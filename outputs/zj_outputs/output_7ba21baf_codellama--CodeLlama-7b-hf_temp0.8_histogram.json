[
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use Kokkos to count in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(Kokkos::View<const int*> const& image, Kokkos::View<size_t[256]> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "Kokkos::parallel_for(\"pixel_counts\", Kokkos::RangePolicy<Kokkos::IndexType<int> >(0, image.extent(0)),\n   KOKKOS_LAMBDA (int i) {\n    bins(image(i)) += 1;\n   });\n\n  }",
            "int num_pixels = image.extent(0);\n   Kokkos::RangePolicy<Kokkos::Serial> policy(0, num_pixels);\n   Kokkos::parallel_for(policy,\n     KOKKOS_LAMBDA(const int i) {\n       int intensity = image(i);\n       bins(intensity)++;\n     });\n }",
            "// TODO: implement\n }",
            "// TODO: Your code here\n\n    //Kokkos::View<int*> v_image(\"v_image\", image.size());\n    //Kokkos::deep_copy(v_image, image);\n\n    Kokkos::parallel_for(image.size(), KOKKOS_LAMBDA (const int i) {\n        Kokkos::atomic_fetch_add(&bins[image(i)], 1);\n    });\n    //Kokkos::deep_copy(image, v_image);\n}",
            "// Your code here\n }",
            "const int image_length = image.dimension(0);\n     for (int i = 0; i < image_length; i++) {\n         int gray = image(i);\n         bins(gray)++;\n     }\n }",
            "// TODO: fill out this function\n    int n = image.size();\n    int i = 0;\n    while (i<n){\n        bins[image(i)]++;\n        i++;\n    }\n}",
            "int num_bins = bins.extent(0);\n\n    //TODO: implement\n    //Kokkos::RangePolicy<host_exec_space> range(0, num_bins);\n    //host_exec_space().parallel_for(range, [=](int i) {\n    //    bins(i) = 0;\n    //});\n    for(int i = 0; i < num_bins; ++i){\n      bins(i) = 0;\n    }\n\n    Kokkos::parallel_for(\"pixelCounts\", Kokkos::RangePolicy<host_exec_space>(0, image.size()),\n    KOKKOS_LAMBDA (const int i) {\n       bins(image(i)) += 1;\n    });\n\n    Kokkos::fence();\n }",
            "// TODO\n }",
            "//TODO: Your code here\n }",
            "}",
            "// Your code here.\n }",
            "// Your code goes here\n   \n }",
            "// TODO\n\n}",
            "// TODO: Your code here\n }",
            "// TODO: Implement this function\n}",
            "// Your code here\n}",
            "// TODO: Your code here\n  int n = image.size();\n\n  Kokkos::parallel_for(n, KOKKOS_LAMBDA(int i) {\n      ++bins(image[i]);\n  });\n }",
            "// TODO: replace the for loop with a Kokkos parallel_for\n     size_t kokkos_loop_depth = 0;\n     bins = 0;\n     for (int j = 0; j < image.size(); ++j) {\n       bins(image(j)) += 1;\n     }\n }",
            "// TODO: Your code here\n \n}",
            "// TODO: implement this function\n   \n   for (int i = 0; i < bins.size(); i++) {\n        bins(i) = 0;\n   }\n\n   for (int i = 0; i < image.extent(0); i++) {\n        int value = image(i);\n        if (value < 0 || value > 255)\n            throw std::out_of_range(\"value out of range\");\n        bins(value)++;\n   }\n}",
            "// The code below is just a draft. You have to fill in the details.\n\n   // initialize bins to all zeros\n   // Note: bins should be an array of 256 size\n   // For example: bins[0] should be initialized to 0\n   // The following code does not initialize bins to all zeros\n   // Please fill in the details\n   // Kokkos::deep_copy(bins, 0);\n\n   // iterate through each pixel in image\n   // For example, if image is [2, 116, 201, 11, 92, 92, 201, 4, 2],\n   // then pixelCounts iterates through 9 pixels\n   // Please fill in the details\n   // for (auto pixel: image) {\n   //   bins[pixel] += 1;\n   // }\n\n   // bins should be [0, 0, 2, 0, 1,...]\n\n }",
            "// TODO: Implement this function\n  Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace> policy(0,bins.size());\n  Kokkos::parallel_for(\"pixelCounts\",policy,[=](int i){\n    bins(i)=0;\n  });\n  Kokkos::parallel_for(\"pixelCounts\",policy,[=](int i){\n    for(int j=0;j<image.size();j++){\n      if(image(j)==i){\n        bins(i)++;\n      }\n    }\n  });\n  \n}",
            "// Your code here\n   //bins = 0;\n   //bins[0] = 2;\n   //bins[116] = 1;\n   //bins[201] = 1;\n   //bins[11] = 1;\n   //bins[92] = 2;\n   //bins[4] = 1;\n   Kokkos::View<int*, Kokkos::Cuda> imageview(\"image\", image.size());\n   Kokkos::deep_copy(imageview, image);\n   Kokkos::View<size_t*, Kokkos::Cuda> binview(\"bins\", 256);\n   Kokkos::deep_copy(binview, bins);\n   Kokkos::parallel_for(\"PixelCounts\", imageview.size(), KOKKOS_LAMBDA(const int i){\n      binview[imageview[i]]++;\n   });\n   Kokkos::deep_copy(bins, binview);\n   //Kokkos::parallel_for(\"PixelCounts\", image.size(), KOKKOS_LAMBDA(const int i){\n   //   bins[image[i]]++;\n   //});\n   //Kokkos::deep_copy(bins, binview);\n }",
            "//TODO\n    // Create an index view on image that will be used to count the pixel intensities\n    auto image_index = Kokkos::create_mirror_view(image);\n    int image_size = image.size();\n    for (int i=0; i<image_size; i++){\n      image_index(i) = image(i);\n    }\n\n    Kokkos::parallel_for(\"pixelCounts\", Kokkos::RangePolicy<Kokkos::IndexType>(0, image_size), [&](const int i) {\n      bins(image_index(i)) += 1;\n    });\n\n    // TODO: Copy the mirrored view back into `bins`\n    Kokkos::deep_copy(bins, image_index);\n\n    // Kokkos::deep_copy(image_index, bins);\n\n    // for (int i=0; i<256; i++){\n    //   std::cout << \"bins[\" << i << \"]=\" << bins(i) << std::endl;\n    // }\n }",
            "// Initialize the bins to zero:\n   Kokkos::parallel_for( \"init_pixel_counts\", 0, image.size(), [&]( size_t idx ) {\n     bins(idx) = 0;\n   });\n\n   // For each pixel in the image:\n   //   count that pixel in its bin\n   //   add the previous bin to the next bin\n   Kokkos::parallel_for( \"pixel_counts\", 0, image.size(), [&]( size_t idx ) {\n     bins(image(idx))++;\n   });\n }",
            "// TODO: implement this function to count the number of pixels with each grayscale intensity in image\n   Kokkos::RangePolicy<Kokkos::Cuda> policy(0, image.size());\n   Kokkos::parallel_for(policy, [=] (int i) {\n     int value = image[i];\n     bins(value)++;\n   });\n }",
            "for (int i = 0; i < 256; ++i)\n     bins(i) = 0;\n   for (int i = 0; i < image.size(); ++i)\n     ++bins(image(i));\n }",
            "Kokkos::deep_copy(bins, 0);\n    const int n = image.size();\n    Kokkos::parallel_for(\"pixelCounts\", Kokkos::RangePolicy<>(0, n), KOKKOS_LAMBDA(const int i) {\n        bins(image(i)) += 1;\n    });\n    Kokkos::deep_copy(bins, bins);\n}",
            "using ExecSpace = Kokkos::DefaultExecutionSpace;\n   using Policy = Kokkos::RangePolicy<ExecSpace>;\n   Kokkos::parallel_for(\"pixelCounts\", Policy(0,image.size()), KOKKOS_LAMBDA(int i) {\n     bins[image(i)] += 1;\n   });\n}",
            "/* NOTE: The following code is not a valid C++ code.\n       However, you are not allowed to change it. */\n\n    int n=image.size();\n\n    Kokkos::View<size_t[256],Kokkos::HostSpace> count_host(\"count_host\",256);\n\n    for(int i=0;i<n;i++) {\n       count_host(image(i))++;\n    }\n\n    Kokkos::deep_copy(bins,count_host);\n }",
            "int N = image.size();\n   Kokkos::RangePolicy<Kokkos::Serial> range(0, N);\n   Kokkos::parallel_for(\"pixelCounts\", range, KOKKOS_LAMBDA(const int& i) {\n     bins(image(i))++;\n   });\n   Kokkos::fence();\n }",
            "int nrows = image.size();\n\t Kokkos::RangePolicy<Kokkos::Serial> policy(0, nrows);\n\t Kokkos::parallel_for(\"pixelCounts\", policy, [=] (const int i) {\n\t\t size_t index = image[i];\n\t\t bins[index] += 1;\n\t });\n }",
            "Kokkos::parallel_for(\"pixelCounts\", image.extent(0), KOKKOS_LAMBDA(const int i) {\n      bins[image[i]]++;\n   });\n   Kokkos::fence();\n}",
            "// Your code here\n    // You will need a loop, and an if statement\n }",
            "// TODO fill in\n\n }",
            "// Fill bins vector with zeros\n    Kokkos::deep_copy(bins, 0);\n    // Increment bins for each pixel\n    Kokkos::parallel_for(image.size(), KOKKOS_LAMBDA (const int i){\n        bins(image[i]) += 1;\n    });\n\n    // Cleanup\n    // Kokkos::finalize();\n\n}",
            "// TODO\n }",
            "Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Dynamic> > policy(0, image.size());\n     Kokkos::parallel_for(\"pixelCounts\", policy, KOKKOS_LAMBDA(int i) {\n         bins(image[i])++;\n     });\n     Kokkos::fence();\n }",
            "Kokkos::parallel_for(\n        \"Pixel Counts\",\n        Kokkos::RangePolicy<Kokkos::Serial>(0,image.size()),\n        KOKKOS_LAMBDA (const int i) {\n            bins[image(i)]++;\n        });\n }",
            "// fill in this function\n }",
            "// TODO: fill out this function\n }",
            "// TODO\n}",
            "// TODO: fill in your solution\n\n     Kokkos::parallel_for(\"pixelCounts\", image.size(), KOKKOS_LAMBDA(const int& i) {\n      bins(image(i))++;\n    });\n\n     Kokkos::finalize();\n\n     return;\n }",
            "//...\n }",
            "// TODO\n }",
            "// TODO: Your code here\n\n}",
            "}",
            "}",
            "//TODO\n   \n   // Count the number of pixels with each intensity.\n   // This works only for images that are 256 in size.\n   Kokkos::parallel_for(\"countPixels\", image.size(), KOKKOS_LAMBDA (const int i) {\n     bins(image(i)) += 1;\n   });\n   \n }",
            "for(int i=0;i<bins.size();i++)\n    bins(i)=0;\n    Kokkos::parallel_for(\"PixelCounts\",image.size(), KOKKOS_LAMBDA(int i) {\n        bins(image(i))++;\n    });\n }",
            "// Your code here.\n\n   Kokkos::parallel_for(image.size(), KOKKOS_LAMBDA(const int &idx){\n       bins[image(idx)] += 1;\n   });\n }",
            "// TODO: Your code here\n   \n }",
            "// Your code here.\n   Kokkos::parallel_for(\"count_intensities\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, image.size()), KOKKOS_LAMBDA (size_t idx) {\n     Kokkos::atomic_increment<int>(&bins[image(idx)]);\n   });\n }",
            "// TODO: Replace this with a single Kokkos kernel\n   //...\n }",
            "// fill with zero\n    bins = Kokkos::View<size_t[256]>(\"bins\");\n    Kokkos::deep_copy(bins, 0);\n\n    // Kokkos::View is a 1-D View of a buffer on the device.\n    // Views are declared in template form with the data type and a\n    // dimension (e.g., Kokkos::View<int[3]>).\n    // Views are accessed as a regular array.\n    // The memory buffer is owned by the View.\n    // Accessing the memory buffer directly is not recommended.\n    auto host_image = Kokkos::create_mirror_view(image);\n    Kokkos::deep_copy(host_image, image);\n    // You can access the buffer using host_image.data().\n\n    // Kokkos::parallel_for uses a functor that has the signature\n    // void operator()(const int i) const.\n    // Kokkos::parallel_for starts a parallel loop and applies the functor\n    // to each element of the range.\n    // The functor is applied to the first index (begin) first.\n    Kokkos::parallel_for(\n        \"Count pixel counts\",\n        Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, image.extent(0)),\n        [&host_image, &bins](const int i) {\n            size_t value = host_image[i];\n            bins(value) += 1;\n    });\n\n    Kokkos::deep_copy(bins, host_image);\n}",
            "for (int i=0; i<bins.size(); i++) {\n     bins[i] = 0;\n   }\n   for (int i=0; i<image.size(); i++) {\n     bins[image(i)]++;\n   }\n }",
            "// TODO: compute the counts for each intensity in image\n   //       using Kokkos::parallel_for\n   //       loop over each pixel value in image\n   //       store the number of pixels with that intensity in bins\n   \n   // Hint: image.size() returns the number of elements in the View\n   //       Kokkos::View<int, Kokkos::LayoutRight, Kokkos::HostSpace, Kokkos::MemoryTraits<Kokkos::Unmanaged> >\n   //       image.data() returns a pointer to the underlying data\n   \n   // TODO: make sure bins has been properly initialized\n\n   Kokkos::parallel_for(\"pixelCounts\",image.size(),KOKKOS_LAMBDA(const int i){\n   int grayscale=image(i);\n   bins(grayscale)++;\n   });\n  \n }",
            "Kokkos::parallel_for(image.size(), KOKKOS_LAMBDA(const int i) {\n        bins(image(i))++;\n    });\n    Kokkos::fence();\n}",
            "// TODO\n    // Loop over all elements of the image, and for each element (call it c) do bins[c]=bins[c]+1\n}",
            "const int numPixels = image.size();\n    const int maxValue = 255;\n    const int numValues = maxValue + 1;\n\n    // Initialize bins to zero.\n    // Kokkos::View<size_t[256]> bins(\"bins\", numValues);\n    for (int i = 0; i < numValues; ++i) {\n        bins(i) = 0;\n    }\n\n    // Count the number of pixels of each intensity value.\n    // Kokkos::parallel_for(numPixels, KOKKOS_LAMBDA(int i) {\n    //     ++bins(image[i]);\n    // });\n\n    // Or use Kokkos::RangePolicy and Kokkos::parallel_reduce.\n    Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static> > policy(0, numPixels);\n    Kokkos::parallel_reduce(policy, KOKKOS_LAMBDA(int i, size_t& tmp) {\n        tmp += ++bins(image(i));\n    }, 0);\n}",
            "// TODO fill in\n}",
            "// This is a 1-dimensional view of a 2-d array\n     // A 2-d array with the first dimension equal to the number of threads\n     // and the second dimension equal to the number of pixels.\n     Kokkos::View<int**, Kokkos::LayoutLeft> pixel_view(\"pixel_view\", Kokkos::Threads, image.size());\n\n     // A 1-d view of the second dimension of the above view\n     // The second dimension is the number of pixels and the first dimension is the number of threads\n     Kokkos::View<int*, Kokkos::LayoutRight> pixel_count_view(\"pixel_count_view\", Kokkos::Threads * image.size());\n\n     // For each thread, copy the 1-d view of the 2-d view into the 1-d view\n     Kokkos::parallel_for(\"pixelCounts\", Kokkos::Threads(), [&](int i) {\n         pixel_view(i, _) = image;\n     });\n\n     // For each pixel, count the number of times it is in the array.\n     // This is an example of Kokkos using a reduction (i.e. parallel_reduce)\n     // where the reduction is to an array of size 256\n     Kokkos::parallel_reduce(\"pixelCounts\", Kokkos::Threads(), Kokkos::RangePolicy<Kokkos::Threads>(0, image.size()),\n                             KOKKOS_LAMBDA(int i, int& sum) {\n                                 // Initialize sum to zero\n                                 sum = 0;\n                                 // for each pixel, count the number of times it is in the array\n                                 for (int j = 0; j < image.size(); ++j) {\n                                     if (pixel_view(i, j) > 0)\n                                         ++sum;\n                                 }\n                             },\n                             pixel_count_view);\n\n     // Copy the results from the view into the array\n     Kokkos::deep_copy(bins, pixel_count_view);\n }",
            "Kokkos::parallel_for(\"pixelCounts\", 0, image.size(), KOKKOS_LAMBDA(const size_t& idx) {\n        int val = image(idx);\n        bins(val)++;\n    });\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<>(0, image.size()), KOKKOS_LAMBDA(int i) {\n  \tbins(image(i))++;\n  });\n }",
            "for (size_t i=0; i<256; ++i) {\n         bins[i] = 0;\n     }\n     for (size_t i=0; i<image.size(); ++i) {\n         ++bins[image[i]];\n     }\n }",
            "// TODO Fill this in\n    // Kokkos::parallel_reduce(image.size(), pixelCounts(image, bins))\n\n\n    /*\n    int* image_ptr = image.data();\n    for (int i = 0; i < image.size(); i++) {\n        bins(image_ptr[i])++;\n    }\n    */\n\n    // bins.resize(256);\n    // for (size_t i = 0; i < image.size(); i++) {\n    //     bins[image[i]]++;\n    // }\n }",
            "int m = image.size();\n\n    Kokkos::parallel_for(\"pixelCounts\", m, KOKKOS_LAMBDA (int i) {\n        bins(image[i])++;\n    });\n\n}",
            "//TODO: Fill this in\n}",
            "// Your code here\n   int m=image.extent(0);\n   for (int i=0; i<256; i++){\n      bins(i)=0;\n   }\n   Kokkos::parallel_for(\"pixelCounts\", Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Dynamic>>(0,m), [=] (const int i) {\n      bins(image(i))+=1;\n   });\n }",
            "// TODO\n   // Kokkos::parallel_for(image.size(),[=](int i){\n   //  //if(image[i]>=0&&image[i]<=255)\n   //  if(image[i]>0&&image[i]<255)\n   //  {\n   //    bins[image[i]]++;\n   //  }\n   //});\n   // Kokkos::parallel_reduce(image.size(),0,[&](int i,size_t &temp){\n   //  if(image[i]>0&&image[i]<255)\n   //  {\n   //    temp++;\n   //    bins[image[i]]++;\n   //  }\n   //  return temp;\n   // });\n   Kokkos::parallel_reduce(image.size(),0,KOKKOS_LAMBDA(int i,size_t &temp){\n      if(image[i]>0&&image[i]<255)\n      {\n        temp++;\n        bins[image[i]]++;\n      }\n      return temp;\n    });\n }",
            "// Kokkos Views can be parallel_for looped over\n  Kokkos::parallel_for(image.size(), KOKKOS_LAMBDA(const int& i) {\n    ++bins(image(i));\n  });\n\n  // Make sure Kokkos is finished\n  Kokkos::finalize();\n}",
            "Kokkos::parallel_for(\"pixelCounts\", image.size(), KOKKOS_LAMBDA(int i) {\n      bins(image(i)) += 1;\n  });\n\n}",
            "// TODO\n   int n = image.size();\n   Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Dynamic>> policy(0, n);\n   Kokkos::parallel_for(\"pixelCounts\",policy,PixelCountFunctor<int>(image,bins));\n }",
            "// TODO\n }",
            "//TODO: Your code here\n }",
            "// TODO: Your code here\n }",
            "// Your code here\n   // You will need to loop over the image\n   // You will need to increment the correct bin.\n   // Don't forget the boundary condition.\n   // You can use the Kokkos memory accessor (Kokkos::deep_copy).\n   \n   Kokkos::deep_copy(bins,0);\n   for(int i=0;i<image.size();i++){\n       bins(image(i)) += 1;\n   }\n }",
            "// TODO: your code here\n}",
            "size_t image_size = image.extent(0);\n    Kokkos::RangePolicy<Kokkos::Serial> policy(0, image_size);\n    Kokkos::parallel_for(\"pixelCounts\", policy, [=] (int i) {\n        int pixel = image(i);\n        bins(pixel)++;\n    });\n}",
            "// TODO\n }",
            "// TODO: complete this function\n }",
            "// Use Kokkos parallel_for to fill the bins\n   Kokkos::parallel_for( \"pixelCounts\", image.size(), KOKKOS_LAMBDA(const int i) {\n      bins(image(i))++;\n   });\n\n   // You can also do this in serial with a for loop\n   // for (int i = 0; i < image.size(); i++) {\n   //   bins(image(i))++;\n   // }\n }",
            "for (int i = 0; i < image.size(); i++) {\n     // This code is very slow\n     // bins[image[i]]++;\n\n     // Kokkos implementation\n     Kokkos::atomic_fetch_add(bins[image[i]], 1);\n   }\n }",
            "/* TODO: Your code goes here */\n}",
            "// TODO\n\n}",
            "//TODO: fill in this function\n }",
            "// Your code here\n\n\n   Kokkos::parallel_for(image.size(), KOKKOS_LAMBDA(const int i){\n        bins(image(i))++;\n   });\n\n\n\n\n }",
            "}",
            "}",
            "// TODO: Fill this in, using the Kokkos::RangePolicy.\n   // You will need to use Kokkos::sum(bins[0]) to get the total number of pixels.\n }",
            "const int n = image.size();\n   Kokkos::parallel_for(\"pixelCounts\", Kokkos::RangePolicy<>(0, n), KOKKOS_LAMBDA(const int i) {\n     bins(image(i))++;\n   });\n   Kokkos::fence();\n}",
            "Kokkos::parallel_for(\"pixelCounts\", image.size(), KOKKOS_LAMBDA (const int i) {\n        bins(image(i))++;\n    });\n\n    Kokkos::fence();\n}",
            "// TODO: your code here\n }",
            "const auto image_size = image.size();\n  auto image_ptr = image.data();\n  auto bins_ptr = bins.data();\n  Kokkos::parallel_for(image_size, KOKKOS_LAMBDA(const int i) {\n    auto pixel = image_ptr[i];\n    bins_ptr[pixel]++;\n  });\n}",
            "for (int i = 0; i < 256; i++)\n        bins[i] = 0;\n    const int n = image.extent_int(0);\n    for (int i = 0; i < n; i++) {\n        auto pixel = image(i);\n        if (pixel >= 256) {\n            std::cout << \"invalid pixel value\" << std::endl;\n            return;\n        }\n        bins(pixel)++;\n    }\n}",
            "}",
            "// Fill bins with zeros.\n  Kokkos::deep_copy(bins, 0);\n\n  // TODO: fill in this function\n}",
            "// TODO: Replace this with a loop over the image\n     // (i.e. parallel_reduce over all pixels)\n     //\n     // For each pixel in the image:\n     // - if the pixel is >= 0 and < 256\n     // - then bins[pixel] += 1\n\n}",
            "// TODO Fill in this function\n }",
            "// TODO: Implement\n     return;\n }",
            "const int length = image.size();\n  for(int i = 0; i < length; i++) {\n    int val = image(i);\n    if(val < 256) bins(val)++;\n  }\n}",
            "// TODO: Implement this function\n}",
            "// Fill this in\n }",
            "}",
            "// TODO: fill in\n\n }",
            "// TODO\n     // Hint: loop over image and increment bin[i]\n     // Note: you can use image.size() to get the number of elements in image\n }",
            "// TODO: Your code here\n    return;\n  }",
            "// TODO: Your code goes here\n   for (int i = 0; i < image.size(); i++){\n      bins(image[i]) += 1;\n   }\n\n   // TODO: End your code here\n\n }",
            "// YOUR CODE HERE\n\n }",
            "// TODO: Count the number of pixels in each bin\n\n   // TODO: Store the results in bins\n\n   // TODO: Use Kokkos to count in parallel\n }",
            "}",
            "// TODO\n }",
            "// TODO: Your code here\n     for(int i=0;i<256;i++){\n       bins(i)=0;\n     }\n     auto image_host=Kokkos::create_mirror_view(image);\n     Kokkos::deep_copy(image_host, image);\n     for (int i=0;i<image_host.size();i++){\n       bins(image_host(i))+=1;\n     }\n }",
            "// Fill in your code here\n    Kokkos::parallel_for(\"counting pixels\", Kokkos::RangePolicy<Kokkos::Serial>(0, image.size()), \n                         KOKKOS_LAMBDA(const int& i) {\n                            // Fill in your code here\n                            bins(image(i))++;\n                         });\n}",
            "// TODO: Your code here\n    // Use Kokkos to sum the image in parallel\n    // This is a parallel sum of the image\n    Kokkos::parallel_reduce(Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static>>(0, image.size()), KOKKOS_LAMBDA(int i, size_t& num_pixels) {\n      num_pixels += (image(i)==i);\n    }, Kokkos::Sum<size_t>(bins));\n\n }",
            "// Your code here\n }",
            "// TODO: implement this function using Kokkos\n}",
            "// TODO: Your code here\n   // YOUR CODE GOES HERE\n\n\n   int numElements = image.size();\n   const int numThreads = 1;\n\n   // Initialize bins\n   Kokkos::parallel_for(numElements, numThreads, [&] (int i) {\n     bins(image(i))++;\n   });\n\n }",
            "}",
            "// TODO: Your code here\n  size_t length=image.size();\n  Kokkos::parallel_for(\"pixelCounts\",Kokkos::RangePolicy<>(0,length),\n                                     [&](const int& i)\n  {\n      bins(image(i))++;\n  });\n}",
            "Kokkos::parallel_for(\"PixelCounts\", image.size(), KOKKOS_LAMBDA (const int i) {\n         Kokkos::atomic_increment(&bins[image(i)]);\n     });\n }",
            "for (int i = 0; i < image.size(); ++i) {\n     ++bins[image(i)];\n   }\n}",
            "int length=image.size();\n\tKokkos::parallel_for(\"count\", length, KOKKOS_LAMBDA(const int& i) {\n\t\tbins[image[i]] += 1;\n\t});\n}",
            "// Fill in your code here\n     auto image_h = Kokkos::create_mirror_view_and_copy(Kokkos::HostSpace{}, image);\n     Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Serial>(0, image_h.size()),\n     [&image_h, &bins] (const int idx) {\n         bins(image_h[idx]) += 1;\n     });\n     Kokkos::deep_copy(bins, bins);\n }",
            "//TODO: Implement this\n  Kokkos::parallel_for(\"\", Kokkos::RangePolicy<>(0, image.size()),\n                       [=](const int i) { bins(image(i))++; });\n}",
            "/* Your code here */\n}",
            "// TODO\n   int n = image.size();\n   Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static>>(0,n),[=](int i){\n     int val = image(i);\n     Kokkos::atomic_fetch_add(&bins[val],1);\n   });\n }",
            "// TODO\n   \n  \n  //  Kokkos::RangePolicy<ExecSpace> policy(0,bins.extent(0));\n  //  Kokkos::parallel_for(\"pixelCounts\", policy, KOKKOS_LAMBDA(int i){\n  //    bins(i) = 0;\n  //    for(int j=0;j<image.extent(0);j++){\n  //      if(image(j) == i){\n  //        bins(i) += 1;\n  //      }\n  //    }\n  //  });\n}",
            "// TODO: Your code here\n }",
            "// Compute the pixel counts.\n    // Each thread works on a single pixel.\n    // Kokkos guarantees that all the pixels are contiguous in memory.\n    // Use `Kokkos::parallel_for` and `Kokkos::atomic_add`\n    Kokkos::parallel_for(image.size(), [&](int idx) {\n        size_t& count = bins(image(idx));\n        Kokkos::atomic_add(&count, 1);\n    });\n\n    // In Kokkos, the memory for the arrays is contiguous,\n    // so it's possible to perform an atomic reduction directly on the array.\n    // Kokkos::atomic_add(&bins[0], 1);\n\n    // Another method to compute the pixel counts is to use `Kokkos::deep_copy`\n    // to copy the array of counts from one view to the other.\n    // In this case, the reduction is performed by the `deep_copy` function.\n    //Kokkos::deep_copy(bins, bins);\n}",
            "int image_size = image.extent(0);\n   Kokkos::parallel_for(\"pixelCounts\", Kokkos::RangePolicy<>(0, image_size), KOKKOS_LAMBDA (const int i) {\n       bins(image(i))++;\n   });\n}",
            "// Fill in this function\n   // Hint: you can use a Kokkos::parallel_reduce\n }",
            "// Your code here\n }",
            "//TODO: Your code here\n}",
            "//TODO\n }",
            "//TODO: implement\n    \n    Kokkos::parallel_for(image.size(),\n                         KOKKOS_LAMBDA (const int& i) {\n                             bins(image(i))++;\n                         });\n}",
            "// TODO: implement\n }",
            "}",
            "int n_pixels = image.size();\n     int n_threads = omp_get_max_threads();\n     // TODO: modify this function to use Kokkos to parallelize \n     #pragma omp parallel for num_threads(n_threads)\n     for (int pixel = 0; pixel < n_pixels; pixel++) {\n         // update the corresponding value in bins\n         // Hint: `bins[image[pixel]] += 1`\n         // Hint: Kokkos::atomic_fetch_add(bins[image[pixel]], 1)\n         // Hint: Kokkos::atomic_fetch_add(&bins[image[pixel]], 1)\n         // Hint: Kokkos::atomic_fetch_add(bins[0], 1)\n         // Hint: Kokkos::atomic_fetch_add(&bins[0], 1)\n         // Hint: Kokkos::atomic_fetch_add(bins[0], 1, 0)\n         // Hint: Kokkos::atomic_fetch_add(&bins[0], 1, 0)\n         // Hint: Kokkos::atomic_fetch_add(bins, 1, 0)\n         // Hint: Kokkos::atomic_fetch_add(&bins, 1, 0)\n         // Hint: Kokkos::atomic_fetch_add(&bins[image[pixel]], 1, 0)\n         // Hint: Kokkos::atomic_fetch_add(bins[image[pixel]], 1, 0)\n         // Hint: Kokkos::atomic_fetch_add(&bins[image[pixel]], 1, 0)\n         // Hint: Kokkos::atomic_fetch_add(&bins[image[pixel]], 1, 0)\n         // Hint: Kokkos::atomic_fetch_add(bins[image[pixel]], 1, 0, 0)\n         // Hint: Kokkos::atomic_fetch_add(&bins[image[pixel]], 1, 0, 0)\n         // Hint: Kokkos::atomic_fetch_add(&bins[image[pixel]], 1, 0, 0)\n         // Hint: Kokkos::atomic_fetch_add(&bins[image[pixel]], 1, 0, 0, 0)\n         // Hint: Kokkos::atomic_fetch_add(bins[image[pixel]], 1, 0, 0, 0)\n     }\n }",
            "const int N = image.extent(0);\n  Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace> policy(0, N);\n  Kokkos::parallel_for(\"pixelCounts\", policy, KOKKOS_LAMBDA(const int& i) {\n    bins(image(i))++;\n  });\n  Kokkos::fence();\n}",
            "// TODO: fill in the function\n }",
            "// TODO: Implement this function\n  }",
            "// your code here\n }",
            "// TODO: YOUR CODE HERE\n     for(int i = 0; i<image.extent(0); i++){\n         bins(image(i)) = bins(image(i)) + 1;\n     }\n     bins.update_host();\n }",
            "// TODO: Your code here.\n\n   auto h_image = image.data();\n   auto h_bins = bins.data();\n\n   for(int i = 0; i < image.size(); i++) {\n     h_bins[h_image[i]]++;\n   }\n\n }",
            "// NOTE: Kokkos doesn't guarantee a specific memory layout for Views,\n    // so we have to loop over the bins explicitly to avoid race conditions.\n    const size_t N = image.extent_int(0);\n    for (size_t i=0; i<N; ++i) {\n      const int value = image(i);\n      bins(value) += 1;\n    }\n  }",
            "auto pixel_count_lambda = [=] (int i) { bins(image(i))++; };\n   Kokkos::parallel_for(image.size(), pixel_count_lambda);\n }",
            "// TODO: Your code goes here\n \n   const size_t n = image.size();\n   Kokkos::RangePolicy<Kokkos::Serial> range_policy(0, n);\n   Kokkos::parallel_for(range_policy, pixelCountsFunctor(image, bins));\n\n }",
            "Kokkos::parallel_for(\"pixelCounts\", image.size(), KOKKOS_LAMBDA(int i) {\n        ++bins(image(i));\n    });\n}",
            "// TODO: Count the number of pixels with each grayscale intensity.\n }",
            "int n = image.size();\n     for (int i = 0; i < n; i++) {\n         bins[image(i)] += 1;\n     }\n }",
            "// TODO: Your code here\n\n  size_t pixelCount = image.extent_int(0);\n  Kokkos::parallel_for(\"count_pixels\", pixelCount, KOKKOS_LAMBDA(const int pixelIndex) {\n    bins[image(pixelIndex)]++;\n  });\n\n}",
            "// your code here\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::IndexType<int>>(0,image.size()),[&](const int i){\n    bins(image(i))+=1;\n  });\n }",
            "// Compute the pixel counts in parallel.\n    for (int i=0; i<image.extent(0); i++) {\n        Kokkos::atomic_fetch_add(&bins(image(i)), 1);\n    }\n    // TODO: Implement this function\n}",
            "// Fill in this function!\n    Kokkos::parallel_for(\"pixelCounts\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, image.size()),\n                             KOKKOS_LAMBDA(int i) {\n                                size_t idx = image(i);\n                                bins(idx)++;\n                             });\n }",
            "// Fill bins with zeros\n   auto hostBins = Kokkos::create_mirror_view(bins);\n   for (size_t i = 0; i < 256; i++) {\n     hostBins(i) = 0;\n   }\n   Kokkos::deep_copy(bins, hostBins);\n\n   // Count pixels\n   Kokkos::parallel_for(image.size(), KOKKOS_LAMBDA(const size_t i) {\n     bins(image[i]) += 1;\n   });\n   Kokkos::deep_copy(hostBins, bins);\n}",
            "// Count the number of pixels in each bin\n\n  Kokkos::parallel_for(\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, image.size()),\n    KOKKOS_LAMBDA (const int i) {\n      const int val = image(i);\n      bins(val) += 1;\n  });\n\n}",
            "int n = image.extent(0);\n   Kokkos::parallel_for(\"pixelCounts\", n, KOKKOS_LAMBDA(int i) {\n      bins[image(i)]++;\n   });\n}",
            "// TODO\n }",
            "// TODO: implement this parallel counting\n}",
            "// TODO\n\n }",
            "// Fill in your solution here\n }",
            "Kokkos::RangePolicy<Kokkos::Serial> range(0, image.size());\n   \n   for (int j = 0; j < 256; j++)\n     bins[j] = 0;\n   \n   Kokkos::parallel_for(\"pixelCounts\", range, KOKKOS_LAMBDA (const int j) {\n       bins[image[j]]++;\n     });\n }",
            "auto size = image.size();\n   Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static> > rangePolicy(0, size);\n   Kokkos::parallel_for(rangePolicy, KOKKOS_LAMBDA(const int i) {\n      Kokkos::atomic_increment(&bins(image(i)));\n   });\n }",
            "size_t num_pixels = image.size();\n    Kokkos::parallel_for(num_pixels, KOKKOS_LAMBDA(const int i) {\n        bins(image(i)) += 1;\n    });\n}",
            "// TODO: Implement pixelCounts\n     //\n     // Hints:\n     //\n     // * Use Kokkos::TeamPolicy\n     // * Use Kokkos::TeamThreadRange\n     // * Use Kokkos::atomic_fetch_add\n     // * Use bins[color] += 1\n\n     auto teamPolicy = Kokkos::TeamPolicy<>(image.extent(0), 4);\n     Kokkos::parallel_for(\n         teamPolicy,\n         KOKKOS_LAMBDA(const Kokkos::TeamPolicy<>::member_type& teamMember) {\n             const int pixel = teamMember.league_rank();\n             Kokkos::parallel_for(\n                 Kokkos::TeamThreadRange(teamMember, 0, image.extent(0)),\n                 [&](int i) {\n                     int color = image[i];\n                     Kokkos::atomic_fetch_add(&bins[color], 1);\n                 });\n         });\n }",
            "// TODO\n}",
            "}",
            "}",
            "// Fill bins with 0, in case the image is smaller than 256 pixels\n   Kokkos::deep_copy(bins, 0);\n\n   // Increment the bin corresponding to pixel intensity in image\n   const size_t num_pixels = image.size();\n   for (size_t i = 0; i < num_pixels; i++) {\n     bins[image(i)] += 1;\n   }\n}",
            "//TODO: Fill in\n  // Initialize bins to 0\n  Kokkos::deep_copy(bins,0);\n\n  // Kokkos view is a type-safe abstraction for arrays.\n  // We can iterate over it using a parallel_for\n  const int N = image.extent(0); // Number of pixels\n  Kokkos::parallel_for(\n      \"PixelCounts\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, N),\n      KOKKOS_LAMBDA(const int i) {\n        //TODO: Increment the i'th bin in bins by 1\n\n      });\n  // Synchronize to ensure the parallel_for has finished\n  Kokkos::fence();\n}",
            "// TODO: Your code here\n\n   Kokkos::RangePolicy<Kokkos::Serial> range_policy(0, 256);\n   Kokkos::parallel_for(\"pixelCounts\", range_policy, [&] (size_t i) {\n    bins(i) = 0;\n   });\n\n   Kokkos::RangePolicy<Kokkos::Serial> range_policy2(0, image.size());\n   Kokkos::parallel_for(\"pixelCounts\", range_policy2, [&] (size_t i) {\n    bins(image(i)) += 1;\n   });\n }",
            "const int imageSize = image.extent(0);\n   const int numThreads = Kokkos::OpenMP::hardware_thread_count();\n   const int numPerThread = imageSize / numThreads;\n\n   Kokkos::RangePolicy<Kokkos::OpenMP> policy(0, numThreads);\n   Kokkos::parallel_for(\"parallelFor\", policy,\n                        KOKKOS_LAMBDA(const int i) {\n                          int start = i * numPerThread;\n                          int end = start + numPerThread;\n                          if (i == numThreads - 1) end = imageSize;\n\n                          for (int j = start; j < end; ++j) {\n                            bins(image(j))++;\n                          }\n                        });\n   Kokkos::fence();\n }",
            "// Your code here\n   \n   return;\n }",
            "// Your code here\n    \n    //Kokkos::parallel_for(image.extent(0),[&](int i){\n    //  auto pixel_value = image(i);\n    //  bins(pixel_value) = bins(pixel_value) + 1;\n    //});\n    //Kokkos::fence();\n    \n    //Kokkos::parallel_reduce(image.extent(0),[&](int i, size_t& l){\n    //  auto pixel_value = image(i);\n    //  l += 1;\n    //  if(i == 0)\n    //    bins(pixel_value) = l;\n    //},Kokkos::Sum<size_t>(bins));\n\n    Kokkos::parallel_reduce(\"pixelCounts\", image.extent(0), KOKKOS_LAMBDA(int i, size_t& l){\n      auto pixel_value = image(i);\n      l += 1;\n      if(i == 0)\n        bins(pixel_value) = l;\n      },Kokkos::Sum<size_t>(bins));\n\n  }",
            "// Count the number of pixels in image with each grayscale intensity.\n   // Store the results in `bins`.\n   // Use Kokkos to count in parallel. Assume Kokkos has already been initialized.\n\n   // Kokkos::View<size_t[256]> bins(Kokkos::ViewAllocateWithoutInitializing(\"bins\"));\n\n   // Use the parallel_reduce function\n   // The range is over the size of the input image.\n   // The accumulate is the element of the output vector.\n   // The lambda function takes the accumulate, the element, and the index.\n   // The lambda function returns the accumulate with the value from the image added.\n   // The input is the image.\n   // The output is the bins.\n   // The lambda function is:\n   //   auto lambda = [&] (size_t& a, const size_t& b, const size_t& c) -> size_t {\n   //     a += image(c);\n   //     return a;\n   //   };\n   Kokkos::parallel_reduce(Kokkos::RangePolicy<>(0,image.size()),\n                           KOKKOS_LAMBDA(size_t &a, const size_t &b, const size_t &c) -> size_t {\n                           a += image(c);\n                           return a;\n                           },\n                           bins);\n }",
            "// TODO: Your code here\n\n}",
            "// TODO: fill in this function\n }",
            "//...\n }",
            "const int num_entries = image.size();\n   // initialize the results bins to zero\n   Kokkos::deep_copy(bins, 0);\n\n   Kokkos::parallel_for(num_entries, KOKKOS_LAMBDA (const int i) {\n     bins[image(i)]++;\n   });\n\n   Kokkos::fence();\n }",
            "// TO DO: YOUR CODE HERE\n   int i;\n   int len = image.size();\n   for (i = 0; i < 256; i++)\n   {\n    bins(i) = 0;\n   }\n   for (i = 0; i < len; i++)\n   {\n       bins(image(i)) += 1;\n   }\n   \n}",
            "// TODO: Fill in this function\n\n }",
            "// TODO\n    \n    \n}",
            "/*\n    YOUR CODE HERE\n    Note:\n      bins[i] is the number of times the value i appears in image.\n      Kokkos::View<size_t[256]> is an array of 256 size_t, each initialized to 0.\n  */\n  const size_t N = image.size();\n\n  // loop over all elements of the image\n  // and increment the bins array at the index of each element\n  // Kokkos::parallel_for: this runs the loop over all elements in parallel\n  // Kokkos::RangePolicy: this loop goes from 0 to N-1\n  Kokkos::parallel_for(\"pixel_counts\",\n                       Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, N),\n                       KOKKOS_LAMBDA (const size_t i) {\n                        bins[image(i)]++;\n                       });\n\n\n  /*\n    END YOUR CODE HERE\n  */\n}",
            "// Your code here\n  const size_t N = image.extent(0);\n  Kokkos::parallel_for(N, [=] (const size_t i) {\n    size_t color = image(i);\n    if (color < bins.size()) {\n      bins(color) += 1;\n    }\n  });\n\n  Kokkos::fence();\n  return;\n }",
            "// Your code here.\n}",
            "/* Your code here */\n }",
            "//...\n     // Hint: use Kokkos to parallelize the following loop\n     //\n     // for(int i = 0; i < image.size(); ++i) {\n     //   bins[image[i]]++;\n     // }\n     //...\n }",
            "// Fill in your code here\n }",
            "// TODO\n    \n    return;\n }",
            "// TODO\n   // HINT: use a kokkos::parallel_for to iterate over the image pixels\n   // HINT: use Kokkos::atomic_fetch_add to increment the counts\n }",
            "// TODO: fill in your code here\n\n }",
            "// TODO: Implement\n\n}",
            "// TODO: Implement this function\n    // You may want to use Kokkos::deep_copy and Kokkos::parallel_for\n    \n    size_t imageSize = image.size();\n\n    for(size_t i = 0; i < imageSize; i++){\n        bins(image(i))++;\n    }\n\n}",
            "// TODO\n }",
            "/*\n      Hint 1: Use an int-to-size_t cast for the indexing into the histogram\n      Hint 2: You may need to initialize the histogram vector\n    */\n}",
            "// 1. fill bins with 0s\n    // 2. for each pixel value p in image, bins[p]++\n    // 3. return bins\n}",
            "// TODO: your code here\n }",
            "auto image_host = Kokkos::create_mirror_view(image);\n    Kokkos::deep_copy(image_host, image);\n\n    constexpr size_t num_blocks = 256;\n    auto num_threads = 256;\n    Kokkos::RangePolicy<Kokkos::HostSpace::execution_space> policy(0, num_blocks);\n\n    Kokkos::parallel_for(policy, KOKKOS_LAMBDA(const int& i) {\n        // int cnt = 0;\n        // for (auto v : image_host) {\n        //     if (v == i) ++cnt;\n        // }\n        // bins[i] = cnt;\n        bins[i] = 0;\n        for (auto v : image_host) {\n            if (v == i) bins[i]++;\n        }\n    });\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use OpenMP to count in parallel.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "int i;\n  #pragma omp parallel for\n  for (i=0; i<image.size(); i++){\n    bins[image[i]]++;\n  }\n}",
            "// Implement this function!\n }",
            "// Count pixels in parallel\n    #pragma omp parallel for\n    for (int i = 0; i < 256; i++) {\n        for (int j = 0; j < image.size(); j++) {\n            if (image[j] == i) {\n                bins[i]++;\n            }\n        }\n    }\n\n\n}",
            "for(int i = 0; i < 256; i++){\n\t\t for(int j = 0; j < image.size(); j++){\n\t\t\t if(i == image[j]){\n\t\t\t\t bins[i]++;\n\t\t\t }\n\t\t }\n\t }\n }",
            "#pragma omp parallel\n  for (size_t i=0; i<image.size(); i++) {\n#pragma omp atomic\n    bins[image[i]]++;\n  }\n}",
            "// fill with zeros\n   for(size_t i=0; i<bins.size(); i++) bins[i] = 0;\n\n   #pragma omp parallel for\n   for(size_t i=0; i<image.size(); i++) {\n     bins[image[i]] += 1;\n   }\n\n }",
            "#pragma omp parallel for\n     for(auto i=0; i < 256; i++) {\n         int count = 0;\n         for (auto j = 0; j < image.size(); ++j) {\n             if(image[j] == i) count++;\n         }\n         bins[i] = count;\n     }\n }",
            "#pragma omp parallel for\n  for(int i=0;i<image.size();i++){\n    bins[image[i]]++;\n  }\n  return;\n }",
            "#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        ++bins[image[i]];\n    }\n}",
            "#pragma omp parallel\n     for (auto i : image) {\n         #pragma omp atomic\n         bins[i]++;\n     }\n }",
            "#pragma omp parallel for\n    for (int i = 0; i < 256; i++)\n    {\n        bins[i] = 0;\n    }\n    for (int i = 0; i < image.size(); i++)\n    {\n        bins[image[i]]++;\n    }\n}",
            "for (int i = 0; i < 256; ++i)\n    {\n        bins[i] = 0;\n    }\n\n    //#pragma omp parallel for\n    //omp_set_num_threads(4);\n    for (int i = 0; i < image.size(); ++i)\n    {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel\n   {\n     #pragma omp for\n     for (int i=0; i < image.size(); ++i) {\n       bins[image[i]]++;\n     }\n   }\n }",
            "#pragma omp parallel for\n    for(auto i = 0; i < image.size(); i++){\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n   for (int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "int width = image.size();\n   int height = 1;\n   int counter = 0;\n\n   std::fill(bins.begin(), bins.end(), 0);\n\n   #pragma omp parallel num_threads(16) shared(bins)\n   {\n     int id = omp_get_thread_num();\n     int per_thread = width/16;\n\n     #pragma omp for schedule(static, per_thread)\n     for (int i = 0; i < width; i++) {\n        bins[image[i]] += 1;\n     }\n   }\n }",
            "// TODO\n#pragma omp parallel for\n  for (int i = 0; i < image.size(); ++i) {\n    bins[image[i]]++;\n  }\n}",
            "#pragma omp parallel for\n  for (auto i = 0u; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "// TODO: Your code here\n }",
            "#pragma omp parallel for\n   for (int i=0;i<256;i++){\n     bins[i]=0;\n   }\n   for (int i=0;i<image.size();i++){\n     bins[image[i]]++;\n   }\n\n }",
            "#pragma omp parallel\n   {\n     #pragma omp for nowait\n     for (int i = 0; i < image.size(); i++) {\n       bins[image[i]]++;\n     }\n   }\n }",
            "#pragma omp parallel for\n\tfor (int i = 0; i < image.size(); i++)\n\t{\n\t\tbins[image[i]]++;\n\t}\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   \n   // TODO: write your code here\n   #pragma omp parallel for\n   for (int i = 0; i < image.size(); i++)\n   {\n       bins[image[i]]++;\n   }\n}",
            "#pragma omp parallel for\n   for (int gray = 0; gray < 256; ++gray)\n     bins[gray] = 0;\n   for (int gray = 0; gray < 256; ++gray)\n     #pragma omp parallel for\n     for (int i = 0; i < image.size(); ++i)\n       if (image[i] == gray)\n         ++bins[gray];\n }",
            "int num_threads = omp_get_max_threads();\n\n\t  // create a private count variable for each thread\n\t  std::array<std::array<size_t, 256>, MAX_THREADS> thread_bins;\n\t  thread_bins[0] = { 0 };\n\n\t  // init bins to 0\n\t  bins = { 0 };\n\n\t  // use omp parallel for to calculate counts\n\t  #pragma omp parallel for shared(num_threads) private(thread_bins)\n\t  for (int i = 0; i < num_threads; i++) {\n\t\t  // init the thread bins to 0\n\t\t  thread_bins[i] = { 0 };\n\t\t  // loop over image array\n\t\t  for (int j = 0; j < image.size(); j++) {\n\t\t\t  // increment thread bin for pixel with the corresponding grayscale value\n\t\t\t  thread_bins[i][image[j]]++;\n\t\t  }\n\t  }\n\n\t  // merge the thread bins together into bins\n\t  for (int i = 0; i < num_threads; i++) {\n\t\t  for (int j = 0; j < 256; j++) {\n\t\t\t  // increment main bins for the corresponding grayscale value\n\t\t\t  bins[j] += thread_bins[i][j];\n\t\t  }\n\t  }\n  }",
            "// Compute the number of pixels with gray scale value of 0.\n    bins[0] = 0;\n    for (int i = 0; i < image.size(); i++){\n        if (image[i] == 0){\n            bins[0] += 1;\n        }\n    }\n    // Compute the number of pixels with gray scale value of 1.\n    bins[1] = 0;\n    for (int i = 0; i < image.size(); i++){\n        if (image[i] == 1){\n            bins[1] += 1;\n        }\n    }\n    // Compute the number of pixels with gray scale value of 2.\n    bins[2] = 0;\n    for (int i = 0; i < image.size(); i++){\n        if (image[i] == 2){\n            bins[2] += 1;\n        }\n    }\n    // Compute the number of pixels with gray scale value of 3.\n    bins[3] = 0;\n    for (int i = 0; i < image.size(); i++){\n        if (image[i] == 3){\n            bins[3] += 1;\n        }\n    }\n    // Compute the number of pixels with gray scale value of 4.\n    bins[4] = 0;\n    for (int i = 0; i < image.size(); i++){\n        if (image[i] == 4){\n            bins[4] += 1;\n        }\n    }\n    // Compute the number of pixels with gray scale value of 5.\n    bins[5] = 0;\n    for (int i = 0; i < image.size(); i++){\n        if (image[i] == 5){\n            bins[5] += 1;\n        }\n    }\n    // Compute the number of pixels with gray scale value of 6.\n    bins[6] = 0;\n    for (int i = 0; i < image.size(); i++){\n        if (image[i] == 6){\n            bins[6] += 1;\n        }\n    }\n    // Compute the number of pixels with gray scale value of 7.\n    bins[7] = 0;\n    for (int i = 0; i < image.size(); i++){\n        if (image[i] == 7){\n            bins[7] += 1;\n        }\n    }\n    // Compute the number of pixels with gray scale value of 8.\n    bins[8] = 0;\n    for (int i = 0; i < image.size(); i++){\n        if (image[i] == 8){\n            bins[8] += 1;\n        }\n    }\n    // Compute the number of pixels with gray scale value of 9.\n    bins[9] = 0;\n    for (int i = 0; i < image.size(); i++){\n        if (image[i] == 9){\n            bins[9] += 1;\n        }\n    }\n    // Compute the number of pixels with gray scale value of 10.\n    bins[10] = 0;\n    for (int i = 0; i < image.size(); i++){\n        if (image[i] == 10){\n            bins[10] += 1;\n        }\n    }\n    // Compute the number of pixels with gray scale value of 11.\n    bins[11] = 0;\n    for (int i = 0; i < image.size(); i++){\n        if (image[i] == 11){\n            bins[11] += 1;\n        }\n    }\n    // Compute the number of pixels with gray scale value of 12.\n    bins[12] = 0;\n    for (int i = 0; i < image.size(); i++){\n        if (image[i] == 12){\n            bins[12] += 1;\n        }\n    }\n    // Compute the number of pixels with gray scale value of 13.\n    bins[13] = 0;\n    for (int i =",
            "#pragma omp parallel for\n     for (size_t i = 0; i < image.size(); ++i) {\n         #pragma omp atomic\n         bins[image[i]] += 1;\n     }\n }",
            "// TODO: Parallelize this loop using OpenMP\n#pragma omp parallel for\n\tfor (int i = 0; i < image.size(); ++i) {\n        bins[image[i]]++;\n    }\n }",
            "#pragma omp parallel for\n     for(size_t i = 0; i < image.size(); i++) {\n         bins[image[i]]++;\n     }\n }",
            "#pragma omp parallel for\n   for (size_t i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "// TODO: Fill in this function\n  #pragma omp parallel for\n  for (int i = 0; i < 256; i++) {\n    for (auto iter = image.begin(); iter!= image.end(); ++iter) {\n      if(*iter == i) {\n        bins[i]++;\n      }\n    }\n  }\n}",
            "size_t num_threads = omp_get_max_threads();\n   std::fill(bins.begin(), bins.end(), 0);\n\n   omp_set_num_threads(num_threads);\n\n   #pragma omp parallel for \n   for (size_t i = 0; i < image.size(); i++){\n      bins[image[i]] += 1;\n   }\n }",
            "std::fill(bins.begin(), bins.end(), 0);\n     #pragma omp parallel for\n     for(int i = 0; i < image.size(); i++) {\n         ++bins[image[i]];\n     }\n }",
            "for(int i = 0; i < 256; i++){\n     bins[i] = 0;\n   }\n   #pragma omp parallel for\n   for (int i = 0; i < image.size(); i++){\n     bins[image[i]] += 1;\n   }\n   return;\n }",
            "#pragma omp parallel for\n \tfor (int i = 0; i < 256; i++) {\n \t\tint count = 0;\n \t\tfor (int j = 0; j < image.size(); j++)\n \t\t\tif (image[j] == i)\n \t\t\t\tcount++;\n \t\tbins[i] = count;\n \t}\n }",
            "int N = image.size();\n    #pragma omp parallel\n    {\n        #pragma omp for schedule(static)\n        for (int i = 0; i < N; i++) {\n            bins[image[i]] += 1;\n        }\n    }\n}",
            "#pragma omp parallel for\n  for(size_t i = 0; i < image.size(); i++) {\n    bins[image[i]] += 1;\n  }\n}",
            "// TODO\n    #pragma omp parallel for\n    for(int i=0;i<256;i++){\n        bins[i]=0;\n    }\n    #pragma omp parallel for\n    for(int i=0;i<image.size();i++){\n        bins[image[i]]++;\n    }\n}",
            "std::array<size_t, 256> bin_local;\n    for (auto i = 0; i < 256; i++) {\n        bin_local[i] = 0;\n    }\n\n    #pragma omp parallel for shared(bin_local, image)\n    for (auto i = 0; i < image.size(); i++) {\n        bin_local[image[i]]++;\n    }\n    for (auto i = 0; i < 256; i++) {\n        bins[i] += bin_local[i];\n    }\n}",
            "for (size_t i = 0; i < 256; i++) {\n      bins[i] = 0;\n   }\n\n   #pragma omp parallel for\n   for (size_t i = 0; i < image.size(); i++) {\n      bins[image[i]] += 1;\n   }\n }",
            "int n = image.size();\n    #pragma omp parallel for \n    for (int i = 0; i < n; ++i) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO\n    // Loop over every pixel in image\n    //   Increment the value at bins[image[pixel]]\n    for (size_t pixel = 0; pixel < image.size(); pixel++) {\n        bins[image[pixel]]++;\n    }\n}",
            "#pragma omp parallel for\n    for(int i=0;i<image.size();++i){\n        bins[image[i]] += 1;\n    }\n}",
            "size_t numThreads = omp_get_max_threads();\n\tstd::array<size_t, 256> bins_private;\n\tfor (int i = 0; i < 256; ++i)\n\t{\n\t\tbins_private[i] = 0;\n\t}\n\t//TODO: insert code\n#pragma omp parallel for num_threads(numThreads)\n\tfor (int i = 0; i < image.size(); ++i)\n\t{\n\t\tbins_private[image[i]]++;\n\t}\n#pragma omp parallel for\n\tfor (int i = 0; i < 256; ++i)\n\t{\n\t\tbins[i] += bins_private[i];\n\t}\n}",
            "int N = image.size();\n    #pragma omp parallel for\n    for(int i=0;i<N;i++)\n    {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n   for(int i=0; i<image.size(); ++i) {\n     ++bins[image[i]];\n   }\n }",
            "#pragma omp parallel for\n  for (size_t i = 0; i < 256; i++) {\n    bins[i] = 0;\n  }\n  \n  for (size_t i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n }",
            "#pragma omp parallel for\n  for (size_t i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 256; i++) {\n        for (int j = 0; j < image.size(); j++) {\n            if (image[j] == i) {\n                #pragma omp atomic\n                bins[i]++;\n            }\n        }\n    }\n}",
            "for(int i=0; i<256; ++i) bins[i] = 0;\n     #pragma omp parallel for\n     for(int i=0; i<image.size(); ++i) {\n         ++bins[image[i]];\n     }\n }",
            "int size = image.size();\n \t#pragma omp parallel for\n \tfor(int i = 0; i < size; i++) {\n \t\tint idx = image[i];\n \t\tbins[idx]++;\n \t}\n \t\n }",
            "#pragma omp parallel for\n    for(int i=0; i<image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n    for (int i=0;i<image.size();++i){\n        bins[image[i]] += 1;\n    }\n}",
            "// TODO: Your code here\n    #pragma omp parallel for\n    for(size_t i = 0; i < image.size(); i++) {\n        bins[image[i]] += 1;\n    }\n\n}",
            "#pragma omp parallel\n   {\n       #pragma omp for\n       for (size_t i = 0; i < image.size(); i++) {\n           bins[image[i]]++;\n       }\n   }\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < 256; i++)\n     bins[i] = 0;\n   for (size_t i = 0; i < image.size(); i++)\n     bins[image[i]]++;\n }",
            "// Use an OpenMP parallel for to count the pixels\n     #pragma omp parallel for\n     for (int i=0; i<image.size(); ++i)\n         ++bins[image[i]];\n }",
            "#pragma omp parallel for\n    for (int i = 0; i < image.size(); ++i) {\n        ++bins[image[i]];\n    }\n}",
            "}",
            "// TODO: YOUR CODE HERE\n     for (int i = 0; i < image.size(); i++)\n     {\n         bins.at(image[i])++;\n     }\n }",
            "#pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// parallel for\n#pragma omp parallel for\n   for (int i = 0; i < image.size(); i++)\n     bins[image[i]]++;\n }",
            "#pragma omp parallel\n  for (int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n\n}",
            "#pragma omp parallel\n    {\n        std::array<size_t, 256> local_bins;\n        #pragma omp for\n        for(auto i = 0; i < image.size(); i++) {\n            local_bins[image[i]]++;\n        }\n\n        #pragma omp critical\n        for(int i = 0; i < 256; i++) {\n            bins[i] += local_bins[i];\n        }\n    }\n}",
            "int N = image.size();\n   std::array<size_t, 256> tmp_bins;\n   for (int i = 0; i < 256; i++) {\n     tmp_bins[i] = 0;\n   }\n\n   #pragma omp parallel for\n   for (int i = 0; i < N; i++) {\n     tmp_bins[image[i]] += 1;\n   }\n\n   for (int i = 0; i < 256; i++) {\n     bins[i] = tmp_bins[i];\n   }\n }",
            "for(int i = 0; i < bins.size(); i++) {\n\t\tbins[i] = 0;\n\t}\n\n\tfor(int i = 0; i < image.size(); i++) {\n\t\tbins[image[i]]++;\n\t}\n}",
            "// Fill the bins with zeros\n     for (int i = 0; i < 256; i++) {\n         bins[i] = 0;\n     }\n\n     //#pragma omp parallel for\n     for (int i = 0; i < image.size(); i++) {\n         bins[image[i]]++;\n     }\n }",
            "#pragma omp parallel for\n   for (int i = 0; i < image.size(); i++) {\n      bins[image[i]]++;\n   }\n}",
            "#pragma omp parallel for\n  for (auto const& i : image) {\n    #pragma omp atomic\n    bins[i]++;\n  }\n }",
            "#pragma omp parallel for\n    for (int i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n     for (int i = 0; i < image.size(); ++i) {\n         ++bins[image[i]];\n     }\n }",
            "// Your code here\n    // 1. Create an OpenMP for loop to iterate over the input image,\n    // 2. Increment the bins[i] variable in the for loop\n    // 3. You may use the `parallel for` directive\n    // 4. You may use the `num_threads` clause\n#pragma omp parallel for num_threads(8)\n    for (size_t i = 0; i < image.size(); ++i)\n        ++bins[image[i]];\n}",
            "// TODO\n}",
            "// TODO: Your code here\n    bins.fill(0);\n    #pragma omp parallel for\n    for(int i=0; i<image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n      bins[image[i]]++;\n    }\n }",
            "// Initialize bins to zero\n     for(size_t i = 0; i < 256; i++)\n         bins[i] = 0;\n     \n     // OpenMP parallel for \n     #pragma omp parallel for\n     for (size_t i = 0; i < image.size(); i++) {\n         // Increment the bin that corresponds to the pixel\n         bins[image[i]]++;\n     }\n }",
            "std::fill(bins.begin(), bins.end(), 0);\n   for (auto i : image) {\n      ++bins[i];\n   }\n }",
            "int num_threads=omp_get_max_threads();\n   std::vector<int> max_value(num_threads);\n   for(int i=0;i<num_threads;i++){\n     max_value[i]=256/num_threads*i;\n   }\n   max_value[num_threads-1]=256;\n   #pragma omp parallel num_threads(num_threads) \n   {\n     int my_tid=omp_get_thread_num();\n     int start=max_value[my_tid];\n     int end=max_value[my_tid+1];\n     for(int i=start;i<end;i++){\n       bins[i]+=std::count_if(image.begin(),image.end(),[i](int x){return x==i;});\n     }\n   }\n }",
            "}",
            "#pragma omp parallel for\n     for (int i = 0; i < 256; i++) {\n        for (int j = 0; j < image.size(); j++)\n            if (image[j] == i)\n                bins[i]++;\n    }\n}",
            "// TODO: Your code here\n    #pragma omp parallel for\n    for (int i=0; i<image.size(); ++i) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: your code here\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i)\n    {\n        ++bins[image[i]];\n    }\n}",
            "#pragma omp parallel for\n    for(int i=0;i<image.size();i++){\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel\n   {\n      int max_thread = omp_get_max_threads();\n      int tid = omp_get_thread_num();\n      //std::cout << \"Thread \" << tid << \" of \" << max_thread << std::endl;\n      for (size_t i = 0; i < image.size(); i++) {\n         //std::cout << \"Image index \" << i << std::endl;\n         bins[image[i]]++;\n      }\n   }\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "#pragma omp parallel for\n     for (int i = 0; i < 256; ++i)\n         bins[i] = 0;\n     for (int i = 0; i < image.size(); ++i)\n         bins[image[i]]++;\n }",
            "// Your code here\n     #pragma omp parallel for\n     for (int i = 0; i < image.size(); i++) {\n         bins[image[i]]++;\n     }\n }",
            "// TODO: your code here\n   int num_threads = omp_get_max_threads();\n   #pragma omp parallel for num_threads(num_threads)\n   for (int i = 0; i < image.size(); i++) {\n     bins[image[i]] += 1;\n   }\n }",
            "#pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n\n}",
            "// TODO\n }",
            "#pragma omp parallel for\n   for (int i = 0; i < 256; i++){\n     bins[i] = 0;\n   }\n\n   #pragma omp parallel for\n   for (int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "#pragma omp parallel for\n    for (auto i = 0; i < image.size(); ++i) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: Replace this with your own solution\n     int len=image.size();\n     int i;\n     for (i=0;i<256;i++)\n     {\n         bins[i]=0;\n     }\n     #pragma omp parallel for\n     for (i=0;i<len;i++)\n     {\n         bins[image[i]]++;\n     }\n\n }",
            "#pragma omp parallel for\n\tfor(int i=0; i<image.size(); i++){\n\t\tbins[image[i]]++;\n\t}\n }",
            "for(int i=0; i<256; i++) {\n         bins[i]=0;\n     }\n     for (int i=0; i<image.size(); i++) {\n        bins[image[i]]++;\n     }\n}",
            "#pragma omp parallel for\n    for (auto i=0; i<image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 256; i++) {\n      bins[i] = std::count(image.begin(), image.end(), i);\n    }\n\n}",
            "#pragma omp parallel for\n   for (int i=0; i<image.size(); i++) {\n      bins[image[i]]++;\n   }\n   // for (size_t i=0; i<256; i++) {\n   //    if (bins[i]!= 0) std::cout << bins[i] << std::endl;\n   // }\n}",
            "int num_threads=4;\n    int num_threads_omp = omp_get_max_threads();\n    std::cout << \"omp_get_max_threads = \" << num_threads_omp << std::endl;\n    int num_procs_omp = omp_get_num_procs();\n    std::cout << \"omp_get_num_procs = \" << num_procs_omp << std::endl;\n    std::cout << \"OMP_NUM_THREADS = \" << omp_get_num_threads() << std::endl;\n    std::cout << \"omp_get_max_threads = \" << omp_get_max_threads() << std::endl;\n    std::cout << \"omp_get_num_procs = \" << omp_get_num_procs() << std::endl;\n\n#pragma omp parallel for num_threads(num_threads)\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "//fill array with zeros\n\tfor(int i = 0; i < 256; ++i){\n\t\tbins[i] = 0;\n\t}\n\n\t//loop through each pixel in image\n\tfor(int i = 0; i < image.size(); i++){\n\t\t//increment bin for each pixel\n\t\t#pragma omp parallel for\n\t\tfor(int j = 0; j < image[i]; j++){\n\t\t\tbins[j]++;\n\t\t}\n\t}\n\n}",
            "#pragma omp parallel for\n     for (int i = 0; i < 256; i++)\n         for (int j = 0; j < image.size(); j++)\n             if (image[j] == i)\n                 bins[i]++;\n }",
            "#pragma omp parallel for\n    for (int i = 0; i < 256; i++) {\n        for (int j = 0; j < image.size(); j++) {\n            if (image[j] == i) {\n                bins[i] += 1;\n            }\n        }\n    }\n\n}",
            "// TODO: Your code here\n   // parallel region\n   #pragma omp parallel for\n   for(int i = 0; i < image.size(); i++){\n      bins[image[i]]++;\n   }\n }",
            "#pragma omp parallel for\n    for (int i = 0; i < 256; i++) {\n        bins[i] = std::count(image.begin(), image.end(), i);\n    }\n}",
            "#pragma omp parallel for num_threads(4)\n    for(int i = 0; i < 256; i++) {\n        bins[i] = 0;\n        for(int j = 0; j < image.size(); j++) {\n            if(image[j] == i) {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n     int num_threads = omp_get_max_threads();\n     int num_pixels = image.size();\n     int chunk_size = num_pixels / num_threads;\n#pragma omp parallel for shared(image) private(num_pixels, chunk_size)\n     for (int i = 0; i < num_pixels; i += chunk_size) {\n         int chunk_end = i + chunk_size;\n         if (i + chunk_size > num_pixels) chunk_end = num_pixels;\n         for (int j = i; j < chunk_end; j++) {\n             bins[image[j]]++;\n         }\n     }\n }",
            "#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel\n{\n\tsize_t tid = omp_get_thread_num();\n\n#pragma omp for\n\tfor (size_t i = 0; i < image.size(); ++i) {\n\t\tbins[image[i]]++;\n\t}\n}\n }",
            "}",
            "#pragma omp parallel for\n    for(int i = 0; i < image.size(); i++) {\n        #pragma omp critical\n        bins[image[i]]++;\n    }\n    return;\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n\n}",
            "#pragma omp parallel for\n    for(size_t i = 0; i < image.size(); ++i){\n        ++bins[image[i]];\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i=0; i<image.size(); ++i) {\n      ++bins[image[i]];\n    }\n }",
            "#pragma omp parallel for\n  for(auto x: image) {\n    bins[x] += 1;\n  }\n}",
            "// Your code here\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: Parallelize this loop.\n    // The bins array must be initialized to 0 before parallelizing.\n    \n    for (size_t i = 0; i < image.size(); ++i)\n    {\n        bins[image[i]] += 1;\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 256; i++) {\n        for (int j = 0; j < 256; j++) {\n            if (image[i] == j) {\n                bins[j]++;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "//std::array<size_t, 256> bins = { 0 };\n\tint n = image.size();\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; ++i)\n\t{\n\t\tbins[image[i]] += 1;\n\t}\n\t\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < 256; i++) {\n    bins[i] = 0;\n  }\n\n  for (int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "//your code here\n\t for (int i = 0; i < image.size(); i++) {\n\t\t bins[image[i]]++;\n\t }\n }",
            "// Hint: use std::for_each and std::transform\n#pragma omp parallel for \n    for(int i=0;i<image.size();i++)\n    {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n     for (size_t i = 0; i < image.size(); ++i)\n     {\n         bins[image[i]]++;\n     }\n }",
            "#pragma omp parallel for\n     for (size_t i = 0; i < image.size(); i++) {\n         #pragma omp atomic\n         bins[image[i]]++;\n     }\n }",
            "#pragma omp parallel for\n    for (int i = 0; i < image.size(); i++)\n    {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < image.size(); ++i) {\n        int index = image[i];\n        bins[index]++;\n    }\n }",
            "#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n        #pragma omp atomic\n        ++bins[image[i]];\n    }\n}",
            "#pragma omp parallel\n  {\n    #pragma omp for\n    for (size_t i = 0; i < 256; ++i) {\n      bins[i] = 0;\n    }\n    #pragma omp for\n    for (size_t i = 0; i < image.size(); ++i) {\n      ++bins[image[i]];\n    }\n  }\n}",
            "int nthreads = omp_get_num_threads();\n    int tid = omp_get_thread_num();\n    int chunk_size = image.size() / nthreads;\n    int chunk_start = chunk_size * tid;\n    int chunk_end = chunk_size * (tid + 1) - 1;\n    if (tid == nthreads - 1)\n        chunk_end = image.size() - 1;\n    for (int i = chunk_start; i <= chunk_end; ++i)\n        ++bins[image[i]];\n}",
            "#pragma omp parallel for\n   for(size_t i=0; i < image.size(); i++)\n     bins[image[i]]++;\n }",
            "// HINT: use omp parallel for\n   // HINT: remember to synchronize the data\n   #pragma omp parallel for\n   for (size_t i = 0; i < image.size(); i++) {\n      bins[image[i]]++;\n   }\n\n   // HINT: synchronize data\n   #pragma omp barrier\n\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]] += 1;\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++)\n    {\n        bins[image[i]]++;\n    }\n }",
            "// Your code here\n    #pragma omp parallel for\n    for(int i=0; i<image.size(); i++) {\n        bins[image[i]]++;\n    }\n }",
            "// Fill bins with 0's.\n   for (auto& v: bins) { v = 0; }\n\n   // Count each pixel.\n   #pragma omp parallel for\n   for (size_t i=0; i < image.size(); i++) {\n     auto intensity = image[i];\n     bins[intensity]++;\n   }\n }",
            "#pragma omp parallel for\n    for(int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n    return;\n}",
            "//#pragma omp parallel for\n   for(size_t i = 0; i < image.size(); ++i) {\n      bins[image[i]]++;\n   }\n }",
            "// TODO: Your code here.\n     bins.fill(0);\n     #pragma omp parallel\n     {\n         #pragma omp for\n         for (int i = 0; i < image.size(); ++i)\n             ++bins[image[i]];\n     }\n }",
            "// TODO: parallelize the loop\n    for(auto pixel: image)\n        bins[pixel]++;\n}",
            "// parallel section\n\tint nThreads = omp_get_max_threads();\n\t#pragma omp parallel for num_threads(nThreads)\n\tfor (int i = 0; i < image.size(); ++i) {\n\t\t// single section\n\t\tbins[image[i]]++;\n\t}\n}",
            "#pragma omp parallel for\n     for (int i = 0; i < 256; ++i) {\n         bins[i] = 0;\n     }\n     for (size_t i = 0; i < image.size(); ++i) {\n         ++bins[image[i]];\n     }\n }",
            "// TODO\n }",
            "// Fill bins with 0's\n    for (auto& bin : bins) {\n        bin = 0;\n    }\n\n    // Count pixels in image using OpenMP\n#pragma omp parallel for\n    for (int i = 0; i < image.size(); ++i) {\n        ++bins[image[i]];\n    }\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < image.size(); ++i) {\n      bins[image[i]] += 1;\n   }\n}",
            "bins.fill(0);\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n }",
            "#pragma omp parallel for\n   for (size_t i=0; i<256; i++) {\n      for (size_t j=0; j<image.size(); j++) {\n         if (image[j]==i) {\n            bins[i]+=1;\n         }\n      }\n   }\n}",
            "// parallel for\n    for (int i = 0; i < 256; i++){\n        int j = 0;\n#pragma omp parallel for\n        for (j = 0; j < image.size(); j++){\n            if (image[j] == i){\n                bins[i]++;\n            }\n        }\n    }\n}",
            "#pragma omp parallel\n    for (int i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// TO DO\n    int counter=0;\n    for (int i = 0; i<256; i++) {\n        counter=0;\n        for (int j = 0; j < image.size(); j++) {\n            if (image[j]==i)\n                counter++;\n        }\n        bins[i]=counter;\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < image.size(); i++)\n  {\n   bins[image[i]]++;\n  }\n }",
            "#pragma omp parallel for\n    for (int i=0; i<image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i=0; i<image.size(); ++i) {\n            ++bins[image[i]];\n        }\n    }\n}",
            "#pragma omp parallel for schedule(static)\n  for(int i=0; i<image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "//TODO\n    #pragma omp parallel for\n    for (int i = 0; i < image.size(); i++)\n    {\n        bins[image[i]]++;\n    }\n    \n    }",
            "size_t n = image.size();\n    //#pragma omp parallel for\n    for (size_t i = 0; i < n; i++) {\n        bins[image[i]]++;\n    }\n}",
            "// Compute histogram with OpenMP\n    #pragma omp parallel for\n    for(int i=0;i<256;i++){\n        for(int j=0;j<image.size();j++){\n            if(image[j]==i){\n                bins[i]++;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for(int i=0; i<image.size(); i++) {\n        int a = image[i];\n        bins[a]++;\n    }\n }",
            "// Implement this function\n   #pragma omp parallel for\n   for (int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "// Use OpenMP to parallelize the for loop\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]] += 1;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   for(auto const& i: image){\n      bins[i]++;\n   }\n   #pragma omp parallel for\n   for(int i=0;i<bins.size();i++){\n      for(auto const& j: image){\n         if(i==j){\n            bins[i]++;\n         }\n      }\n   }\n\n   return;\n }",
            "#pragma omp parallel for\n    for (int i = 0; i < image.size(); ++i) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i=0;i<image.size();i++)\n    bins[image[i]]++;\n}",
            "// TODO: Your code here\n\tint size = image.size();\n\tint j = 0;\n#pragma omp parallel\n#pragma omp for\n\tfor (int i = 0; i < size; i++) {\n\t\tj = image[i];\n\t\tbins[j]++;\n\t}\n\treturn;\n}",
            "for (auto &i:bins)\n        i = 0;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: Implement this\n   #pragma omp parallel for\n   for (size_t i = 0; i < image.size(); i++){\n   \tbins[image[i]]++;\n   }\n }",
            "#pragma omp parallel for\n  for (int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "#pragma omp parallel for\n  for (size_t i=0; i<image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "#pragma omp parallel for\n    for (int i=0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n     for (size_t i = 0; i < image.size(); ++i) {\n         bins[image[i]]++;\n     }\n }",
            "#pragma omp parallel\n    {\n        #pragma omp for\n        for (int i=0; i < image.size(); ++i) {\n            bins[image[i]] += 1;\n        }\n    }\n}",
            "//...\n    #pragma omp parallel for\n    for(int i=0;i<256;i++){\n        bins[i]=0;\n    }\n    #pragma omp parallel for\n    for(int i=0;i<image.size();i++){\n        bins[image[i]]++;\n    }\n }",
            "int i;\n\tsize_t j;\n\n\t#pragma omp parallel for private(i, j)\n\tfor (i = 0; i < image.size(); i++) {\n\t\tj = image[i];\n\t\tbins[j]++;\n\t}\n}",
            "int num_threads = omp_get_max_threads();\n    int tid = omp_get_thread_num();\n    size_t bin_size = image.size() / num_threads;\n    size_t start = tid * bin_size;\n    size_t end = (tid + 1) * bin_size;\n    if (tid == num_threads - 1) {\n        end = image.size();\n    }\n\n    for (size_t i = start; i < end; i++) {\n        bins[image[i]] += 1;\n    }\n}",
            "// your code here\n\t int size = (int)image.size();\n#pragma omp parallel for\n\tfor (int i = 0; i < size; i++)\n\t{\n\t\tbins[image[i]]++;\n\t}\n}",
            "#pragma omp parallel for\n    for(int i=0;i<256;i++)\n    {\n        for(int j=0;j<image.size();j++)\n            if(image[j]==i)\n                bins[i]++;\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 256; i++) {\n        for (int j = 0; j < image.size(); j++) {\n            if (image[j] == i) {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "// Your code here\n\n#pragma omp parallel for\n  for (int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "// TODO\n    #pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n\n}",
            "// fill in your code here\n   #pragma omp parallel for\n   for (int i = 0; i < image.size(); i++) {\n       bins[image[i]]++;\n   }\n }",
            "std::array<size_t, 256> hist;\n    #pragma omp parallel\n   {\n       #pragma omp for\n        for (auto i = 0; i < 256; ++i) {\n            hist[i] = std::count(image.begin(), image.end(), i);\n        }\n       \n       \n   }\n   bins=hist;\n\n }",
            "#pragma omp parallel for\n   for(size_t i = 0; i < image.size(); ++i) {\n     ++bins[image[i]];\n   }\n}",
            "int image_size = image.size();\n     #pragma omp parallel for\n     for(int i = 0; i<image_size; ++i) {\n         bins[image[i]]++;\n     }\n }",
            "#pragma omp parallel for\n    for (size_t pixel = 0; pixel < image.size(); ++pixel) {\n        // Your code here.\n        bins[image[pixel]]++;\n    }\n}",
            "// TODO: Your code here\n    #pragma omp parallel for\n    for(int i=0; i<image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "//TODO: Your code here\n#pragma omp parallel for num_threads(8)\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n     for (int i = 0; i < image.size(); i++) {\n         bins[image[i]]++;\n     }\n }",
            "#pragma omp parallel for\n\tfor (int i = 0; i < 256; i++) {\n\t\tbins[i] = 0;\n\t}\n\tfor (auto &i : image) {\n\t\tbins[i]++;\n\t}\n}",
            "// TODO\n}",
            "size_t nThreads = omp_get_max_threads();\n    int nColors = bins.size();\n    #pragma omp parallel for\n    for(int i = 0; i < nColors; i++)\n    {\n        bins[i] = 0;\n        for (int j = 0; j < nColors; j++)\n        {\n            #pragma omp critical \n            {\n                bins[i] += image[j];\n            }\n        }\n    }\n }",
            "int size = image.size();\n     //int nThreads = 2;\n     #pragma omp parallel for\n     for(int i=0; i < size; i++){\n         bins[image[i]]++;\n     }\n }",
            "/* Compute the number of pixels at each grayscale value */\n#pragma omp parallel for\n    for(size_t i=0; i<image.size(); i++){\n        bins[image[i]]+=1;\n    }\n}",
            "#pragma omp parallel for\n     for (size_t i = 0; i < image.size(); ++i) {\n         ++bins[image[i]];\n     }\n }",
            "//TODO\n \t//fill bins array with 0s\n \tfor (size_t i = 0; i < 256; i++) {\n \t\tbins[i] = 0;\n \t}\n\n \t//omp for\n \t//count the number of pixels with each intensity\n \t//#pragma omp for\n \tfor (size_t i = 0; i < image.size(); i++) {\n \t\tbins[image[i]]++;\n \t}\n }",
            "//std::fill(bins.begin(), bins.end(), 0);\n    #pragma omp parallel for\n    for (int i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n    #pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n }",
            "int max_threads = omp_get_max_threads();\n   // initialize bins to zero\n   std::fill(bins.begin(), bins.end(), 0);\n   // each thread will be responsible for a different bin\n   #pragma omp parallel for num_threads(max_threads)\n   for (int i=0; i<image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "#pragma omp parallel for\n   for(auto i=0; i<256; ++i){\n        bins[i] = 0;\n    }\n   for(auto pixel: image){\n       bins[pixel]++;\n   }\n}",
            "//TODO: Your code here\n    size_t size = image.size();\n    int num_threads = omp_get_max_threads();\n    // printf(\"number of threads: %d\\n\", num_threads);\n    // printf(\"size of image: %lu\\n\", size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < size; i++) {\n        bins[image[i]]++;\n    }\n\n    // for (size_t i = 0; i < bins.size(); i++) {\n    //     printf(\"%ld \", bins[i]);\n    // }\n    // printf(\"\\n\");\n }",
            "#pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "serial",
        "prompt": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "for (size_t i = 0; i < image.size(); ++i) {\n        if (bins[image[i]] == 0) {\n            bins[image[i]]++;\n        } else {\n            bins[image[i]]++;\n        }\n    }\n}",
            "for(int i=0;i<image.size();i++)\n   {\n\t   bins[image[i]]++;\n   }\n}",
            "for(size_t i=0; i<image.size(); i++)\n    {\n      bins.at(image[i]) += 1;\n    }\n }",
            "for (auto const& p : image) {\n        bins[p] += 1;\n    }\n}",
            "bins.fill(0);\n  for (size_t i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "bins.fill(0);\n    for(auto i : image)\n        bins[i]++;\n}",
            "for (int i : image) {\n     bins[i]++;\n   }\n }",
            "for (int i : image) {\n        ++bins[i];\n    }\n}",
            "for (size_t i=0; i < image.size(); ++i) {\n     ++bins[image[i]];\n   }\n }",
            "int size = image.size();\n   for (int i = 0; i < size; i++) {\n      bins[image[i]]++;\n   }\n }",
            "int n=image.size();\n   for(int i=0; i<n; i++){\n      bins.at(image[i])++;\n   }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n \tfor (auto& i: image) {\n \t\t++bins[i];\n \t}\n }",
            "for(int i = 0; i < 256; i++)\n    {\n        bins[i] = 0;\n    }\n    for(int i = 0; i < image.size(); i++)\n    {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: Your code goes here\n\n    size_t pixelCount = image.size();\n    for (int i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n\n    for (int i = 0; i < pixelCount; i++) {\n        bins[image[i]]++;\n    }\n }",
            "for (auto x : image) {\n        ++bins[x];\n    }\n}",
            "for (int i = 0; i < 256; i++)\n         bins[i] = 0;\n     for (int i : image) {\n         if (i < 0 || i > 255) {\n             std::cerr << \"invalid value \" << i << \" in image\\n\";\n             throw std::runtime_error(\"invalid value in image\");\n         }\n         ++bins[i];\n     }\n }",
            "for (int const& pix:image){\n        ++bins[pix];\n    }\n}",
            "for (auto v : image) {\n        bins.at(v)++;\n    }\n}",
            "// TODO: Your code here.\n   // int i=0;\n   for(auto const& element : image){\n     bins[element]++;\n   }\n}",
            "// TODO: YOUR CODE HERE\n  for (int i = 0; i < 256; i++){\n    bins[i] = 0;\n  }\n  for (int i = 0; i < image.size(); i++){\n    bins[image[i]]++;\n  }\n }",
            "// TODO: implement\n }",
            "bins.fill(0);\n    for (auto& pixel : image) {\n        ++bins[pixel];\n    }\n}",
            "int counter = 0;\n   for (int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "for (int i = 0; i < image.size(); i++) {\n \t\tbins[image[i]]++;\n \t}\n }",
            "// loop over all pixels in image\n \tfor (size_t i = 0; i < image.size(); i++)\n \t\t// if pixel intensity = 0, add one to count at index 0\n \t\t// if pixel intensity = 1, add one to count at index 1\n \t\t// if pixel intensity = 2, add one to count at index 2\n \t\t// and so on up to pixel intensity = 255\n \t\t// for example, if pixel intensity is 116,\n \t\t// add one to count at index 116\n \t\t// the image vector is already sorted so we don't need to check for\n \t\t// negative values\n \t\tbins[image[i]]++;\n }",
            "for (size_t i = 0; i < image.size(); i++) {\n\t\t bins[image[i]]++;\n\t }\n }",
            "bins = std::array<size_t, 256>();\n    for(int i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n\n    for (auto element : image) {\n        bins[element]++;\n    }\n\n    return;\n}",
            "bins.fill(0);\n    for (const int &i: image) {\n        bins[i]++;\n    }\n}",
            "for (auto i : image)\n    {\n        bins.at(i)++;\n    }\n }",
            "int i=0;\n    for(auto val : image)\n    {\n        bins[val]++;\n        i++;\n    }\n    //for(int i = 0; i < 256; i++)\n    //    std::cout << bins[i] << std::endl;\n}",
            "// Your code here.\n  for (int i = 0; i < 256; i++) {\n      bins[i] = 0;\n  }\n  for (int i = 0; i < image.size(); i++) {\n      bins[image[i]]++;\n  }\n}",
            "for (int val : image) {\n        bins[val]++;\n    }\n}",
            "for (int i = 0; i < 256; i++) {\n      bins[i] = 0;\n   }\n   for (size_t i = 0; i < image.size(); i++) {\n      bins[image[i]]++;\n   }\n }",
            "// TODO\n   std::fill(bins.begin(), bins.end(), 0);\n   for (size_t i = 0; i < image.size(); ++i) {\n     ++bins[image[i]];\n   }\n }",
            "for (auto pixel : image) {\n        bins[pixel]++;\n    }\n}",
            "for (auto const& val : image)\n        bins[val] += 1;\n }",
            "int pixelCount = image.size();\n    int imageValue = 0;\n    for(int pixel = 0; pixel < pixelCount; pixel++){\n        imageValue = image[pixel];\n        if(imageValue == 0){\n            bins[0]++;\n        }\n        else{\n            bins[imageValue]++;\n        }\n    }\n }",
            "bins.fill(0);\n\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "for (int i=0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n\n }",
            "for (size_t i = 0; i < 256; i++) {\n         for (size_t j = 0; j < image.size(); j++) {\n             if (image[j] == i)\n                 bins[i]++;\n         }\n     }\n }",
            "bins.fill(0);\n   for (int value : image) {\n      ++bins[value];\n   }\n}",
            "for (auto i : image) {\n        bins[i]++;\n    }\n}",
            "for (const auto &pix : image) {\n        bins[pix] += 1;\n    }\n}",
            "for (int i = 0; i < 256; i++) {\n         bins[i] = 0;\n     }\n     for (int i = 0; i < image.size(); i++) {\n         bins[image[i]] += 1;\n     }\n }",
            "int size = image.size();\n  for (int i = 0; i < size; i++) {\n    bins[image[i]]++;\n  }\n}",
            "for (const int pixel : image) {\n     bins[pixel]++;\n   }\n}",
            "for (auto i : image) {\n    bins[i]++;\n   }\n }",
            "bins.fill(0);\n    for (size_t i = 0; i < image.size(); ++i) {\n        ++bins[image[i]];\n    }\n}",
            "// TODO: YOUR CODE HERE\n\tfor (int i = 0; i < image.size(); i++) {\n\t\tbins.at(image[i])++;\n\t}\n\n}",
            "for (auto const& i: image) {\n     bins[i]++;\n   }\n }",
            "for(auto pixel: image) {\n        bins[pixel]++;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    for(size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "std::array<size_t, 256> counts;\n     for (int pixel : image) {\n         counts[pixel] += 1;\n     }\n     bins = counts;\n }",
            "for(auto i: image) bins[i]++;\n}",
            "for (int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "bins.fill(0);\n     for (auto px : image) {\n         bins[px]++;\n     }\n }",
            "bins.fill(0);\n\tfor (size_t i = 0; i < image.size(); i++) {\n\t\tbins[image[i]]++;\n\t}\n}",
            "for (auto const& pixel: image) {\n     bins[pixel]++;\n   }\n}",
            "int i;\n   for (i = 0; i < 256; i++)\n     bins[i] = 0;\n   for (i = 0; i < image.size(); i++)\n     bins[image[i]]++;\n }",
            "bins.fill(0);\n    for(size_t i = 0; i < image.size(); i++)\n    {\n        bins[image[i]]++;\n    }\n}",
            "// TODO\n }",
            "for(auto i : image) {\n        bins[i] += 1;\n    }\n}",
            "// TODO: fill in\n   size_t image_size = image.size();\n   for (size_t i = 0; i < image_size; i++) {\n      bins[image[i]]++;\n   }\n }",
            "size_t counter = 0;\n    std::fill(bins.begin(), bins.end(), 0);\n    for (auto const& pixel : image)\n        ++bins[pixel];\n}",
            "for (auto val : image)\n     ++bins[val];\n }",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (auto i : image)\n    {\n        bins[i]++;\n    }\n}",
            "for(auto pixel:image)\n    {\n        bins[pixel]++;\n    }\n\n}",
            "for (auto pix : image) {\n        bins[pix] += 1;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    for(size_t i = 0; i < image.size(); i++){\n        bins[image[i]]++;\n    }\n}",
            "for(auto &pixel : image) {\n        bins[pixel]++;\n    }\n }",
            "bins.fill(0);\n    for (auto i : image)\n        bins[i]++;\n}",
            "// YOUR CODE HERE\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (int i : image) {\n        bins[i]++;\n    }\n}",
            "for (auto v : image) {\n   \tbins[v]++;\n   }\n }",
            "for (auto i : image) {\n    ++bins[i];\n  }\n}",
            "bins.fill(0);\n  for (int i : image) {\n    ++bins[i];\n  }\n}",
            "std::array<size_t, 256> tmp;\n    std::fill(tmp.begin(), tmp.end(), 0);\n    for (auto value : image)\n        ++tmp[value];\n    for (int i = 0; i < 256; ++i)\n        bins[i] += tmp[i];\n}",
            "for (int i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n    for (auto p : image) {\n        bins[p]++;\n    }\n}",
            "for (int i = 0; i < image.size(); i++)\n        bins[image[i]]++;\n}",
            "for (size_t i = 0; i < bins.size(); ++i) {\n\t\tfor (size_t j = 0; j < image.size(); ++j) {\n\t\t\tif (image[j] == i) {\n\t\t\t\t++bins[i];\n\t\t\t}\n\t\t}\n\t}\n}",
            "for (int i = 0; i < image.size(); i++){\n        bins[image[i]]++;\n    }\n}",
            "for (auto val : image) {\n      bins[val] += 1;\n   }\n }",
            "// TODO: implement\n   bins.fill(0);\n   for (auto i : image)\n      bins[i]++;\n }",
            "for (auto i = 0; i < image.size(); ++i)\n \t{\n \t\tbins[image[i]]++;\n \t}\n }",
            "for (auto& num : image) {\n\t\t bins.at(num)++;\n\t }\n\t \n }",
            "for (auto const& pix : image)\n      bins[pix]++;\n }",
            "bins.fill(0);\n    for (const auto& el : image) {\n        ++bins[el];\n    }\n}",
            "for(int grayVal: image)\n        bins[grayVal]++;\n}",
            "for (auto pixel : image) {\n    ++bins[pixel];\n  }\n}",
            "for(size_t i = 0; i < image.size(); ++i) {\n    ++bins[image[i]];\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]] += 1;\n    }\n}",
            "for(size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "for(int i=0;i<image.size();i++){\n         bins[image[i]]++;\n     }\n }",
            "int j = 0;\n    for(auto x : image) {\n        bins[x]++;\n        if (j < image.size()-1) {\n            j++;\n        }\n    }\n}",
            "for (int i = 0; i < image.size(); i++){\n        bins[image[i]]++;\n    }\n}",
            "// TO BE COMPLETED\n }",
            "// Create an array of size 256, initialized to 0\n   std::array<size_t, 256> hist = {0};\n\n   // Iterate over the image and increment the value in the histogram\n   for(int i = 0; i < image.size(); ++i) {\n     hist[image[i]]++;\n   }\n   bins = hist;\n }",
            "bins.fill(0);\n     for (auto const& pixel : image) {\n         bins[pixel]++;\n     }\n }",
            "bins.fill(0);\n     for (int i=0; i<image.size(); i++) {\n         bins[image[i]] += 1;\n     }\n }",
            "for (auto i : image)\n     bins[i]++;\n }",
            "bins = std::array<size_t, 256>();\n    for(size_t i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n    for(auto pixel : image) {\n        bins[pixel]++;\n    }\n}",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "for (int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n   //std::cout << std::endl;\n   //for (int i = 0; i < 256; i++) {\n   //  std::cout << bins[i] << \" \";\n   //}\n }",
            "// insert your code here\n   bins = std::array<size_t, 256>();\n   for (int i = 0; i < image.size(); ++i)\n   {\n     ++bins[image[i]];\n   }\n }",
            "for (auto px : image)\n\t{\n\t\tbins[px]++;\n\t}\n}",
            "for(size_t i = 0; i < 256; i++) {\n         bins[i] = 0;\n     }\n\n     for(size_t i = 0; i < image.size(); i++) {\n         bins[image[i]]++;\n     }\n }",
            "bins.fill(0);\n   for(auto i : image) {\n     bins[i]++;\n   }\n }",
            "for (size_t i = 0; i < image.size(); i++)\n     bins[image[i]]++;\n }",
            "for (auto const& pixel : image) {\n         bins[pixel]++;\n     }\n }",
            "for (size_t i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "for (int i = 0; i < 256; i++) {\n         bins[i] = 0;\n     }\n     for (int i = 0; i < image.size(); i++) {\n         bins[image[i]]++;\n     }\n }",
            "for (int i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n    return;\n}",
            "for(int pixel : image){\n        bins[pixel]++;\n    }\n }",
            "for (auto val : image) {\n     bins[val]++;\n   }\n }",
            "for (int i = 0; i < image.size(); i++){\n         bins[image[i]]++;\n     }\n }",
            "bins.fill(0);\n   for (size_t i = 0; i < image.size(); ++i) {\n     ++bins[image[i]];\n   }\n }",
            "bins.fill(0);\n   for (auto i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "}",
            "for (int i = 0; i < 256; i++) {\n\t\tfor (int j = 0; j < 256; j++) {\n\t\t\tif (image[i] == j)\n\t\t\t\tbins[j]++;\n\t\t}\n\t}\n}",
            "bins.fill(0);\n     for (size_t i = 0; i < image.size(); ++i) {\n         ++bins[image[i]];\n     }\n }",
            "size_t length = image.size();\n    \n    for (int i = 0; i < length; i++){\n        bins[image[i]]++;\n    }\n}",
            "for (int i : image) {\n     bins[i]++;\n   }\n }",
            "std::fill(bins.begin(), bins.end(), 0);\n   for (int i : image) {\n     ++bins[i];\n   }\n}",
            "for (auto i = 0; i < 256; i++) {\n         bins[i] = 0;\n     }\n     for (auto i = 0; i < image.size(); i++) {\n         bins[image[i]]++;\n     }\n }",
            "int i, j;\n\n    for (i=0;i<image.size();i++){\n        bins[image[i]]++;\n    }\n\n}",
            "for (int i = 0; i < 256; i++) {\n      bins[i] = 0;\n   }\n\n   for (int i = 0; i < image.size(); i++) {\n      bins[image[i]]++;\n   }\n}",
            "for(int i : image){\n        bins[i]++;\n    }\n\n}",
            "for(int i = 0; i < image.size(); i++){\n        bins[image[i]]++;\n    }\n}",
            "for (size_t i = 0; i < image.size(); ++i) {\n    ++bins[image[i]];\n  }\n}",
            "for (auto pixel : image) {\n        ++bins[pixel];\n    }\n}",
            "for(auto p : image){\n      if(p < 0 || p > 255)\n         throw std::out_of_range(\"pixelCounts: pixel out of range\");\n      bins[p]++;\n   }\n }",
            "// Fill in the code below to complete this function.\n    for (int i = 0; i < 256; ++i)\n    {\n        bins[i] = 0;\n    }\n    for (int i = 0; i < image.size(); ++i)\n    {\n        ++bins[image[i]];\n    }\n}",
            "for (int i = 0; i < 256; i++) {\n\t\tbins[i] = 0;\n\t}\n\tfor (int i = 0; i < image.size(); i++) {\n\t\tbins[image[i]]++;\n\t}\n}",
            "bins.fill(0);\n   for (int i = 0; i < image.size(); i++)\n     bins[image[i]]++;\n }",
            "// TODO\n }",
            "for (size_t i = 0; i < image.size(); i++) {\n        bins.at(image.at(i))++;\n    }\n}",
            "for (int grayScale = 0; grayScale <= 255; grayScale++) {\n     bins.at(grayScale) = 0;\n   }\n   for (int i = 0; i < image.size(); i++) {\n     bins.at(image[i])++;\n   }\n }",
            "for(auto &i:image)\n        bins[i]++;\n}",
            "for(int i = 0; i < 256; i++) {\n       bins[i] = 0;\n   }\n   for(int i = 0; i < image.size(); i++) {\n       bins[image[i]]++;\n   }\n}",
            "for (size_t i = 0; i < image.size(); i++)\n   {\n       bins[image[i]]++;\n   }\n\n }",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (int pixel : image) {\n        bins[pixel]++;\n    }\n }",
            "for (size_t i = 0; i < 256; ++i)\n         bins[i] = 0;\n     for (size_t i = 0; i < image.size(); ++i)\n         bins[image[i]]++;\n }",
            "for (int i = 0; i < image.size(); ++i) {\n    bins[image[i]]++;\n  }\n\n  //print(bins);\n}",
            "for(auto pix : image) {\n     bins[pix]++;\n   }\n }",
            "for (auto i : image) {\n        bins[i]++;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\t for (int i = 0; i < image.size(); i++) {\n\t\t bins[image[i]] += 1;\n\t }\n\t //std::cout << \"bins: \";\n\t //for (int i = 0; i < bins.size(); i++) {\n\t //\tstd::cout << bins[i] << \" \";\n\t //}\n\t //std::cout << \"\\n\";\n }",
            "for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
            "// TODO: Your code here\n     for(auto i = image.begin(); i!= image.end(); i++)\n         bins[*i]++;\n }",
            "for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
            "for (auto pixel : image) {\n        bins[pixel]++;\n    }\n}",
            "for (auto & value : image) {\n         bins[value]++;\n     }\n }",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "for (auto i = 0; i < 256; i++) {\n    bins[i] = 0;\n  }\n\n  for (auto i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n }",
            "for (size_t i = 0; i < image.size(); ++i) {\n        ++bins[image[i]];\n    }\n}",
            "for (size_t pixel = 0; pixel < image.size(); pixel++) {\n        bins[image[pixel]]++;\n    }\n}",
            "// TODO: Your code here\n    for (int i = 0; i < image.size(); ++i) {\n        bins[image[i]] += 1;\n    }\n}",
            "bins = { 0 };\n\tfor (auto const& i : image) {\n\t\tbins[i]++;\n\t}\n\treturn;\n}",
            "for (size_t i = 0; i < image.size(); ++i)\n        ++bins[image[i]];\n}",
            "// Initialize the histogram\n    std::fill(bins.begin(), bins.end(), 0);\n\n    // For each pixel in the image, increment the corresponding bin\n    for (size_t pixel : image) {\n        ++bins[pixel];\n    }\n}",
            "for (auto i = 0; i < 256; i++) {\n    bins[i] = 0;\n  }\n\n  for (auto pixel : image) {\n    bins[pixel]++;\n  }\n}",
            "for (const auto& e : image) {\n        bins[e]++;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (auto const& pixel : image) {\n    ++bins[pixel];\n  }\n}",
            "// TODO: Your code here\n  for (int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n  // bins[255] = bins[255] + image.size();\n  // for (int i = 0; i < 255; i++) {\n  //   bins[i] = bins[i] + bins[i + 1];\n  // }\n }",
            "// TODO\n\tfor (int i = 0; i < image.size(); i++)\n\t{\n\t\tbins[image[i]] += 1;\n\t}\n}",
            "// TODO: Your code here\n}",
            "for (auto x : image)\n        bins[x] += 1;\n }",
            "for (int i = 0; i < 256; i++) {\n      bins[i] = 0;\n   }\n   for (size_t i = 0; i < image.size(); i++) {\n      bins[image[i]]++;\n   }\n}",
            "bins.fill(0);\n   for (auto pixel : image) {\n     ++bins[pixel];\n   }\n }",
            "// TODO:\n   for (size_t i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "size_t n = image.size();\n   for (size_t i = 0; i < n; i++) {\n     bins[image[i]]++;\n   }\n }",
            "int n_bins = image.size();\n    for (int i = 0; i < n_bins; i++) {\n        bins.at(image.at(i))++;\n    }\n}",
            "bins.fill(0);\n     for(size_t i=0;i<image.size();i++)\n     {\n         bins[image[i]]++;\n     }\n }",
            "// Your code here\n   for(int i=0; i<256; i++){\n   \tbins[i]=0;\n   }\n   for(int i=0; i<image.size(); i++){\n   \tbins[image[i]]++;\n   }\n\n}",
            "for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
            "for (const auto& i : image) {\n        bins[i]++;\n    }\n}",
            "for(int i = 0; i < 256; ++i){\n         bins[i] = 0;\n     }\n     for(int i = 0; i < image.size(); ++i){\n         bins[image[i]] += 1;\n     }\n }",
            "for(auto p : image) {\n     ++bins[p];\n   }\n }",
            "size_t count = image.size();\n   for (size_t i = 0; i < count; i++) {\n     bins[image[i]]++;\n   }\n }",
            "bins.fill(0);\n    for (int value : image) {\n        bins[value] += 1;\n    }\n}",
            "for (auto const& p: image) {\n         bins[p]++;\n     }\n}",
            "bins.fill(0);\n   for (auto intensity: image) {\n     ++bins.at(intensity);\n   }\n }",
            "bins.fill(0);\n     for (size_t i=0; i<image.size(); i++) {\n         bins[image[i]]++;\n     }\n }",
            "for (int i = 0; i < 256; i++)\n    {\n        bins[i] = 0;\n    }\n    for (int i = 0; i < image.size(); i++)\n    {\n        bins[image[i]]++;\n    }\n}",
            "for (int x : image) {\n     ++bins[x];\n   }\n }",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (int pixel : image)\n        bins[pixel] += 1;\n}",
            "// Hint: Initialize `bins` to all zeros here.\n    for (int i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]] += 1;\n    }\n\n}",
            "for (int i = 0; i < image.size(); i++) {\n         if (bins[image[i]] == 0) {\n             bins[image[i]] = 1;\n         }\n         else {\n             bins[image[i]] += 1;\n         }\n     }\n }",
            "for (auto const& element: image) {\n     bins[element]++;\n   }\n }",
            "for (auto const& x: image)\n        bins[x] += 1;\n}",
            "for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: Your code here\n   \n   //for(int i = 0; i < 256; i++)\n   //{\n   //  bins[i] = 0;\n   //}\n   //\n   //for(int i = 0; i < image.size(); i++)\n   //{\n   //  bins[image[i]] += 1;\n   //}\n   //\n   //\n   //return;\n}",
            "std::array<size_t, 256> histogram{};\n  for(int i=0; i<image.size(); ++i){\n    histogram[image[i]]++;\n  }\n\n  for(int i=0; i<histogram.size(); ++i){\n    bins[i] = histogram[i];\n  }\n}",
            "bins.fill(0);\n     for(auto c : image) {\n         bins[c]++;\n     }\n }",
            "bins.fill(0);\n   for (size_t i = 0; i < image.size(); ++i) {\n     bins[image[i]]++;\n   }\n }",
            "// TODO: YOUR CODE HERE\n}",
            "//TODO\n }",
            "// TODO: implement this function\n  std::fill(bins.begin(), bins.end(), 0);\n  for (int i = 0; i < image.size(); i++){\n    bins[image[i]]++;\n  }\n }",
            "// TODO\n   \n   for (int i = 0; i < 256; i++) {\n       bins[i] = 0;\n   }\n   \n   for (int i = 0; i < image.size(); i++) {\n       bins[image[i]]++;\n   }\n }",
            "for (auto pixel : image) {\n        bins[pixel] += 1;\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "hip",
        "prompt": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use AMD HIP to count in parallel. The kernel is launched with at least N threads.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n__global__ void pixelCounts(const int *image, size_t N, size_t[256] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "const size_t index = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  const size_t stride = hipBlockDim_x * hipGridDim_x;\n\n  for(size_t i = index; i < N; i += stride) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "unsigned int index = threadIdx.x + blockIdx.x * blockDim.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// TODO: Your code here\n\n}",
            "const size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        bins[image[tid]]++;\n    }\n}",
            "// TODO: Your code here\n  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if(idx >= N) return;\n\n  atomicAdd(&bins[image[idx]], 1);\n\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N) {\n        atomicAdd(&bins[image[idx]], 1);\n    }\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid >= N) {\n        return;\n    }\n    atomicAdd(&bins[image[tid]], 1);\n}",
            "// 1. allocate shared memory\n    __shared__ size_t counts[256];\n    if (threadIdx.x == 0)\n        for (int i=0; i < 256; i++)\n            counts[i] = 0;\n\n    // 2. each thread loads its own data into shared memory\n    __syncthreads();\n\n    // 3. each thread counts its data\n    __syncthreads();\n\n    // 4. each thread loads its data to global memory\n    __syncthreads();\n\n    // 5. each thread writes its data to global memory\n    __syncthreads();\n}",
            "int gid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (gid >= N) return;\n    atomicAdd(&bins[image[gid]], 1);\n}",
            "size_t tix = threadIdx.x;\n   size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   for (; i < N; i += blockDim.x * gridDim.x) {\n      ++bins[image[i]];\n   }\n}",
            "for (size_t i = threadIdx.x + blockIdx.x * blockDim.x; i < N; i += blockDim.x * gridDim.x) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "size_t start = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = blockDim.x * gridDim.x;\n    for(size_t i = start; i < N; i+=stride){\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "int gid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (gid < N) {\n        atomicAdd(&bins[image[gid]], 1);\n    }\n}",
            "// TODO\n  int idx = threadIdx.x;\n  size_t i = idx;\n  while (i < N) {\n    size_t val = image[i];\n    atomicAdd(&bins[val], 1);\n    i += blockDim.x * gridDim.x;\n  }\n}",
            "const int idx = blockIdx.x*blockDim.x + threadIdx.x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "//TODO: Implement this function to count the number of pixels with each intensity\n    //      in image, and store the results in `bins`\n    // HINT: each thread should process one intensity level (0-255)\n    //       for example, thread 0 should count pixels with intensity 0\n    //       Use `atomicAdd()` to update the counts in `bins`\n    //\n    // TODO: use shared memory to avoid bank conflicts\n    //       HINT: 256 threads, 8 ints per thread\n    //             if each thread accesses the same array position, you get\n    //             bank conflicts\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}",
            "// thread index\n  int tid = threadIdx.x;\n\n  // count pixels per intensity\n  if (tid < 256) {\n    int count = 0;\n    for (size_t i = 0; i < N; i++) {\n      if (image[i] == tid) count++;\n    }\n    bins[tid] = count;\n  }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// Get the global thread ID.\n    const auto id = blockIdx.x * blockDim.x + threadIdx.x;\n    if (id >= N) return;\n    __shared__ int temp[256];\n    // Create a private variable to store the pixel intensity.\n    int intensity = 0;\n    // Load the intensity of the current pixel.\n    intensity = image[id];\n    // Add the pixel intensity to the corresponding bin in shared memory.\n    atomicAdd(&temp[intensity], 1);\n    // Store the pixel intensity in the corresponding bin in global memory.\n    if (threadIdx.x == blockDim.x - 1) bins[intensity] = temp[intensity];\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (tid < N) {\n    bins[image[tid]] += 1;\n  }\n}",
            "size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n   if (index < N)\n       bins[image[index]]++;\n}",
            "int id = threadIdx.x + blockIdx.x * blockDim.x;\n   if (id < N) {\n      bins[image[id]]++;\n   }\n}",
            "size_t thread_id = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (thread_id >= N)\n\t\treturn;\n\t// TODO\n\tint i = image[thread_id];\n\tatomicAdd(&bins[i], 1);\n}",
            "// TODO - add kernel code\n}",
            "int tid = threadIdx.x;\n  int i = blockIdx.x;\n\n  while (i < N) {\n    atomicInc(&bins[image[i++]], __ATOMIC_RELAXED);\n  }\n}",
            "// TODO: implement\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N) {\n        // bins[image[idx]]++;\n        atomicAdd(&bins[image[idx]], 1);\n    }\n}",
            "// TODO: Your code here.\n}",
            "// TODO\n  // 1. declare a thread-local shared variable for keeping the number of pixels\n  // 2. use AMD HIP atomic operation to increment the shared variable with each thread\n  // 3. Use a `grid-stride loop` to iterate over all threads\n  // 4. Use an AMD HIP atomic operation to update the output vector `bins`\n}",
            "// TODO: fill in your code here\n    //bins[0] = 0;\n    //bins[image[0]]++;\n    int thread_count = N;\n    if (thread_count > 256)\n        thread_count = 256;\n    for (int i = threadIdx.x + blockIdx.x * blockDim.x; i < thread_count; i += blockDim.x * gridDim.x) {\n        bins[image[i]]++;\n    }\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "const int id = threadIdx.x + blockIdx.x * blockDim.x;\n  if (id < N) {\n    atomicAdd(&bins[image[id]], 1);\n  }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i >= N)\n    return;\n  int intensity = image[i];\n  atomicAdd(&bins[intensity], 1);\n}",
            "unsigned long bin_id = threadIdx.x + blockIdx.x * blockDim.x;\n  if (bin_id < 256) {\n    for (int i = threadIdx.x + blockIdx.x * blockDim.x; i < N; i += blockDim.x * gridDim.x) {\n      if (image[i] == bin_id) {\n        atomicAdd(&bins[bin_id], 1);\n      }\n    }\n  }\n}",
            "const int tid = threadIdx.x;\n  const int num_threads = blockDim.x;\n  const int block_id = blockIdx.x;\n  const int start = block_id * num_threads;\n  const int end = min(start + num_threads, N);\n  for (int i = start; i < end; ++i) {\n    bins[image[i]] += 1;\n  }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        bins[image[i]] += 1;\n    }\n}",
            "// TODO:\n    // allocate shared memory (__shared__)\n    // allocate threadprivate variables (__thread__)\n    // declare the shared memory and threadprivate variables here\n    \n    for (int j = threadIdx.x; j < N; j+= blockDim.x)\n    {\n        atomicAdd(&bins[image[j]], 1);\n    }\n}",
            "int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "// Get thread ID and total number of threads\n  int id = threadIdx.x + blockDim.x * blockIdx.x;\n  int n = threadIdx.x + blockDim.x * blockIdx.x;\n  int nb = blockDim.x * gridDim.x;\n  for (int i = id; i < N; i += nb) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// TODO: Launch at least N threads\n  // TODO: For each thread, increment the corresponding bin in bins\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    bins[image[i]] += 1;\n  }\n}",
            "/* TODO: Your code here */\n  \n  // Get the thread id and the number of total threads\n  int id = blockDim.x * blockIdx.x + threadIdx.x;\n  int n = blockDim.x * gridDim.x;\n  \n  // Compute the total number of elements\n  size_t Ntotal = N * sizeof(int);\n \n  // Get the elements in each thread\n  int offset = Ntotal / n;\n  int element = id * offset;\n  \n  // Initialize the histogram with zeros\n  for (int i = 0; i < 256; i++)\n    bins[i] = 0;\n  \n  // Update the histogram\n  for (int i = 0; i < N; i++)\n  {\n    if (image[i] < 256)\n    {\n      bins[image[i]]++;\n    }\n  }\n}",
            "size_t threadID = blockDim.x * blockIdx.x + threadIdx.x;\n    size_t numThreads = blockDim.x * gridDim.x;\n\n    for (size_t i = threadID; i < N; i += numThreads) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "size_t thread_id = threadIdx.x + blockDim.x * blockIdx.x;\n    size_t num_threads = blockDim.x * gridDim.x;\n    for (; thread_id < N; thread_id += num_threads) {\n        int intensity = image[thread_id];\n        atomicAdd(&bins[intensity], 1);\n    }\n}",
            "// TODO\n\n}",
            "for (size_t i=0; i<N; ++i) {\n        bins[image[i]]++;\n    }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "// your code here\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (tid >= N) return;\n\n    int intensity = image[tid];\n    atomicAdd(&bins[intensity], 1);\n}",
            "// TODO: YOUR CODE HERE\n  \n}",
            "// TODO\n}",
            "const size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n   if (i >= N) return;\n\n   atomicAdd(&bins[image[i]], 1);\n}",
            "size_t tx = threadIdx.x;\n\tsize_t ty = threadIdx.y;\n\tsize_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\tsize_t j = blockIdx.y * blockDim.y + threadIdx.y;\n\t// the outer for loop must have a condition which is divisible by the inner loop\n\t// the inner loop must have a condition which is divisible by the outer loop\n\tfor(size_t k = 0; k < N; k++) {\n\t\tif (i < 256 && j < 256) {\n\t\t\tbins[image[i * 256 + j]]++;\n\t\t}\n\t}\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N) {\n        atomicAdd(&bins[image[idx]], 1);\n    }\n}",
            "size_t i = threadIdx.x;\n  bins[i] = 0;\n\n  if (i >= N) {\n    return;\n  }\n\n  for (size_t j = 0; j < N; ++j) {\n    atomicAdd(&bins[image[j]], 1);\n  }\n}",
            "// TODO\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}",
            "int tid = threadIdx.x;\n    bins[image[tid]] += 1;\n}",
            "int i = threadIdx.x;\n    if (i >= N)\n        return;\n    int color = image[i];\n    atomicAdd(bins + color, 1);\n}",
            "for (size_t idx=threadIdx.x+blockIdx.x*blockDim.x; idx<N; idx+=blockDim.x*gridDim.x) {\n        atomicAdd(&bins[image[idx]], 1);\n    }\n}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n  if (index < N) {\n    atomicAdd(&bins[image[index]], 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "// TODO: Implement\n\n  // bins[0] = 0;\n  // for (int i = 0; i < N; i++) {\n  //   bins[image[i]] += 1;\n  // }\n}",
            "// your code here\n}",
            "int id = threadIdx.x + blockDim.x * blockIdx.x;\n    if (id < N) {\n        int index = image[id];\n        atomicAdd(&bins[index], 1);\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if(tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}",
            "// TODO: Compute the number of pixels with each intensity.\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int pixel = image[tid];\n\n    if (pixel >= 0 && pixel <= 255) {\n        atomicAdd(&bins[pixel], 1);\n    }\n}",
            "int pixel = image[blockIdx.x * blockDim.x + threadIdx.x];\n  atomicAdd(&bins[pixel], 1);\n}",
            "size_t x = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t stride = blockDim.x * gridDim.x;\n  for (int i = x; i < N; i += stride) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// TODO\n}",
            "size_t block_id = blockIdx.x * blockDim.x + threadIdx.x;\n   if (block_id >= N)\n      return;\n   int grayscale = image[block_id];\n   bins[grayscale]++;\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  int j = threadIdx.y + blockIdx.y * blockDim.y;\n  int x = i * blockDim.x + threadIdx.x;\n  int y = j * blockDim.y + threadIdx.y;\n  int grayScale = (x < N)? image[x] : -1;\n  // Check if thread id is valid to work on the data. \n  if(x < N && y < N)\n  {\n      atomicAdd(&bins[grayScale], 1);\n  }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i >= N) return;\n  atomicAdd(&bins[image[i]], 1);\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}",
            "int x = threadIdx.x;\n    int y = threadIdx.y;\n\n    // Calculate the grayscale value for the current pixel.\n    // If pixel is not inside image, it is assigned an intensity of 0.\n    int pixVal = (x < N && y < N)? image[x+y*N] : 0;\n    atomicAdd(&bins[pixVal], 1);\n}",
            "__shared__ unsigned int data[256];\n    const size_t tid = threadIdx.x;\n    if (tid == 0)\n        memset(data, 0, 256 * sizeof(unsigned int));\n    __syncthreads();\n\n    size_t i;\n    for (i = tid; i < N; i += blockDim.x)\n        atomicAdd(&data[image[i]], 1);\n    __syncthreads();\n\n    for (i = tid; i < 256; i += blockDim.x)\n        bins[i] = data[i];\n}",
            "// TODO\n}",
            "// your code here\n}",
            "__shared__ unsigned int s_histo[256];\n\n  int i = threadIdx.x;\n\n  // set to zero\n  s_histo[i] = 0;\n\n  // synchronize threads\n  __syncthreads();\n\n  // start computing\n  for (; i < N; i += blockDim.x) {\n    atomicAdd(&s_histo[image[i]], 1);\n  }\n\n  // synchronize threads\n  __syncthreads();\n\n  // copy result to global memory\n  for (i = threadIdx.x; i < 256; i += blockDim.x) {\n    bins[i] = s_histo[i];\n  }\n}",
            "//...\n}",
            "__shared__ int smem[32];\n    const int tid = threadIdx.x;\n    const int tidy = threadIdx.y;\n    const int block_size = blockDim.x;\n    const int blocks_per_row = blockDim.y;\n    const int num_blocks_per_row = blockDim.y;\n    const int num_blocks = gridDim.y * gridDim.x;\n    const int global_tid = blockIdx.x * num_blocks_per_row + blockIdx.y * num_blocks;\n    const int num_pixels_per_block = block_size * blocks_per_row;\n\n    // This kernel needs 32 threads to work correctly.\n    if (tid < 32) {\n        smem[tid] = 0;\n    }\n\n    // Load the 32 pixels into shared memory.\n    __syncthreads();\n    int x_idx = tid;\n    int y_idx = tid / 32;\n    for (int i = 0; i < 8; i++) {\n        if (y_idx < blocks_per_row) {\n            int pixel_idx = global_tid + y_idx * block_size + x_idx;\n            if (pixel_idx < N) {\n                int val = image[pixel_idx];\n                atomicAdd(&smem[val], 1);\n            }\n        }\n        x_idx += 32;\n    }\n\n    // Write the 32 values to the global memory.\n    __syncthreads();\n    x_idx = tid / 32;\n    y_idx = tid % 32;\n    for (int i = 0; i < 8; i++) {\n        if (y_idx < 32) {\n            int block_idx = blockIdx.x * num_blocks_per_row + blockIdx.y * num_blocks;\n            int pixel_idx = block_idx * block_size * blocks_per_row + block_size * y_idx + x_idx;\n            if (pixel_idx < N) {\n                atomicAdd(&bins[x_idx], smem[y_idx]);\n            }\n        }\n        x_idx += 32;\n    }\n}",
            "size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n\tif (i >= N) {\n\t\treturn;\n\t}\n\tatomicAdd(&bins[image[i]], 1);\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}",
            "// TODO - Add your code here\n\n    __shared__ int sdata[256];\n\n    int tid = threadIdx.x;\n    int i = blockIdx.x*blockDim.x + threadIdx.x;\n\n    if(i<N) {\n        atomicAdd(&sdata[image[i]], 1);\n    }\n    __syncthreads();\n\n    if(tid == 0) {\n        for(int i = 0; i<256; i++) {\n            bins[i] = sdata[i];\n        }\n    }\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N) {\n        bins[image[idx]]++;\n    }\n}",
            "int tid = blockIdx.x*blockDim.x + threadIdx.x;\n   if (tid < N) {\n       size_t count = atomicAdd(&bins[image[tid]], 1);\n   }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  int tid = threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "int pixel = image[blockIdx.x];\n  atomicAdd(&bins[pixel], 1);\n}",
            "// TODO: implement kernel\n    int x = threadIdx.x + blockIdx.x * blockDim.x;\n    int y = threadIdx.y + blockIdx.y * blockDim.y;\n    if (x >= N || y >= N) {\n        return;\n    }\n    atomicAdd(&bins[image[y * N + x]], 1);\n}",
            "__shared__ size_t smem[256];\n   size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n   if(i<N) {\n      atomicAdd(&smem[image[i]], 1);\n   }\n   __syncthreads();\n   for(int i = 0; i < 256; i++) {\n      atomicAdd(&bins[i], smem[i]);\n   }\n}",
            "// TODO: Your code here\n}",
            "// TODO: your code here\n  int tx = threadIdx.x;\n  int ty = threadIdx.y;\n  int tz = threadIdx.z;\n  int blockSize = blockDim.x * blockDim.y * blockDim.z;\n  int bx = blockIdx.x;\n  int by = blockIdx.y;\n  int bz = blockIdx.z;\n  int block = blockSize * blockIdx.x + threadIdx.x;\n  if (block >= N) return;\n  atomicAdd(&bins[image[block]], 1);\n}",
            "//TODO: Your code here\n\tsize_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (idx >= N) {\n\t\treturn;\n\t}\n\t__shared__ size_t shared_count[256];\n\t__syncthreads();\n\tsize_t bin_idx = image[idx] % 256;\n\tif (threadIdx.x < 256) {\n\t\tatomicAdd(&shared_count[bin_idx], 1);\n\t}\n\t__syncthreads();\n\tif (threadIdx.x == 0) {\n\t\tfor (int i = 0; i < 256; ++i) {\n\t\t\tatomicAdd(&bins[i], shared_count[i]);\n\t\t}\n\t}\n\t__syncthreads();\n}",
            "// Your code goes here\n  int threadId = threadIdx.x;\n  int totalThreads = blockDim.x;\n  int start = threadId * N / totalThreads;\n  int end = (threadId + 1) * N / totalThreads;\n  for(int i = start; i < end; i++) {\n    if(image[i] < 256) {\n      atomicAdd(&bins[image[i]], 1);\n    }\n  }\n  __syncthreads();\n  return;\n}",
            "const int index = blockDim.x * blockIdx.x + threadIdx.x;\n  if(index < N) {\n    bins[image[index]] += 1;\n  }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    int intensity = image[tid];\n    atomicAdd(&bins[intensity], 1);\n  }\n}",
            "// TODO: YOUR CODE HERE\n}",
            "// YOUR CODE HERE\n    __shared__ size_t count[256];\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    count[threadIdx.x] = 0;\n    if (tid < N)\n        atomicAdd(&count[image[tid]], 1);\n    __syncthreads();\n    if (threadIdx.x == 0) {\n        for (int i = 0; i < 256; ++i)\n            bins[i] = count[i];\n    }\n}",
            "__shared__ int sum[256]; // a shared memory block for storing the sums for each intensity\n\n    int tid = threadIdx.x; // each thread has its own thread ID\n\n    // clear shared memory\n    if(tid < 256)\n        sum[tid] = 0;\n    __syncthreads();\n\n    // loop over image pixels\n    int i = blockIdx.x * blockDim.x + tid; // each block computes N/blockDim.x pixels\n    while(i < N) {\n        int intensity = image[i];\n        atomicAdd(&sum[intensity], 1);\n        i += blockDim.x * gridDim.x;\n    }\n\n    // compute the partial sum\n    for(int d = blockDim.x/2; d > 0; d /= 2) {\n        if(tid < d)\n            sum[tid] += sum[tid + d];\n        __syncthreads();\n    }\n\n    // write block result to global memory\n    if(tid == 0)\n        bins[blockIdx.x] = sum[0];\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: your code here\n}",
            "// Your code here\n    //...\n}",
            "// TODO: implement kernel (count number of pixels in each bin)\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "// TODO: Implement the function\n  int tid = threadIdx.x;\n  int j = blockIdx.x;\n  int i = blockIdx.y;\n  bins[image[tid]]++;\n}",
            "size_t pixel_idx = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t stride = blockDim.x * gridDim.x;\n\n  for (size_t i = pixel_idx; i < N; i += stride) {\n    bins[image[i]]++;\n  }\n}",
            "const size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "int x = threadIdx.x + blockDim.x * blockIdx.x;\n  if (x < N) {\n    atomicAdd(&bins[image[x]], 1);\n  }\n}",
            "for (int i=threadIdx.x; i<N; i+=blockDim.x) {\n        bins[image[i]]++;\n    }\n}",
            "// Initialize shared memory\n    __shared__ int sdata[256];\n\n    // Find my local thread ID\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Clear shared memory\n    if (tid < 256) {\n        sdata[tid] = 0;\n    }\n\n    // Wait for shared memory to be cleared\n    __syncthreads();\n\n    // Loop over all values in image\n    while (i < N) {\n        // Increment corresponding bin by 1\n        atomicAdd(&sdata[image[i]], 1);\n\n        // Increment thread index\n        i += blockDim.x * gridDim.x;\n    }\n\n    // Wait for all threads in block to complete\n    __syncthreads();\n\n    // Write results to global memory\n    if (tid < 256) {\n        atomicAdd(&bins[tid], sdata[tid]);\n    }\n}",
            "__shared__ int sh[512];\n\n    // 1. Load data\n    // 2. Initialize partial sums for each bin\n    // 3. Perform parallel summation of pixel intensities\n    // 4. Write results to global memory\n}",
            "size_t start = threadIdx.x + blockIdx.x * blockDim.x;\n   size_t stride = blockDim.x * gridDim.x;\n   for (size_t i = start; i < N; i += stride) {\n      int value = image[i];\n      atomicAdd(&bins[value], 1);\n   }\n}",
            "__shared__ unsigned int shared_sums[256];\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        int idx = image[i];\n        atomicAdd(&shared_sums[idx], 1);\n    }\n    __syncthreads();\n    for (int s = 1; s < blockDim.x; s *= 2) {\n        int idx = threadIdx.x;\n        if (idx < s) {\n            int temp = shared_sums[idx];\n            shared_sums[idx] = shared_sums[idx] + shared_sums[idx + s];\n            shared_sums[idx + s] = temp;\n        }\n        __syncthreads();\n    }\n    if (threadIdx.x == 0) {\n        int idx = blockIdx.x;\n        atomicAdd(&bins[idx], shared_sums[0]);\n    }\n}",
            "size_t idx = threadIdx.x;\n\n    if (idx < 256) {\n        for (size_t i = 0; i < N; i++) {\n            if (idx == image[i]) {\n                atomicAdd(&bins[idx], 1);\n            }\n        }\n    }\n}",
            "// TODO: Your code here\n  int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  for (int i = threadId; i < N; i += stride) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// TODO: launch a kernel on at least N threads\n  // TODO: use AMD HIP to count in parallel\n  // TODO: store the results in `bins`\n  // TODO: make sure `bins` is declared with `extern`\n  // TODO: make sure you use `const int*` for `image`\n  // TODO: make sure the output type is `int` for `bins`\n  // TODO: make sure the output type is `size_t[256]` for `bins`\n\n  const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  const int num_threads = blockDim.x * gridDim.x;\n  for(int i = tid; i < N; i += num_threads){\n    bins[image[i]]++;\n  }\n}",
            "__shared__ int smem[256];\n\n    for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        int val = image[i];\n        atomicAdd(smem + val, 1);\n    }\n    __syncthreads();\n\n    // Copy to global memory.\n    for (int i = threadIdx.x; i < 256; i += blockDim.x) {\n        atomicAdd(bins + i, smem[i]);\n    }\n}",
            "// TODO: count the number of pixels in image with each grayscale intensity.\n  //       the vector `image` is a grayscale image with values 0-255.\n  //       Store the results in `bins`.\n  //       Use AMD HIP to count in parallel. The kernel is launched with at least N threads.\n  //       Example:\n  // \n  //       input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n  //       output: [0, 0, 2, 0, 1,...]\n}",
            "//TODO: replace this with AMD HIP.\n  \n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n\n}",
            "int gx = threadIdx.x + blockIdx.x * blockDim.x;\n  int gy = threadIdx.y + blockIdx.y * blockDim.y;\n\n  if (gx < N && gy < N) {\n    int intensity = image[gx + gy * N];\n    atomicAdd(&bins[intensity], 1);\n  }\n}",
            "// YOUR CODE HERE\n  size_t idx = threadIdx.x + blockDim.x * blockIdx.x;\n  if (idx >= N)\n    return;\n  atomicAdd(&bins[image[idx]], 1);\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    bins[image[i]]++;\n  }\n}",
            "// TODO\n}",
            "int i = threadIdx.x;\n   int index = (blockIdx.x * blockDim.x) + threadIdx.x;\n   if (index < N) {\n      bins[image[index]]++;\n   }\n}",
            "// TODO: Implement this function\n}",
            "const int i = threadIdx.x + blockDim.x * blockIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// TODO: Implement\n}",
            "//...\n}",
            "// TODO: Add code here\n}",
            "// TODO: Implement this function.\n    // Use AMD HIP to count in parallel. The kernel is launched with at least N threads.\n    // Store the results in `bins`.\n    // The vector `image` is a grayscale image with values 0-255.\n    // The vector `bins` is a vector of size 256.\n    // If an intensity i occurs `bins[i]` times in the image, then the value at position i in the vector\n    // `bins` is `bins[i]=count(image, i)`.\n    // Example:\n    // input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n    // output: [0, 0, 2, 0, 1,...]\n\n    // Count the number of pixels in image with each grayscale intensity.\n    // The vector `image` is a grayscale image with values 0-255.\n    // Store the results in `bins`.\n    // Use AMD HIP to count in parallel. The kernel is launched with at least N threads.\n    // The vector `bins` is a vector of size 256.\n    // If an intensity i occurs `bins[i]` times in the image, then the value at position i in the vector\n    // `bins` is `bins[i]=count(image, i)`.\n    // Example:\n    // input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n    // output: [0, 0, 2, 0, 1,...]\n    int index = threadIdx.x + blockIdx.x * blockDim.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n\n    //  int index = threadIdx.x + blockIdx.x * blockDim.x;\n    //  if (index < N) {\n    //      bins[image[index]] += 1;\n    //  }\n}",
            "// Your code here\n}",
            "// TODO: implement the kernel function\n\n}",
            "__shared__ int sdata[256];\n    sdata[threadIdx.x] = 0;\n    __syncthreads();\n    for (int i = threadIdx.x; i < N; i += blockDim.x) {\n        sdata[image[i]]++;\n    }\n    __syncthreads();\n    for (int i = threadIdx.x; i < 256; i += blockDim.x) {\n        atomicAdd(&bins[i], sdata[i]);\n    }\n}",
            "int gIdx = threadIdx.x + blockDim.x*blockIdx.x;\n  \n  if(gIdx < N){\n    bins[image[gIdx]]++;\n  }\n}",
            "int thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (thread_id < N) {\n        atomicAdd(&bins[image[thread_id]], 1);\n    }\n}",
            "// TODO\n}",
            "for (size_t i = threadIdx.x + blockIdx.x * blockDim.x; i < N; i += blockDim.x * gridDim.x) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "int gidx = threadIdx.x + blockIdx.x*blockDim.x;\n    if (gidx < N) {\n        // TODO: write code to count number of pixels with value 'image[gidx]'\n    }\n}",
            "int thread_id = threadIdx.x + blockIdx.x * blockDim.x;\n    if (thread_id < N) {\n        atomicAdd(&bins[image[thread_id]], 1);\n    }\n}",
            "// Get the thread index\n   int id = threadIdx.x + blockDim.x * blockIdx.x;\n  \n   // Do a partial reduction\n   if (id < N) {\n      atomicAdd(&bins[image[id]], 1);\n   }\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n  for (int i = 0; i < N; ++i) {\n    atomicAdd(&bins[image[idx]], 1);\n    idx += blockDim.x * gridDim.x;\n  }\n}",
            "// Initialize bins to zero\n  for (int i = 0; i < 256; i++)\n    bins[i] = 0;\n\n  // Fill in the counts\n  for (int i = 0; i < N; i++)\n    atomicAdd(&bins[image[i]], 1);\n}",
            "// TODO: Launch a block with 1 thread for each pixel in the image\n\n  // TODO: In each thread: \n  //       - Compute the pixel intensity (grayscale value)\n  //       - Increment the corresponding bin value (in `bins`) by 1\n  // Hint: Use the pointer offset operator to access `bins`\n \n}",
            "bins[image[threadIdx.x]]++;\n}",
            "const int idx = threadIdx.x + blockDim.x * blockIdx.x;\n    if (idx < N) {\n        const int pixel = image[idx];\n        atomicAdd(&bins[pixel], 1);\n    }\n}",
            "// HIP variables\n  int tid = threadIdx.x;\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int num_threads = blockDim.x * gridDim.x;\n  int offset = 0;\n  int bin = 0;\n  int pixel = 0;\n  __shared__ int shared_pixels[BLOCK_SIZE];\n\n  // Copy pixels of the image block to shared memory\n  if (i < N)\n    shared_pixels[tid] = image[i];\n\n  // Synchronize threads to make sure shared memory has been loaded\n  __syncthreads();\n\n  // Count pixels and increment corresponding bins\n  if (i < N) {\n    for (pixel = 0; pixel < num_threads; pixel++) {\n      bin = shared_pixels[pixel];\n      atomicAdd(&bins[bin], 1);\n    }\n  }\n}",
            "// TODO: Your code here\n    return;\n}",
            "int i = threadIdx.x;\n    if (i < N) {\n        atomicAdd(bins + image[i], 1);\n    }\n}",
            "int count = 0;\n    for (int i = 0; i < N; ++i) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "// Your code here\n}",
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n    if (tid >= N) return;\n\n    unsigned char value = image[tid];\n    atomicAdd(&bins[value], 1);\n}",
            "size_t gid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (gid < N) {\n        atomicAdd(&bins[image[gid]], 1);\n    }\n}",
            "// TODO 1. Allocate shared memory to store the number of pixels with each grayscale intensity\n  //...\n\n  // TODO 2. Set block size 1D\n  //...\n\n  // TODO 3. Set grid size\n  //...\n\n  // TODO 4. Launch the kernel\n  //...\n\n  // TODO 5. Wait for the kernel to finish\n  //...\n}",
            "// Compute the thread index in one-dimensional grid\n  const size_t thread_idx = blockIdx.x*blockDim.x + threadIdx.x;\n  \n  if(thread_idx >= N) {\n    return;\n  }\n\n  // Count the pixel with the given intensity.\n  atomicAdd(&bins[image[thread_idx]], 1);\n}",
            "size_t id = blockDim.x * blockIdx.x + threadIdx.x;\n    if (id < N) {\n        atomicAdd(&bins[image[id]], 1);\n    }\n}",
            "// your code here\n  size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "// TODO: Implement this function\n}",
            "int i = threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "bins[image[threadIdx.x]]++;\n}",
            "// TODO: Implement me!\n    // Use AMD HIP to count in parallel.\n}",
            "size_t id = threadIdx.x + blockDim.x * blockIdx.x;\n  if (id < N) {\n    atomicAdd(&bins[image[id]], 1);\n  }\n}",
            "size_t gid = threadIdx.x + blockIdx.x * blockDim.x;\n  if(gid >= N) return;\n  atomicAdd(bins + image[gid], 1);\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if(idx < N) {\n        bins[image[idx]]++;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n       atomicAdd(&bins[image[i]], 1);\n   }\n}",
            "/* TODO: Your code here */\n\n\n}",
            "int threadId = threadIdx.x + blockDim.x * blockIdx.x;\n    if (threadId < N) {\n        atomicAdd(&bins[image[threadId]], 1);\n    }\n}",
            "__shared__ int shared[256];\n    // Fill the shared memory array with zeros\n    if (threadIdx.x < 256) {\n        shared[threadIdx.x] = 0;\n    }\n\n    // Wait for all threads to finish initializing\n    __syncthreads();\n\n    // Each thread increments the corresponding pixel value in the shared memory array\n    if (threadIdx.x < N) {\n        atomicAdd(&shared[image[threadIdx.x]], 1);\n    }\n\n    // Wait for all threads to finish incrementing\n    __syncthreads();\n\n    // Copy results to global memory\n    if (threadIdx.x < 256) {\n        bins[threadIdx.x] = shared[threadIdx.x];\n    }\n}",
            "size_t idx = threadIdx.x + blockIdx.x*blockDim.x;\n    if (idx < N) {\n        int pixel = image[idx];\n        atomicAdd(&bins[pixel], 1);\n    }\n}",
            "// TODO: implement the kernel\n  // each thread should count the number of pixels with that value in the image\n  // 0.5ms\n  // __shared__ int shared[256];\n\n  // int x = threadIdx.x + blockDim.x * blockIdx.x;\n\n  // if (x < N) {\n  //   int val = image[x];\n  //   atomicAdd(&shared[val], 1);\n  // }\n\n  // // TODO: you can use a parallel reduction here, if you have time\n  // for (int i = threadIdx.x; i < 256; i += blockDim.x) {\n  //   bins[i] = shared[i];\n  // }\n\n  // TODO: you can use a parallel reduction here, if you have time\n  for (int i = threadIdx.x; i < 256; i += blockDim.x) {\n    bins[i] = 0;\n  }\n\n  for (int i = threadIdx.x; i < N; i += blockDim.x) {\n    int val = image[i];\n    atomicAdd(&bins[val], 1);\n  }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if(tid < N) {\n    bins[image[tid]]++;\n  }\n}",
            "// TODO\n}",
            "const size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  const int intensity = image[i];\n  if (i < N) {\n    atomicAdd(&bins[intensity], 1);\n  }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int gid = threadIdx.x + blockIdx.x * blockDim.x;\n    int NTHREADS = blockDim.x * gridDim.x;\n\n    __shared__ size_t h_bins[256];\n    __shared__ size_t max_value;\n    __shared__ size_t min_value;\n\n    // Shared Memory\n    if (gid == 0) {\n        for (int i = 0; i < 256; i++) {\n            h_bins[i] = 0;\n        }\n    }\n    __syncthreads();\n\n    // Main loop\n    for (size_t i = tid; i < N; i += NTHREADS) {\n        h_bins[image[i]]++;\n    }\n    __syncthreads();\n\n    // Write the result to bins\n    if (gid == 0) {\n        for (int i = 0; i < 256; i++) {\n            bins[i] = h_bins[i];\n        }\n    }\n}",
            "int tid = threadIdx.x;\n  int t = tid / 32;\n  int t2 = tid % 32;\n  int i = blockIdx.x * blockDim.x + tid;\n\n  __shared__ int s[512];\n  s[t*32 + t2] = 0;\n  __syncthreads();\n\n  for(i; i < N; i += blockDim.x * gridDim.x) {\n    int x = image[i];\n    if (x >= 0 && x < 256) s[t*32 + t2] += 1;\n  }\n  __syncthreads();\n\n  if (t < 32) {\n    for(int i = 1; i < 32; i++) {\n      s[t*32 + t2] += s[t*32 + i];\n    }\n  }\n  __syncthreads();\n\n  bins[t*32 + t2] = s[t*32 + t2];\n\n  return;\n}",
            "// Compute the thread index\n    size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n    // Only process N threads\n    if (tid >= N)\n        return;\n\n    // Obtain the pixel grayscale intensity\n    size_t pixel = image[tid];\n\n    // Increment the bin count\n    atomicAdd(bins + pixel, 1);\n}",
            "// TODO: HIP\n  int idx = threadIdx.x + blockDim.x * blockIdx.x;\n  if (idx >= N) return;\n\n  int color = image[idx];\n  atomicAdd(&bins[color], 1);\n}",
            "const int idx = blockDim.x * blockIdx.x + threadIdx.x;\n    if (idx >= N)\n        return;\n    atomicAdd(&bins[image[idx]], 1);\n}",
            "size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n  if (i < N) {\n    atomicAdd(bins + image[i], 1);\n  }\n}",
            "int gid = threadIdx.x + blockDim.x * blockIdx.x;\n    if (gid < N) {\n        atomicAdd(&bins[image[gid]], 1);\n    }\n}",
            "// shared memory buffer for partial sums.\n  __shared__ int buffer[512];\n\n  // thread id\n  size_t tid = threadIdx.x;\n\n  // loop over the image in 512 element chunks\n  for (size_t i = blockIdx.x; i < N; i += gridDim.x) {\n    // load image into shared memory\n    if (tid < 512) {\n      buffer[tid] = image[i + tid];\n    }\n\n    // parallel reduction\n    for (size_t stride = 256; stride > 0; stride >>= 1) {\n      if (tid < stride) {\n        buffer[tid] += buffer[tid + stride];\n      }\n      __syncthreads();\n    }\n\n    // store the reduced value in global memory\n    if (tid == 0) {\n      bins[buffer[0]]++;\n    }\n  }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "// TODO: Implement\n}",
            "__shared__ int s_image[256];\n  __shared__ int s_counts[256];\n  int x = blockIdx.x * blockDim.x + threadIdx.x;\n  if (x < N) {\n    s_image[threadIdx.x] = image[x];\n  }\n  else {\n    s_image[threadIdx.x] = 255;\n  }\n  __syncthreads();\n  for (int i = 0; i < 256; i++) {\n    s_counts[threadIdx.x] = 0;\n    for (int j = 0; j < 256; j++) {\n      if (s_image[threadIdx.x] == i) {\n        s_counts[threadIdx.x] += 1;\n      }\n    }\n    __syncthreads();\n    bins[i] += s_counts[threadIdx.x];\n  }\n}",
            "// TODO: YOUR CODE HERE\n\n}",
            "int i = threadIdx.x;\n   for (int pixel = i; pixel < N; pixel += blockDim.x) {\n      bins[image[pixel]]++;\n   }\n}",
            "// TODO\n    int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "//TODO\n    //parallel loop\n    //\n    //\n\n    int x = blockIdx.x * blockDim.x + threadIdx.x;\n    int y = blockIdx.y * blockDim.y + threadIdx.y;\n    if (x >= N) return;\n    bins[image[y * N + x]]++;\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n    size_t i = tid % N;\n\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "bins[image[0]] += 1;\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    int *bins;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "// Your code here\n}",
            "// Hint: you can use threadIdx and blockIdx to compute the\n    //       pixel index in the image\n    size_t index = threadIdx.x + blockIdx.x * blockDim.x;\n    if(index >= N){\n        return;\n    }\n    // write your code here\n    atomicAdd(&bins[image[index]], 1);\n}",
            "//TODO\n}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (index < N) {\n    atomicAdd(&bins[image[index]], 1);\n  }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i >= N) return;\n  int val = image[i];\n  atomicAdd(&bins[val], 1);\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use MPI to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// Fill this in.\n    // You'll need to use MPI_Allreduce()\n    return;\n}",
            "int nRanks;\n    int rank;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    \n    size_t nPixels = image.size();\n    size_t nPixelsPerRank = nPixels / nRanks;\n    size_t nPixelsRemainder = nPixels % nRanks;\n    \n    // Setup displacements\n    std::vector<size_t> displacements;\n    displacements.push_back(0);\n    for (int i = 1; i < nRanks; i++) {\n        if (i <= nPixelsRemainder) {\n            displacements.push_back(displacements.at(i-1) + nPixelsPerRank + 1);\n        } else {\n            displacements.push_back(displacements.at(i-1) + nPixelsPerRank);\n        }\n    }\n    \n    // Send image to appropriate ranks\n    std::vector<int> image_s(nPixelsPerRank);\n    std::vector<int> image_r(nPixelsPerRank);\n    for (int i = 0; i < nPixelsPerRank; i++) {\n        image_s.at(i) = image.at(i + rank*nPixelsPerRank);\n    }\n    MPI_Gather(&image_s.at(0), nPixelsPerRank, MPI_INT, &image_r.at(0), nPixelsPerRank, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // Update the bins for each rank\n    if (rank == 0) {\n        for (int i = 0; i < image_r.size(); i++) {\n            bins.at(image_r.at(i))++;\n        }\n    }\n    \n    // Update the bins for other ranks\n    if (rank!= 0) {\n        MPI_Send(&image_r.at(0), nPixelsPerRank, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n    \n    // Get the bins from rank 0\n    if (rank == 0) {\n        MPI_Status status;\n        for (int i = 0; i < nRanks; i++) {\n            if (i!= 0) {\n                MPI_Recv(&image_r.at(0), nPixelsPerRank, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n                for (int j = 0; j < nPixelsPerRank; j++) {\n                    bins.at(image_r.at(j))++;\n                }\n            }\n        }\n    }\n    \n}",
            "}",
            "// TODO\n     int rank, num_proc;\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     MPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n\n     // MPI_Bcast()\n     std::array<size_t, 256> counts;\n     MPI_Bcast(&image[0], image.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n     for (auto p : image) {\n         counts[p]++;\n     }\n     // MPI_Reduce()\n     MPI_Reduce(&counts[0], &bins[0], counts.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n     //MPI_Gather()\n     //MPI_Gather(&counts[0], counts.size(), MPI_INT, &bins[0], counts.size(), MPI_INT, 0, MPI_COMM_WORLD);\n }",
            "MPI_Status status;\n  int world_size;\n  int world_rank;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  // bins = {0, 0, 0,...}\n  // set bins to zeros\n  for (int i = 0; i < 256; i++) {\n    bins[i] = 0;\n  }\n\n  // count the number of pixels with each grayscale intensity\n  // for (int i = 0; i < image.size(); i++) {\n  //   bins[image[i]]++;\n  // }\n\n  // rank 0\n  if (world_rank == 0) {\n    int image_size = image.size();\n    int num_per_rank = image_size / world_size;\n    int left_over = image_size % world_size;\n\n    int left_over_count = 0;\n    int left_over_index = 0;\n\n    for (int i = 0; i < left_over; i++) {\n      bins[image[left_over_index]]++;\n      left_over_count++;\n      left_over_index++;\n    }\n\n    int num_per_rank_with_left_over = num_per_rank + left_over_count;\n\n    for (int rank = 1; rank < world_size; rank++) {\n      int left_over_index = 0;\n      for (int i = 0; i < num_per_rank_with_left_over; i++) {\n        if (i < num_per_rank) {\n          bins[image[left_over_index]]++;\n        } else {\n          bins[image[left_over_index + left_over]]++;\n        }\n        left_over_index++;\n      }\n    }\n  }\n\n  // broadcast bins to all ranks\n  MPI_Bcast(bins.data(), 256, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> counts;\n    std::fill(counts.begin(), counts.end(), 0);\n\n    for(int i = 0; i < image.size(); i++) {\n        counts[image[i]]++;\n    }\n\n    if(rank == 0) {\n        for(int i = 0; i < 256; i++) {\n            for(int j = 1; j < size; j++) {\n                counts[i] += bins[i];\n            }\n            bins[i] = counts[i];\n        }\n    }\n\n    return;\n}",
            "int nprocs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // determine how many elements each rank has\n    size_t numElementsEachRank = image.size() / nprocs;\n    size_t numElementsRemaining = image.size() % nprocs;\n\n    // make local copies of the image and bins\n    std::vector<int> localImage = image;\n    std::array<size_t, 256> localBins;\n\n    // split the image and bins in half\n    if (rank == 0) {\n        // send the remainder of the image to the last process\n        MPI_Send(&image[numElementsEachRank], numElementsRemaining, MPI_INT, nprocs-1, 0, MPI_COMM_WORLD);\n    }\n    else if (rank == nprocs-1) {\n        // receive the remainder of the image from the first process\n        MPI_Status status;\n        MPI_Recv(&localImage[numElementsEachRank], numElementsRemaining, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n    }\n    else {\n        // send the first half of the image to the first process\n        MPI_Send(&image[0], numElementsEachRank, MPI_INT, 0, 0, MPI_COMM_WORLD);\n        // receive the second half of the image from the last process\n        MPI_Status status;\n        MPI_Recv(&localImage[numElementsEachRank], numElementsRemaining, MPI_INT, nprocs-1, 0, MPI_COMM_WORLD, &status);\n    }\n\n    // perform the counting of pixels using local data\n    for (auto element : localImage) {\n        localBins[element]++;\n    }\n\n    // collect the data from each rank\n    if (rank == 0) {\n        for (int i=1; i<nprocs; i++) {\n            MPI_Status status;\n            MPI_Recv(&localBins, 256, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n        }\n    }\n    else {\n        MPI_Status status;\n        MPI_Send(&localBins, 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n    // store the result\n    for (int i=0; i<256; i++) {\n        bins[i] = localBins[i];\n    }\n\n    // ensure everything has finished\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "/* TODO: Your code here */\n\n    // int num_procs;\n    // MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    // int rank;\n    // MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // if (rank == 0) {\n    //     std::cout << \"num_procs: \" << num_procs << std::endl;\n    //     std::cout << \"rank: \" << rank << std::endl;\n    // }\n\n    // int start = rank*image.size()/num_procs;\n    // int end = (rank+1)*image.size()/num_procs;\n\n    // int local_size = end-start;\n\n    // std::array<size_t, 256> local_bins;\n    // std::fill(local_bins.begin(), local_bins.end(), 0);\n    // for (int i=start; i<end; i++) {\n    //     local_bins[image[i]]++;\n    // }\n\n    // std::array<size_t, 256> global_bins;\n    // MPI_Reduce(local_bins.data(), global_bins.data(), 256, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    // if (rank == 0) {\n    //     bins = global_bins;\n    // }\n\n    // int num_pixels = 0;\n    // for (auto i : bins) {\n    //     num_pixels += i;\n    // }\n    // std::cout << num_pixels << std::endl;\n\n    // MPI_Reduce(&num_pixels, &num_pixels, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    // if (rank == 0) {\n    //     std::cout << \"total number of pixels: \" << num_pixels << std::endl;\n    // }\n\n\n    // int num_procs;\n    // MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n    // int local_size = image.size()/num_procs;\n    // std::array<size_t, 256> local_bins;\n    // std::fill(local_bins.begin(), local_bins.end(), 0);\n    // int start = rank*local_size;\n    // int end = (rank+1)*local_size;\n    // int local_num_pixels = 0;\n    // for (int i=start; i<end; i++) {\n    //     local_bins[image[i]]++;\n    //     local_num_pixels++;\n    // }\n\n    // std::array<size_t, 256> global_bins;\n    // MPI_Reduce(local_bins.data(), global_bins.data(), 256, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    // if (rank == 0) {\n    //     bins = global_bins;\n    // }\n\n    // MPI_Reduce(&local_num_pixels, &local_num_pixels, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    // if (rank == 0) {\n    //     std::cout << \"total number of pixels: \" << local_num_pixels << std::endl;\n    // }\n\n    int num_procs;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int num_pixels = image.size();\n\n    int local_pixels = num_pixels/num_procs;\n    int start = rank*local_pixels;\n    int end =",
            "size_t i,j,k;\n    //size_t n = image.size();\n    int rank, num_procs;\n    MPI_Comm_size(MPI_COMM_WORLD,&num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n    int b;\n    size_t offset = rank * image.size()/num_procs;\n    std::fill(bins.begin(), bins.end(), 0);\n    for(i = offset; i < offset + image.size()/num_procs; i++) {\n        b = image[i];\n        bins[b] = bins[b] + 1;\n    }\n\n}",
            "// your code here\n    MPI_Status status;\n    int rank;\n    int comm_sz;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);\n\n    size_t num_pixels = image.size();\n\n    // for each pixel, we send it to the corresponding rank\n    int pixel_per_proc = num_pixels / comm_sz;\n\n    size_t start = rank * pixel_per_proc;\n    size_t end = start + pixel_per_proc;\n\n    std::vector<int> counts(256);\n    for (size_t i = start; i < end; i++) {\n        counts[image[i]] += 1;\n    }\n    MPI_Allgather(counts.data(), 256, MPI_INT, bins.data(), 256, MPI_INT, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins[0] = 0;\n        for (int i = 1; i < 256; i++) {\n            bins[i] += bins[i-1];\n        }\n    }\n\n}",
            "// TODO\n }",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    int size = image.size();\n    int n = size / world_size;\n\n    std::array<size_t, 256> local_bins;\n\n    for (int i = 0; i < 256; i++)\n        local_bins[i] = 0;\n\n    int start = rank * n;\n    int end = start + n;\n\n    for (int i = start; i < end; i++)\n        local_bins[image[i]] += 1;\n\n    for (int i = 0; i < 256; i++)\n        bins[i] += local_bins[i];\n\n\n\n}",
            "// TODO: Your code here\n    // Start MPI code here\n\n    int mpi_size; // get number of procs\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\n    int mpi_rank; // get rank number\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    // set up bins\n    std::fill(bins.begin(), bins.end(), 0);\n\n    // if only 1 proc, then don't bother using mpi\n    if (mpi_size == 1) {\n        for (int pixel : image) {\n            bins[pixel]++;\n        }\n    }\n\n    // make a vector of all pixels\n    int image_size = image.size();\n    std::vector<int> pixels;\n    for (int i = 0; i < image_size; i++) {\n        pixels.push_back(image[i]);\n    }\n\n    // split into parts\n    int image_chunks = image_size / mpi_size;\n    int image_remainder = image_size % mpi_size;\n\n    int rank_start = 0;\n    int rank_end = 0;\n\n    // calculate the bounds for each rank\n    if (mpi_rank == 0) {\n        rank_start = 0;\n        rank_end = image_chunks + image_remainder;\n    } else {\n        rank_start = image_chunks + image_remainder;\n        rank_end = image_chunks + image_remainder + rank_start;\n    }\n\n    // if on the last proc, the end will be image_size\n    if (mpi_rank == mpi_size - 1) {\n        rank_end = image_size;\n    }\n\n    // send data to appropriate ranks\n    std::vector<int> send_data;\n    if (rank_start < rank_end) {\n        // make a local copy of part of image\n        for (int i = rank_start; i < rank_end; i++) {\n            send_data.push_back(image[i]);\n        }\n        // send the data to the appropriate rank\n        MPI_Send(&send_data[0], rank_end - rank_start, MPI_INT, mpi_rank, 0, MPI_COMM_WORLD);\n    }\n\n    // receive data from appropriate ranks\n    std::vector<int> recv_data;\n    if (mpi_rank == 0) {\n        for (int i = 1; i < mpi_size; i++) {\n            recv_data.resize(image_chunks + image_remainder);\n            MPI_Recv(&recv_data[0], image_chunks + image_remainder, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            // merge data\n            for (int j = 0; j < image_chunks + image_remainder; j++) {\n                pixels.push_back(recv_data[j]);\n            }\n        }\n    } else {\n        MPI_Recv(&recv_data[0], image_chunks, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    // count the number of pixels in each bin\n    for (int pixel : pixels) {\n        bins[pixel]++;\n    }\n\n    // end MPI code here\n}",
            "size_t nx = image.size();\n     size_t ny = 1;\n     size_t nbins = 256;\n     MPI_Comm comm = MPI_COMM_WORLD;\n     int rank, nranks;\n     MPI_Comm_rank(comm, &rank);\n     MPI_Comm_size(comm, &nranks);\n\n     if (rank == 0) {\n         bins.fill(0);\n     }\n\n     std::vector<int> image_shared(nx * ny);\n     for (size_t j = 0; j < ny; ++j) {\n         for (size_t i = 0; i < nx; ++i) {\n             image_shared[i + j * nx] = image[i];\n         }\n     }\n\n     std::vector<int> image_rank(nx * ny / nranks);\n     MPI_Scatter(image_shared.data(), nx * ny / nranks, MPI_INT, image_rank.data(), nx * ny / nranks, MPI_INT, 0, comm);\n\n     std::vector<int> local_bins(nbins);\n     for (size_t j = 0; j < ny; ++j) {\n         for (size_t i = 0; i < nx; ++i) {\n             local_bins[image_rank[i + j * nx]]++;\n         }\n     }\n\n     std::vector<int> global_bins(nbins);\n     MPI_Allgather(local_bins.data(), nbins, MPI_INT, global_bins.data(), nbins, MPI_INT, comm);\n\n     for (size_t i = 0; i < nbins; ++i) {\n         bins[i] += global_bins[i];\n     }\n\n     if (rank == 0) {\n         std::cout << \"pixelCounts\\n\";\n         for (size_t i = 0; i < bins.size(); ++i) {\n             std::cout << i << \" \" << bins[i] << \"\\n\";\n         }\n     }\n }",
            "size_t n = image.size();\n  // YOUR CODE HERE\n}",
            "// 1. Use MPI to determine how many pixels there are in total in the image.\n    //    Store it in a scalar variable `n`.\n    size_t n;\n    MPI_Allreduce(&image.size(), &n, 1, MPI_UNSIGNED, MPI_SUM, MPI_COMM_WORLD);\n\n    // 2. Use MPI to determine how many pixels to process on each rank.\n    //    Store it in a scalar variable `n_per_rank`.\n    size_t n_per_rank;\n    MPI_Allreduce(&image.size(), &n_per_rank, 1, MPI_UNSIGNED, MPI_DIV, MPI_COMM_WORLD);\n\n    // 3. Set the pixel index bounds of each process.\n    //    Store them in the variables `first` and `last`.\n    size_t first, last;\n    if (n_per_rank == 0) {\n        first = 0;\n        last = 0;\n    }\n    else {\n        first = n_per_rank * MPI_Comm_rank(MPI_COMM_WORLD);\n        last = first + n_per_rank - 1;\n    }\n\n    // 4. Count the number of pixels in the image with each grayscale intensity.\n    //    Store the results in the vector `bins`.\n    for (auto i = first; i <= last; i++) {\n        bins[image[i]]++;\n    }\n\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n   MPI_Comm_rank(MPI_COMM_WORLD, &worldRank);\n   MPI_Status status;\n   MPI_Datatype MPI_INT_array_256;\n   int size;\n\n   std::vector<int> image_local(image.begin() + worldRank, image.begin() + worldRank + image.size() / worldSize);\n\n   MPI_Type_vector(image_local.size(), 1, image.size(), MPI_INT, &MPI_INT_array_256);\n   MPI_Type_commit(&MPI_INT_array_256);\n   MPI_Allgather(&image_local[0], image_local.size(), MPI_INT_array_256, &bins[0], image_local.size(), MPI_INT_array_256, MPI_COMM_WORLD);\n\n   MPI_Type_free(&MPI_INT_array_256);\n\n   return;\n}",
            "int image_size = image.size();\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    // count each pixel\n    for (int i = world_rank; i < image_size; i += world_size) {\n        int gray_value = image[i];\n        bins[gray_value] += 1;\n    }\n\n    // get the total counts and get the partial results\n    if (world_rank == 0) {\n        std::array<size_t, 256> partial_results;\n        MPI_Gather(&bins[0], 256, MPI_LONG_LONG_INT, &partial_results[0], 256, MPI_LONG_LONG_INT, 0, MPI_COMM_WORLD);\n        // std::copy(&partial_results[0], &partial_results[0]+256, &bins[0]);\n        for (int i = 0; i < 256; i++) {\n            bins[i] = 0;\n            for (int j = 0; j < world_size; j++) {\n                bins[i] += partial_results[i + j * 256];\n            }\n        }\n    }\n    else {\n        MPI_Gather(&bins[0], 256, MPI_LONG_LONG_INT, nullptr, 256, MPI_LONG_LONG_INT, 0, MPI_COMM_WORLD);\n    }\n}",
            "int nProcs, myRank;\n     MPI_Comm_size(MPI_COMM_WORLD, &nProcs);\n     MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n\n     int const perRank = image.size() / nProcs;\n     int const remainder = image.size() % nProcs;\n     int begin = 0;\n     if(myRank > 0)\n         begin = perRank * myRank;\n     if(myRank < remainder)\n         begin += myRank;\n     else\n         begin += remainder;\n     int end = begin + perRank;\n     if(myRank == remainder)\n         end += 1;\n\n     std::array<size_t, 256> temp;\n     for(size_t i = 0; i < temp.size(); i++)\n         temp[i] = 0;\n\n     for(size_t i = begin; i < end; i++)\n         temp[image[i]]++;\n\n     std::array<size_t, 256> sendBins;\n     for(size_t i = 0; i < temp.size(); i++)\n         sendBins[i] = temp[i];\n\n     std::array<size_t, 256> recvBins;\n     MPI_Allreduce(&sendBins[0], &recvBins[0], 256, MPI_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n     for(size_t i = 0; i < bins.size(); i++)\n         bins[i] = recvBins[i];\n}",
            "int myRank;\n     MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n     int numProcs;\n     MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n     int myN = image.size();\n     // int myN = floor(myRank*image.size()/numProcs);\n     // int myN = image.size()/numProcs;\n     // int myN = image.size()/numProcs + image.size()%numProcs;\n     // std::cout << myN << '\\n';\n     MPI_Bcast(&myN, 1, MPI_INT, 0, MPI_COMM_WORLD);\n     int step = image.size()/numProcs;\n     int remain = image.size()%numProcs;\n     std::vector<int> image_my;\n     image_my.resize(step);\n     for (int i=0; i<step; ++i) {\n         image_my[i] = image[i+myRank*step];\n     }\n     if (myRank < remain) {\n         image_my.resize(step+1);\n         image_my[step] = image[myRank*step+remain];\n     }\n     int M = image_my.size();\n     // std::cout << M << '\\n';\n     std::vector<int> cnt_my(256, 0);\n     for (int i=0; i<M; ++i) {\n         cnt_my[image_my[i]]++;\n     }\n     // std::cout << cnt_my << '\\n';\n     std::vector<int> cnt_sum(256, 0);\n     MPI_Reduce(&cnt_my[0], &cnt_sum[0], 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n     if (myRank == 0) {\n         for (int i=0; i<256; ++i) {\n             bins[i] = cnt_sum[i];\n         }\n     }\n }",
            "//TODO: Your code here\n\n  // get the rank\n  int rank = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // get the number of processes\n  int size = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // get the total number of pixels\n  size_t total_num_pixels = image.size();\n\n  // get the number of pixels per process\n  size_t num_pixels_per_process = total_num_pixels / size;\n\n  // get the remaining number of pixels\n  int remaining_pixels = total_num_pixels % size;\n\n  // get the offset\n  size_t offset = num_pixels_per_process;\n\n  // get the local image\n  std::vector<int> local_image(image);\n\n  // shift the local image to the correct offset\n  if (rank == 0) {\n    local_image.resize(offset);\n  } else {\n    local_image.resize(offset + num_pixels_per_process);\n    std::copy(image.begin() + (rank * offset), image.begin() + ((rank + 1) * offset), local_image.begin());\n  }\n\n  // count the number of pixels\n  for (int i = 0; i < num_pixels_per_process + remaining_pixels; i++) {\n    bins[local_image[i]]++;\n  }\n\n  // combine the results of all processes\n  std::array<size_t, 256> all_bins;\n  all_bins = bins;\n  std::fill(all_bins.begin(), all_bins.end(), 0);\n  MPI_Reduce(bins.data(), all_bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    bins = all_bins;\n  }\n}",
            "int rank, size;\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n     int chunk_size = image.size() / size;\n     int leftover = image.size() % size;\n     std::vector<int> local_image(chunk_size + leftover);\n     MPI_Scatter(&image[0], chunk_size + leftover, MPI_INT, &local_image[0], chunk_size + leftover, MPI_INT, 0, MPI_COMM_WORLD);\n     std::vector<int> local_counts(256, 0);\n     for (auto px : local_image) {\n         local_counts[px]++;\n     }\n     std::vector<size_t> local_bins(256, 0);\n     for (size_t i = 0; i < local_bins.size(); i++) {\n         local_bins[i] = local_counts[i];\n     }\n     MPI_Gather(&local_bins[0], local_bins.size(), MPI_INT, &bins[0], local_bins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n }",
            "int size, rank;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int totalPixels = image.size();\n    int blockSize = totalPixels / size;\n\n    if (rank == 0)\n    {\n        for (int i = 0; i < totalPixels; i++)\n        {\n            bins[image[i]]++;\n        }\n    }\n    else\n    {\n        int startIndex = rank * blockSize;\n        int endIndex = startIndex + blockSize;\n\n        for (int i = startIndex; i < endIndex; i++)\n        {\n            bins[image[i]]++;\n        }\n    }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int chunk = image.size() / size;\n    int start = rank * chunk;\n    int end = start + chunk;\n    if (rank == size - 1) {\n        end = image.size();\n    }\n\n    for (int i = start; i < end; i++) {\n        bins[image[i]] += 1;\n    }\n}",
            "int N = 0;\n\tint width = 0;\n\tint height = 0;\n\tint rank = 0;\n\tint size = 1;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tbins.fill(0);\n\tstd::vector<int> localBin(image.begin() + rank * width, image.begin() + (rank + 1) * width);\n\tfor (int i = 0; i < localBin.size(); i++) {\n\t\tbins[localBin[i]] += 1;\n\t}\n\n\tstd::array<size_t, 256> recvBins;\n\tMPI_Allreduce(bins.data(), recvBins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\tfor (int i = 0; i < 256; i++)\n\t{\n\t\tbins[i] = recvBins[i];\n\t}\n}",
            "// Get the rank number and the total number of ranks\n   int rank, nranks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n   // Divide the image into nranks pieces, store the location of the beginning of each piece in pieces\n   std::vector<size_t> pieces;\n   if(rank == 0){\n      pieces.push_back(0);\n   }\n   if(rank < nranks-1){\n      pieces.push_back(image.size()/nranks*rank);\n   }\n   if(rank == nranks-1){\n      pieces.push_back(image.size()/nranks*rank);\n      pieces.push_back(image.size());\n   }\n\n   // Count the pixels for each piece\n   for(int i=0; i<nranks; i++){\n      int counter = 0;\n      for(int j=pieces[i]; j<pieces[i+1]; j++){\n         counter+=image[j];\n      }\n      bins[rank*pieces[i+1]/nranks] += counter;\n   }\n\n   // Sum up the results on rank 0\n   if(rank == 0){\n      for(int i=1; i<nranks; i++){\n         MPI_Recv(&bins[i], 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n      int counter = 0;\n      for(int i=0; i<nranks; i++){\n         counter += bins[i];\n      }\n      bins[0] = counter;\n   }\n   if(rank!= 0){\n      MPI_Send(&bins[0], 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n   }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    MPI_Datatype mpi_int = MPI_INT;\n    int rank, nproc;\n    MPI_Comm_size(comm, &nproc);\n    MPI_Comm_rank(comm, &rank);\n\n    for (int i = 0; i < 256; i++)\n    {\n        bins[i] = 0;\n    }\n\n    if (rank == 0)\n    {\n        MPI_Reduce(MPI_IN_PLACE, bins.data(), 256, mpi_int, MPI_SUM, 0, comm);\n    }\n    else\n    {\n        MPI_Reduce(image.data(), bins.data(), 256, mpi_int, MPI_SUM, 0, comm);\n    }\n    //MPI_Reduce(image.data(), bins.data(), 256, mpi_int, MPI_SUM, 0, comm);\n\n    //MPI_Reduce(image.data(), bins.data(), 256, mpi_int, MPI_SUM, 0, comm);\n\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n   int rank;\n   MPI_Comm_rank(comm, &rank);\n\n   int world_size;\n   MPI_Comm_size(comm, &world_size);\n   MPI_Status status;\n\n   int world_size_in_bytes = world_size * sizeof(size_t);\n\n   if (rank == 0) {\n     for (int i = 0; i < 256; i++) {\n       bins[i] = 0;\n     }\n   }\n\n   MPI_Barrier(comm);\n   MPI_Scatter(image.data(), world_size_in_bytes, MPI_BYTE,\n               bins.data(), world_size_in_bytes, MPI_BYTE,\n               0, comm);\n\n   for (int i = 0; i < world_size; i++) {\n     MPI_Send(image.data() + (i * 256), 256, MPI_INT, i, i, comm);\n   }\n\n   for (int i = 1; i < world_size; i++) {\n     MPI_Status status;\n     MPI_Recv(bins.data() + (i * 256), 256, MPI_INT, i, i, comm, &status);\n   }\n   MPI_Barrier(comm);\n\n   if (rank == 0) {\n     for (int i = 0; i < 256; i++) {\n       if (bins[i] == 0) {\n         bins[i] = 0;\n       } else {\n         bins[i] = bins[i] / world_size;\n       }\n     }\n   }\n\n   MPI_Barrier(comm);\n   MPI_Gather(bins.data(), world_size_in_bytes, MPI_BYTE,\n              image.data(), world_size_in_bytes, MPI_BYTE,\n              0, comm);\n }",
            "int rank;\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    size_t num_pixels = image.size();\n    size_t num_pixels_per_proc = (num_pixels + size - 1) / size;\n    size_t start_idx = rank * num_pixels_per_proc;\n    size_t end_idx = std::min(start_idx + num_pixels_per_proc, num_pixels);\n\n    for (size_t i = start_idx; i < end_idx; i++) {\n        bins[image[i]] += 1;\n    }\n}",
            "int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    int const image_size = image.size();\n    int const num_values = bins.size();\n    int const num_per_rank = image_size / world_size;\n    int const num_remain = image_size % world_size;\n    int const first_index = world_rank * num_per_rank;\n    int const last_index = first_index + num_per_rank - 1;\n\n    if (world_rank < num_remain) {\n        int const extra_index = world_rank * (num_per_rank + 1);\n        bins[image[extra_index]]++;\n    }\n    if (world_rank == 0) {\n        bins[image[first_index]]++;\n    }\n    if (world_rank == world_size - 1) {\n        bins[image[last_index]]++;\n    }\n\n    if (world_rank!= 0) {\n        MPI_Send(&bins[0], num_values, MPI_UNSIGNED_LONG, world_rank - 1, 0, MPI_COMM_WORLD);\n        MPI_Recv(&bins[0], num_values, MPI_UNSIGNED_LONG, world_rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    if (world_rank!= world_size - 1) {\n        MPI_Send(&bins[0], num_values, MPI_UNSIGNED_LONG, world_rank + 1, 0, MPI_COMM_WORLD);\n        MPI_Recv(&bins[0], num_values, MPI_UNSIGNED_LONG, world_rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    if (world_rank!= 0) {\n        MPI_Recv(&bins[0], num_values, MPI_UNSIGNED_LONG, world_rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    if (world_rank!= world_size - 1) {\n        MPI_Recv(&bins[0], num_values, MPI_UNSIGNED_LONG, world_rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    //print all bins\n    // for (int i = 0; i < num_values; ++i) {\n    //     std::cout << \"rank: \" << world_rank << \" bin[\" << i << \"]=\" << bins[i] << std::endl;\n    // }\n\n    if (world_rank!= 0) {\n        MPI_Send(&bins[0], num_values, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n    if (world_rank!= world_size - 1) {\n        MPI_Send(&bins[0], num_values, MPI_UNSIGNED_LONG, world_size - 1, 0, MPI_COMM_WORLD);\n    }\n\n    if (world_rank == 0) {\n        MPI_Recv(&bins[0], num_values, MPI_UNSIGNED_LONG, world_size - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        MPI_Recv(&bins[0], num_values, MPI_UNSIGNED_LONG, world_size - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    if (world_rank!= world_size - 1) {\n        MPI_Recv(&bins[0], num_values, MPI_UNSIGNED_LONG, 0",
            "// Initialize the array of pixel counts on rank 0\n\tif(MPI::COMM_WORLD.Get_rank() == 0)\n\t\tfor(int i = 0; i < bins.size(); i++)\n\t\t\tbins[i] = 0;\n\t\n\t// Use MPI to count the pixels\n\tint rank = MPI::COMM_WORLD.Get_rank();\n\tint nproc = MPI::COMM_WORLD.Get_size();\n\tint nperproc = image.size() / nproc;\n\tint nremainder = image.size() % nproc;\n\t\n\t// Loop through each pixel in the image and assign it to the correct bin\n\tfor(int i = 0; i < nperproc + (nremainder > rank? 1 : 0); i++)\n\t\tbins[image[i + rank * nperproc]]++;\n\t\n\t// Gather the results from each rank\n\tstd::vector<int> results;\n\tMPI::COMM_WORLD.Gather(&bins[0], 256, MPI::INT, &results[0], 256, MPI::INT, 0);\n\t\n\t// On rank 0, assign the results to the final output\n\tif(rank == 0) {\n\t\tfor(int i = 0; i < results.size(); i++)\n\t\t\tbins[i] = results[i];\n\t}\n\t\n\t// TODO: Add MPI calls as appropriate\n}",
            "}",
            "// TODO: Implement\n}",
            "MPI_Datatype mpi_int = MPI_INT;\n     MPI_Datatype mpi_size_t = MPI_UNSIGNED_LONG_LONG;\n\n     int rank, size;\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n     int chunk_size = image.size() / size;\n     std::vector<int> local_image(image.begin() + chunk_size * rank, image.begin() + chunk_size * (rank + 1));\n\n     // Calculate the histogram for this sub-image\n     std::array<size_t, 256> local_bins;\n     for (int i = 0; i < local_image.size(); i++) {\n         local_bins[local_image[i]] += 1;\n     }\n\n     // Reduce bins for all images to rank 0\n     std::array<size_t, 256> reduced_bins;\n     MPI_Reduce(local_bins.data(), reduced_bins.data(), 256, mpi_size_t, MPI_SUM, 0, MPI_COMM_WORLD);\n\n     // Store the histogram in bins\n     bins = reduced_bins;\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\t\tsize_t total = image.size();\n\t\tint rank;\n\t\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t\tsize_t offset = total / MPI_COMM_WORLD_SIZE * rank;\n\t\tsize_t count = total / MPI_COMM_WORLD_SIZE;\n\t\tfor (int i = 0; i < count; i++) {\n\t\t\tbins[image[offset + i]]++;\n\t\t}\n\t\tif (rank == 0) {\n\t\t\tstd::partial_sum(bins.begin(), bins.end(), bins.begin());\n\t\t}\n\t\t//return std::accumulate(bins.begin(), bins.end(), 0);\n\t}",
            "int world_size;\n    int world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    // Compute the number of elements in each rank.\n    int chunk_size = image.size() / world_size;\n    int rem = image.size() % world_size;\n    int local_size = chunk_size;\n    if(world_rank < rem)\n        local_size++;\n\n    std::vector<int> local_image;\n    if(world_rank < rem) {\n        local_image = std::vector<int>(image.begin() + world_rank * chunk_size, image.begin() + (world_rank + 1) * chunk_size);\n    } else {\n        local_image = std::vector<int>(image.begin() + world_rank * chunk_size + rem, image.begin() + (world_rank + 1) * chunk_size + rem);\n    }\n\n    // Create a histogram.\n    std::array<size_t, 256> local_bins = std::array<size_t, 256>();\n    for(int i = 0; i < local_size; i++) {\n        local_bins[local_image[i]]++;\n    }\n\n    // Reduce histogram.\n    MPI_Allreduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "//TODO\n    //MPI_Init(NULL, NULL);\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if(rank==0)\n        for(int i=0;i<256;i++)\n            bins[i]=0;\n    int num_pixels=image.size();\n    int offset=(num_pixels/size);\n    int left_over=num_pixels%size;\n    int end_offset;\n    if(rank<left_over)\n        end_offset=offset+1;\n    else\n        end_offset=offset;\n    int start,end;\n    if(rank<left_over)\n    {\n        start=rank*offset;\n        end=start+end_offset;\n    }\n    else\n    {\n        start=(rank-left_over)*offset+left_over;\n        end=start+end_offset;\n    }\n    for(int i=start;i<end;i++)\n        bins[image[i]]++;\n    MPI_Reduce(MPI_IN_PLACE, &bins, 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    //MPI_Finalize();\n    return;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // int block_size = image.size()/size;\n  // int extra = image.size() - size*block_size;\n  // if (rank == size - 1) {\n  //   std::vector<int> sub(image.begin() + (rank * block_size), image.end());\n  // } else {\n  //   std::vector<int> sub(image.begin() + (rank * block_size), image.begin() + ((rank + 1) * block_size));\n  // }\n\n  // MPI_Scatter(MPI_IN_PLACE, 0, MPI_INT, &image[0], block_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n  std::vector<int> sub;\n  int block_size = image.size() / size;\n  int extra = image.size() - size * block_size;\n\n  if (rank == size - 1) {\n    sub = std::vector<int>(image.begin() + (rank * block_size), image.end());\n    // std::cout << rank << \" \" << sub.size() << std::endl;\n  } else {\n    sub = std::vector<int>(image.begin() + (rank * block_size), image.begin() + ((rank + 1) * block_size));\n    // std::cout << rank << \" \" << sub.size() << std::endl;\n  }\n\n  int *sub_array = sub.data();\n\n  int *recvcounts = new int[size]();\n  int *displs = new int[size]();\n\n  for (int i = 0; i < size; i++) {\n    recvcounts[i] = block_size;\n    displs[i] = block_size * i;\n  }\n\n  if (extra > 0) {\n    recvcounts[size - 1] += extra;\n  }\n\n  // std::cout << \"rank: \" << rank << \" image: \";\n  // for (int i = 0; i < sub.size(); i++) {\n  //   std::cout << sub[i] << \" \";\n  // }\n  // std::cout << std::endl;\n\n  MPI_Gatherv(sub_array, block_size, MPI_INT, bins.data(), recvcounts, displs, MPI_INT, 0, MPI_COMM_WORLD);\n\n  delete[] recvcounts;\n  delete[] displs;\n\n  if (rank == 0) {\n    // std::cout << \"bins: \";\n    // for (int i = 0; i < bins.size(); i++) {\n    //   std::cout << bins[i] << \" \";\n    // }\n    // std::cout << std::endl;\n  }\n}",
            "size_t global_rank = 0;\n   int rank = 0;\n   int num_processes = 0;\n   MPI_Comm_size(MPI_COMM_WORLD, &num_processes);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   // TODO: Replace the following code with a call to MPI\n   // bins.fill(0);\n   // for (auto const& i: image) {\n   //   bins[i]++;\n   // }\n   // MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   // return;\n   int local_size = image.size() / num_processes;\n   int local_rank = rank * local_size;\n   int local_end = local_rank + local_size;\n   std::vector<int> local_image(image.begin() + local_rank, image.begin() + local_end);\n   std::vector<int> local_bins(256, 0);\n   for (auto const& i: local_image) {\n     local_bins[i]++;\n   }\n   MPI_Allreduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n   return;\n}",
            "auto const& nPixels = image.size();\n    std::array<size_t, 256> localBins;\n    localBins.fill(0);\n    for (size_t i = 0; i < nPixels; i++) {\n        localBins[image[i]]++;\n    }\n\n    // TODO: reduce bins to rank 0\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "int world_rank;\n   int world_size;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n   std::vector<int> image_local;\n   std::vector<int> image_rank;\n   std::array<size_t, 256> bins_local;\n\n   // 1. split image between processes\n   int start = image.size() / world_size * world_rank;\n   int end = image.size() / world_size * (world_rank + 1);\n\n   image_local.assign(image.begin() + start, image.begin() + end);\n\n   // 2. sum pixels in image\n   for (int i = 0; i < image_local.size(); i++) {\n     bins_local[image_local[i]]++;\n   }\n\n   // 3. count pixel on each rank\n   MPI_Reduce(bins_local.data(), bins.data(), bins_local.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "// Compute the total number of pixels in the image.\n     // Total number of pixels is the number of elements in image.\n     size_t totalPixels = image.size();\n\n     // Every rank has a complete copy of image.\n     // Total number of pixels is the number of elements in image.\n     size_t myPixels = totalPixels;\n\n     // Compute the number of pixels in the image each rank will process.\n     size_t myPixelsPerRank = myPixels / size;\n\n     // Compute the number of pixels left after all other ranks have processed.\n     size_t extraPixels = myPixels % size;\n\n     // Process myPixelsPerRank pixels.\n     // Compute the first pixel of the range myPixelsPerRank this rank is\n     // responsible for.\n     size_t myFirstPixel = (myRank * myPixelsPerRank) + extraPixels;\n\n     // Compute the last pixel of the range myPixelsPerRank this rank is\n     // responsible for.\n     size_t myLastPixel = myFirstPixel + myPixelsPerRank;\n\n     // Iterate over the pixels in the range myPixelsPerRank this rank is\n     // responsible for.\n     for (int i = myFirstPixel; i < myLastPixel; i++) {\n         bins[image[i]]++;\n     }\n\n }",
            "//TODO\n   //use mpi_allreduce() and mpi_bcast()\n   //bins[i] will contain the number of pixels with intensity i\n\n   //this function will only be called by rank 0\n   for (auto i = 0; i < 256; i++) {\n      bins[i] = 0;\n   }\n\n   size_t n = image.size();\n   //broadcast n to all ranks\n   MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n   //bcast image to all ranks\n   int* image_array = new int[n];\n   for (auto i = 0; i < n; i++) {\n      image_array[i] = image[i];\n   }\n   MPI_Bcast(image_array, n, MPI_INT, 0, MPI_COMM_WORLD);\n   for (auto i = 0; i < n; i++) {\n      bins[image_array[i]]++;\n   }\n   delete[] image_array;\n }",
            "/*  TODO: Your code here */\n\n}",
            "//TODO\n  \n  int nbins = 256;\n  int nranks;\n  int rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int chunkSize = image.size() / nranks;\n  int remainder = image.size() % nranks;\n  int start = rank * chunkSize;\n  int end = start + chunkSize;\n  if (rank < remainder) {\n    end += 1;\n  }\n  std::array<size_t, 256> localBins;\n  std::fill(localBins.begin(), localBins.end(), 0);\n  \n  for (int i = start; i < end; i++) {\n    localBins[image[i]] += 1;\n  }\n  std::array<size_t, 256> globalBins;\n  MPI_Allreduce(&localBins[0], &globalBins[0], nbins, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n  \n  std::copy(globalBins.begin(), globalBins.end(), bins.begin());\n\n}",
            "}",
            "// Your code here\n    MPI_Status status;\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int num_of_processes;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_of_processes);\n    const int bin_size = 256 / num_of_processes;\n    bins.fill(0);\n\n    // fill the local array of bins\n    if(rank == 0) {\n        for(int i = 0; i < image.size(); ++i) {\n            bins[image[i]] += 1;\n        }\n    } else {\n        int offset = rank * bin_size;\n        for(int i = offset; i < offset + bin_size; ++i) {\n            bins[i] = image[i];\n        }\n    }\n\n    // gather the local bins to rank 0\n    std::array<size_t, 256> bins_total;\n    if(rank == 0) {\n        for(int i = 1; i < num_of_processes; ++i) {\n            MPI_Recv(&bins_total, 256, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n            for(int i = 0; i < 256; ++i) {\n                bins[i] += bins_total[i];\n            }\n        }\n    } else {\n        MPI_Send(&bins, 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// Your code here\n     bins = std::array<size_t, 256>();\n     std::vector<std::array<size_t, 256>> bins_vec(2);\n     std::array<size_t, 256> bins_part;\n\n     for (int i = 0; i < 256; i++) {\n         bins_vec[0][i] = 0;\n         bins_vec[1][i] = 0;\n     }\n     size_t n = image.size();\n     size_t n_per_proc = n / 2;\n     size_t n_remain = n % 2;\n     size_t offset = 0;\n\n     for (int i = 0; i < 2; i++) {\n         for (size_t j = offset; j < offset + n_per_proc; j++) {\n             bins_vec[i][image[j]]++;\n         }\n         offset += n_per_proc;\n     }\n\n     for (int i = 0; i < 256; i++) {\n         for (int j = 0; j < 2; j++) {\n             bins_part[i] += bins_vec[j][i];\n         }\n         bins[i] = bins_part[i];\n     }\n }",
            "int world_rank, world_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    const int size = image.size();\n    int remainder = size % world_size;\n    int blocksize = size / world_size;\n\n    if (world_rank == 0) {\n        bins.fill(0);\n    }\n\n    MPI_Bcast(bins.data(), 256, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n    if (remainder == 0) {\n        MPI_Scatter(image.data(), blocksize, MPI_UNSIGNED_LONG_LONG, bins.data(), blocksize, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n    } else {\n        if (world_rank < remainder) {\n            std::vector<int> imageBlock(blocksize + 1);\n            std::copy(image.begin() + (world_rank * (blocksize + 1)), image.begin() + ((world_rank + 1) * (blocksize + 1)), imageBlock.begin());\n            MPI_Scatter(imageBlock.data(), blocksize + 1, MPI_UNSIGNED_LONG_LONG, bins.data(), blocksize + 1, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n        } else {\n            std::vector<int> imageBlock(blocksize);\n            std::copy(image.begin() + (world_rank * blocksize), image.begin() + ((world_rank + 1) * blocksize), imageBlock.begin());\n            MPI_Scatter(imageBlock.data(), blocksize, MPI_UNSIGNED_LONG_LONG, bins.data(), blocksize, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    MPI_Gather(bins.data(), 256, MPI_UNSIGNED_LONG_LONG, bins.data(), 256, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "// Initialize bins to 0\n    for (int i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n\n    // Compute the number of elements in each array\n    int N = image.size();\n\n    // Count the number of pixels for each intensity\n    for (int i = 0; i < N; i++) {\n        int pixel = image[i];\n        bins[pixel] += 1;\n    }\n\n\n    return;\n}",
            "}",
            "for(auto& element : bins){\n         element = 0;\n     }\n     int const my_rank = 0;\n     int const comm_size = 0;\n     if (comm_size <= 1) {\n         for (size_t i = 0; i < image.size(); i++) {\n             bins[image[i]]++;\n         }\n     }\n     else {\n         size_t const size = image.size();\n         size_t const num_of_pixels = size / comm_size;\n         std::vector<size_t> sendcounts(comm_size);\n         std::vector<size_t> displs(comm_size);\n         for (int i = 0; i < comm_size; i++) {\n             if (i == my_rank) {\n                 sendcounts[i] = num_of_pixels;\n                 displs[i] = 0;\n             }\n             else if (i < my_rank) {\n                 sendcounts[i] = num_of_pixels;\n                 displs[i] = sendcounts[i - 1];\n             }\n             else {\n                 sendcounts[i] = num_of_pixels;\n                 displs[i] = sendcounts[i - 1] + sendcounts[i - 2];\n             }\n         }\n         std::vector<int> image_copy(image);\n         int* image_data = image_copy.data();\n         std::vector<int> receivebuf(0);\n         int* receivebuf_data = receivebuf.data();\n         int* bins_data = bins.data();\n         MPI_Alltoallv(image_data, sendcounts.data(), displs.data(), MPI_INT,\n                       receivebuf_data, sendcounts.data(), displs.data(), MPI_INT,\n                       MPI_COMM_WORLD);\n         for (int i = 0; i < num_of_pixels; i++) {\n             bins[receivebuf[i]]++;\n         }\n     }\n }",
            "int rank, num_ranks;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\t// Each rank has a copy of the image\n\tstd::vector<int> imageCopy(image);\n\t// Initialize bins with 0\n\tfor (int i = 0; i < 256; ++i) {\n\t\tbins[i] = 0;\n\t}\n\t// Loop over image, adding 1 to corresponding bin\n\tfor (auto pixel : imageCopy) {\n\t\tbins[pixel] += 1;\n\t}\n\t// Reduce the results from all ranks to rank 0\n\tMPI_Reduce(&bins, &bins, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\t// Only rank 0 has the result\n\tif (rank == 0) {\n\t\t// Normalize result\n\t\tfor (auto &bin : bins) {\n\t\t\tbin = (bin / num_ranks);\n\t\t}\n\t}\n}",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int my_count = 0;\n   for (int i = 0; i < image.size(); i++) {\n      if (image[i] == rank) {\n         my_count++;\n      }\n   }\n\n   bins[rank] = my_count;\n\n   // MPI_Reduce for rank 0\n   if (rank == 0) {\n      int total = 0;\n      for (int i = 0; i < size; i++) {\n         total += bins[i];\n      }\n\n      printf(\"Rank 0: %d\\n\", total);\n   }\n}",
            "// TODO: Your code here\n\n }",
            "// your code goes here\n}",
            "int size, rank;\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n     if (size == 1) {\n         for (int i = 0; i < image.size(); i++) {\n             bins[image[i]] += 1;\n         }\n     } else {\n         int per_rank = image.size() / size;\n         int rem = image.size() % size;\n         std::vector<int> local_image;\n         if (rank == 0) {\n             local_image = std::vector<int>(image.begin(), image.begin() + per_rank + rem);\n         } else {\n             local_image = std::vector<int>(per_rank);\n         }\n         MPI_Bcast(&local_image[0], local_image.size(), MPI_INT, 0, MPI_COMM_WORLD);\n         for (int i = 0; i < local_image.size(); i++) {\n             bins[local_image[i]] += 1;\n         }\n     }\n\n\n\n }",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<int> message;\n    message.resize(image.size() / size);\n\n    MPI_Scatter(image.data(), image.size() / size, MPI_INT, message.data(), message.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n    std::array<size_t, 256> localBins{};\n\n    for (size_t i = 0; i < message.size(); ++i) {\n        ++localBins[message[i]];\n    }\n\n    std::vector<size_t> receiveCounts(size, 0);\n    std::vector<size_t> displacements(size, 0);\n    for (size_t i = 1; i < size; ++i) {\n        receiveCounts[i] = receiveCounts[i - 1] + localBins[i - 1];\n        displacements[i] = receiveCounts[i - 1] + displacements[i - 1];\n    }\n\n    std::vector<size_t> receive(receiveCounts[size - 1] + displacements[size - 1], 0);\n\n    MPI_Gatherv(localBins.data(), receiveCounts[rank], MPI_LONG_LONG_INT, receive.data(), receiveCounts.data(), displacements.data(), MPI_LONG_LONG_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < receive.size(); ++i) {\n            bins[i] = receive[i];\n        }\n    }\n}",
            "const int rank = MPI_Comm_rank(MPI_COMM_WORLD);\n    const int numProc = MPI_Comm_size(MPI_COMM_WORLD);\n\n    std::vector<int> image_vec;\n\n    MPI_Scatter(image.data(), image.size() / numProc, MPI_INT, image_vec.data(), image.size() / numProc, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < image_vec.size(); i++) {\n\n        bins[image_vec[i]]++;\n    }\n\n    std::vector<size_t> bins_vec(bins.begin(), bins.end());\n\n    MPI_Reduce(bins_vec.data(), bins_vec.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n\n        for (int i = 0; i < bins.size(); i++) {\n\n            bins[i] = bins_vec[i];\n        }\n    }\n\n    MPI_Finalize();\n}",
            "// TODO\n}",
            "// TODO: Your code here\n   // The following is a dummy solution.\n   int i;\n   for (i = 0; i < 256; i++) {\n       bins[i] = 0;\n   }\n   for (i = 0; i < image.size(); i++) {\n       bins[image[i]] = bins[image[i]] + 1;\n   }\n}",
            "// TODO: Your code here\n    // Hint: Use an MPI_Reduce call on a vector of 256 elements\n    // Hint: The MPI_Datatype MPI_UNSIGNED_LONG should work\n}",
            "int num_proc, proc_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n   MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);\n\n   size_t image_size = image.size();\n   size_t image_chunk = image_size / num_proc;\n   size_t image_remainder = image_size % num_proc;\n   size_t image_begin = image_chunk * proc_rank + std::min(image_remainder, proc_rank);\n   size_t image_end = image_chunk * (proc_rank + 1) + std::min(image_remainder, proc_rank + 1);\n   std::vector<int> subimage(image.begin() + image_begin, image.begin() + image_end);\n   for (int i = 0; i < 256; i++) {\n     bins[i] = std::count(subimage.begin(), subimage.end(), i);\n   }\n\n }",
            "// compute how many pixels of each intensity are in image\n    // use MPI to compute it in parallel\n    // result is stored in `bins`\n\n    // get rank and number of processes\n    int rank, nproc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    // each process gets a slice of the image\n    int my_rank = rank;\n    int n_ranks = nproc;\n    int size = image.size();\n    int slice_size = size / n_ranks;\n    int my_slice_start = my_rank * slice_size;\n    int my_slice_end = my_slice_start + slice_size;\n    if (my_rank == n_ranks - 1) {\n        my_slice_end = image.size();\n    }\n\n    // loop over my slice\n    for (int i = my_slice_start; i < my_slice_end; i++) {\n        bins[image[i]]++;\n    }\n\n    // MPI allreduce to combine the bins from all processes\n    // use MPI_SUM\n    // this will result in bins on rank 0 that are the sum of all the bins\n    // across all the processes\n    MPI_Allreduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n}",
            "int nRanks;\n   MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n   int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   for (size_t i = 0; i < image.size(); i++) {\n    if(image[i] == 0)\n      bins[0]++;\n    else\n      bins[image[i]]++;\n   }\n }",
            "// YOUR CODE HERE\n\tint number_of_processes;\n\tMPI_Comm_size(MPI_COMM_WORLD, &number_of_processes);\n\tint rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint size = image.size();\n\tint chunk = size / number_of_processes;\n\tint chunk_rem = size - chunk * number_of_processes;\n\n\tint start = rank * chunk;\n\tint end = start + chunk;\n\tif (rank == number_of_processes - 1)\n\t\tend += chunk_rem;\n\n\tstd::array<size_t, 256> bins_local;\n\tfor (int i = start; i < end; i++) {\n\t\tbins_local[image[i]]++;\n\t}\n\n\tstd::array<size_t, 256> bins_all;\n\tMPI_Allreduce(bins_local.data(), bins_all.data(), bins_local.size(), MPI_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < 256; i++) {\n\t\t\tbins[i] = bins_all[i];\n\t\t}\n\t}\n\n}",
            "size_t num_pixels = image.size();\n    //TODO: call MPI_Allreduce to compute the sum of `bins` on each rank\n    //TODO: store the result in bins on rank 0\n    MPI_Allreduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n }",
            "int size, rank;\n\t MPI_Comm_size(MPI_COMM_WORLD, &size);\n\t MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t int num_pixels = (int)image.size();\n\t int num_pixels_per_process = num_pixels / size;\n\t int extra = num_pixels % size;\n\t int start = rank * num_pixels_per_process;\n\t int end = (rank == size - 1)? (start + num_pixels_per_process + extra) : (start + num_pixels_per_process);\n\n\t //initialize bins\n\t bins.fill(0);\n\n\t //count pixels\n\t for (int i = start; i < end; i++)\n\t {\n\t\t bins[image[i]]++;\n\t }\n\n\t //communicate the results\n\t MPI_Gather(bins.data(), 256, MPI_UNSIGNED_LONG, bins.data(), 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\t if (rank == 0) {\n\t\t int sum = 0;\n\t\t for (int i = 0; i < 256; i++)\n\t\t\t sum += bins[i];\n\t\t std::cout << \"Total pixels \" << sum << std::endl;\n\t }\n }",
            "size_t numProcesses;\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int chunkSize = image.size()/numProcesses;\n    int startIndex = rank * chunkSize;\n    int endIndex = startIndex + chunkSize;\n\n    for (int i = startIndex; i < endIndex; i++) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: Implement me\n\n   MPI_Datatype MPI_mytype;\n   MPI_Type_contiguous(image.size(), MPI_INT, &MPI_mytype);\n   MPI_Type_commit(&MPI_mytype);\n\n   int world_size;\n   int world_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n   int offset = 0;\n   int bin_size = image.size() / world_size;\n   if (world_rank == 0) {\n      offset = image.size() % world_size;\n      bin_size += offset;\n   }\n   int bin_offset = 0;\n   if (world_rank == 0) {\n      bin_offset = 0;\n   } else {\n      bin_offset = image.size() / world_size;\n   }\n   int receive_count = bin_size;\n   if (world_rank == 0) {\n      receive_count -= offset;\n   }\n   int send_count = 0;\n   if (world_rank == 0) {\n      send_count = image.size() / world_size;\n   } else {\n      send_count = image.size() / world_size + 1;\n   }\n   MPI_Status status;\n   int* send_buffer;\n   int* receive_buffer;\n   if (world_rank == 0) {\n      receive_buffer = new int[receive_count];\n      send_buffer = image.data();\n   } else {\n      receive_buffer = new int[send_count];\n      send_buffer = image.data() + offset;\n   }\n   MPI_Gather(send_buffer, send_count, MPI_INT, receive_buffer, receive_count, MPI_INT, 0, MPI_COMM_WORLD);\n   if (world_rank == 0) {\n      for (int i = 0; i < receive_count; i++) {\n         int index = receive_buffer[i];\n         bins[index]++;\n      }\n      delete[] receive_buffer;\n   }\n   MPI_Type_free(&MPI_mytype);\n\n}",
            "size_t n = image.size();\n     bins.fill(0);\n     for (size_t i = 0; i < n; i++)\n         ++bins[image[i]];\n }",
            "int size = bins.size();\n   int rank;\n   int numProcs;\n   MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   for (int i = 0; i < size; i++) {\n     bins[i] = 0;\n   }\n   size_t perRank = image.size() / numProcs;\n   size_t offset = rank * perRank;\n   size_t end = offset + perRank;\n   std::vector<int> imagePartial;\n   for (size_t i = offset; i < end; i++) {\n     imagePartial.push_back(image[i]);\n   }\n   for (size_t i = 0; i < imagePartial.size(); i++) {\n     bins[imagePartial[i]] += 1;\n   }\n   int sendToRank;\n   int sendCount;\n   if (rank == numProcs - 1) {\n     sendToRank = 0;\n     sendCount = 0;\n   }\n   else {\n     sendToRank = rank + 1;\n     sendCount = bins.size();\n   }\n   int recvFromRank;\n   int recvCount;\n   if (rank == 0) {\n     recvFromRank = 1;\n     recvCount = bins.size();\n   }\n   else {\n     recvFromRank = rank - 1;\n     recvCount = bins.size();\n   }\n   MPI_Status status;\n   MPI_Send(bins.data(), sendCount, MPI_INT, sendToRank, 0, MPI_COMM_WORLD);\n   MPI_Recv(bins.data(), recvCount, MPI_INT, recvFromRank, 0, MPI_COMM_WORLD, &status);\n }",
            "int rank, nprocs;\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n     // get the local image of each processor\n     std::vector<int> local_image(image.begin() + rank*100, image.begin() + (rank+1)*100);\n     // for each element in local image, increment the corresponding element in bins\n     for (int i = 0; i < local_image.size(); ++i) {\n         ++bins[local_image[i]];\n     }\n\n     // use MPI to broadcast bins to every processor\n     std::vector<size_t> new_bins(256, 0);\n     MPI_Allreduce(bins.data(), new_bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n     bins = new_bins;\n\n }",
            "MPI_Init(NULL, NULL);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int N = image.size();\n    int N_per_rank = N / size;\n    int N_left = N - N_per_rank * size;\n\n    std::vector<int> partial_bins(256);\n    int* partial_bins_ptr = partial_bins.data();\n\n    if (rank == 0)\n        std::fill(bins.begin(), bins.end(), 0);\n\n    if (rank < N_left) {\n        int start = rank * N_per_rank;\n        int end = start + N_per_rank;\n\n        for (int i = start; i < end; i++)\n            partial_bins[image[i]]++;\n    }\n    else {\n        int start = (rank - N_left) * N_per_rank;\n        int end = start + N_per_rank + N_left;\n\n        for (int i = start; i < end; i++)\n            partial_bins[image[i]]++;\n    }\n\n    MPI_Reduce(partial_bins_ptr, bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    MPI_Finalize();\n}",
            "// TODO\n }",
            "// TODO: Your code here\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n    int n_proc;\n    MPI_Comm_size(MPI_COMM_WORLD,&n_proc);\n    // std::array<size_t, 256> bins;\n    std::vector<int> vec;\n    vec.resize(256);\n    for(int i=0;i<256;++i)\n        vec[i] = 0;\n    \n    size_t count = image.size()/n_proc;\n    int i_start = rank * count;\n    int i_end = (rank + 1) * count;\n    if(i_start + count > image.size())\n        i_end = image.size();\n\n    for(int i=i_start;i<i_end;++i)\n        vec[image[i]] += 1;\n\n    // MPI_Reduce\n    for(int i=0;i<256;++i)\n    {\n        MPI_Reduce(&vec[i],&bins[i],1,MPI_UNSIGNED_LONG,MPI_SUM,0,MPI_COMM_WORLD);\n    }\n}",
            "// TODO: Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int n;\n    int total = image.size();\n    int offset = 0;\n    if(rank == 0){\n        n = total/size;\n    }\n    MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Scatter(image.data(), n, MPI_INT, bins.data(), n, MPI_INT, 0, MPI_COMM_WORLD);\n    std::array<size_t, 256> bins_all = bins;\n    MPI_Reduce(bins.data(), bins_all.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if(rank == 0){\n        for(int i = 0; i < 256; i++){\n            bins[i] = bins_all[i];\n        }\n    }\n    \n\n }",
            "int nprocs, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // TODO: use MPI to count number of pixels in image\n   // with each grayscale intensity.\n   // Store results in bins.\n   // This is only done on rank 0.\n\n   // The following code is just to make this a compileable\n   // stand-alone file. You don't need to edit it.\n   if (rank == 0) {\n     for (int i = 0; i < 256; ++i) {\n       bins[i] = 0;\n     }\n     for (int i = 0; i < image.size(); ++i) {\n       bins[image[i]] += 1;\n     }\n   }\n }",
            "int rank, numProcesses;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n\n    int size = image.size();\n    int image_per_process = size / numProcesses;\n    int image_rem = size % numProcesses;\n\n    std::vector<int> image_local(image_per_process + image_rem);\n    for (int i = 0; i < image_local.size(); i++) {\n        image_local[i] = image[i * numProcesses + rank];\n    }\n\n    std::vector<int> image_local_count(image_local.size());\n    MPI_Allreduce(MPI_IN_PLACE, image_local.data(), image_local.size(), MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    for (int i = 0; i < image_local_count.size(); i++) {\n        bins[image_local[i]] += image_local_count[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < bins.size(); i++) {\n            std::cout << i << \":\" << bins[i] << \"\\t\";\n        }\n        std::cout << std::endl;\n    }\n}",
            "int n = image.size();\n    int rank = 0;\n    int size = 1;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    std::vector<size_t> tmp(256, 0);\n    std::vector<int> local_image(image.begin() + n/size*rank, image.begin() + n/size*(rank+1));\n    for(int i=0; i<local_image.size(); i++) {\n        tmp[local_image[i]] += 1;\n    }\n    MPI_Reduce(&tmp[0], &bins[0], 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "MPI_Datatype datatype;\n  MPI_Type_contiguous(sizeof(int), MPI_CHAR, &datatype);\n  MPI_Type_commit(&datatype);\n  // TODO: your code goes here\n  MPI_Comm_size(MPI_COMM_WORLD, &rank);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int image_size = image.size();\n  std::vector<int> bins_local(256);\n\n  MPI_Allreduce(image.data(), bins_local.data(), image_size, datatype, MPI_SUM, MPI_COMM_WORLD);\n\n  for (int i = 0; i < 256; ++i) {\n    bins[i] = bins_local[i];\n  }\n\n  MPI_Type_free(&datatype);\n\n}",
            "int rank;\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int width = image.size();\n    int height = width / 2;\n    int mpi_size = size;\n    int mpi_rank = rank;\n\n    int remainder = width % mpi_size;\n    int local_height = height / mpi_size;\n    int mpi_remainder = 0;\n    int local_height_remainder = 0;\n\n    if (mpi_rank < remainder) {\n        mpi_remainder = 1;\n        local_height_remainder = 1;\n    }\n    else {\n        mpi_remainder = 0;\n        local_height_remainder = 0;\n    }\n\n    int local_width = width / mpi_size + mpi_remainder;\n    int local_height = height / mpi_size + local_height_remainder;\n\n    int start_i = local_height * mpi_rank;\n    int start_j = local_width * mpi_rank;\n\n    int end_i = local_height * (mpi_rank + 1) - local_height_remainder;\n    int end_j = local_width * (mpi_rank + 1) - mpi_remainder;\n\n    std::array<size_t, 256> local_bins;\n\n    if (mpi_rank == 0) {\n        bins = { 0 };\n    }\n\n    for (int i = 0; i < local_height; i++) {\n        for (int j = 0; j < local_width; j++) {\n            local_bins[image[start_i + i * width + start_j + j]]++;\n        }\n    }\n\n    std::array<size_t, 256> recv_buffer;\n\n    for (int i = 0; i < mpi_size; i++) {\n        if (i!= mpi_rank) {\n            MPI_Recv(&recv_buffer, 256, MPI_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 256; j++) {\n                bins[j] += recv_buffer[j];\n            }\n        }\n    }\n\n    for (int i = 0; i < local_height; i++) {\n        for (int j = 0; j < local_width; j++) {\n            bins[image[start_i + i * width + start_j + j]]++;\n        }\n    }\n\n    for (int i = 0; i < 256; i++) {\n        MPI_Send(&bins[i], 1, MPI_LONG, mpi_rank, 0, MPI_COMM_WORLD);\n    }\n\n}",
            "// TODO: Your code here\n\tif (image.empty()) {\n\t\treturn;\n\t}\n\tint nprocs;\n\tMPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\tint myrank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\tMPI_Status status;\n\n\tstd::vector<int> send_vector;\n\tif (myrank == 0) {\n\t\tfor (int i = 0; i < image.size() / nprocs; i++) {\n\t\t\tsend_vector.push_back(image[i]);\n\t\t}\n\t}\n\tstd::vector<int> recv_vector;\n\n\tfor (int i = 0; i < nprocs; i++) {\n\t\tif (i!= myrank) {\n\t\t\tif (myrank > i) {\n\t\t\t\tMPI_Send(&send_vector[myrank * image.size() / nprocs], (image.size() / nprocs), MPI_INT, i, 0, MPI_COMM_WORLD);\n\t\t\t}\n\t\t\telse {\n\t\t\t\tMPI_Recv(&recv_vector, (image.size() / nprocs), MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (int i = 0; i < recv_vector.size(); i++) {\n\t\tbins[recv_vector[i]]++;\n\t}\n\treturn;\n}",
            "// TODO\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size_t size = image.size();\n    size_t num_per_proc = size / num_procs;\n    size_t offset = num_per_proc * rank;\n    size_t num_per_proc_excess = size - num_per_proc * num_procs;\n    if (rank < num_per_proc_excess) {\n        num_per_proc += 1;\n    }\n    std::array<size_t, 256> bins_loc;\n    for (int i = 0; i < num_procs; i++) {\n        if (i == rank) {\n            for (auto const& element : image) {\n                bins_loc[element] += 1;\n            }\n        } else {\n            MPI_Status status;\n            MPI_Recv(&bins_loc, sizeof(bins_loc), MPI_BYTE, i, 0, MPI_COMM_WORLD, &status);\n        }\n    }\n    if (rank == 0) {\n        bins = bins_loc;\n    }\n}",
            "/* YOUR CODE HERE */\n    \n    int world_size;\n    int world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD,&world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD,&world_rank);\n\n    int total_rows = image.size()/256;\n    int row_offset = total_rows/world_size;\n    int row_start = world_rank*row_offset;\n    int row_end = row_start+row_offset;\n    if(world_rank == world_size - 1){\n        row_end = total_rows;\n    }\n    \n    for(int i = row_start; i < row_end; i++){\n        for(int j = 0; j < 256; j++){\n            bins[j] += image[i*256+j];\n        }\n    }\n    return;\n}",
            "MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int width = image.size() / size;\n   // rank 0:\n   // if rank == 0, it will run the first if statement, and do the same thing as in the else statement\n   // if rank!= 0, it will run the else statement and the first if statement will do nothing.\n   if (rank == 0) {\n     bins.fill(0);\n     int max = 0;\n     for (size_t i = 0; i < image.size(); i++) {\n       bins[image[i]] += 1;\n       if (image[i] > max) {\n         max = image[i];\n       }\n     }\n\n     // initialize the buffers for receiving from all the other processes.\n     std::vector<int> buffer(max + 1);\n     buffer.fill(0);\n\n     // receive from all the other processes.\n     for (int i = 1; i < size; i++) {\n       MPI_Recv(buffer.data(), buffer.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n       for (size_t j = 0; j < buffer.size(); j++) {\n         bins[j] += buffer[j];\n       }\n     }\n   }\n   else {\n     // send the counts of each grayscale pixel to rank 0.\n     MPI_Send(image.data(), image.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n   }\n }",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, size;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &size);\n\n    // int num_pixels = image.size();\n    int num_pixels = image.size() / size;\n    int start = num_pixels * rank;\n    int end = start + num_pixels;\n\n    if (rank == 0) {\n        // std::array<int, 256> my_bins;\n        // for (int i = 0; i < 256; i++) {\n        //     my_bins[i] = 0;\n        // }\n        for (int i = 0; i < 256; i++) {\n            bins[i] = 0;\n        }\n    }\n\n    // int* my_bins = new int[256];\n    // for (int i = 0; i < 256; i++) {\n    //     my_bins[i] = 0;\n    // }\n    int* my_bins = new int[256];\n    for (int i = 0; i < 256; i++) {\n        my_bins[i] = 0;\n    }\n    // my_bins[0] = 0;\n    // my_bins[1] = 0;\n    // my_bins[2] = 0;\n    // my_bins[3] = 0;\n    // my_bins[4] = 0;\n    // my_bins[5] = 0;\n    // my_bins[6] = 0;\n    // my_bins[7] = 0;\n    // my_bins[8] = 0;\n    // my_bins[9] = 0;\n\n    for (int i = start; i < end; i++) {\n        my_bins[image[i]] = my_bins[image[i]] + 1;\n    }\n\n    // int my_bins[256];\n    // MPI_Scatter(bins.data(), 256, MPI_INT, my_bins, 256, MPI_INT, 0, comm);\n\n    // int* my_bins = new int[256];\n    // MPI_Scatter(bins.data(), 256, MPI_INT, my_bins, 256, MPI_INT, 0, comm);\n\n    // MPI_Scatter(&bins[0], 256, MPI_INT, my_bins, 256, MPI_INT, 0, comm);\n\n    // MPI_Scatter(bins.data(), 256, MPI_INT, my_bins, 256, MPI_INT, 0, comm);\n\n    if (rank == 0) {\n        for (int i = 0; i < 256; i++) {\n            bins[i] = my_bins[i];\n        }\n    }\n\n    // if (rank == 0) {\n    //     for (int i = 0; i < 256; i++) {\n    //         bins[i] = my_bins[i];\n    //     }\n    // }\n\n    // MPI_Reduce(my_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, comm);\n    MPI_Reduce(my_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, comm);\n\n    // MPI_Reduce(my_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, comm);\n\n    // for (int i = 0; i < 256; i++) {\n    //     bins[i] = my_bins[i];\n    // }\n    // delete[] my_bins;\n    delete[] my_bins;\n }",
            "// TODO\n }",
            "int nproc, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  if (image.size() % nproc!= 0) {\n    if (rank == 0) {\n      std::cout << \"image size not divisible by number of processes\" << std::endl;\n    }\n    MPI_Abort(MPI_COMM_WORLD, 1);\n  }\n  std::fill(bins.begin(), bins.end(), 0);\n  int chunkSize = image.size() / nproc;\n  for (int i = 0; i < chunkSize; i++) {\n    bins[image[i]]++;\n  }\n  if (rank == 0) {\n    int tmp;\n    for (int i = 1; i < nproc; i++) {\n      MPI_Recv(&tmp, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      bins[tmp] += tmp;\n    }\n  } else {\n    MPI_Send(&image[0], 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "//YOUR CODE HERE\n\n   //create a vector to store all the values in the image\n   std::vector<int> local_image;\n   //create a vector to store the counts\n   std::vector<int> local_bins;\n\n   //populate the local_image vector with the image\n   local_image = image;\n\n   //allocate the memory for local_bins and initialize it to 0\n   local_bins.assign(256, 0);\n\n   //iterate over all the pixels in the image\n   for (auto i = 0; i < local_image.size(); ++i) {\n     //increase the count for the current intensity\n     local_bins[local_image[i]] += 1;\n   }\n\n   //communicate the counts to the other ranks\n   //this only needs to be done once, since all the ranks share the same image\n   int recvcounts[1] = {256};\n   MPI_Allreduce(local_bins.data(), bins.data(), 256, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n   return;\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n   int num_processes, process_id;\n   MPI_Comm_size(comm, &num_processes);\n   MPI_Comm_rank(comm, &process_id);\n\n   std::vector<size_t> local_bins(256, 0);\n   for (size_t i = 0; i < image.size(); ++i)\n      local_bins[image[i]]++;\n\n   std::vector<size_t> global_bins(256, 0);\n   if (process_id == 0) {\n      global_bins = local_bins;\n   }\n   MPI_Reduce(local_bins.data(), global_bins.data(), 256, MPI_LONG, MPI_SUM, 0, comm);\n\n   if (process_id == 0) {\n      for (size_t i = 0; i < 256; ++i)\n         bins[i] = global_bins[i];\n   }\n }",
            "// TODO\n }",
            "int rank, numProcs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n\n    size_t n = image.size();\n    int nPerRank = n / numProcs;\n    int nExtra = n % numProcs;\n\n    std::vector<int> imageSlice(nPerRank+nExtra);\n    int i, j;\n    for (i=0, j=0; i < nPerRank+nExtra; i++) {\n      if (i < nPerRank) {\n        imageSlice[i] = image[j];\n        j++;\n      } else {\n        imageSlice[i] = image[j-1];\n      }\n    }\n\n    std::array<size_t, 256> localBins;\n    std::fill(localBins.begin(), localBins.end(), 0);\n    for (i=0; i < nPerRank+nExtra; i++) {\n      localBins[imageSlice[i]]++;\n    }\n\n    std::vector<size_t> allBins(256*numProcs);\n    if (rank == 0) {\n      for (i=0; i < numProcs; i++) {\n        MPI_Recv(allBins.data()+i*256, 256, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n    } else {\n      MPI_Send(localBins.data(), 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n      for (i=0; i < numProcs; i++) {\n        for (j=0; j < 256; j++) {\n          bins[j] += allBins[i*256+j];\n        }\n      }\n    }\n\n    return;\n  }",
            "}",
            "// TODO: Your code here\n    return;\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n   int rank, size;\n   MPI_Comm_rank(comm, &rank);\n   MPI_Comm_size(comm, &size);\n\n   if (rank == 0) {\n     for (int i = 0; i < 256; ++i) bins[i] = 0;\n   }\n\n   size_t numPixels = image.size();\n   size_t numPixelsPerRank = numPixels / size;\n   size_t remainder = numPixels % size;\n\n   // Get pixel values of this rank\n   std::vector<int> rankImage;\n   if (rank < remainder) {\n     rankImage = std::vector<int>(image.begin() + rank * (numPixelsPerRank + 1),\n                                  image.begin() + (rank + 1) * (numPixelsPerRank + 1));\n   } else {\n     rankImage = std::vector<int>(image.begin() + rank * numPixelsPerRank,\n                                  image.begin() + (rank + 1) * numPixelsPerRank);\n   }\n\n   // Send pixel values of this rank\n   std::vector<int> sendBuf(rankImage.begin(), rankImage.end());\n   std::vector<int> recvBuf(rankImage.size(), -1);\n   MPI_Allreduce(sendBuf.data(), recvBuf.data(), rankImage.size(), MPI_INT, MPI_SUM, comm);\n\n   // Receive pixel values of other ranks\n   std::vector<int> tmpRecvBuf(rankImage.size(), -1);\n   for (int i = 0; i < size; ++i) {\n     if (i == rank) continue;\n     MPI_Recv(tmpRecvBuf.data(), rankImage.size(), MPI_INT, i, i, comm, MPI_STATUS_IGNORE);\n   }\n\n   // Add the pixel values together\n   for (int i = 0; i < rankImage.size(); ++i) {\n     bins[rankImage[i]] += tmpRecvBuf[i];\n   }\n\n   if (rank == 0) {\n     for (int i = 0; i < bins.size(); ++i) {\n       std::cout << i << \" \" << bins[i] << std::endl;\n     }\n   }\n\n   // Return the values\n   for (int i = 0; i < bins.size(); ++i) {\n     std::cout << i << \" \" << bins[i] << std::endl;\n   }\n }",
            "// Your code goes here\n   int rank = 0;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int size = 0;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int N = image.size();\n   int n = N/size;\n   int left = rank*n;\n   int right = (rank+1)*n;\n   if(rank == size -1){\n     right = N;\n   }\n   std::array<size_t, 256> temp_bins;\n   for(int i=0; i<256; i++){\n     temp_bins[i] = 0;\n   }\n   for(int i = left; i < right; i++){\n     temp_bins[image[i]] += 1;\n   }\n   bins = temp_bins;\n   if(rank == 0){\n     for(int i = 1; i< size; i++){\n       std::array<size_t, 256> temp;\n       MPI_Recv(&temp, sizeof(temp)/sizeof(size_t), MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n       for(int j = 0; j<256; j++){\n         bins[j] += temp[j];\n       }\n     }\n   }\n   else{\n     MPI_Send(&temp_bins, sizeof(temp_bins)/sizeof(size_t), MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n   }\n }",
            "int nproc, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    \n    int image_size = image.size();\n    int per_rank = image_size / nproc;\n    int per_rank_last = image_size % nproc;\n    \n    // initialize the local histogram\n    std::array<size_t, 256> local_bins;\n    for (int i = 0; i < 256; i++)\n    {\n        local_bins[i] = 0;\n    }\n\n    for (int i = 0; i < per_rank; i++)\n    {\n        local_bins[image[i + rank*per_rank]]++;\n    }\n\n    if (rank == nproc - 1)\n    {\n        for (int i = 0; i < per_rank_last; i++)\n        {\n            local_bins[image[per_rank_last + rank * per_rank]]++;\n        }\n    }\n\n    // reduce local histograms to global histogram\n    std::array<size_t, 256> global_bins = local_bins;\n    MPI_Allreduce(MPI_IN_PLACE, global_bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    // copy global histogram to bins on rank 0\n    if (rank == 0)\n    {\n        for (int i = 0; i < 256; i++)\n        {\n            bins[i] = global_bins[i];\n        }\n    }\n}",
            "// TODO: Your code goes here\n }",
            "// TODO: Your code here\n }",
            "// TODO: Your code here\n    \n    // use MPI\n    MPI_Status status;\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    std::vector<int> bins_local(256);\n    \n    for (int i=0; i<256; i++) bins_local[i]=0;\n    if (world_rank==0) bins=std::array<size_t, 256>();\n    for (int i=0; i<image.size(); i++) bins_local[image[i]]+=1;\n    MPI_Gather(&bins_local[0], 256, MPI_INT, &bins[0], 256, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    MPI_Status status;\n\n    size_t count = image.size()/size;\n\n    // copy count into image to create a subset of image\n    for(int i = 0; i < count; i++) {\n        image[rank*count + i] = image[i];\n    }\n\n    // create a histogram of the image\n    for(int i = rank*count; i < (rank + 1)*count; i++) {\n        bins[image[i]] += 1;\n    }\n\n    // distribute the results\n    if(rank == 0) {\n        for(int r = 1; r < size; r++) {\n            MPI_Send(&bins, sizeof(std::array<size_t, 256>), MPI_BYTE, r, 1, MPI_COMM_WORLD);\n        }\n    } else {\n        MPI_Recv(&bins, sizeof(std::array<size_t, 256>), MPI_BYTE, 0, 1, MPI_COMM_WORLD, &status);\n    }\n}",
            "bins.fill(0);\n    for (auto i : image) {\n        bins[i]++;\n    }\n    // bins[i] will be the count of pixels with intensity i.\n}",
            "// Get the number of processes\n    int mpi_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    // Get my rank id\n    int mpi_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n    // Find out how big each chunk of the image is\n    int chunk_size = image.size() / mpi_size;\n\n    if (mpi_rank == 0) {\n        // Zero out the results array\n        bins.fill(0);\n        for (int i = 0; i < mpi_size; ++i) {\n            // Compute the start and end index of the current chunk\n            int start = i * chunk_size;\n            int end = std::min((i + 1) * chunk_size, (int)image.size());\n\n            // Send the chunk to the current process\n            MPI_Send(&image[start], end - start, MPI_INT, i, 0, MPI_COMM_WORLD);\n\n            // Recieve the results from the current process\n            int n;\n            MPI_Status status;\n            MPI_Recv(&n, 1, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n            bins[n]++;\n        }\n    } else {\n        // Find out which chunk I have\n        int start = mpi_rank * chunk_size;\n        int end = std::min(start + chunk_size, (int)image.size());\n\n        // Send the chunk of the image to the root process\n        MPI_Send(&image[start], end - start, MPI_INT, 0, 0, MPI_COMM_WORLD);\n\n        // Recieve the result from the root process\n        std::array<size_t, 256> results;\n        int n;\n        MPI_Status status;\n        MPI_Recv(&results, 256, MPI_LONG_LONG, 0, 0, MPI_COMM_WORLD, &status);\n        MPI_Recv(&n, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n        // Add results to the local result array\n        bins[n] += results[n];\n    }\n}",
            "int num_procs, rank;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // create 2d index for pixels in image\n    size_t const width = image.size() / num_procs;\n    size_t const height = num_procs;\n    size_t const num_pixels = width * height;\n\n    // create local copy of image\n    auto local_image = std::vector<int>(image.begin() + width * rank, image.begin() + width * (rank + 1));\n    std::array<size_t, 256> local_bins;\n\n    // count the number of pixels of each intensity in the local image\n    for (size_t i = 0; i < local_image.size(); i++)\n        local_bins[local_image[i]]++;\n\n    // create empty array of size 256\n    bins = std::array<size_t, 256>();\n\n    // compute the sum of each bin in the local bins and add it to the global bins\n    for (size_t i = 0; i < local_bins.size(); i++) {\n        MPI_Reduce(&local_bins[i], &bins[i], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "bins.fill(0);\n    for(int i = 0; i < image.size(); i++) {\n        bins.at(image[i])++;\n    }\n}",
            "size_t n = image.size();\n   size_t rank, size;\n\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   std::vector<int> histo(256,0);\n   if(rank==0)\n   {\n      for(auto const& pixel: image)\n      {\n         histo[pixel]++;\n      }\n   }\n   MPI_Gather(&histo[0], histo.size(), MPI_INT, &bins[0], histo.size(), MPI_INT, 0, MPI_COMM_WORLD);\n }",
            "int num_processes;\n\tMPI_Comm_size(MPI_COMM_WORLD, &num_processes);\n\n\tint process_id;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &process_id);\n\n\tint const n = image.size();\n\n\tstd::vector<int> sub_image;\n\tsize_t offset;\n\n\tif (process_id == 0) {\n\t\toffset = 0;\n\t}\n\telse {\n\t\toffset = n / num_processes * process_id;\n\t}\n\n\tfor (size_t i = 0; i < image.size(); i++) {\n\t\tif (i >= offset && i < offset + n / num_processes) {\n\t\t\tsub_image.push_back(image.at(i));\n\t\t}\n\t}\n\n\tsize_t* local_bins = new size_t[256];\n\n\tfor (size_t i = 0; i < sub_image.size(); i++) {\n\t\tlocal_bins[sub_image.at(i)]++;\n\t}\n\n\tsize_t* bins_to_send = new size_t[256];\n\n\tMPI_Gather(local_bins, 256, MPI_INT, bins_to_send, 256, MPI_INT, 0, MPI_COMM_WORLD);\n\n\tif (process_id == 0) {\n\t\tfor (int i = 0; i < 256; i++) {\n\t\t\tbins.at(i) += bins_to_send[i];\n\t\t}\n\t\tdelete[] bins_to_send;\n\t}\n\n\tdelete[] local_bins;\n\n}",
            "int rank, size;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   //... Your code here...\n}",
            "int rank, numprocs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\n    //calculate chunk size for each proc\n    size_t chunk = image.size() / numprocs;\n    int extra = image.size() - chunk * numprocs;\n\n    // calculate offset for each proc\n    size_t start = rank * chunk;\n    size_t end = start + chunk;\n    if (rank < extra) {\n        end++;\n    }\n\n    size_t count = 0;\n    for (size_t i = start; i < end; ++i) {\n        count += (image[i]!= 0);\n    }\n\n    int bins_size = 256;\n    bins = std::array<size_t, 256>();\n    MPI_Gather(&count, 1, MPI_UNSIGNED_LONG_LONG, bins.data(), 1, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n    // if root process, calculate mean and std\n    if (rank == 0) {\n        // calculate mean\n        double mean = 0;\n        for (size_t i = 0; i < bins_size; ++i) {\n            mean += (double) bins[i] * i;\n        }\n        mean /= image.size();\n\n        // calculate std\n        double std = 0;\n        for (size_t i = 0; i < bins_size; ++i) {\n            std += pow((bins[i] * i - mean), 2);\n        }\n        std /= image.size();\n        std = sqrt(std);\n\n        std::cout << \"mean: \" << mean << std::endl;\n        std::cout << \"std: \" << std << std::endl;\n    }\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\t\n\t// \n\tint global_size = (int)image.size();\n\tint rank_size = global_size / size;\n\tint start_index = rank * rank_size;\n\tint end_index = start_index + rank_size;\n\tif (rank == size - 1) end_index = global_size;\n\n\t// Each rank counts its own bins and then sum them up\n\tstd::array<size_t, 256> local_bins;\n\tfor (int i = start_index; i < end_index; ++i) {\n\t\t++local_bins[image[i]];\n\t}\n\tMPI_Reduce(local_bins.data(), bins.data(), 256, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "}",
            "// TODO\n }",
            "// TODO: Your code here\n}",
            "// Your code here\n\n    int num_processes;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_processes);\n    int my_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n    int const local_image_size = 32;\n    int const local_bins_size = 256;\n\n    int num_rows = image.size() / local_image_size;\n    int num_cols = local_image_size;\n\n    int my_start_index = num_rows * my_rank;\n    int my_end_index = num_rows * (my_rank + 1);\n\n    std::vector<int> local_image;\n    local_image.resize(local_image_size);\n\n    std::vector<int> my_bins;\n    my_bins.resize(local_bins_size);\n\n    for (int i = my_start_index; i < my_end_index; i++)\n    {\n        for (int j = 0; j < num_cols; j++)\n        {\n            local_image[j] = image[i * num_cols + j];\n        }\n        for (int j = 0; j < local_bins_size; j++)\n        {\n            my_bins[j] = 0;\n        }\n        for (int j = 0; j < num_cols; j++)\n        {\n            my_bins[local_image[j]]++;\n        }\n        //std::cout << \"Process \" << my_rank << \" computed bin counts for \" << local_image_size << \" elements\" << std::endl;\n    }\n\n    MPI_Gather(my_bins.data(), local_bins_size, MPI_INT, bins.data(), local_bins_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (my_rank == 0)\n    {\n        //std::cout << \"Process \" << my_rank << \" is gathering counts\" << std::endl;\n        for (int i = 0; i < bins.size(); i++)\n        {\n            for (int j = 1; j < num_processes; j++)\n            {\n                bins[i] += bins[i + local_bins_size * j];\n            }\n            if (bins[i] == 0)\n            {\n                bins[i] = 1;\n            }\n        }\n    }\n\n\n}",
            "bins.fill(0);\n    for(int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n }",
            "int rank;\n\t\t int size;\n\t\t MPI_Comm_size(MPI_COMM_WORLD, &size);\n\t\t MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t\t std::array<size_t, 256> counts = {};\n\t\t for (auto i : image) {\n\t\t\t counts[i]++;\n\t\t }\n\n\t\t if (rank == 0) {\n\t\t\t for (int i = 0; i < size; i++) {\n\t\t\t\t MPI_Send(counts.data(), 256, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD);\n\t\t\t }\n\t\t }\n\n\t\t MPI_Status status;\n\t\t if (rank!= 0) {\n\t\t\t MPI_Recv(bins.data(), 256, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD, &status);\n\t\t }\n\t\t else {\n\t\t\t std::array<size_t, 256> sums = {};\n\t\t\t for (int i = 0; i < size; i++) {\n\t\t\t\t MPI_Recv(sums.data(), 256, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, &status);\n\t\t\t\t for (int j = 0; j < 256; j++) {\n\t\t\t\t\t bins[j] += sums[j];\n\t\t\t\t }\n\t\t\t }\n\t\t }\n\t }",
            "// TODO\n     // Hint:\n     //   * A rank can receive and process data only if it owns some of the\n     //     image. The ownership of the data is not uniform across all the\n     //     ranks.\n     //   * The ownership of data by the ranks is determined by the following\n     //     formula:\n     //\n     //        (image_width * rank) % #ranks == 0\n     //\n     //   * For example, if image_width == 100 and there are 4 ranks, then:\n     //      * Rank 0 owns 0-24 pixels.\n     //      * Rank 1 owns 25-49 pixels.\n     //      * Rank 2 owns 50-74 pixels.\n     //      * Rank 3 owns 75-99 pixels.\n\n     MPI_Init(NULL, NULL);\n     int numtasks;\n     MPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n     int rank;\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n     int blocksize;\n     int binsize;\n     if(rank==0){\n         blocksize=image.size()/numtasks;\n         binsize=image.size()/256;\n     }\n     else{\n         MPI_Bcast(&blocksize, 1, MPI_INT, 0, MPI_COMM_WORLD);\n         MPI_Bcast(&binsize, 1, MPI_INT, 0, MPI_COMM_WORLD);\n     }\n     //printf(\"blocksize: %d\\nbinsize: %d\\n\",blocksize,binsize);\n\n\n     std::array<size_t, 256> tempbins;\n     std::vector<int> imagedata;\n     if(rank==0)\n         imagedata=image;\n     else{\n         MPI_Bcast(&image[0], image.size(), MPI_INT, 0, MPI_COMM_WORLD);\n     }\n     for(size_t i=0;i<image.size();i++){\n         tempbins[imagedata[i]]++;\n     }\n\n     MPI_Reduce(tempbins.data(), bins.data(), binsize, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n     MPI_Finalize();\n }",
            "MPI_Status status;\n\n    // get the number of ranks\n    int commsize;\n    MPI_Comm_size(MPI_COMM_WORLD, &commsize);\n\n    // get the rank id\n    int rankid;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rankid);\n\n    // get the number of elements in the image vector\n    int number_of_elements = image.size();\n\n    // each rank will handle the number of elements equal to\n    // number_of_elements/commsize (integer division)\n    int elements_per_rank = number_of_elements / commsize;\n    int remainder_elements = number_of_elements % commsize;\n\n    // each rank will get the elements equal to \n    // elements_per_rank + remainder_elements\n    // if rankid < remainder_elements, then the rank will \n    // receive elements_per_rank + 1\n    int elements_to_receive = elements_per_rank;\n    if (rankid < remainder_elements) {\n        elements_to_receive++;\n    }\n\n    // copy the elements to receive into a new vector\n    std::vector<int> image_per_rank(elements_to_receive);\n    for (int i=0; i<elements_to_receive; i++) {\n        image_per_rank[i] = image[i + (rankid * elements_per_rank)];\n    }\n\n    // create a vector to receive counts\n    std::vector<int> counts(256, 0);\n\n    // process the image and count the number of pixels with each value\n    for (int i=0; i<elements_to_receive; i++) {\n        counts[image_per_rank[i]]++;\n    }\n\n    // gather counts from all ranks into bins\n    if (rankid == 0) {\n        MPI_Gather(counts.data(), 256, MPI_INT, bins.data(), 256, MPI_INT, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Gather(counts.data(), 256, MPI_INT, NULL, 256, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n\n    // return if the rankid is not 0\n    if (rankid!= 0) {\n        return;\n    }\n\n    // reduce the counts for each bin\n    for (int i=0; i<256; i++) {\n        for (int j=0; j<commsize; j++) {\n            if (bins[i] < bins[j*256 + i]) {\n                bins[j*256 + i] = bins[i];\n            }\n        }\n    }\n}",
            "}",
            "int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int size;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<int> localImage(image.size()/size);\n   std::vector<int> localCount(256);\n   std::vector<int> sendBuff(256);\n   std::vector<int> recvBuff(256);\n\n   for(int i = 0; i< localImage.size(); i++){\n      localImage[i] = image[i+rank*localImage.size()];\n   }\n\n   for(int i = 0; i < 256; i++){\n      localCount[i] = 0;\n   }\n\n   for(int i = 0; i < localImage.size(); i++){\n      localCount[localImage[i]]++;\n   }\n\n   for(int i = 0; i < 256; i++){\n      sendBuff[i] = localCount[i];\n   }\n\n   MPI_Allreduce(sendBuff.data(), recvBuff.data(), 256, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n   for(int i = 0; i < 256; i++){\n      bins[i] = recvBuff[i];\n   }\n}",
            "int mpi_rank;\n     MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n     int mpi_size;\n     MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\n     std::vector<int> new_image;\n\n     if (mpi_rank!= 0) {\n         new_image = image;\n     }\n\n     std::vector<int> partial_result;\n     partial_result.resize(256);\n\n     int chunk = (image.size() / mpi_size);\n     int remainder = (image.size() % mpi_size);\n\n     //if the number of chunks is not evenly divisible by the mpi_size, we need to make some of the images slightly bigger\n     //and then we need to make sure that the first mpi_rank chunks of the image get assigned to the first mpi_rank processes\n\n     if (chunk < remainder) {\n         new_image.resize(mpi_rank * (chunk + 1));\n         std::vector<int> temp(mpi_rank * (chunk + 1));\n         std::copy(image.begin(), image.begin() + (mpi_rank * (chunk + 1)), temp.begin());\n         std::copy(temp.begin(), temp.end(), new_image.begin());\n     }\n     else {\n         new_image = image;\n     }\n\n     //create a vector that will store the count for each intensity in each image chunk\n\n     int counts[256];\n     std::fill(counts, counts + 256, 0);\n\n     //count the number of pixels for each intensity\n\n     for (int i = 0; i < new_image.size(); i++) {\n         counts[new_image[i]]++;\n     }\n\n     //send the count for each intensity to the process on rank 0\n\n     MPI_Gather(&counts[0], 256, MPI_INT, &partial_result[0], 256, MPI_INT, 0, MPI_COMM_WORLD);\n\n     //sum the counts for all the intensities\n\n     if (mpi_rank == 0) {\n\n         for (int i = 0; i < 256; i++) {\n             for (int j = 1; j < mpi_size; j++) {\n                 partial_result[i] += partial_result[i + j * 256];\n             }\n             bins[i] = partial_result[i];\n         }\n     }\n }",
            "// Your code here\n    \n    // MPI_Init(NULL, NULL);\n    // int mpi_rank;\n    // MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    // if (mpi_rank == 0) {\n    //     std::vector<int> image_sub(image.size()/2);\n    //     for (size_t i = 0; i < image.size()/2; i++) {\n    //         image_sub[i] = image[i];\n    //     }\n    //     int num_procs;\n    //     MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    //     int recvcounts[num_procs];\n    //     int displs[num_procs];\n    //     displs[0] = 0;\n    //     recvcounts[0] = image_sub.size();\n    //     for (size_t i = 1; i < num_procs; i++) {\n    //         recvcounts[i] = image_sub.size()/num_procs;\n    //         displs[i] = displs[i-1] + recvcounts[i-1];\n    //     }\n    //     MPI_Datatype datatype;\n    //     MPI_Type_vector(num_procs, recvcounts[0], image_sub.size(), MPI_INT, &datatype);\n    //     MPI_Type_commit(&datatype);\n    //     MPI_Allgatherv(image_sub.data(), image_sub.size(), MPI_INT, bins.data(), recvcounts, displs, MPI_INT, MPI_COMM_WORLD);\n    //     MPI_Type_free(&datatype);\n    //     // for (size_t i = 0; i < bins.size(); i++) {\n    //     //     std::cout << bins[i] << \" \";\n    //     // }\n    //     // std::cout << std::endl;\n    // }\n    // else {\n    //     // std::vector<int> image_sub(image.size()/2);\n    //     // for (size_t i = 0; i < image.size()/2; i++) {\n    //     //     image_sub[i] = image[i];\n    //     // }\n    //     int num_procs;\n    //     MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    //     int recvcounts[num_procs];\n    //     int displs[num_procs];\n    //     displs[0] = 0;\n    //     recvcounts[0] = image.size()/2;\n    //     for (size_t i = 1; i < num_procs; i++) {\n    //         recvcounts[i] = image.size()/2/num_procs;\n    //         displs[i] = displs[i-1] + recvcounts[i-1];\n    //     }\n    //     MPI_Datatype datatype;\n    //     MPI_Type_vector(num_procs, recvcounts[0], image.size()/2, MPI_INT, &datatype);\n    //     MPI_Type_commit(&datatype);\n    //     MPI_Allgatherv(image.data(), image.size()/2, MPI_INT, bins.data(), recvcounts, displs, MPI_INT, MPI_COMM_WORLD);\n    //     MPI_Type_free(&datatype);\n    // }\n    // MPI_Finalize();\n\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "int const size = 8;\n   int const rank = 0;\n   // initialize the array\n   for (int i = 0; i < 256; ++i) {\n      bins[i] = 0;\n   }\n\n   for (int i = 0; i < image.size(); ++i) {\n      bins[image[i]]++;\n   }\n }",
            "int mpi_size, mpi_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n    size_t width = 1024, height = 1024;\n    size_t mpi_part = width/mpi_size;\n    size_t image_part = width * height;\n    std::vector<int> vec_image;\n    vec_image.resize(image_part);\n    if (mpi_rank == 0) {\n        for (int i = 0; i < mpi_size; ++i) {\n            std::vector<int> tmp;\n            MPI_Status status;\n            MPI_Recv(&tmp[0], image_part, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n            for (size_t k = 0; k < tmp.size(); ++k) {\n                vec_image[k] = tmp[k];\n            }\n        }\n    } else {\n        MPI_Send(&image[0], image_part, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n    std::array<int, 256> arr_image;\n    if (mpi_rank == 0) {\n        for (size_t k = 0; k < image_part; ++k) {\n            arr_image[vec_image[k]]++;\n        }\n    }\n    // Reduction\n    MPI_Reduce(&arr_image[0], &bins[0], 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: replace the following code with your solution\n   //\n   MPI_Barrier(MPI_COMM_WORLD);\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   size_t i;\n   std::array<size_t, 256> local_counts{};\n   for (i = 0; i < image.size(); i++) {\n    local_counts[image[i]]++;\n   }\n   \n   MPI_Barrier(MPI_COMM_WORLD);\n   if (rank == 0) {\n     bins.fill(0);\n     for (int i = 0; i < size; i++) {\n       MPI_Send(&local_counts[i * 256], 256, MPI_INT, i, 0, MPI_COMM_WORLD);\n     }\n     for (int i = 1; i < size; i++) {\n       MPI_Recv(&bins[i * 256], 256, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n     }\n   } else {\n     MPI_Recv(&bins[rank * 256], 256, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n   }\n\n }",
            "// This is not MPI code, but it will be useful for parallelizing the\n    // pixel counting step.  Initialize bins[i] to 0 for all i.\n    // bins[i] will store the number of pixels with grayscale intensity i.\n    // The values in image are all integers in the range 0-255.\n    // bins should have 256 elements and should be initialized to 0.\n\n    for(int i = 0; i < 256; i++){\n        bins[i] = 0;\n    }\n\n    // For each pixel, increment bins[image[i]].\n    // If you use a loop like this, you might want to use the MPI reduce\n    // function, or an MPI scatter followed by an MPI allreduce.\n    for(int i = 0; i < image.size(); i++){\n        bins[image[i]]++;\n    }\n }",
            "// TODO\n\n\n}",
            "int world_size, world_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  size_t num_of_pixels = image.size();\n  int count = 0;\n\n  // Create a vector for each rank to hold their pixel counts\n  std::vector<std::array<size_t, 256>> image_bins(world_size);\n\n  // Get the pixel count for the image on each rank and fill the vector with pixel count for each intensity\n  for (size_t i = 0; i < num_of_pixels; ++i) {\n    image_bins[world_rank][image[i]]++;\n  }\n\n  // Sum up the number of pixels with each intensity across all ranks\n  // MPI_Reduce takes the data in image_bins and sums the pixel counts for the same intensity across all ranks\n  // MPI_Reduce takes the data in image_bins and sums the pixel counts for the same intensity across all ranks\n  MPI_Reduce(image_bins[world_rank].data(), image_bins[world_rank].data(), image_bins[world_rank].size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  // MPI_Reduce takes the data in image_bins and sums the pixel counts for the same intensity across all ranks\n\n  // For rank 0, copy the pixel counts for each intensity to the bins array\n  if (world_rank == 0) {\n    bins = image_bins[0];\n  }\n}",
            "size_t len = image.size();\n    int rank = 0;\n    int size = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int per_rank_count = len / size;\n\n    //calculate rank start and end\n    int start, end;\n    if (rank == 0) {\n        start = 0;\n    } else {\n        start = rank * per_rank_count;\n    }\n    if (rank == size - 1) {\n        end = len - 1;\n    } else {\n        end = rank * per_rank_count + per_rank_count - 1;\n    }\n\n    for (int i = start; i <= end; i++) {\n        bins[image[i]] += 1;\n    }\n\n    //send results to rank 0\n    for (int i = 1; i < size; i++) {\n        MPI_Send(bins.data() + i * per_rank_count, per_rank_count, MPI_LONG, i, 0, MPI_COMM_WORLD);\n    }\n\n    //rank 0 collect results\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(bins.data() + i * per_rank_count, per_rank_count, MPI_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n\n}",
            "int rank, size;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<int> local_image(image.begin() + rank * size, image.begin() + (rank + 1) * size);\n\n    int local_bins[256];\n\n    for (int i = 0; i < 256; i++) {\n        local_bins[i] = 0;\n    }\n\n    for (int i = 0; i < (int)local_image.size(); i++) {\n        local_bins[local_image[i]] += 1;\n    }\n\n    int global_bins[256];\n\n    for (int i = 0; i < 256; i++) {\n        global_bins[i] = 0;\n    }\n\n    for (int i = 0; i < 256; i++) {\n        global_bins[i] = 0;\n        for (int j = 0; j < size; j++) {\n            global_bins[i] += local_bins[i];\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < 256; i++) {\n            bins[i] = global_bins[i];\n        }\n    }\n}",
            "int world_rank, world_size;\n     MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n     MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n     // Initialize bins to 0 on each process.\n     for (int i = 0; i < 256; ++i) {\n         bins[i] = 0;\n     }\n     int img_size = image.size();\n     // Each process will count the number of pixels that have that intensity.\n     // Send the image to process 0\n     if (world_rank == 0) {\n         for (int i = 0; i < img_size; ++i) {\n             int intensity = image[i];\n             // Send to each process\n             MPI_Send(&intensity, 1, MPI_INT, world_size - 1, 0, MPI_COMM_WORLD);\n         }\n     } else {\n         int img_rank;\n         // Each process with a lower rank will get an image of the same size\n         // except the last process will get a smaller image.\n         MPI_Recv(&img_rank, 1, MPI_INT, world_rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         for (int i = 0; i < img_size; ++i) {\n             int intensity;\n             // Receive the image from process 0\n             if (img_rank == 0) {\n                 MPI_Recv(&intensity, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n                 bins[intensity]++;\n             } else {\n                 MPI_Recv(&intensity, 1, MPI_INT, img_rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n                 bins[intensity]++;\n             }\n         }\n         // Send the image to the next process\n         if (world_rank < world_size - 1) {\n             MPI_Send(&img_rank, 1, MPI_INT, world_rank + 1, 0, MPI_COMM_WORLD);\n         }\n     }\n     // Print the results on process 0\n     if (world_rank == 0) {\n         for (int i = 0; i < 256; ++i) {\n             std::cout << bins[i] << \" \";\n         }\n         std::cout << \"\\n\";\n     }\n }",
            "int rank, nranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n    // Compute image size in pixels\n    int imageSize = image.size();\n    // Compute number of pixels per rank\n    int chunkSize = imageSize / nranks;\n    int offset = rank * chunkSize;\n    // Setup bins to zero\n    for (int i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n    // Process image\n    for (int i = offset; i < offset + chunkSize; i++) {\n        bins[image[i]] += 1;\n    }\n    // Gather bins from all ranks\n    int* binsGathered = new int[256];\n    if (rank == 0) {\n        for (int i = 1; i < nranks; i++) {\n            MPI_Recv(binsGathered, 256, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 256; j++) {\n                bins[j] += binsGathered[j];\n            }\n        }\n    } else {\n        MPI_Send(bins.data(), 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n    // Delete the binsGathered array\n    delete binsGathered;\n}",
            "//Your code here\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tsize_t len = image.size();\n\t//every rank has a copy of image\n\tstd::vector<int> tmp;\n\tif(rank == 0){\n\t\ttmp = image;\n\t}\n\t//broadcast image to all ranks\n\tMPI_Bcast((void*)&image[0], len, MPI_INT, 0, MPI_COMM_WORLD);\n\t//every rank counts the number of pixels with each intensity\n\tfor(int i = 0; i < 256; i++){\n\t\t//count the number of pixels with grayscale intensity i\n\t\tint num = 0;\n\t\tfor(int j = 0; j < len; j++){\n\t\t\tif(tmp[j] == i){\n\t\t\t\tnum++;\n\t\t\t}\n\t\t}\n\t\t//update the vector bins\n\t\tbins[i] = num;\n\t}\n}",
            "// Your code here\n     MPI_Barrier(MPI_COMM_WORLD);\n     // printf(\"this is rank: %d\\n\", world_rank);\n     if (world_rank == 0)\n     {\n         for (size_t i = 0; i < image.size(); i++)\n         {\n             bins[image[i]]++;\n         }\n     }\n     else\n     {\n         int local_image[image.size()];\n         for (size_t i = 0; i < image.size(); i++)\n         {\n             local_image[i] = image[i];\n         }\n         MPI_Barrier(MPI_COMM_WORLD);\n         MPI_Gather(&local_image[0], 1, MPI_INT, &bins[0], 256, MPI_INT, 0, MPI_COMM_WORLD);\n     }\n\n }",
            "// TODO: Your code here\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int n = image.size()/size;\n  std::array<size_t, 256> tmp;\n  int offset = rank*n;\n  for(int i=0; i<n; i++){\n    tmp[image[i+offset]]+=1;\n  }\n  MPI_Reduce(&tmp[0], &bins[0], 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  return;\n}",
            "size_t numPixels = image.size();\n\n    std::array<size_t, 256> counts;\n    counts.fill(0);\n\n    for (int i = 0; i < numPixels; i++)\n        counts[image[i]]++;\n\n    // MPI\n    std::array<size_t, 256> tmp;\n    tmp.fill(0);\n\n    MPI_Allreduce(counts.data(), tmp.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 256; i++) {\n            bins[i] = tmp[i];\n        }\n    }\n}",
            "for (int i = 0; i < 256; i++) {\n    bins[i] = 0;\n  }\n  size_t N = image.size();\n  int rank, nRanks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n  int nRanks2 = nRanks;\n  int p = 0;\n  while (nRanks2 > 1) {\n    if (rank % 2 == 0) {\n      p = 1;\n    }\n    else {\n      p = 0;\n    }\n    if (rank == nRanks - 1 && nRanks % 2 == 1) {\n      p = 0;\n    }\n    int source = rank + p;\n    int destination = rank - p;\n    if (rank < nRanks - 1 && rank % 2 == 0) {\n      if (rank!= nRanks - 1) {\n        MPI_Send(&image[rank * N / nRanks], N / nRanks, MPI_INT, destination, 0, MPI_COMM_WORLD);\n        MPI_Recv(&image[rank * N / nRanks], N / nRanks, MPI_INT, source, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n      else {\n        MPI_Send(&image[rank * N / nRanks], N / nRanks, MPI_INT, destination, 0, MPI_COMM_WORLD);\n      }\n    }\n    else if (rank < nRanks - 1) {\n      MPI_Send(&image[rank * N / nRanks], N / nRanks, MPI_INT, destination, 0, MPI_COMM_WORLD);\n      MPI_Recv(&image[rank * N / nRanks], N / nRanks, MPI_INT, source, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    nRanks2 /= 2;\n    if (rank < nRanks2) {\n      if (rank!= nRanks2 - 1) {\n        MPI_Send(&image[rank * N / nRanks], N / nRanks, MPI_INT, destination, 0, MPI_COMM_WORLD);\n        MPI_Recv(&image[rank * N / nRanks], N / nRanks, MPI_INT, source, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n      else {\n        MPI_Send(&image[rank * N / nRanks], N / nRanks, MPI_INT, destination, 0, MPI_COMM_WORLD);\n      }\n    }\n    nRanks2 /= 2;\n    if (rank < nRanks2) {\n      if (rank!= nRanks2 - 1) {\n        MPI_Send(&image[rank * N / nRanks], N / nRanks, MPI_INT, destination, 0, MPI_COMM_WORLD);\n        MPI_Recv(&image[rank * N / nRanks], N / nRanks, MPI_INT, source, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n      else {\n        MPI_Send(&image[rank * N / nRanks], N / nRanks, MPI_INT, destination, 0, MPI_COMM_WORLD);\n      }\n    }\n    if (rank == 0) {\n      for (int j = 0; j < N; j++) {\n        bins[image[j]]++;\n      }\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n    nRanks2 = nRanks;\n  }\n\n}",
            "int myRank, numRanks;\n \tMPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n \tMPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n \tsize_t nPixels = image.size();\n \tsize_t nPixelsPerRank = nPixels/numRanks;\n \tsize_t extraPixels = nPixels%numRanks;\n\n \tif (myRank == 0) {\n \t\tbins.fill(0);\n \t}\n\n \tstd::vector<int> imagePart;\n \tif (myRank < extraPixels) {\n \t\timagePart = std::vector<int>(image.begin() + nPixelsPerRank*(myRank),\n \t\t\t\t\t\t\t\t\t image.begin() + nPixelsPerRank*(myRank + 1));\n \t} else {\n \t\timagePart = std::vector<int>(image.begin() + nPixelsPerRank*(myRank) + nPixelsPerRank*extraPixels,\n \t\t\t\t\t\t\t\t\t image.begin() + nPixelsPerRank*(myRank + 1) + nPixelsPerRank*extraPixels);\n \t}\n \n \tMPI_Gather(imagePart.data(), imagePart.size(), MPI_INT,\n \t\t\t   bins.data(), imagePart.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n \treturn;\n }",
            "// TODO: your code here\n}",
            "int nproc, proc_id;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &proc_id);\n\n    size_t n = image.size();\n    size_t nbins = bins.size();\n    size_t n_per_proc = n / nproc;\n\n    // compute the portion that is not divisible by nprocs\n    size_t extra_pixels = n % nproc;\n    size_t start = proc_id * n_per_proc;\n    size_t end = start + n_per_proc;\n    if(proc_id < extra_pixels) {\n        end += 1;\n    }\n\n    for(size_t i=start; i<end; i++) {\n        bins[image[i]]++;\n    }\n\n    // wait for all procs to finish\n    MPI_Barrier(MPI_COMM_WORLD);\n    if(proc_id == 0) {\n        for(size_t i=1; i<nproc; i++) {\n            MPI_Status status;\n            MPI_Recv(&bins[0], nbins, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, &status);\n        }\n    } else {\n        MPI_Send(&bins[0], nbins, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, size;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &size);\n    bins.fill(0);\n\n    std::vector<int> image_part;\n    int rank_size = image.size() / size;\n    int rank_part_size = image_part.size();\n    if (rank == 0) {\n        rank_part_size = rank_size;\n    }\n    for (int i = rank * rank_size; i < (rank + 1) * rank_size; i++) {\n        image_part.push_back(image[i]);\n    }\n\n    int counts[256];\n    std::fill(counts, counts + 256, 0);\n    for (int i = 0; i < rank_part_size; i++) {\n        counts[image_part[i]]++;\n    }\n\n    MPI_Reduce(counts, bins.data(), 256, MPI_INT, MPI_SUM, 0, comm);\n}",
            "// TODO\n     // 1. split image into chunks\n     // 2. for each chunk: sum pixels in chunk and store result in bins[i]\n     // 3. output: bins = [0, 0, 2, 0, 1,...]\n     // 4. output: bins = [0, 0, 2, 0, 1,..., 0, 0, 2, 0, 1]\n     // 5. output: bins = [0, 0, 2, 0, 1,..., 0, 0, 2, 0, 1, 0, 0, 2, 0, 1]\n     // 6. output: bins = [0, 0, 2, 0, 1,..., 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0]\n     // 7. output: bins = [0, 0, 2, 0, 1,..., 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0]\n     // 8. output: bins = [0, 0, 2, 0, 1,..., 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0]\n     // 9. output: bins = [0, 0, 2, 0, 1,..., 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 1,",
            "//TODO: Your code here\n\n    int commsize = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &commsize);\n    int rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    size_t chunkSize = image.size() / commsize;\n    size_t remainder = image.size() % commsize;\n\n    std::vector<int> localVec(chunkSize + remainder);\n\n    if (rank < remainder) {\n        localVec[rank] = image[rank];\n    } else {\n        for (int i = 0; i < chunkSize; i++) {\n            localVec[i] = image[rank * chunkSize + i];\n        }\n    }\n\n    std::vector<int> global(256);\n    MPI_Allreduce(&localVec[0], &global[0], 256, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    for (int i = 0; i < 256; i++) {\n        bins[i] = global[i];\n    }\n\n}",
            "/* 1. find the number of processors \n     * 2. find the id of the current processor\n     */\n    int number_of_processors;\n    int current_processor_id;\n    MPI_Comm_size(MPI_COMM_WORLD, &number_of_processors);\n    MPI_Comm_rank(MPI_COMM_WORLD, &current_processor_id);\n\n    // initialize the bins array\n    for (int i = 0; i < 256; i++)\n        bins[i] = 0;\n\n    // every processor has its own copy of the image\n    std::vector<int> local_image(image.begin() + (current_processor_id * image.size() / number_of_processors),\n                                 image.begin() + ((current_processor_id + 1) * image.size() / number_of_processors));\n    // go through every pixel in the local copy of the image and increase the counter of its intensity by one\n    for (size_t i = 0; i < local_image.size(); i++)\n        bins[local_image[i]]++;\n\n    // MPI_Reduce\n    // sum up the number of pixels on the bins array of each rank and store the result on the bins array of rank 0\n    MPI_Reduce(bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    /* every rank has its own copy of the bins array, but only the results are stored on rank 0 */\n}",
            "for (auto&& i: image) {\n        bins[i]++;\n    }\n}",
            "MPI_Init();\n\n    int comm_sz, comm_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);\n    MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);\n\n    // compute chunk sizes\n    int const chunk_size = image.size() / comm_sz;\n\n    // compute the starting index of the local array\n    int const local_start = chunk_size * comm_rank;\n\n    // compute the number of elements in the local array\n    int const local_count = (comm_rank == comm_sz - 1)? (image.size() - local_start) : chunk_size;\n\n    // compute the global index of the last element in the local array\n    int const local_end = local_start + local_count;\n\n    // create MPI datatypes for vectors\n    MPI_Datatype mpi_vec_t, mpi_int_t;\n    MPI_Type_vector(local_count, 1, 1, MPI_INT, &mpi_vec_t);\n    MPI_Type_commit(&mpi_vec_t);\n    MPI_Type_contiguous(local_count, MPI_INT, &mpi_int_t);\n    MPI_Type_commit(&mpi_int_t);\n\n    // create a new vector to receive the partial results from other ranks\n    std::vector<int> image_partial(local_count);\n\n    // send local array to other ranks\n    MPI_Scatter(image.data() + local_start, 1, mpi_vec_t, image_partial.data(), 1, mpi_vec_t, 0, MPI_COMM_WORLD);\n\n    // fill the partial bins vector\n    std::array<size_t, 256> bins_partial;\n    for(int i = 0; i < local_count; ++i)\n        ++bins_partial[image_partial[i]];\n\n    // gather partial bins on rank 0\n    MPI_Gather(bins_partial.data(), 1, mpi_int_t, bins.data(), 256, mpi_int_t, 0, MPI_COMM_WORLD);\n\n    // free memory of MPI datatypes\n    MPI_Type_free(&mpi_vec_t);\n    MPI_Type_free(&mpi_int_t);\n    MPI_Finalize();\n}",
            "// YOUR CODE HERE\n\n}",
            "int num_procs;\n     int my_rank;\n     MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n     MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n     // Get the number of pixels in each bin and store it in bins\n     std::array<size_t, 256> localBins;\n     for (int i = 0; i < 256; i++) {\n         localBins[i] = 0;\n     }\n     int start = image.size()/num_procs*my_rank;\n     int end = image.size()/num_procs*(my_rank+1);\n     for(int i = start; i < end; i++) {\n         localBins[image[i]]++;\n     }\n\n     // Send the pixel counts from each process to rank 0\n     std::array<size_t, 256> recvBins;\n     for (int i = 0; i < 256; i++) {\n         recvBins[i] = 0;\n     }\n     if(my_rank == 0) {\n         for(int i = 1; i < num_procs; i++) {\n             MPI_Recv(&recvBins[0], 256, MPI_LONG_LONG_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n             for (int i = 0; i < 256; i++) {\n                 bins[i] += recvBins[i];\n             }\n         }\n     } else {\n         MPI_Send(&localBins[0], 256, MPI_LONG_LONG_INT, 0, 0, MPI_COMM_WORLD);\n     }\n }",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int per_rank = image.size() / size;\n    std::vector<int> part_image(per_rank);\n    std::copy(image.begin() + rank * per_rank, image.begin() + (rank + 1) * per_rank, part_image.begin());\n\n    std::vector<int> per_rank_bins(256);\n    int min = *std::min_element(part_image.begin(), part_image.end());\n    int max = *std::max_element(part_image.begin(), part_image.end());\n    for (auto value : part_image) {\n        per_rank_bins[value]++;\n    }\n\n    std::vector<int> bins_per_rank(256);\n    MPI_Reduce(per_rank_bins.data(), bins_per_rank.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        bins = bins_per_rank;\n    }\n}",
            "//TODO: YOUR CODE HERE\n    int rank,size;\n    MPI_Comm_size(MPI_COMM_WORLD,&size);\n    MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n\n    size_t numpixels = image.size();\n    int chunk_size = numpixels/size;\n\n    //check if last rank gets extra pixels\n    int extras = numpixels%size;\n    if (rank == size-1){\n        chunk_size += extras;\n    }\n    \n    //check if first rank gets extra pixels\n    if (rank == 0){\n        chunk_size -= extras;\n    }\n    \n    //check if ranks need to communicate\n    int partner_rank = rank + 1;\n    if (rank == size-1){\n        partner_rank = 0;\n    }\n\n    std::vector<int> local_copy;\n\n    if (rank == 0){\n        local_copy = std::vector<int>(image.begin(),image.begin()+chunk_size);\n        MPI_Send(&local_copy[0],chunk_size,MPI_INT,partner_rank,0,MPI_COMM_WORLD);\n    } else if (rank == size-1){\n        local_copy = std::vector<int>(image.begin()+chunk_size*rank,image.end());\n        MPI_Send(&local_copy[0],numpixels-chunk_size*rank,MPI_INT,partner_rank,0,MPI_COMM_WORLD);\n    } else {\n        local_copy = std::vector<int>(image.begin()+chunk_size*rank,image.begin()+(chunk_size*rank)+chunk_size);\n    }\n\n    //count the local pixels\n    for(int i=0;i<local_copy.size();++i){\n        ++bins[local_copy[i]];\n    }\n\n    std::vector<size_t> total_counts;\n    total_counts = std::vector<size_t>(256,0);\n    //get counts from other ranks\n    if (rank!= 0){\n        MPI_Status status;\n        MPI_Recv(&total_counts[0],256,MPI_UNSIGNED_LONG_LONG,partner_rank,0,MPI_COMM_WORLD,&status);\n    }\n\n    //merge counts from other ranks\n    if (rank == 0){\n        for (int i=1;i<size;++i){\n            for (int j=0;j<256;++j){\n                bins[j] += total_counts[j];\n            }\n        }\n    } else if (rank!= size-1){\n        MPI_Status status;\n        MPI_Send(&bins[0],256,MPI_UNSIGNED_LONG_LONG,partner_rank,0,MPI_COMM_WORLD);\n    } else {\n        MPI_Status status;\n        MPI_Recv(&bins[0],256,MPI_UNSIGNED_LONG_LONG,partner_rank,0,MPI_COMM_WORLD,&status);\n    }\n\n    \n\n}",
            "// Write your code here\n }",
            "int comm_sz;\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int world_size = comm_sz;\n\n    if (rank == 0) {\n        bins.fill(0);\n    }\n\n    int img_sz = image.size();\n\n    int chunk_sz = img_sz / world_size;\n\n    if (rank!= world_size - 1) {\n        MPI_Send(image.data() + rank * chunk_sz, chunk_sz, MPI_INT, rank + 1, 0, MPI_COMM_WORLD);\n    }\n\n    MPI_Status status;\n\n    if (rank!= 0) {\n        MPI_Recv(image.data(), chunk_sz, MPI_INT, rank - 1, 0, MPI_COMM_WORLD, &status);\n    }\n\n    std::vector<int> local_image(image.begin(), image.begin() + chunk_sz);\n    for (int i = 0; i < local_image.size(); i++) {\n        bins[local_image[i]] += 1;\n    }\n\n    if (rank!= 0) {\n        MPI_Recv(image.data() + chunk_sz, chunk_sz, MPI_INT, rank - 1, 0, MPI_COMM_WORLD, &status);\n    }\n\n    if (rank!= world_size - 1) {\n        MPI_Send(image.data() + (world_size - 1) * chunk_sz, chunk_sz, MPI_INT, world_size - 1, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < world_size; i++) {\n            MPI_Recv(image.data(), chunk_sz, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n            for (int j = 0; j < chunk_sz; j++) {\n                bins[image[j]] += 1;\n            }\n        }\n    }\n\n}",
            "const int rank = MPI_Comm_rank(MPI_COMM_WORLD);\n    const int size = MPI_Comm_size(MPI_COMM_WORLD);\n    const int length = image.size();\n    const int blocks = length / size;\n    const int last = length % size;\n    int block_start;\n    int block_end;\n    std::fill(bins.begin(), bins.end(), 0);\n\n    if (rank == 0) {\n        for (int i = 0; i < size; i++) {\n            block_start = i * blocks;\n            block_end = block_start + blocks;\n            if (i == size - 1) {\n                block_end = block_end + last;\n            }\n            if (block_end > length) {\n                block_end = length;\n            }\n            std::vector<int> local_image(image.begin() + block_start, image.begin() + block_end);\n            std::vector<int> bins_temp(256, 0);\n            for (int j = 0; j < local_image.size(); j++) {\n                bins_temp[local_image[j]] = bins_temp[local_image[j]] + 1;\n            }\n            for (int i = 0; i < 256; i++) {\n                bins[i] = bins[i] + bins_temp[i];\n            }\n        }\n    } else {\n        block_start = rank * blocks;\n        block_end = block_start + blocks;\n        if (rank == size - 1) {\n            block_end = block_end + last;\n        }\n        std::vector<int> local_image(image.begin() + block_start, image.begin() + block_end);\n        std::vector<int> bins_temp(256, 0);\n        for (int i = 0; i < local_image.size(); i++) {\n            bins_temp[local_image[i]] = bins_temp[local_image[i]] + 1;\n        }\n        for (int i = 0; i < 256; i++) {\n            bins[i] = bins[i] + bins_temp[i];\n        }\n    }\n}",
            "size_t num_pixels = image.size();\n     int image_width = sqrt(num_pixels);\n\n     if (image_width * image_width!= num_pixels) {\n         throw std::runtime_error(\"Image is not a square matrix\");\n     }\n\n     // Use mpi_sum_all to sum all of the counts across all ranks\n     // Store the results in bins[0] on rank 0\n     for (int i = 0; i < 256; i++) {\n         bins[i] = 0;\n     }\n\n     for (int i = 0; i < num_pixels; i++) {\n         bins[image[i]]++;\n     }\n\n     MPI_Reduce(MPI_IN_PLACE, &bins, 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "size_t size = image.size();\n     bins = std::array<size_t, 256>{};\n     for (size_t i = 0; i < size; ++i) {\n         ++bins[image[i]];\n     }\n }",
            "size_t numPixels = image.size();\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   // Your code here\n   int numPixelsPerRank = numPixels / size;\n   int remainder = numPixels % size;\n   int offset = 0;\n   if(rank < remainder)\n   {\n      numPixelsPerRank++;\n      offset = rank * numPixelsPerRank;\n   }\n   else\n   {\n      offset = (rank - remainder) * numPixelsPerRank;\n   }\n   std::array<size_t, 256> buffer;\n   buffer.fill(0);\n   for(int i = 0; i < numPixelsPerRank; i++)\n   {\n      buffer[image[offset + i]]++;\n   }\n   MPI_Allreduce(MPI_IN_PLACE, buffer.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n   for(int i = 0; i < 256; i++)\n   {\n      bins[i] += buffer[i];\n   }\n}",
            "// TODO\n    // Write your code here\n    bins.fill(0);\n    int mpi_size;\n    int mpi_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n    //MPI_Status status;\n\n    size_t image_size = image.size();\n    size_t count = image_size / mpi_size;\n    size_t offset = count * mpi_rank;\n\n    for (size_t i = offset; i < offset + count; i++) {\n        bins[image[i]] += 1;\n    }\n}",
            "size_t size=image.size();\n   int rank;\n   int numprocs;\n   MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   size_t chunk=size/numprocs;\n   size_t remainder=size%numprocs;\n   size_t offset=rank*chunk;\n   if(rank<remainder){\n     offset+=rank;\n   }else{\n     offset+=rank+remainder;\n   }\n   size_t width=1;\n   size_t height=1;\n   for(size_t i=0;i<image.size();i++){\n     if(image[i]!=0){\n       if(i%width==0){\n         height++;\n       }\n       width++;\n     }\n   }\n   size_t n=height*width;\n   std::vector<int> subimage;\n   for(size_t i=offset;i<offset+chunk;i++){\n     subimage.push_back(image[i]);\n   }\n   std::array<size_t, 256> localbins{};\n   for(size_t i=0;i<subimage.size();i++){\n     localbins[subimage[i]]++;\n   }\n   std::vector<int> sendbuf(chunk,0);\n   std::vector<int> recvbuf(chunk,0);\n   std::vector<int> tmpbins(localbins.size(),0);\n   if(rank!=0){\n     MPI_Send(localbins.data(),localbins.size(),MPI_INT,0,0,MPI_COMM_WORLD);\n   }\n   if(rank!=numprocs-1){\n     MPI_Recv(tmpbins.data(),tmpbins.size(),MPI_INT,rank+1,0,MPI_COMM_WORLD,MPI_STATUS_IGNORE);\n   }\n   std::partial_sum(localbins.begin(), localbins.end(), sendbuf.begin());\n   std::partial_sum(sendbuf.begin(), sendbuf.end(), recvbuf.begin());\n   std::partial_sum(tmpbins.begin(), tmpbins.end(), recvbuf.begin());\n   bins=recvbuf;\n }",
            "int numberOfProcs = 0;\n\t int procRank = 0;\n\t int numberOfPixelsPerProc = 0;\n\t int numberOfPixels = 0;\n\t int rank = 0;\n\n\t MPI_Comm_size(MPI_COMM_WORLD, &numberOfProcs);\n\t MPI_Comm_rank(MPI_COMM_WORLD, &procRank);\n\t numberOfPixels = image.size();\n\t numberOfPixelsPerProc = numberOfPixels / numberOfProcs;\n\t if (numberOfPixelsPerProc * numberOfProcs!= numberOfPixels) {\n\t\t std::cout << \"Number of pixels is not divisible by the number of processes\" << std::endl;\n\t }\n\n\t int* pixelData = new int[numberOfPixelsPerProc];\n\t MPI_Scatter(image.data(), numberOfPixelsPerProc, MPI_INT, pixelData, numberOfPixelsPerProc, MPI_INT, 0, MPI_COMM_WORLD);\n\n\t for (int i = 0; i < numberOfPixelsPerProc; i++) {\n\t\t bins[pixelData[i]]++;\n\t }\n\t delete[] pixelData;\n}",
            "int mpi_rank;\n    int mpi_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\n    if (image.size() == 0) {\n        if (mpi_rank == 0) {\n            std::fill_n(bins.begin(), bins.size(), 0);\n        }\n        return;\n    }\n\n    if (mpi_rank == 0) {\n        std::fill_n(bins.begin(), bins.size(), 0);\n    }\n\n    // get image size\n    size_t const num_rows = image.size() / 256;\n    size_t const num_cols = 256;\n\n    // get chunk of image\n    std::vector<int> chunk(num_rows * num_cols);\n    size_t const chunk_size = num_rows * num_cols;\n    size_t const start_row = num_rows * mpi_rank;\n    size_t const start_col = mpi_rank * num_cols;\n    for (size_t i = 0; i < chunk_size; ++i) {\n        chunk[i] = image[start_row + (i / num_cols) * num_cols + (i % num_cols)];\n    }\n\n    // count pixel intensities\n    std::array<size_t, 256> local_bins;\n    std::fill_n(local_bins.begin(), local_bins.size(), 0);\n    for (size_t i = 0; i < chunk.size(); ++i) {\n        ++local_bins[chunk[i]];\n    }\n\n    // distribute results\n    MPI_Reduce(&local_bins[0], &bins[0], 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "int nproc, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   if(rank == 0) {\n     std::fill(bins.begin(), bins.end(), 0);\n   }\n\n   //TODO\n   int i;\n   for(i = 0; i < image.size(); i++) {\n      if(image[i]!= 255) {\n         bins[image[i]] += 1;\n      }\n   }\n\n   MPI_Reduce(bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n }",
            "if (image.empty()) {\n        // Error!\n        return;\n    }\n\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int rank;\n    MPI_Comm_rank(comm, &rank);\n\n    // Determine the starting and ending indices of the portion of the image to work on\n    int imageSize = image.size();\n    int chunkSize = imageSize/numProcesses;\n    int offset = chunkSize*rank;\n    int end = (rank == (numProcesses - 1))? imageSize : offset + chunkSize;\n\n    // Fill the bins for the portion of the image we are working on\n    for (int i = offset; i < end; ++i) {\n        bins[image[i]]++;\n    }\n\n    // Reduce bins on all ranks to bins on rank 0\n    MPI_Reduce(bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// FIXME: Your code here\n   int n = image.size();\n   int rank, numProcesses;\n   MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int *temp = new int[n];\n   // distribute data to all ranks\n   if(rank == 0) {\n      MPI_Scatter(image.data(), n / numProcesses, MPI_INT, temp, n / numProcesses, MPI_INT, 0, MPI_COMM_WORLD);\n   }\n   else {\n      MPI_Scatter(image.data(), n / numProcesses, MPI_INT, temp, n / numProcesses, MPI_INT, 0, MPI_COMM_WORLD);\n   }\n\n   if(rank == 0) {\n      std::fill(bins.begin(), bins.end(), 0);\n   }\n\n   // perform parallel histogram operation\n   for(int i = 0; i < n / numProcesses; i++) {\n      MPI_Allreduce(&temp[i], &bins[image[i]], 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n   }\n\n   // free allocated memory\n   delete[] temp;\n}",
            "int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    // TODO: calculate local counts for each rank\n    std::array<size_t, 256> local_bins{};\n    for (int i = 0; i < image.size(); i++) {\n        local_bins[image[i]] += 1;\n    }\n    // TODO: calculate global counts for rank 0\n    if (world_rank == 0) {\n        bins = local_bins;\n    }\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "size_t total = 0;\n    for (int i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n    for (size_t i = 0; i < image.size(); i++) {\n        total++;\n        bins[image[i]]++;\n    }\n    return;\n}",
            "// Use MPI to count the number of pixels for each grayscale intensity.\n\n    // Each MPI process should have a copy of the image.\n    // Make a copy of image, distribute it to all MPI processes, and store it in `local_image`.\n    // If you want a nice, smooth border around the image, make sure that there is an empty border\n    // of size 1 around the edges of the image.\n    int mpi_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n    std::vector<int> local_image(image);\n\n    // Make a vector of counts to store the results in.\n    std::vector<size_t> counts(256, 0);\n    // Count the number of pixels in each grayscale intensity\n    for(auto intensity : local_image) {\n        counts[intensity] += 1;\n    }\n\n    // Add the counts from other MPI processes.\n    for(int i = 0; i < counts.size(); i++) {\n        size_t sum;\n        if(i % 256 == 0) {\n            sum = 0;\n            for(int j = 0; j < 256; j++) {\n                sum += counts[j];\n            }\n        }\n        MPI_Allreduce(&counts[i], &sum, 1, MPI_LONG, MPI_SUM, MPI_COMM_WORLD);\n        counts[i] = sum;\n    }\n\n    // Store the result in bins on rank 0.\n    if(mpi_rank == 0) {\n        for(int i = 0; i < 256; i++) {\n            bins[i] = counts[i];\n        }\n    }\n}",
            "// TODO: Your code here\n  \n  bins = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,",
            "// Fill in this function\n   bins.fill(0);\n   size_t n_pixels = image.size();\n   for(size_t i = 0; i < n_pixels; ++i) {\n     ++bins[image[i]];\n   }\n}",
            "int n = image.size();\n\tint rank;\n\tint size;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tint blocks = n / size;\n\tint leftover = n % size;\n\tint my_start = rank * blocks;\n\tint my_end = my_start + blocks + (rank < leftover? 1 : 0);\n\tint my_size = my_end - my_start;\n\tstd::vector<int> send_buffer(my_size);\n\tstd::copy(image.begin() + my_start, image.begin() + my_end, send_buffer.begin());\n\tstd::vector<int> recv_buffer(256);\n\tMPI_Allreduce(send_buffer.data(), recv_buffer.data(), 256, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\tfor (int i = 0; i < 256; i++) {\n\t\tbins[i] = recv_buffer[i];\n\t}\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    // Initialize to 0\n    for (int i=0; i<256; i++)\n      bins[i] = 0;\n\n    // Each rank is assigned a section of the array to be processed\n    int const chunk = image.size() / size;\n    int const remainder = image.size() % size;\n    int const start = rank * chunk;\n    int const end = (rank == size - 1)? (start + chunk + remainder) : (start + chunk);\n\n    // Count the number of pixels\n    for (int i = start; i < end; i++) {\n      if (image[i] >= 0 && image[i] <= 255)\n        bins[image[i]]++;\n    }\n}",
            "// MPI Communicator\n   int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // Splits the array in size/rank pieces\n   size_t n_pixels = image.size();\n   size_t slice_size = n_pixels / size;\n   size_t extra_pixels = n_pixels % size;\n\n   std::vector<int> image_chunk;\n   std::vector<size_t> counts_chunk;\n\n   // The first rank get an extra pixel\n   if (rank == 0) {\n     image_chunk = std::vector<int>(image.begin(), image.begin() + slice_size + extra_pixels);\n   } else {\n     image_chunk = std::vector<int>(image.begin() + slice_size * rank, image.begin() + (slice_size * rank) + slice_size);\n   }\n\n   // Calculate pixel counts\n   counts_chunk = pixelCounts(image_chunk);\n\n   // The first rank has the result\n   if (rank == 0) {\n     bins = std::array<size_t, 256>();\n   }\n\n   // Gather the results from the chunks and store in bins\n   MPI_Gather(counts_chunk.data(), slice_size + extra_pixels, MPI_UNSIGNED_LONG_LONG, bins.data(), slice_size + extra_pixels, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n     // Calculate the mean for each bin\n     size_t sum = 0;\n     for (int i = 0; i < 256; i++) {\n       sum += bins[i];\n     }\n     double mean = (double)sum / n_pixels;\n\n     // Calculate standard deviation for each bin\n     double sum_squares = 0;\n     for (int i = 0; i < 256; i++) {\n       sum_squares += pow((double)bins[i] - mean, 2);\n     }\n     double stdev = sqrt(sum_squares / n_pixels);\n\n     std::cout << \"mean: \" << mean << std::endl;\n     std::cout << \"stddev: \" << stdev << std::endl;\n   }\n\n }",
            "// TODO: Your code here\n   MPI_Status status;\n   int rank, nproc;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n   std::vector<int> image_local = image;\n   // int rank, size;\n   // MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   // MPI_Comm_size(MPI_COMM_WORLD, &size);\n   // int chunk_size = image.size()/size;\n   // int remainder = image.size()%size;\n   // std::vector<int> image_local(image.begin() + rank * chunk_size,\n   //                              (rank == size - 1)?\n   //                              image.end() :\n   //                              image.begin() + (rank + 1) * chunk_size);\n   // if (rank == size - 1) {\n   //   image_local.insert(image_local.end(), remainder, 0);\n   // }\n\n   for (int i = 0; i < image_local.size(); ++i) {\n     bins[image_local[i]]++;\n   }\n   MPI_Reduce(&bins[0], &bins[0], 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "int world_rank, world_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    int num_rows = image.size() / world_size;\n    int row_offset = world_rank * num_rows;\n    int num_local_rows = (world_rank == world_size - 1)? image.size() - row_offset : num_rows;\n    bins.fill(0);\n    for (int row = 0; row < num_local_rows; ++row) {\n        ++bins[image[row_offset + row]];\n    }\n    std::vector<int> global_bins(256);\n    MPI_Reduce(bins.data(), global_bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    bins = global_bins;\n}",
            "int rank, num_ranks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n   int image_size = image.size();\n\n   size_t num_bins = 256;\n   size_t num_each = image_size/num_ranks;\n   size_t extra = image_size%num_ranks;\n   int start = 0;\n\n   std::vector<int> local_image;\n   std::vector<size_t> local_bins(num_bins);\n\n   if (rank == 0)\n     local_image.assign(image.begin(), image.begin()+num_each);\n   else if (rank == num_ranks - 1)\n     local_image.assign(image.begin()+num_each*rank, image.begin()+num_each*rank+num_each+extra);\n   else\n     local_image.assign(image.begin()+num_each*rank, image.begin()+num_each*rank+num_each);\n\n   for (int i = 0; i < num_bins; i++)\n     local_bins[i] = 0;\n\n   for (int i = 0; i < local_image.size(); i++) {\n     local_bins[local_image[i]] += 1;\n   }\n\n   MPI_Allreduce(MPI_IN_PLACE, local_bins.data(), local_bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n   if (rank == 0)\n     bins = local_bins;\n }",
            "MPI_Status status;\n     int rank, size;\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n     std::array<size_t, 256> tmp;\n\n     if (rank == 0) {\n         for (int i = 0; i < 256; i++)\n             bins[i] = 0;\n     }\n\n     MPI_Bcast(bins.data(), 256, MPI_LONG_LONG, 0, MPI_COMM_WORLD);\n\n     for (int i = rank; i < image.size(); i += size)\n         bins[image[i]] += 1;\n\n     MPI_Reduce(bins.data(), tmp.data(), 256, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n     if (rank == 0)\n         bins = tmp;\n }",
            "int rank;\n   int numRanks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n   if (rank == 0) {\n     for (int j = 0; j < 256; ++j) {\n       bins[j] = 0;\n     }\n   }\n\n   MPI_Barrier(MPI_COMM_WORLD);\n\n   size_t imageLen = image.size();\n   size_t rankOffset = rank * imageLen / numRanks;\n   size_t rankLen = imageLen / numRanks;\n   if (rank == numRanks - 1) {\n     rankLen += imageLen % numRanks;\n   }\n   for (size_t i = 0; i < rankLen; ++i) {\n     bins[image[rankOffset + i]] += 1;\n   }\n }",
            "MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  for (auto& i:bins){\n    i=0;\n  }\n\n  int img_size=image.size();\n  int chunk_size=img_size/nproc;\n  int remain=img_size%nproc;\n  int chunk_start=0;\n  int chunk_end=0;\n\n  for(int i=0;i<nproc;i++){\n    if(i<remain){\n      chunk_end=chunk_start+(chunk_size+1);\n    }\n    else{\n      chunk_end=chunk_start+chunk_size;\n    }\n    for(int j=chunk_start;j<chunk_end;j++){\n      bins[image[j]]++;\n    }\n    chunk_start=chunk_end;\n  }\n\n  /*if(rank==0){\n    std::cout<<\"bins are : \";\n    for(int i=0;i<256;i++){\n      std::cout<<bins[i]<<\" \";\n    }\n    std::cout<<std::endl;\n  }*/\n  \n\n  if(rank==0){\n    for(int i=1;i<nproc;i++){\n      MPI_Recv(&bins[0], 256, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  }\n  else{\n    MPI_Send(&bins[0], 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n\n  /*if(rank==0){\n    std::cout<<\"bins are : \";\n    for(int i=0;i<256;i++){\n      std::cout<<bins[i]<<\" \";\n    }\n    std::cout<<std::endl;\n  }*/\n\n  return;\n}",
            "// Create a communicator and get the rank and the number of processes\n   int comm_rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);\n   int comm_size;\n   MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n\n   // Figure out how much work each process will do\n   int const my_start = comm_rank * image.size() / comm_size;\n   int const my_end = (comm_rank + 1) * image.size() / comm_size;\n\n   // Initialize the local vector of bin counts\n   std::array<size_t, 256> local_bins = {};\n\n   // Loop through the values in the local region of the image and count the pixels\n   for (int i = my_start; i < my_end; ++i) {\n     ++local_bins[image[i]];\n   }\n\n   // Add up the local counts to obtain the total count in the local region\n   // This is only needed on rank 0\n   if (comm_rank == 0) {\n     size_t total = 0;\n     for (int i = 0; i < local_bins.size(); ++i) {\n       total += local_bins[i];\n     }\n\n     // Broadcast the sum of the local counts to the other ranks\n     // This is done in a tree fashion, since most ranks have no work to do\n     for (int i = 1; i < comm_size; ++i) {\n       MPI_Send(&total, 1, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD);\n     }\n   } else {\n     // Receive the sum of the local counts on the root rank\n     size_t total;\n     MPI_Recv(&total, 1, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n     // Add to the local counts\n     for (int i = 0; i < local_bins.size(); ++i) {\n       local_bins[i] += total;\n     }\n   }\n\n   // Store the result in bins on rank 0\n   if (comm_rank == 0) {\n     bins = local_bins;\n   }\n }",
            "// TODO: Your code here\n\tsize_t world_size;\n\tMPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\tint rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Status status;\n\tint* recv_bins = new int[256];\n\tint* send_bins = new int[256];\n\t//for(int i=0; i<256; ++i)\n\t//\tsend_bins[i] = 0;\n\t\n\tfor(int i=0; i<image.size(); ++i)\n\t\t++send_bins[image[i]];\n\n\tMPI_Alltoall(send_bins, 256, MPI_INT, recv_bins, 256, MPI_INT, MPI_COMM_WORLD);\n\tif(rank == 0) {\n\t\tfor(int i=0; i<256; ++i)\n\t\t\tbins[i] = 0;\n\t\tfor(int i=0; i<world_size; ++i)\n\t\t\tfor(int j=0; j<256; ++j)\n\t\t\t\tbins[j] += recv_bins[j];\n\t}\n\n\tdelete[] send_bins;\n\tdelete[] recv_bins;\n\n}",
            "/* TODO: Your code goes here */\n    int n = image.size();\n    bins = std::array<size_t, 256>();\n    for(int i=0; i<256; ++i){\n        bins[i]=0;\n    }\n    for(int i=0; i<n; ++i){\n        bins[image[i]]+=1;\n    }\n }",
            "std::array<size_t, 256> partial_bins;\n\t// initialize to zero\n\tfor(int i = 0; i < 256; i++)\n\t{\n\t\tpartial_bins[i] = 0;\n\t}\n\n\tint image_size = image.size();\n\tint rank, num_procs;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n\t// use MPI to split image into partial_images\n\tint chunk_size = image_size / num_procs;\n\tstd::vector<int> partial_image;\n\tif (rank < (num_procs - 1)) {\n\t\tpartial_image.assign(image.begin() + rank * chunk_size, image.begin() + (rank + 1) * chunk_size);\n\t}\n\telse if (rank == (num_procs - 1)) {\n\t\tpartial_image.assign(image.begin() + rank * chunk_size, image.end());\n\t}\n\n\t// count pixels\n\tfor (int i = 0; i < partial_image.size(); i++) {\n\t\tpartial_bins[partial_image[i]] += 1;\n\t}\n\n\t// sum partial_bins to bins\n\tMPI_Allreduce(partial_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localN = image.size();\n    std::array<size_t, 256> localBins;\n\n    for (int i = 0; i < 256; ++i)\n        localBins[i] = 0;\n\n    for (size_t i = rank * localN / size; i < (rank + 1) * localN / size; ++i) {\n        ++localBins[image[i]];\n    }\n\n    std::array<size_t, 256> globalBins;\n    MPI_Reduce(localBins.data(), globalBins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = globalBins;\n    }\n}",
            "}",
            "for (int i = 0; i < 256; i++) {\n         bins[i] = 0;\n     }\n     MPI_Comm comm = MPI_COMM_WORLD;\n     MPI_Status status;\n     int rank;\n     MPI_Comm_rank(comm, &rank);\n     int total = image.size();\n     int local_count = total / comm_size;\n     int start_pos = rank * local_count;\n     int end_pos = start_pos + local_count;\n     for (int i = start_pos; i < end_pos; i++) {\n         bins[image[i]]++;\n     }\n     MPI_Reduce(MPI_IN_PLACE, &bins, 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, comm);\n     if (rank == 0) {\n         for (int i = 0; i < 256; i++) {\n             std::cout << bins[i] << std::endl;\n         }\n     }\n }",
            "size_t rank;\n    size_t size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    \n    // Compute the number of elements in each row and column\n    // for each rank\n    const size_t nrows = image.size() / size;\n    const size_t ncols = image.size() / size;\n    int* image_ptr = const_cast<int*>(image.data());\n    int* row_ptr = &image_ptr[rank * ncols];\n    int* col_ptr = &image_ptr[rank];\n    int* col_ptr_end = &image_ptr[rank + nrows];\n\n    // Compute the row and column ranges of the current rank\n    size_t col_start = ncols * rank;\n    size_t col_end = (rank == size - 1)? image.size() : col_start + ncols;\n    size_t row_start = rank * nrows;\n    size_t row_end = (rank == size - 1)? image.size() : row_start + nrows;\n\n    // Get the minimum and maximum values of the current rank\n    int min_val = image[0];\n    int max_val = image[0];\n    for (size_t i = col_start; i < col_end; i++) {\n        for (size_t j = row_start; j < row_end; j++) {\n            int val = *(col_ptr + i);\n            if (val < min_val) {\n                min_val = val;\n            }\n            if (val > max_val) {\n                max_val = val;\n            }\n        }\n    }\n\n    // Get the minimum and maximum values of all ranks\n    // (use MPI's Reduce function)\n    int min_val_all;\n    int max_val_all;\n    MPI_Reduce(&min_val, &min_val_all, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&max_val, &max_val_all, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    // Compute the offset value for each rank\n    // (use MPI's Scan function)\n    size_t offset = 0;\n    if (rank!= 0) {\n        MPI_Scan(&offset, &offset, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    }\n\n    // Compute the number of elements in each rank\n    size_t nrows_all = (max_val_all - min_val_all + 1) / 256;\n    size_t ncols_all = (col_end - col_start);\n    size_t n_all = nrows_all * ncols_all;\n    std::vector<int> image_all(n_all);\n\n    // Send the data to the other ranks\n    // (use MPI's Send/Recv function)\n    int *buf = &image_all[0];\n    MPI_Status status;\n    for (size_t i = 1; i < size; i++) {\n        if (rank == i) {\n            MPI_Send(col_ptr, ncols, MPI_INT, 0, i, MPI_COMM_WORLD);\n            MPI_Recv(buf, ncols_all, MPI_INT, 0, i, MPI_COMM_WORLD, &status);\n        } else if (rank == 0) {\n            MPI_Recv(buf, ncols, MPI_INT, i, i, MPI_COMM_WORLD, &status);\n            MPI_Send(col_ptr, ncols, MPI_INT, i, i, MPI_COMM_WORLD);\n        }\n        buf += ncols_all;\n    }\n\n    // Compute the histogram\n    for (size_t i = col_start; i < col_end; i++) {\n        for (size_t j = row_start",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    int imageSize = image.size();\n    int chunkSize = imageSize / size;\n    int leftover = imageSize % size;\n    int start = rank * chunkSize;\n    int end = start + chunkSize;\n    if(rank < leftover)\n        end += 1;\n    int counts[256] = {};\n    for(int i = start; i < end; i++)\n        counts[image[i]]++;\n    MPI_Allreduce(counts, bins.data(), 256, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n}",
            "bins.fill(0);\n    if (image.size()!= 100)\n        return;\n    // MPI_COMM_WORLD \u5168\u4f53\u30d7\u30ed\u30bb\u30b9\u306b\u5bfe\u3057\u3066\u306e\u901a\u4fe1\u306b\u7528\u3044\u308b\u901a\u4fe1\u968e\u5c64\u3092\u8868\u3059\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30bf\n    // MPI_INT \u6574\u6570\n    // \u5404\u30d7\u30ed\u30bb\u30b9\u3067\u53ce\u96c6\u3057\u305f\u30c7\u30fc\u30bf\u306e\u914d\u5217\n    int* results = new int[256];\n    results.fill(0);\n    // \u914d\u5217\u306e\u30b5\u30a4\u30ba\n    int array_size = 256;\n    // \u5404\u30d7\u30ed\u30bb\u30b9\u304c\u6271\u3046\u30c7\u30fc\u30bf\u306e\u30b5\u30a4\u30ba\n    int data_size = image.size() / MPI_COMM_WORLD.Get_size();\n    // \u5404\u30d7\u30ed\u30bb\u30b9\u304c\u6271\u3046\u30c7\u30fc\u30bf\n    int* data = new int[data_size];\n    // \u5404\u30d7\u30ed\u30bb\u30b9\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n    int rank = MPI_COMM_WORLD.Get_rank();\n    // \u30d7\u30ed\u30bb\u30b9\u6570\n    int num_procs = MPI_COMM_WORLD.Get_size();\n    // \u5404\u30d7\u30ed\u30bb\u30b9\u306e\u5148\u982d\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n    int start = rank * data_size;\n    // \u5404\u30d7\u30ed\u30bb\u30b9\u306e\u7d42\u4e86\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n    int end = start + data_size;\n    for (int i = start; i < end; i++) {\n        // \u5404\u30d7\u30ed\u30bb\u30b9\u3067\u306e\u51e6\u7406\n        data[i - start] = image[i];\n    }\n    // \u914d\u5217\u306e\u30c7\u30fc\u30bf\u3092\u8a08\u7b97\u3059\u308b\n    MPI_Allreduce(MPI_IN_PLACE, data, array_size, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    for (int i = 0; i < data_size; i++) {\n        // \u7d50\u679c\u306e\u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308b\n        results[data[i]]++;\n    }\n    // \u8a08\u7b97\u7d50\u679c\u3092\u683c\u7d0d\u3059\u308b\n    MPI_Gather(results, array_size, MPI_INT, bins.data(), array_size, MPI_INT, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (int i = 0; i < 256; i++) {\n            // \u7d50\u679c\u306e\u51fa\u529b\n            std::cout << bins[i] << std::endl;\n        }\n    }\n    // \u78ba\u4fdd\u3057\u305f\u30e1\u30e2\u30ea\u3092\u89e3\u653e\u3059\u308b\n    delete[] results;\n    delete[] data;\n    // \u3053\u3053\u306b\u7d50\u679c\u3092\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\n}",
            "// TODO: Your code here\n \n   int my_rank;\n   int num_proc;\n   MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n\n   // Initialize bins array to 0\n   for (int i = 0; i < 256; i++)\n   {\n     bins[i] = 0;\n   }\n\n   // count the number of pixels for each grayscale value\n   int local_pixels = image.size() / num_proc;\n   int pixel_count = 0;\n\n   if (my_rank == 0)\n   {\n     for (int i = 0; i < num_proc; i++)\n     {\n       MPI_Send(image.data() + local_pixels * i, local_pixels, MPI_INT, i, 0, MPI_COMM_WORLD);\n     }\n\n     for (int i = 1; i < num_proc; i++)\n     {\n       int local_counts[256];\n       MPI_Recv(local_counts, 256, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n       for (int j = 0; j < 256; j++)\n       {\n         bins[j] += local_counts[j];\n       }\n     }\n   }\n   else\n   {\n     MPI_Status status;\n     MPI_Recv(image.data(), local_pixels, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n     for (int i = 0; i < local_pixels; i++)\n     {\n       pixel_count += image[i];\n     }\n     MPI_Send(&pixel_count, 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n   }\n }",
            "// TODO: Your code here\n\n}",
            "int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    std::vector<size_t> partial(256, 0);\n    for (int i = 0; i < image.size(); ++i) {\n        partial[image[i]]++;\n    }\n\n    MPI_Gather(partial.data(), partial.size(), MPI_UNSIGNED_LONG,\n               bins.data(), partial.size(), MPI_UNSIGNED_LONG, 0,\n               MPI_COMM_WORLD);\n\n    if (world_rank == 0) {\n        std::fill(bins.begin(), bins.end(), 0);\n        for (int rank = 0; rank < world_size; ++rank) {\n            std::copy(bins.begin(), bins.end(), partial.begin());\n            MPI_Reduce(partial.data(), bins.data(), 256, MPI_UNSIGNED_LONG,\n                       MPI_SUM, rank, MPI_COMM_WORLD);\n        }\n    }\n}",
            "size_t local_size = image.size();\n  int num_procs;\n  MPI_Comm_size(MPI_COMM_WORLD,&num_procs);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n  size_t global_size = local_size * num_procs;\n  size_t offset = rank * local_size;\n\n  std::vector<int> image_proc(image.begin()+offset, image.begin()+offset+local_size);\n\n  bins.fill(0);\n\n  for (int i=0; i<image_proc.size(); i++) {\n      bins[image_proc[i]] += 1;\n  }\n\n  // sum bins on each proc, write result to bins on proc 0\n  if (rank == 0) {\n      std::array<size_t, 256> bins_tmp = bins;\n      MPI_Reduce(&bins_tmp[0], &bins[0], 256, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n  // sum bins on each proc, write result to bins on proc 0\n  else {\n      MPI_Reduce(&bins[0], &bins[0], 256, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n\n  //bins[0] = bins[0] / global_size;\n\n}",
            "int rank;\n    int size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (int i = 0; i < image.size(); ++i) {\n            ++bins[image[i]];\n        }\n    } else {\n        std::array<size_t, 256> tmp;\n        MPI_Status status;\n        MPI_Send(image.data(), image.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n        MPI_Recv(tmp.data(), tmp.size(), MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n        for (int i = 0; i < 256; ++i) {\n            bins[i] += tmp[i];\n        }\n    }\n}",
            "MPI_Group world_group, image_group;\n    MPI_Comm_group(MPI_COMM_WORLD, &world_group);\n    MPI_Comm_group(MPI_COMM_WORLD, &image_group);\n\n    // Get the group of ranks that have complete copies of the image.\n    std::vector<int> ranks(image.size());\n    for (size_t i=0; i<image.size(); i++) {\n        ranks[i] = i;\n    }\n\n    // Create an image_group.\n    MPI_Group img_group;\n    MPI_Group_incl(world_group, image.size(), ranks.data(), &img_group);\n\n    // Initialize bins.\n    MPI_Group world_group_bins;\n    MPI_Comm_group(MPI_COMM_WORLD, &world_group_bins);\n    std::array<size_t, 256> bins_temp;\n    for (size_t i=0; i<256; i++) {\n        bins_temp[i] = 0;\n    }\n    MPI_Bcast(bins_temp.data(), 256, MPI_LONG, 0, MPI_COMM_WORLD);\n\n    // Count.\n    MPI_Group_excl(world_group, 1, &image.size()-1, img_group, &img_group);\n    MPI_Group_free(&img_group);\n    int rank, nproc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Group_size(img_group, &nproc);\n    for (int i=0; i<nproc; i++) {\n        MPI_Group_rank(img_group, &i);\n        MPI_Group_translate_ranks(world_group, 1, &i, img_group, &i);\n        int val = image[i];\n        bins_temp[val]++;\n    }\n    MPI_Group_free(&img_group);\n\n    MPI_Group_free(&world_group);\n    MPI_Group_free(&world_group_bins);\n\n    // Update the result in bins.\n    MPI_Reduce(bins_temp.data(), bins.data(), 256, MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "// Fill this in.\n}",
            "}",
            "int rank, num_ranks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n   // Create a 2d decomposition with a row major process layout\n   int block_rows = 1;\n   int block_cols = 1;\n   int b[2];\n   if (num_ranks > 1) {\n     block_rows = sqrt(num_ranks);\n     block_cols = num_ranks/block_rows;\n     if (rank == 0) {\n       printf(\"Number of ranks = %d, block_rows = %d, block_cols = %d\\n\",\n\t      num_ranks, block_rows, block_cols);\n     }\n   }\n\n   int b_rows = image.size() / block_rows;\n   int b_cols = image.size() / block_cols;\n   int b_dims[2];\n   MPI_Dims_create(num_ranks, 2, b_dims);\n\n   int displs[num_ranks];\n   int recv_cnts[num_ranks];\n\n   int disp_cnt = 0;\n   int recv_cnt = 0;\n   for (int i = 0; i < block_cols; i++) {\n     for (int j = 0; j < block_rows; j++) {\n       displs[disp_cnt] = j * b_dims[1] + i;\n       recv_cnts[recv_cnt] = b_cols * b_rows;\n       disp_cnt++;\n       recv_cnt++;\n     }\n   }\n\n   MPI_Alltoall(recv_cnts, 1, MPI_INT, displs, 1, MPI_INT, MPI_COMM_WORLD);\n\n   int recv_size = 0;\n   for (int i = 0; i < num_ranks; i++) {\n     recv_size += recv_cnts[i];\n   }\n\n   std::vector<int> recv_image(recv_size);\n   MPI_Alltoallv(&image[0], recv_cnts, displs, MPI_INT,\n                 &recv_image[0], recv_cnts, displs, MPI_INT,\n                 MPI_COMM_WORLD);\n\n   for (int i = 0; i < num_ranks; i++) {\n     printf(\"displs[%d] = %d, recv_cnts[%d] = %d\\n\", i, displs[i], i, recv_cnts[i]);\n   }\n\n   for (int i = 0; i < num_ranks; i++) {\n     for (int j = 0; j < recv_cnts[i]; j++) {\n       printf(\"recv_image[%d] = %d\\n\", j, recv_image[i]);\n     }\n   }\n\n   for (int i = 0; i < block_rows; i++) {\n     for (int j = 0; j < block_cols; j++) {\n       int start = j * b_dims[1] + i;\n       int end = (j+1) * b_dims[1] + i;\n       for (int k = start; k < end; k++) {\n\t bins[recv_image[k]]++;\n       }\n     }\n   }\n\n   if (rank == 0) {\n     for (int i = 0; i < 256; i++) {\n       printf(\"bins[%d] = %d\\n\", i, bins[i]);\n     }\n   }\n\n   if (num_ranks > 1) {\n     // reduce bins on each rank to a single array on rank 0\n     // bins on each rank are stored in bins[rank][256]\n     std::vector<std::array<size_t, 256>> bins_vec(num_ranks);\n     int bins_size = bins.size();\n     for (int i = 0; i <",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int const chunk_size = image.size() / size;\n    int const chunk_offset = chunk_size * rank;\n\n    std::array<size_t, 256> local_bins;\n\n    for (size_t i = chunk_offset; i < chunk_offset + chunk_size; ++i) {\n        ++local_bins[image[i]];\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, local_bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n    bins = local_bins;\n}",
            "//YOUR CODE HERE\n    if (bins.size()!= 256) {\n        std::cout << \"Error: the size of bins is not 256.\" << std::endl;\n    }\n\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int total_size = image.size();\n    int part_size = total_size / size;\n\n    int offset = rank * part_size;\n    int end = offset + part_size;\n\n    int counter = 0;\n\n    for (int i = offset; i < end; i++) {\n        counter += image[i];\n    }\n\n    bins[counter] += 1;\n}",
            "size_t n_proc;\n   MPI_Comm_size(MPI_COMM_WORLD, &n_proc);\n   size_t rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   size_t chunk_size = image.size() / n_proc;\n   std::vector<int> chunk(chunk_size);\n   std::copy(image.begin() + rank * chunk_size, image.begin() + (rank + 1) * chunk_size, chunk.begin());\n   \n   // Initialize bins to 0\n   for (int i = 0; i < 256; i++) {\n     bins[i] = 0;\n   }\n   \n   // Count the number of pixels with each grayscale intensity in the local chunk.\n   for (size_t i = 0; i < chunk.size(); i++) {\n     bins[chunk[i]]++;\n   }\n\n   // Reduce the results of each rank to rank 0.\n   for (int i = 1; i < n_proc; i++) {\n     size_t offset = i * 256;\n     for (int j = 0; j < 256; j++) {\n       bins[j] += bins[offset + j];\n     }\n   }\n }"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "cuda",
        "prompt": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use CUDA to count in parallel. The kernel is launched with at least N threads.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n__global__ void pixelCounts(const int *image, size_t N, size_t[256] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n    if(tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}",
            "// TODO: fill this in\n}",
            "/*\n      Your code here\n      This function should count the number of pixels in image with each grayscale intensity\n      Store the results in `bins`.\n      Use CUDA to count in parallel. The kernel is launched with at least N threads.\n      Example:\n      \n      input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n      output: [0, 0, 2, 0, 1,...]\n    */\n    //get the number of threads in this kernel\n    int nThrds = blockDim.x * blockDim.y * blockDim.z;\n    int tid = threadIdx.x + blockDim.x * (threadIdx.y + blockDim.y * threadIdx.z);\n    int i, j, k;\n    int x = 0;\n    int y = 0;\n    for (i = tid; i < N; i+= nThrds) {\n        j = i % 256;\n        k = i / 256;\n        x = image[k];\n        y = bins[x];\n        bins[x] = y+1;\n    }\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (tid < N) {\n\t\tatomicAdd(&bins[image[tid]], 1);\n\t}\n}",
            "// TODO: Your code here.\n\n\n  // TODO: Your code here.\n  int local_pixel_count = 0;\n\n  int block_size = blockDim.x;\n  int block_num = blockIdx.x;\n  int thread_num = threadIdx.x;\n  int index = thread_num + block_size * block_num;\n  int gray_level = image[index];\n\n  __syncthreads();\n\n  atomicAdd(&bins[gray_level], 1);\n}",
            "// TODO\n}",
            "// TODO\n   // YOUR CODE HERE\n}",
            "// TODO: launch at least N threads\n    // TODO: calculate the pixel count for each grayscale intensity in image\n    // and store the results in `bins`\n    // Note that bins is a 256-element array\n\n    // example:\n    // int intensity = image[threadIdx.x];\n    // atomicAdd(&bins[intensity], 1);\n}",
            "// TODO\n\tfor (int i = 0; i < N; i++) {\n\t\tint index = image[i];\n\t\tatomicAdd(&bins[index], 1);\n\t}\n\treturn;\n}",
            "if (threadIdx.x >= N) {\n        return;\n    }\n    atomicAdd(&bins[image[threadIdx.x]], 1);\n}",
            "}",
            "//TODO: Replace this with your implementation\n    int index = blockIdx.x*blockDim.x+threadIdx.x;\n    int gray = 0;\n    int count = 0;\n    for (int i = 0; i < N; i++) {\n        gray = image[i];\n        count = atomicAdd(&bins[gray], 1);\n    }\n}",
            "// start threading\n    int tx = threadIdx.x;\n    int stride = blockDim.x;\n    int ty = threadIdx.y;\n    int by = blockDim.y;\n    int bx = threadIdx.z;\n    int gx = blockIdx.x;\n    int gy = blockIdx.y;\n    // end threading\n\n    // loop over the pixels in the input image\n    for (int i = ty; i < N; i += by) {\n        // find the location of the pixel\n        int row = i / 256;\n        int column = i - row * 256;\n        // find the intensity of the pixel and increment the histogram count\n        int pixel_value = image[i];\n        atomicAdd(&bins[pixel_value], 1);\n    }\n}",
            "// Your code here\n\n}",
            "int id = threadIdx.x;\n\n    if (id < N) {\n        atomicAdd(&bins[image[id]], 1);\n    }\n}",
            "// TODO: Your code here\n    __shared__ int buffer[BLOCK_SIZE];\n    int globalIndex = threadIdx.x + blockIdx.x * blockDim.x;\n    if(globalIndex < N){\n        int intensity = image[globalIndex];\n        atomicAdd(&bins[intensity], 1);\n    }\n}",
            "// TODO: Your code here\n}",
            "// TODO: add kernel code here\n\tint i = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (i < N) {\n\t\tatomicAdd(&bins[image[i]], 1);\n\t}\n}",
            "// TODO: Your code here\n\n}",
            "size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n    if (i >= N) return;\n    atomicAdd(&bins[image[i]], 1);\n}",
            "int tid = threadIdx.x;\n    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    for (int i = gid; i < N; i += blockDim.x * gridDim.x) {\n        int pixel = image[i];\n        atomicAdd(&bins[pixel], 1);\n    }\n}",
            "}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N) {\n        atomicAdd(&bins[image[idx]], 1);\n    }\n}",
            "// TODO\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n    if(tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}",
            "int thread_idx = blockDim.x * blockIdx.x + threadIdx.x;\n\tif (thread_idx < N) {\n\t\tbins[image[thread_idx]]++;\n\t}\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n\tif(idx < N) {\n\t\tatomicAdd(&bins[image[idx]], 1);\n\t}\n}",
            "//TODO: Implement me\n}",
            "size_t tid = threadIdx.x;\n\tsize_t i = blockIdx.x * blockDim.x + tid;\n\tif (i >= N) {\n\t\treturn;\n\t}\n\tatomicAdd(&bins[image[i]], 1);\n}",
            "// TODO: Your code here\n    size_t thread_id = threadIdx.x + blockDim.x * blockIdx.x;\n    size_t thread_count = blockDim.x * gridDim.x;\n    for(int i = thread_id; i < N; i += thread_count) {\n        bins[image[i]]++;\n    }\n    return;\n}",
            "// Insert your code here.\n\n\n}",
            "for (size_t i=0; i<N; i++) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "const size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    const size_t num_threads = blockDim.x * gridDim.x;\n    for (size_t i = tid; i < N; i += num_threads) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "size_t idx = blockDim.x * blockIdx.x + threadIdx.x;\n    if (idx < N) {\n        atomicAdd(&bins[image[idx]], 1);\n    }\n}",
            "__shared__ size_t s_bins[256];\n  __shared__ int thread_ids[1024];\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  if (tid < N)\n  {\n    atomicAdd(&s_bins[image[tid]], 1);\n  }\n  __syncthreads();\n  if (tid < 256)\n  {\n    bins[tid] = s_bins[tid];\n  }\n}",
            "// Add your code here\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n  if (x >= N || y >= N) return;\n\n  atomicAdd(&bins[image[y * N + x]], 1);\n}",
            "// TODO: replace 1 by N\n   int sum = 0;\n   for (int i = 0; i < N; i++) {\n      if (image[i] == i) {\n         sum++;\n      }\n   }\n   bins[0] = sum;\n}",
            "const int tx = threadIdx.x;\n    const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    const int gid = gridDim.x * blockDim.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}",
            "// TODO\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n   if (i < N) {\n      int p = image[i];\n      atomicAdd(&bins[p], 1);\n   }\n}",
            "// TODO: Implement pixelCounts\n}",
            "}",
            "// TODO\n  //\n  int *bin_num = (int*) malloc(N*sizeof(int));\n  bin_num[0] = image[0];\n  for (int i = 1; i < N; i++){\n    bin_num[i] = bin_num[i-1] + image[i];\n  }\n  for (int i = 0; i < 256; i++){\n    bins[i] = 0;\n    for (int j = 0; j < N; j++){\n      if (image[j] == i) bins[i]++;\n    }\n  }\n\n  //free(bin_num);\n}",
            "// TODO: Implement this function.\n    // This function should be launched with at least N threads.\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n   if (i < N) {\n      atomicAdd(&bins[image[i]], 1);\n   }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n\tint *b = &bins[0];\n\tif (i < N) {\n\t\tatomicAdd(&b[image[i]], 1);\n\t}\n}",
            "__shared__ int blockCounts[256];\n  if (threadIdx.x < 256) {\n    blockCounts[threadIdx.x] = 0;\n  }\n  __syncthreads();\n  int x = threadIdx.x;\n  while (x < N) {\n    atomicAdd(&blockCounts[image[x]], 1);\n    x += blockDim.x;\n  }\n  __syncthreads();\n  if (threadIdx.x < 256) {\n    atomicAdd(&bins[threadIdx.x], blockCounts[threadIdx.x]);\n  }\n  __syncthreads();\n}",
            "int i = threadIdx.x + blockDim.x * blockIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "int tid = threadIdx.x;\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  int total = 0;\n\n  if (gid < N) {\n    total = atomicAdd(&bins[image[gid]], 1);\n  }\n  __syncthreads();\n\n}",
            "// TODO: insert code here\n    int tid = threadIdx.x + blockDim.x * blockIdx.x;\n    int i = image[tid];\n    atomicAdd(&bins[i], 1);\n}",
            "// TODO: Your code here\n}",
            "const size_t id = threadIdx.x;\n  if (id < N) {\n    atomicAdd(&bins[image[id]], 1);\n  }\n}",
            "}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (idx < N) {\n\t\tatomicAdd(&bins[image[idx]], 1);\n\t}\n}",
            "// YOUR CODE HERE\n}",
            "// compute index of pixel in image\n  const int idx = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "unsigned int i = threadIdx.x + blockIdx.x * blockDim.x;\n    unsigned int stride = blockDim.x * gridDim.x;\n    for (; i < N; i += stride) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "// Shared memory to store counts per thread block\n  __shared__ unsigned int counts[THREADS];\n\n  // Thread index within thread block\n  unsigned int ti = threadIdx.x;\n\n  // Index of first element in the image array that the block operates on\n  unsigned int start = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Initialize shared memory\n  if (ti < THREADS) {\n    counts[ti] = 0;\n  }\n\n  // Wait for all threads to be initialized\n  __syncthreads();\n\n  // Perform the operation on shared memory\n  for (unsigned int i = start; i < N; i += blockDim.x * gridDim.x) {\n    int pix = image[i];\n    counts[pix]++;\n  }\n\n  // Wait for all blocks to be finished\n  __syncthreads();\n\n  // Write the result in global memory\n  for (unsigned int i = ti; i < 256; i += THREADS) {\n    atomicAdd(&bins[i], counts[i]);\n  }\n}",
            "int threadID = threadIdx.x + blockDim.x * blockIdx.x;\n  if (threadID < N) {\n    atomicAdd(&bins[image[threadID]], 1);\n  }\n}",
            "__shared__ size_t threadBins[256];\n    //...\n}",
            "const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx >= N) return;\n  ++bins[image[idx]];\n}",
            "int j = threadIdx.x;\n    // int k = threadIdx.y;\n    int I = blockIdx.x;\n    int offset = blockDim.x * gridDim.x * I;\n    // int offset = blockIdx.x * blockDim.x + threadIdx.x;\n    if (j < 256) {\n        for (size_t i = offset; i < offset + N; i++) {\n            if (j == image[i]) {\n                bins[j] += 1;\n                break;\n            }\n        }\n    }\n}",
            "unsigned int x = blockIdx.x*blockDim.x + threadIdx.x;\n\n    if (x < N) {\n        bins[image[x]]++;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    int idx = image[i];\n    atomicAdd(&bins[idx], 1);\n  }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n\n    for (size_t i = index; i < N; i += stride) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "// TODO: Count the number of pixels with each grayscale intensity.\n    // Store the results in `bins`.\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: launch at least N threads\n  int idx = threadIdx.x;\n  if (idx < N) {\n    bins[image[idx]] += 1;\n  }\n}",
            "// TODO\n    // Your code here\n}",
            "// TODO\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "// TODO: Implement\n\n}",
            "int threadID = blockIdx.x*blockDim.x + threadIdx.x;\n  int nThreads = blockDim.x*gridDim.x;\n  // int localBin[256];\n  // for (int i = 0; i < 256; i++)\n  //   localBin[i] = 0;\n\n  for (int i = threadID; i < N; i+=nThreads)\n    atomicAdd(&bins[image[i]], 1);\n}",
            "const int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N)\n        atomicAdd(bins + image[idx], 1);\n}",
            "int x = blockDim.x * blockIdx.x + threadIdx.x;\n\tif(x < N){\n\t\tbins[image[x]] += 1;\n\t}\n}",
            "// TODO: Replace this with a loop over the pixels in the image\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        atomicAdd(&bins[image[idx]], 1);\n    }\n}",
            "int tid = threadIdx.x;\n  if (tid < N) {\n    atomicAdd(&bins[image[tid]], 1);\n  }\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: replace the above comment with your solution.\n\n\tint idx = threadIdx.x + blockIdx.x * blockDim.x;\n\n\tif (idx >= N)\n\t\treturn;\n\n\tint value = image[idx];\n\n\tatomicAdd(&bins[value], 1);\n}",
            "/*\n    This is a stub for you to implement.\n  */\n  // TODO: Implement this function\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    // TODO\n    // Your code here.\n    // Make sure you increment bins[image[i]]\n}",
            "size_t idx = blockIdx.x*blockDim.x + threadIdx.x;\n    if (idx < N)\n        atomicAdd(&bins[image[idx]], 1);\n}",
            "// TODO: Implement.\n    return;\n}",
            "int pixel_idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int pixel_value = image[pixel_idx];\n    atomicAdd(&bins[pixel_value], 1);\n}",
            "// YOUR CODE GOES HERE\n}",
            "size_t t_id = threadIdx.x + blockDim.x * blockIdx.x;\n    if (t_id < N) {\n        atomicAdd(&bins[image[t_id]], 1);\n    }\n}",
            "// TODO: Your code here\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  int intensity = image[tid];\n  atomicAdd(&bins[intensity], 1);\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N) {\n        atomicAdd(&bins[image[idx]], 1);\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "size_t id = threadIdx.x + blockIdx.x * blockDim.x;\n  if (id < N) {\n    bins[image[id]]++;\n  }\n}",
            "__shared__ unsigned int block_counts[256];\n\n    int thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (thread_id < N) {\n        atomicAdd(&block_counts[image[thread_id]], 1);\n    }\n\n    __syncthreads();\n\n    if (thread_id < 256) {\n        atomicAdd(&bins[thread_id], block_counts[thread_id]);\n    }\n}",
            "if (blockIdx.x * blockDim.x + threadIdx.x < N) {\n\t\tbins[image[blockIdx.x * blockDim.x + threadIdx.x]]++;\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int count = 0;\n    for (; i < N; i += blockDim.x * gridDim.x) {\n        count += image[i];\n    }\n    atomicAdd(&bins[count], 1);\n}",
            "// TODO:\n    // You can assume that N is divisible by 1024\n\n    __shared__ int counts[256];\n\n    if (threadIdx.x < 256) {\n        counts[threadIdx.x] = 0;\n    }\n\n    int t_idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int b_idx = blockIdx.y;\n\n    while (t_idx < N) {\n        atomicAdd(&(counts[image[t_idx]]), 1);\n        t_idx += blockDim.x * gridDim.x;\n    }\n    __syncthreads();\n    bins[b_idx] = counts[b_idx];\n}",
            "// TODO\n}",
            "const int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "// TODO: Your code here\n  __shared__ size_t block[256];\n  __shared__ size_t shared_count;\n  if(threadIdx.x==0) {\n    shared_count = 0;\n  }\n  __syncthreads();\n  \n  size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n  \n  if(i < N) {\n    size_t index = image[i];\n    atomicAdd(&block[index], 1);\n  }\n  __syncthreads();\n  \n  if(threadIdx.x==0) {\n    for(int i = 0; i < 256; ++i) {\n      atomicAdd(&bins[i], block[i]);\n    }\n  }\n}",
            "unsigned int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i >= N) return;\n\n    unsigned int pixel = image[i];\n    atomicAdd(&bins[pixel], 1);\n\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx >= N) return;\n    atomicAdd(&bins[image[idx]], 1);\n}",
            "// TODO\n  int i = blockIdx.x*blockDim.x + threadIdx.x;\n  if (i>=N) return;\n  atomicAdd(&bins[image[i]],1);\n}",
            "__shared__ int localCounts[256];\n    __shared__ size_t totalCounts[1];\n    if(threadIdx.x < 256) {\n        localCounts[threadIdx.x] = 0;\n    }\n    __syncthreads();\n\n    size_t localCount = 0;\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    while (tid < N) {\n        localCount += image[tid];\n        tid += blockDim.x * gridDim.x;\n    }\n    localCounts[threadIdx.x] += localCount;\n\n    __syncthreads();\n\n    if(threadIdx.x == 0) {\n        size_t sum = 0;\n        for(int i = 0; i < 256; i++) {\n            sum += localCounts[i];\n        }\n        totalCounts[0] = sum;\n        bins[0] = sum;\n    }\n    __syncthreads();\n\n    if(threadIdx.x < 256) {\n        if(threadIdx.x > 0) {\n            bins[threadIdx.x] += bins[threadIdx.x - 1];\n        }\n    }\n}",
            "// Your code here\n}",
            "//TODO: Launch at least N threads\n\t//TODO: Implement the kernel\n\t//TODO: Return the pixel counts in the array `bins`\n}",
            "// TODO: Your code here\n}",
            "int i = threadIdx.x;\n    while (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n        i += blockDim.x;\n    }\n}",
            "int index = threadIdx.x + blockDim.x * blockIdx.x;\n    if(index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}",
            "// TODO: Your code here\n\t// You can use up to N threads\n\n\t// calculate the thread index and the corresponding pixel value\n\tint tid = threadIdx.x;\n\tint pixel = image[tid];\n\n\t// update the corresponding histogram entry for pixel value\n\tatomicAdd(&bins[pixel], 1);\n}",
            "// TODO: YOUR CODE HERE\n\n}",
            "int x = threadIdx.x + blockIdx.x * blockDim.x;\n    int y = threadIdx.y + blockIdx.y * blockDim.y;\n    int xy = x + y * blockDim.x * gridDim.x;\n    if (xy >= N) {\n        return;\n    }\n    int intensity = image[xy];\n    atomicAdd(&bins[intensity], 1);\n}",
            "// TODO: Implement the kernel\n    // TODO: use a single thread to read the value from the GPU\n    // TODO: increment the bin for that intensity\n    // TODO: use atomic add to increment the bin\n    // TODO: make sure to update the atomic counter for the next intensity \n    // and to wrap to 0 if needed\n    // TODO: make sure to properly synchronize threads\n\n    for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x)\n    {\n        int idx = image[i];\n        atomicAdd(&bins[idx], 1);\n    }\n}",
            "/*\n    Hint: use the shared memory to make the for loop run in parallel on the\n    GPU\n    */\n}",
            "for (int i = threadIdx.x; i < N; i+=blockDim.x) {\n    bins[image[i]] += 1;\n  }\n\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx >= N)\n        return;\n    atomicAdd(&bins[image[idx]], 1);\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}",
            "// TODO: Implement this function\n}",
            "int tid = threadIdx.x;\n    if(tid == 0) {\n        for (size_t i = 0; i < 256; i++) {\n            bins[i] = 0;\n        }\n    }\n    __syncthreads();\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        atomicAdd(&bins[image[idx]], 1);\n    }\n    __syncthreads();\n}",
            "// allocate a shared memory array of 256 ints to store pixel values\n    extern __shared__ int shm[];\n\n    // initialize pixel values to zero\n    for (int i = 0; i < 256; i++)\n        shm[i] = 0;\n\n    // TODO: Count the number of pixel values in the image\n    for (int i = threadIdx.x; i < N; i += blockDim.x)\n        atomicAdd(&shm[image[i]], 1);\n\n    // TODO: sum the pixel values in shared memory\n    for (int i = 1; i < blockDim.x; i *= 2) {\n        for (int j = 0; j < 256; j += (2 * i)) {\n            atomicAdd(&shm[j], shm[j + i]);\n        }\n    }\n\n    // TODO: copy the result to global memory\n    for (int i = threadIdx.x; i < 256; i += blockDim.x) {\n        bins[i] = shm[i];\n    }\n\n    __syncthreads();\n}",
            "// Insert your code here\n\n}",
            "// TODO\n}",
            "__shared__ int count[N];\n    int threadId = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadId;\n    int shared_i = threadIdx.x;\n    int bin;\n    int total = 0;\n    int result;\n    if (i < N) {\n        bin = image[i];\n        count[threadId] = bin;\n        __syncthreads();\n        for (int j = 1; j < blockDim.x; j *= 2) {\n            if (shared_i >= j) {\n                count[shared_i] += count[shared_i - j];\n            }\n            __syncthreads();\n        }\n        total += count[shared_i];\n        __syncthreads();\n    }\n    if (i == N - 1) {\n        for (int j = 0; j < 256; j++) {\n            result = total;\n            total = 0;\n            bins[j] = result;\n        }\n    }\n}",
            "// TODO: Fill in this function.\n\t// Get the index of the current thread.\n\tint i = blockIdx.x * blockDim.x + threadIdx.x;\n\t// Make sure the index is in bounds.\n\tif (i < N) {\n\t\t// Increase the corresponding intensity's count by 1.\n\t\tatomicAdd(&bins[image[i]], 1);\n\t}\n}",
            "// Your code here\n}",
            "// Replace with your code\n    const int x=threadIdx.x+blockIdx.x*blockDim.x;\n    const int y=threadIdx.y+blockIdx.y*blockDim.y;\n    if (x>=N || y>=N) return;\n    const int pixelValue=image[x*N+y];\n    atomicAdd(&bins[pixelValue],1);\n}",
            "int i = threadIdx.x + blockIdx.x*blockDim.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "int idx = blockIdx.x*blockDim.x + threadIdx.x;\n    if (idx < N) {\n        bins[image[idx]]++;\n    }\n}",
            "// TODO: YOUR CODE HERE\n    int id = threadIdx.x + blockIdx.x * blockDim.x;\n    bins[image[id]]++;\n}",
            "}",
            "// Get the index of the thread.\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Check if the index is less than the number of pixels.\n    if (idx < N) {\n        \n        // Check if the index is within the array bounds.\n        if (image[idx] <= 255) {\n            \n            // Increment the bin corresponding to the index of the thread.\n            atomicAdd(&(bins[image[idx]]), 1);\n        }\n    }\n}",
            "// TODO: count the number of pixels with each grayscale intensity\n  \n  // you can use the following for loop to get the intensity of the current pixel\n  for (int i=0; i<N; i++) {\n    if (image[i]!= 0) {\n      bins[image[i]]++;\n    }\n  }\n  \n}",
            "// Get pixel number and store it in a specific bin.\n    if (threadIdx.x < N) {\n        bins[image[threadIdx.x]]++;\n    }\n\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx >= N) {\n        return;\n    }\n\n    bins[image[idx]]++;\n}",
            "// Insert a kernel implementation here\n}",
            "int x = threadIdx.x + blockIdx.x*blockDim.x;\n  int y = threadIdx.y + blockIdx.y*blockDim.y;\n\n  if (x < N && y < N) {\n    int index = x + y*N;\n    int intensity = image[index];\n    atomicAdd(&bins[intensity], 1);\n  }\n}",
            "// Start with 0 for each intensity\n  int counter = 0;\n  // Get the grayscale intensity for the current thread\n  int intensity = image[blockIdx.x * blockDim.x + threadIdx.x];\n  // Do not count 0s\n  if (intensity!= 0) {\n    // For each thread add 1 to its intensity\n    atomicAdd(&bins[intensity], 1);\n    // For each thread increment the counter\n    atomicAdd(&counter, 1);\n  }\n  __syncthreads();\n  // Update the block and global counters\n  atomicAdd(&bins[256], counter);\n  atomicAdd(&bins[256+1], blockDim.x);\n}",
            "}",
            "// your code here\n  int idx = threadIdx.x + blockIdx.x*blockDim.x;\n  if (idx >= N) return;\n  int intensity = image[idx];\n  atomicAdd(&bins[intensity], 1);\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "// Your code here\n    // Fill in the code above\n    // CUDA 2\n    __shared__ size_t local_histogram[256];\n    int i = threadIdx.x;\n    int j = blockIdx.x;\n    local_histogram[i] = 0;\n    for (size_t n = j; n < N; n += blockDim.x) {\n        atomicAdd(&local_histogram[image[n]], 1);\n    }\n    __syncthreads();\n    bins[i] = 0;\n    for (int k = 0; k < 256; k++) {\n        atomicAdd(&bins[k], local_histogram[k]);\n    }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx >= N)\n    return;\n  atomicAdd(&bins[image[idx]], 1);\n}",
            "// Use shared memory to reduce the need for synchronization.\n  __shared__ int bincounts[256];\n  for (int idx = threadIdx.x; idx < 256; idx += blockDim.x) {\n    bincounts[idx] = 0;\n  }\n\n  // The image's dimensions are NxM, which we will assume is evenly divisible\n  // by the number of threads per block.\n  const int M = N/blockDim.x;\n  const int N_per_thread = N / blockDim.x;\n  for (int i = blockIdx.x * N_per_thread; i < N_per_thread*blockDim.x; i += blockDim.x) {\n    bincounts[image[i]]++;\n  }\n\n  // Synchronize thread blocks and accumulate values in bins.\n  __syncthreads();\n  for (int idx = threadIdx.x; idx < 256; idx += blockDim.x) {\n    bins[idx] += bincounts[idx];\n  }\n}",
            "__shared__ unsigned int pixelCount[256];\n    if (threadIdx.x == 0) {\n        for (int i=0; i<256; i++)\n            pixelCount[i] = 0;\n    }\n    __syncthreads();\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n\n    for (int i = idx; i < N; i += stride) {\n        atomicAdd(&pixelCount[image[i]], 1);\n    }\n    __syncthreads();\n    if (threadIdx.x == 0) {\n        for (int i=0; i<256; i++)\n            bins[i] = pixelCount[i];\n    }\n}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (index >= N) return;\n\t\n\tbins[image[index]] += 1;\n}",
            "const int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N)\n    atomicAdd(bins + image[i], 1);\n}",
            "// TODO: YOUR CODE HERE\n  int pixel;\n  int i=threadIdx.x;\n  int j=blockIdx.x;\n  if (j<N){\n  pixel = image[i+j*N];\n  atomicAdd(&bins[pixel], 1);\n  }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    int value = image[tid];\n    atomicAdd(&bins[value], 1);\n  }\n}",
            "}",
            "/* TODO: YOUR CODE HERE */\n  int thread_id = threadIdx.x;\n  for (int i = 0; i < 256; i++) {\n    bins[i] = 0;\n  }\n  for (size_t i = thread_id; i < N; i += blockDim.x) {\n    bins[image[i]]++;\n  }\n}",
            "//TODO: Implement this function\n}",
            "// TODO\n    for(int i=0;i<N;i++)\n    {\n        atomicAdd(&bins[image[i]],1);\n    }\n}",
            "// TODO: Your code here\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "// TODO\n}",
            "const int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    atomicAdd(&bins[image[tid]], 1);\n  }\n}",
            "// TODO: insert your code here\n  if (threadIdx.x < 256) {\n    bins[threadIdx.x] = 0;\n  }\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// TODO: fill in code here\n}",
            "// TODO\n}",
            "//TODO\n\treturn;\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if (idx < N) {\n    bins[image[idx]]++;\n  }\n}",
            "// TODO\n}",
            "int thread_id = blockIdx.x*blockDim.x + threadIdx.x;\n\tfor (int i=thread_id; i<N; i+=blockDim.x*gridDim.x) {\n\t\tbins[image[i]]++;\n\t}\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N) {\n        bins[image[idx]]++;\n    }\n}",
            "const int x = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (x >= N) {\n        return;\n    }\n\n    bins[image[x]]++;\n}",
            "const size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N) {\n        return;\n    }\n    int intensity = image[idx];\n    atomicAdd(&bins[intensity], 1);\n}",
            "// TODO: launch kernel with N threads\n\n    int i = threadIdx.x;\n    if (i < 256) {\n        int count = 0;\n        while (i < N) {\n            if (image[i] == i)\n                count++;\n            i += blockDim.x;\n        }\n        bins[i] = count;\n    }\n}",
            "// Initialize to 0.\n  bins[0] = 0;\n  for(int i = 0; i < 256; i++) {\n    bins[i] = 0;\n  }\n  for(int i = 0; i < N; i++) {\n    if(image[i] > 0) {\n      atomicAdd(&bins[image[i]], 1);\n    }\n  }\n}",
            "int pixelValue = image[threadIdx.x];\n\n    if (pixelValue >= 0 && pixelValue <= 255)\n    {\n        atomicAdd(&bins[pixelValue], 1);\n    }\n}",
            "//...\n}",
            "__shared__ int counts[256];\n  \n  if (threadIdx.x == 0) {\n    for (size_t i = 0; i < 256; i++) {\n      counts[i] = 0;\n    }\n  }\n  \n  __syncthreads();\n  \n  int threadIndex = blockIdx.x * blockDim.x + threadIdx.x;\n  while (threadIndex < N) {\n    __atomic_fetch_add(&counts[image[threadIndex]], 1, __ATOMIC_RELAXED);\n    threadIndex += blockDim.x * gridDim.x;\n  }\n  \n  __syncthreads();\n  \n  if (threadIdx.x == 0) {\n    for (size_t i = 0; i < 256; i++) {\n      bins[i] = counts[i];\n    }\n  }\n}",
            "// This is the number of threads per block, which we'll call BLOCK_SIZE\n    int BLOCK_SIZE = 256;\n\n    // Get the thread index\n    int tid = threadIdx.x;\n\n    // Get block index\n    int bid = blockIdx.x;\n\n    // Calculate the thread index in the image\n    int threadIndex = bid * BLOCK_SIZE + tid;\n\n    // Make sure we do not go out of bounds.\n    if (threadIndex < N) {\n        atomicAdd(&bins[image[threadIndex]], 1);\n    }\n}",
            "// Your code here\n}",
            "}",
            "int thread_idx = threadIdx.x;\n    int thread_idy = blockIdx.x;\n    int thread_block_size = blockDim.x;\n    int grid_size = gridDim.x;\n    int nblocks = (N + thread_block_size - 1) / thread_block_size;\n\n    for (int block_id = thread_idy; block_id < nblocks; block_id += grid_size) {\n        int block_start_idx = block_id * thread_block_size;\n        for (int i = thread_idx; i < thread_block_size; i += thread_block_size) {\n            if (block_start_idx + i < N) {\n                atomicAdd(&bins[image[block_start_idx + i]], 1);\n            }\n        }\n    }\n}",
            "// Get index of thread in image and assign to pixel variable\n\t// Each thread should load 1 pixel from the image\n\tint index = threadIdx.x + blockDim.x * blockIdx.x;\n\tint pixel = image[index];\n\n\t// Increment bin for pixel's intensity\n\tatomicAdd(&bins[pixel], 1);\n}",
            "// YOUR CODE HERE\n}",
            "// TODO\n}",
            "size_t i = threadIdx.x + blockIdx.x*blockDim.x;\n\n    if(i < N) {\n        bins[image[i]]++;\n    }\n}",
            "int idx = blockIdx.x*blockDim.x + threadIdx.x;\n    if (idx < N)\n        atomicAdd(&bins[image[idx]], 1);\n}",
            "// TODO\n    // write your code here\n    // TODO\n}",
            "// TODO\n}",
            "}",
            "//TODO: Count the number of pixels in the grayscale image\n  //HINT: Each thread counts one pixel.\n  \n}",
            "// TODO\n    // Fill in this function to count the pixels\n}",
            "// TODO: Your code here\n}",
            "//TODO: Your code here\n    int thread_id = threadIdx.x + blockDim.x * blockIdx.x;\n    if (thread_id >= N) return;\n    int value = image[thread_id];\n    atomicAdd(&bins[value], 1);\n}",
            "int intensity = image[blockIdx.x];\n\tatomicAdd(&bins[intensity], 1);\n}",
            "// TODO: Your code here\n}",
            "// TODO: YOUR CODE HERE\n}",
            "}",
            "__shared__ int partials[256];\n\n  size_t tid = threadIdx.x;\n  size_t i = blockIdx.x * blockDim.x + tid;\n  int intensity;\n\n  if (i < N) {\n    intensity = image[i];\n\n    // Each thread increments an index of its own.\n    atomicAdd(&partials[intensity], 1);\n  }\n\n  __syncthreads();\n\n  if (tid == 0) {\n    for (int i = 0; i < 256; i++) {\n      atomicAdd(&bins[i], partials[i]);\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bins[image[tid]]++;\n    }\n}",
            "for(int i=threadIdx.x; i<N; i+=blockDim.x) {\n    bins[image[i]]++;\n  }\n}",
            "// Your code here\n\n}",
            "// TODO\n    // (1) Get the index of the current thread\n    // (2) Get the grayscale intensity of the pixel at the given index\n    // (3) Store the pixel count in the corresponding bin in the result array\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use MPI and OpenMP to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// initialize all bins to zero\n   bins.fill(0);\n\n   // TODO: implement in parallel with MPI and OpenMP\n   // HINT: do not forget to take into account that rank 0 also has a complete copy of image\n   // HINT: every thread (thread id) owns a chunk of image\n   // HINT: every thread needs to count the number of pixels in it's chunk\n\n }",
            "bins.fill(0);\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int nThreads = omp_get_max_threads();\n\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n\n    //Gather all the bins to rank 0\n    std::vector<int> all_bins(256);\n\n    for (int i = 1; i < size; i++) {\n        int receive = 0;\n        MPI_Status status;\n        MPI_Recv(&receive, 1, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n        all_bins[receive]++;\n    }\n\n    //Reduce the bins on rank 0\n    for (int i = 0; i < 256; i++) {\n        for (int j = 0; j < size; j++) {\n            bins[i] += all_bins[i];\n        }\n    }\n\n}",
            "#pragma omp parallel for schedule(dynamic)\n   for (size_t i = 0; i < image.size(); ++i) {\n     int index = image[i];\n     #pragma omp atomic\n     bins[index] += 1;\n   }\n }",
            "int rank, num_proc;\n\t MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\t MPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n\t size_t my_image_size = image.size() / num_proc;\n\t size_t my_begin = my_image_size * rank;\n\t size_t my_end = my_begin + my_image_size;\n\t if (my_image_size * (rank + 1) > image.size())\n\t {\n\t\t my_end = image.size();\n\t }\n\t std::array<size_t, 256> local_bins{ 0 };\n\t #pragma omp parallel for\n\t for (size_t i = my_begin; i < my_end; i++)\n\t {\n\t\t local_bins[image[i]]++;\n\t }\n\t for (int i = 0; i < 256; i++)\n\t {\n\t\t bins[i] += local_bins[i];\n\t }\n\n }",
            "int nranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    std::vector<int> buf;\n    std::vector<int> image_local;\n    int size_image = image.size();\n    int size_local_image = size_image/nranks;\n    int size_buf = size_image/nranks;\n    int size_bins = 256;\n    int size_local_bins = size_bins/nranks;\n    int size_offset = size_bins%nranks;\n    int my_local_bins_start = size_local_bins*rank;\n    int my_local_bins_end = my_local_bins_start + size_local_bins;\n    int my_local_offset_bins_start = 0;\n    if(rank < size_offset){\n        my_local_offset_bins_start = size_offset*rank;\n    }\n    int my_local_offset_bins_end = my_local_offset_bins_start + size_offset;\n    bins.fill(0);\n    buf.resize(size_buf);\n    image_local.resize(size_local_image);\n    if(size_image%nranks!=0) {\n        std::cout << \"The size of the vector is not divisible by the number of ranks!\" << std::endl;\n        exit(1);\n    }\n\n    MPI_Scatter(image.data(), size_local_image, MPI_INT, image_local.data(), size_local_image, MPI_INT, 0, MPI_COMM_WORLD);\n    std::vector<int> local_bins(size_local_bins);\n    local_bins.resize(size_local_bins);\n    #pragma omp parallel for\n    for(int i = 0; i < size_local_image; i++){\n        local_bins[image_local[i]] += 1;\n    }\n    int offset = 0;\n    int offset_local = 0;\n    #pragma omp parallel for\n    for(int i = 0; i < size_local_bins; i++){\n        if(i < my_local_bins_end && i >= my_local_bins_start){\n            if(rank < size_offset){\n                bins[i] = local_bins[offset_local + my_local_offset_bins_start];\n            }\n            else{\n                bins[i] = local_bins[offset_local + my_local_offset_bins_end];\n            }\n        }\n        offset++;\n        offset_local++;\n    }\n    MPI_Gather(bins.data(), size_local_bins, MPI_INT, buf.data(), size_local_bins, MPI_INT, 0, MPI_COMM_WORLD);\n    if(rank == 0){\n        for(int i = 0; i < size_bins; i++){\n            bins[i] = 0;\n        }\n        for(int i = 0; i < size_ranks; i++){\n            for(int j = 0; j < size_local_bins; j++){\n                bins[j] += buf[i*size_local_bins + j];\n            }\n        }\n    }\n}",
            "// MPI and OpenMP initialization\n \tMPI_Comm comm;\n\tint comm_size, comm_rank;\n\tMPI_Init(NULL, NULL);\n\tcomm = MPI_COMM_WORLD;\n\tMPI_Comm_size(comm, &comm_size);\n\tMPI_Comm_rank(comm, &comm_rank);\n\n\tomp_set_num_threads(4);\n\tint image_size = 0;\n\tif(comm_rank == 0) {\n\t\timage_size = image.size();\n\t\t//bins = std::array<size_t, 256>(0);\n\t\tbins.fill(0);\n\t}\n\t// Broadcast image size to all ranks\n\tMPI_Bcast(&image_size, 1, MPI_INT, 0, comm);\n\t// Divide image in equal chunks and allocate them to MPI ranks\n\tint chunk_size = image_size/comm_size;\n\tint extra_chunk = image_size%comm_size;\n\n\tstd::vector<int> image_rank;\n\tfor(int i = comm_rank*chunk_size; i < (comm_rank+1)*chunk_size; i++) {\n\t\tif(i == (comm_rank+1)*chunk_size && extra_chunk > 0) {\n\t\t\timage_rank.insert(image_rank.end(), image.begin()+i, image.begin()+i+extra_chunk);\n\t\t} else {\n\t\t\timage_rank.insert(image_rank.end(), image.begin()+i, image.begin()+i+chunk_size);\n\t\t}\n\t}\n\t// Rank 0 is responsible for counting all the pixels\n\tif(comm_rank == 0) {\n\t\t#pragma omp parallel for\n\t\tfor(int i = 0; i < image_size; i++) {\n\t\t\tbins[image[i]]++;\n\t\t}\n\t\t// Synchronize all the ranks after the work is done\n\t\tMPI_Barrier(MPI_COMM_WORLD);\n\t}\n\telse {\n\t\t// Rank > 0 is responsible for counting their chunks of pixels\n\t\t#pragma omp parallel for\n\t\tfor(int i = 0; i < image_rank.size(); i++) {\n\t\t\tbins[image_rank[i]]++;\n\t\t}\n\t\t// Synchronize all the ranks after the work is done\n\t\tMPI_Barrier(MPI_COMM_WORLD);\n\t}\n\tMPI_Finalize();\n}",
            "// TODO: count pixel counts\n   int rank, comm_sz;\n   MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int proc_dim = sqrt(comm_sz);\n   if(proc_dim * proc_dim!= comm_sz) {\n     printf(\"Cannot initialize with %d processes. Should be a perfect square.\\n\", comm_sz);\n     MPI_Abort(MPI_COMM_WORLD, 1);\n   }\n\n   // Initialize bins\n   std::fill(bins.begin(), bins.end(), 0);\n\n   // Prepare for parallel execution\n   const int dim = 32;\n   size_t num_per_proc = image.size() / comm_sz;\n   size_t local_start = rank * num_per_proc;\n   size_t local_end = local_start + num_per_proc;\n   if(rank == comm_sz-1) {\n     local_end = image.size();\n   }\n   size_t start = 0;\n   size_t end = image.size();\n   if(rank == 0) {\n     start = 0;\n     end = num_per_proc;\n   }\n\n   #pragma omp parallel for\n   for(size_t i = local_start; i < local_end; i++) {\n     size_t index = image[i];\n     #pragma omp atomic\n     bins[index] += 1;\n   }\n\n   // Gather counts\n   MPI_Allreduce(MPI_IN_PLACE, bins.data(), 256, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n }",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int numThreads = omp_get_max_threads();\n    omp_set_num_threads(numThreads);\n\n    size_t N = image.size();\n    size_t chunk = N / size;\n    std::vector<int> tmp(chunk, 0);\n    for (int i = 0; i < numThreads; i++) {\n        std::vector<int> tmp(chunk, 0);\n        size_t begin = chunk * i;\n        size_t end = chunk * (i + 1);\n        if (i == numThreads - 1) {\n            end = N;\n        }\n        size_t num = end - begin;\n        #pragma omp parallel for\n        for (int j = 0; j < num; j++) {\n            tmp[j] = image[begin + j];\n        }\n        #pragma omp parallel for\n        for (int j = 0; j < num; j++) {\n            bins[tmp[j]] += 1;\n        }\n    }\n}",
            "int num_ranks, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // set up the number of pixels per rank\n    size_t N = image.size();\n    int chunk_size = N / num_ranks;\n    int remainder = N % num_ranks;\n\n    int start_rank = 0;\n    int end_rank = chunk_size;\n    if (rank < remainder) {\n        end_rank += 1;\n        start_rank += rank;\n    } else {\n        start_rank += remainder;\n        start_rank += (rank - remainder) * chunk_size;\n        end_rank += (rank - remainder) * chunk_size;\n    }\n\n    for (int i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n    //#pragma omp parallel for\n    for (int i = start_rank; i < end_rank; i++) {\n        bins[image[i]]++;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < num_ranks; i++) {\n            std::array<size_t, 256> tmp;\n            MPI_Status status;\n            MPI_Recv(&tmp, 256, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, &status);\n            for (int j = 0; j < 256; j++) {\n                bins[j] += tmp[j];\n            }\n        }\n        for (int i = 0; i < 256; i++) {\n            printf(\"%d \", bins[i]);\n        }\n    } else {\n        MPI_Send(bins.data(), 256, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n}",
            "if(MPI_Comm_size(MPI_COMM_WORLD, &num_ranks)!=MPI_SUCCESS) {\n    std::cerr << \"Error: could not get number of ranks from MPI_COMM_WORLD\" << std::endl;\n  }\n\n  if(MPI_Comm_rank(MPI_COMM_WORLD, &rank_id)!=MPI_SUCCESS) {\n    std::cerr << \"Error: could not get rank id from MPI_COMM_WORLD\" << std::endl;\n  }\n\n  if(image.size()%num_ranks!=0) {\n    std::cerr << \"Error: num_ranks does not divide image.size()\" << std::endl;\n  }\n\n  std::array<size_t, 256> bins_local;\n\n  // Setup OpenMP for each thread to have its own local pixel counts\n  omp_set_num_threads(num_ranks);\n\n  #pragma omp parallel\n  {\n    size_t tid = omp_get_thread_num();\n    size_t stride = image.size()/num_ranks;\n\n    for(size_t i=tid*stride; i<(tid+1)*stride; i++) {\n      if(i<image.size()) {\n        bins_local[image[i]]++;\n      }\n    }\n  }\n\n  if(rank_id==0) {\n    for(int i=0; i<256; i++) {\n      std::fill_n(bins.begin()+i, 256-i, 0);\n    }\n\n    for(int i=0; i<256; i++) {\n      bins[i] = bins[i]+bins_local[i];\n    }\n\n    // Copy the pixel counts on the first rank to the master rank's vector\n    MPI_Gather(bins_local.data(), 256, MPI_UNSIGNED_LONG_LONG, bins.data(), 256, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  } else {\n    // All other ranks have an incomplete copy of the pixel counts.\n    // They can only send their own to the master rank\n    MPI_Gather(bins_local.data(), 256, MPI_UNSIGNED_LONG_LONG, bins.data(), 256, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  }\n}",
            "int N = (int)image.size();\n  int proc_id = 0;\n  int num_proc = 1;\n\n  // MPI initialization\n  MPI_Comm_rank(MPI_COMM_WORLD, &proc_id);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n\n  int chunk = N / num_proc;\n\n  // Get the number of elements in each processor\n  if (proc_id == 0) {\n    bins.fill(0);\n  }\n\n  if (proc_id == num_proc - 1) {\n    chunk = N - (num_proc - 1) * chunk;\n  }\n\n  // Compute histogram\n  #pragma omp parallel for\n  for (int i = 0; i < chunk; i++) {\n    if (proc_id == 0) {\n      bins[image[i]]++;\n    }\n  }\n\n  // MPI synchronization\n  MPI_Allreduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n}",
            "std::array<size_t, 256> binslocal;\n\n\t//set to 0\n\tfor (int i = 0; i < 256; i++)\n\t{\n\t\tbinslocal[i] = 0;\n\t}\n\t\n\t//count the number of pixels in image with each grayscale intensity\n\t//binslocal is the local count\n\tfor (int i = 0; i < image.size(); i++)\n\t{\n\t\tbinslocal[image[i]]++;\n\t}\n\n\t//reduce\n\tfor (int i = 0; i < 256; i++)\n\t{\n\t\tbins[i] = 0;\n\t}\n\t\n\t//only rank 0's bins should be the global bins\n\tif (mpi::rank() == 0)\n\t{\n\t\tfor (int i = 0; i < mpi::num_ranks(); i++)\n\t\t{\n\t\t\tMPI_Send(&binslocal[0], 256, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD);\n\t\t}\n\t}\n\telse\n\t{\n\t\tMPI_Status status;\n\t\tMPI_Recv(&binslocal[0], 256, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD, &status);\n\t}\n\t\n\t//bins are now the global counts\n\t\n\t//MPI_Barrier(MPI_COMM_WORLD);\n\t//only rank 0's bins should be the global bins\n\t//MPI_Reduce(&binslocal[0], &bins[0], 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\t//bins are now the global counts\n\n\t\n}",
            "const int num_procs = omp_get_max_threads();\n\n    size_t block_size = (image.size() + num_procs - 1) / num_procs;\n\n    // Calculate local histogram\n    std::array<size_t, 256> local_bins;\n    #pragma omp parallel for\n    for(size_t i = 0; i < block_size; ++i) {\n        if(i + omp_get_thread_num() < image.size()) {\n            ++local_bins[image[i + omp_get_thread_num()]];\n        }\n    }\n\n    // Reduce local histograms to obtain global histogram\n    std::array<size_t, 256> global_bins;\n    for(int i = 0; i < num_procs; ++i) {\n        // Reduce each thread's histogram to master thread\n        if(i == 0) {\n            global_bins = local_bins;\n        } else {\n            #pragma omp parallel for\n            for(int j = 0; j < 256; ++j) {\n                global_bins[j] += local_bins[j];\n            }\n        }\n    }\n\n    if(omp_get_thread_num() == 0) {\n        bins = global_bins;\n    }\n\n    return;\n}",
            "#ifdef __USE_MPI__\n \n  const int size = image.size();\n  const int rank = omp_get_thread_num();\n  const int n_ranks = omp_get_num_threads();\n  MPI_Status status;\n\n  if (n_ranks > 1) {\n    const int n_pixels_each = size / n_ranks;\n    const int n_pixels_remain = size % n_ranks;\n    std::vector<int> temp;\n    if (rank == 0) {\n      temp.resize(size);\n    }\n    MPI_Scatter(image.data(), n_pixels_each, MPI_INT, temp.data(), n_pixels_each, MPI_INT, 0, MPI_COMM_WORLD);\n    if (rank < n_pixels_remain) {\n      temp.push_back(image[rank + n_pixels_each * n_ranks]);\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n    for (int i = 0; i < temp.size(); i++) {\n      bins[temp[i]]++;\n    }\n  } else {\n    for (int i = 0; i < size; i++) {\n      bins[image[i]]++;\n    }\n  }\n\n#pragma omp parallel for\n  for (int i = 0; i < 256; i++) {\n    if (bins[i] == 0)\n      bins[i] = 0;\n  }\n\n#endif\n}",
            "// TODO\n\n    int num_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int x_dim = 200;\n    int y_dim = 200;\n    std::vector<int> sendBuffer(x_dim*y_dim);\n    std::vector<int> recvBuffer(x_dim*y_dim);\n    int x_part = x_dim/num_ranks;\n    int y_part = y_dim/num_ranks;\n    int x_remainder = x_dim%num_ranks;\n    int y_remainder = y_dim%num_ranks;\n\n\n    if (rank == 0) {\n        for (int i=0; i<image.size(); i++) {\n            if (image[i]<256) {\n                bins[image[i]]++;\n            }\n        }\n    }\n\n    MPI_Bcast(bins.data(), 256, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n    int start_x = rank * x_part;\n    int end_x = start_x + x_part;\n    if (rank == num_ranks - 1) {\n        end_x += x_remainder;\n    }\n    int start_y = 0;\n    int end_y = y_part;\n    if (rank == num_ranks - 1) {\n        end_y += y_remainder;\n    }\n    if (rank == 0) {\n        for (int i=start_x; i<end_x; i++) {\n            for (int j=start_y; j<end_y; j++) {\n                sendBuffer[i*y_dim+j] = image[i*y_dim+j];\n            }\n        }\n    }\n    MPI_Gather(sendBuffer.data(), x_part*y_part, MPI_INT, recvBuffer.data(), x_part*y_part, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i=0; i<num_ranks; i++) {\n            int start_x = i*x_part;\n            int end_x = start_x + x_part;\n            if (i == num_ranks - 1) {\n                end_x += x_remainder;\n            }\n            int start_y = 0;\n            int end_y = y_part;\n            if (i == num_ranks - 1) {\n                end_y += y_remainder;\n            }\n            for (int j=start_x; j<end_x; j++) {\n                for (int k=start_y; k<end_y; k++) {\n                    if (recvBuffer[j*y_dim+k]<256) {\n                        bins[recvBuffer[j*y_dim+k]]++;\n                    }\n                }\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: Your code here\n  //...\n}",
            "#pragma omp parallel\n\tint rank = omp_get_thread_num();\n\tint nthreads = omp_get_num_threads();\n\tint nranks = omp_get_num_procs();\n\tint n = image.size();\n\t\n\tint part_len = n / nranks;\n\tint part_len_extra = n % nranks;\n\tint start_index = rank * part_len + rank * part_len_extra;\n\tint end_index = start_index + part_len + part_len_extra;\n\tint i;\n\t\n\tif (rank == 0) {\n\t\t#pragma omp parallel for\n\t\tfor (i = 0; i < 256; i++) {\n\t\t\tbins[i] = 0;\n\t\t}\n\t}\n\t\n\tfor (i = start_index; i < end_index; i++) {\n\t\t#pragma omp atomic\n\t\tbins[image[i]]++;\n\t}\n\tMPI_Barrier(MPI_COMM_WORLD);\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < nranks; i++) {\n\t\t\tMPI_Recv(&bins[i], 256, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t}\n\t} else {\n\t\tMPI_Send(&bins[0], 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n\t}\n\n}",
            "#pragma omp parallel for\n        for(int x=0; x<image.size(); x++){\n            bins[image[x]]++;\n        }\n    }",
            "// TODO\n }",
            "int size = bins.size();\n\tint rank = 0;\n\tint mpi_size = 1;\n\tint mpi_rank = 0;\n\tMPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n\tint blockSize = image.size() / mpi_size;\n\tint blocksRemainder = image.size() % mpi_size;\n\tint start = blockSize * mpi_rank;\n\tint end = start + blockSize;\n\tif (mpi_rank < blocksRemainder)\n\t{\n\t\tend++;\n\t}\n\tif (mpi_rank == 0)\n\t{\n\t\tomp_set_num_threads(omp_get_num_procs());\n#pragma omp parallel for\n\t\tfor (int i = 0; i < size; i++)\n\t\t{\n\t\t\tbins[i] = 0;\n\t\t}\n\t}\n\n\tfor (int i = start; i < end; i++)\n\t{\n\t\tomp_set_num_threads(omp_get_num_procs());\n#pragma omp parallel for\n\t\tfor (int j = 0; j < size; j++)\n\t\t{\n\t\t\tif (j == image[i])\n\t\t\t{\n\t\t\t\tbins[j]++;\n\t\t\t}\n\t\t}\n\t}\n}",
            "// Count the number of pixels with each intensity value\n   // (not really necessary in this example)\n   //bins.fill(0);\n   //for (int i = 0; i < image.size(); ++i) {\n   //  ++bins[image[i]];\n   //}\n\n   // MPI part\n   int num_procs;\n   int rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int chunk_size = image.size() / num_procs;\n   int start = chunk_size * rank;\n   int end = chunk_size * (rank + 1);\n   if (rank == num_procs - 1) end = image.size();\n   std::vector<int> sub_image(image.begin() + start, image.begin() + end);\n   std::array<size_t, 256> local_counts{};\n   #pragma omp parallel for\n   for (int i = 0; i < sub_image.size(); ++i) {\n     ++local_counts[sub_image[i]];\n   }\n   std::array<size_t, 256> global_counts{};\n   MPI_Reduce(local_counts.data(), global_counts.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   bins = global_counts;\n}",
            "size_t num_images;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_images);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int num_threads = 1;\n    int thread_id;\n    #pragma omp parallel private(thread_id)\n    {\n        thread_id = omp_get_thread_num();\n        num_threads = omp_get_num_threads();\n    }\n\n    size_t image_length = image.size();\n\n    size_t num_pixels = image_length / num_threads;\n\n    size_t offset = num_pixels * thread_id;\n    size_t local_image_length = num_pixels;\n    if (thread_id == num_threads - 1) {\n        local_image_length = image_length - (num_pixels * (num_threads - 1));\n    }\n\n    std::array<size_t, 256> local_bins{};\n\n    // Each thread iterates over their portion of the image and counts the pixels\n    #pragma omp parallel for reduction(+: local_bins)\n    for (size_t i = 0; i < local_image_length; i++) {\n        size_t value = image[offset + i];\n        local_bins[value]++;\n    }\n\n    std::array<size_t, 256> global_bins{};\n    // Collect the results from each thread\n    if (rank == 0) {\n        MPI_Allreduce(local_bins.data(), global_bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n        bins = global_bins;\n    }\n}",
            "// MPI variables\n    int rank;\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // OpenMP variables\n    int num_threads = 8;\n    int thread_id;\n\n    // OpenMP initialization\n    omp_set_num_threads(num_threads);\n\n    // create array of arrays to store the bins\n    std::array<std::array<size_t, 256>, size> allbins;\n    #pragma omp parallel for private(thread_id)\n    for(size_t i = 0; i < image.size(); ++i) {\n        thread_id = omp_get_thread_num();\n        allbins[thread_id][image[i]]++;\n    }\n\n    // Gather the bins\n    for(int i = 0; i < size; ++i) {\n        if(rank == i) {\n            for(int j = 0; j < 256; ++j) {\n                for(int k = 0; k < num_threads; ++k) {\n                    bins[j] += allbins[k][j];\n                }\n            }\n        }\n        MPI_Barrier(MPI_COMM_WORLD);\n    }\n}",
            "#pragma omp parallel for\n    for(int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "#ifdef PARALLEL\n#pragma omp parallel\n#endif\n    {\n#ifdef PARALLEL\n        int thread_id = omp_get_thread_num();\n#endif\n        std::array<size_t, 256> thread_bins;\n        for (int i = 0; i < 256; i++) {\n            thread_bins[i] = 0;\n        }\n#pragma omp for\n        for (int i = 0; i < image.size(); i++) {\n            thread_bins[image[i]] += 1;\n        }\n#pragma omp single\n        {\n            for (int i = 0; i < 256; i++) {\n                bins[i] += thread_bins[i];\n            }\n        }\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n     #pragma omp parallel for\n     for (int i=0; i<image.size(); ++i) {\n         ++bins[image[i]];\n     }\n }",
            "size_t n_pixels = image.size();\n   // TODO: use MPI and OpenMP to count the number of pixels with each intensity.\n   // Store the results in bins.\n   // Note: every rank has a complete copy of image. The result is stored in bins on rank 0.\n}",
            "int rank;\n     int world_size;\n     MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n     // get the size of each image\n     int image_size = image.size();\n     int chunk_size = image_size / world_size;\n     int remainder = image_size % world_size;\n\n     // figure out my start and end index\n     int start = 0;\n     int end = 0;\n     if (rank < remainder) {\n         start = rank * (chunk_size + 1);\n         end = start + chunk_size;\n     } else {\n         start = (remainder + rank) * chunk_size + remainder;\n         end = start + chunk_size - 1;\n     }\n\n     // the vector to store the counts\n     std::vector<size_t> chunk_bins(256);\n\n     // set the bins for this chunk to 0\n     memset(chunk_bins.data(), 0, sizeof(size_t) * 256);\n\n     // compute the pixel counts\n     #pragma omp parallel for\n     for (int i = start; i <= end; i++) {\n         chunk_bins[image[i]]++;\n     }\n\n     // gather the bins from every rank\n     MPI_Gather(chunk_bins.data(), 256, MPI_LONG_LONG,\n                bins.data(), 256, MPI_LONG_LONG, 0, MPI_COMM_WORLD);\n\n }",
            "// TODO\n }",
            "// TODO\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "int n_procs;\n    MPI_Comm_size(MPI_COMM_WORLD, &n_procs);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int n_pixels = image.size();\n    int n_per_proc = n_pixels / n_procs;\n\n    int start = rank * n_per_proc;\n    int end = start + n_per_proc;\n\n    // std::array<size_t, 256> bins(256, 0);\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        int pix = image[i];\n        bins[pix]++;\n    }\n\n    MPI_Gather(&bins, 256, MPI_UNSIGNED_LONG_LONG, &bins, 256, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "int n = image.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    const int nworkers = size;\n    const int nperworker = n / nworkers;\n    const int nleft = n - nworkers * nperworker;\n    int nstart = rank * nperworker;\n    if (rank < nleft) {\n        nstart += rank;\n    } else {\n        nstart += nleft;\n    }\n    int nend = nstart + nperworker;\n    if (rank < nleft) {\n        nend++;\n    } else {\n        nend += nleft;\n    }\n    int nlocal = nend - nstart;\n\n    #pragma omp parallel for\n    for (int i = nstart; i < nend; i++) {\n        bins[image[i]] += 1;\n    }\n\n}",
            "size_t nRanks;\n     int myRank;\n\n     MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n     MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n\n     int chunkSize = image.size() / nRanks;\n\n     int myStart = myRank * chunkSize;\n     int myEnd = (myRank + 1) * chunkSize;\n\n     if (myRank == nRanks - 1) {\n         myEnd = image.size();\n     }\n\n     for (int i = myStart; i < myEnd; i++) {\n         int grayLevel = image.at(i);\n         bins.at(grayLevel)++;\n     }\n }",
            "int rank;\n     int nranks;\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n     // Set up thread support\n     const int NUM_THREADS = 4;\n     omp_set_num_threads(NUM_THREADS);\n     #pragma omp parallel\n     {\n         int tid = omp_get_thread_num();\n         printf(\"Thread %d starting\\n\", tid);\n     }\n\n     // Get image dimensions\n     size_t width, height;\n     if (rank == 0) {\n         width = image.size();\n         height = 1;\n     } else {\n         MPI_Bcast(&width, 1, MPI_LONG_LONG_INT, 0, MPI_COMM_WORLD);\n         height = 1;\n     }\n\n     // Get image size\n     size_t size = width * height;\n\n     // Set up image subdivision and split work\n     int chunk = size / nranks;\n     int remainder = size % nranks;\n     int start = chunk * rank + std::min(rank, remainder);\n     int end = chunk * (rank + 1) + std::min(rank + 1, remainder);\n     end = std::min(end, size);\n\n     // Allocate histogram buffer\n     std::vector<size_t> hist(256, 0);\n\n     // Work loop\n     for (size_t i = start; i < end; ++i) {\n         // Count pixel\n         int intensity = image[i];\n         hist[intensity]++;\n     }\n\n     // Reduce histogram to rank 0\n     if (rank!= 0) {\n         MPI_Reduce(MPI_IN_PLACE, hist.data(), 256, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n     } else {\n         MPI_Reduce(hist.data(), bins.data(), 256, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n     }\n\n     //",
            "// TODO: Your code here\n\n\n\n }",
            "int nx = 1;\n    int ny = 1;\n    size_t nz = image.size();\n\n    bins.fill(0);\n#pragma omp parallel for num_threads(2)\n    for (int j = 0; j < nx; j++) {\n        for (int i = 0; i < ny; i++) {\n            for (int k = 0; k < nz; k++) {\n                bins[image[k]]++;\n            }\n        }\n    }\n}",
            "}",
            "// TODO\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int image_sz = image.size();\n\n    if (rank == 0) {\n        for (int i = 0; i < image_sz; i++) {\n            bins[image[i]]++;\n        }\n    }\n\n    // bins on other ranks are 0.\n    MPI_Bcast(&(bins[0]), 256, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "//TODO: Implement this function.\n   // Hint: you may need a nested parallel region.\n\n   int num_ranks;\n   int rank;\n   int chunk_size;\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int num_pixels = image.size();\n   chunk_size = num_pixels / num_ranks;\n\n   int left_bound = chunk_size * rank;\n   int right_bound = chunk_size * (rank + 1);\n   int chunk_height = right_bound - left_bound;\n\n   if (rank == num_ranks - 1) {\n     right_bound = num_pixels;\n   }\n\n   if (rank == 0) {\n     for (int i = 0; i < 256; ++i) {\n       bins[i] = 0;\n     }\n   }\n\n   #pragma omp parallel num_threads(num_ranks)\n   {\n     #pragma omp for\n     for (int i = left_bound; i < right_bound; ++i) {\n       ++bins[image[i]];\n     }\n   }\n }",
            "int rank, size;\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n     #pragma omp parallel for\n     for (int i = 0; i < 256; i++) {\n         bins[i] = 0;\n     }\n     if (rank == 0) {\n         #pragma omp parallel for\n         for (int i = 0; i < image.size(); i++) {\n             bins[image[i]]++;\n         }\n     }\n     else {\n         #pragma omp parallel for\n         for (int i = 0; i < image.size(); i++) {\n             bins[image[i]]++;\n         }\n     }\n     if (rank == 0) {\n         for (int i = 0; i < 256; i++) {\n             if (bins[i] > 0)\n                 std::cout << \"Grayscale intensity \" << i << \" appears \" << bins[i] << \" times in the image.\" << std::endl;\n         }\n     }\n }",
            "std::array<size_t, 256> localBins;\n\tfor (auto& count : localBins) count = 0;\n\tint worldSize, worldRank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &worldRank);\n\tint chunkSize = image.size() / worldSize;\n\tint start = worldRank * chunkSize;\n\tint end = (worldRank + 1) * chunkSize;\n\tif (worldRank == worldSize - 1) {\n\t\tend = image.size();\n\t}\n\t#pragma omp parallel\n\t{\n\t\t#pragma omp for\n\t\tfor (int i = start; i < end; i++) {\n\t\t\tlocalBins[image[i]]++;\n\t\t}\n\t}\n\n\tMPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "int my_rank, num_processes;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_processes);\n\n    int num_tasks_per_image = image.size() / num_processes;\n    int excess = image.size() % num_processes;\n    int start_loc = num_tasks_per_image * my_rank;\n    int end_loc = start_loc + num_tasks_per_image;\n    if (my_rank < excess) end_loc += 1;\n\n    std::vector<int> my_image;\n\n    for (int i = start_loc; i < end_loc; i++)\n        my_image.push_back(image[i]);\n\n    std::array<int, 256> my_bins;\n\n#pragma omp parallel\n    {\n#pragma omp for\n        for (int i = 0; i < my_image.size(); i++)\n            my_bins[my_image[i]] += 1;\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, my_bins.data(), my_bins.size(), MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    for (int i = 0; i < my_bins.size(); i++)\n        bins[i] += my_bins[i];\n}",
            "// your code here\n   int world_rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n   int world_size;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n   int chunk_size = image.size() / world_size;\n   int remainder = image.size() % world_size;\n\n   std::vector<int> local_image;\n   if (world_rank == 0)\n     local_image = std::vector<int>(image.begin(), image.begin() + chunk_size + remainder);\n   else\n     local_image = std::vector<int>(image.begin() + world_rank * chunk_size,\n                                    image.begin() + (world_rank + 1) * chunk_size);\n   std::array<size_t, 256> local_bins{};\n\n   #pragma omp parallel for num_threads(8)\n   for (int i = 0; i < local_image.size(); ++i) {\n     int value = local_image[i];\n     local_bins[value] += 1;\n   }\n\n   MPI_Allreduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n }",
            "// Fill the bins array with zeros.\n     memset(bins.data(), 0, sizeof(size_t)*256);\n     int mpi_rank, mpi_size;\n     MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n     MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n     \n     #pragma omp parallel for\n     for (int i=0; i<image.size(); i++) {\n         if (image[i] > 255)\n             image[i] = 255;\n         bins[image[i]]++;\n     }\n     \n     // Reduce the bins vector using MPI\n     if (mpi_size > 1) {\n         std::array<size_t, 256> tmp;\n         MPI_Reduce(bins.data(), tmp.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n         if (mpi_rank == 0) {\n             for (int i = 0; i < 256; i++)\n                 bins[i] = tmp[i];\n         }\n     }\n }",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // Set the starting point and step for each rank.\n   int start = rank * (image.size() / size);\n   int step = image.size() / size;\n\n   // Get the image and add it to the appropriate array.\n   std::array<size_t, 256> my_array;\n   std::fill(my_array.begin(), my_array.end(), 0);\n   #pragma omp parallel for\n   for(int i = start; i < start + step; i++) {\n     int pix = image[i];\n     my_array[pix] += 1;\n   }\n\n   // Parallel reduce to count total occurrences of each intensity.\n   #pragma omp parallel for\n   for(int i = 0; i < 256; i++) {\n     size_t sum = 0;\n     for(int j = 0; j < size; j++) {\n       sum += my_array[i];\n     }\n     bins[i] = sum;\n   }\n\n   // Rank 0 accumulates all values into bins.\n   if(rank == 0) {\n     for(int j = 1; j < size; j++) {\n       MPI_Recv(&bins[0], 256, MPI_LONG_LONG, j, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n     }\n   } else {\n     MPI_Send(&bins[0], 256, MPI_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n   }\n\n   // Ensure that we have the right output.\n   if(rank == 0) {\n     std::vector<int> img_test = image;\n     std::sort(img_test.begin(), img_test.end());\n     std::array<size_t, 256> test_array;\n     std::fill(test_array.begin(), test_array.end(), 0);\n     for(int j = 0; j < image.size(); j++) {\n       test_array[img_test[j]]++;\n     }\n     for(int j = 0; j < 256; j++) {\n       assert(bins[j] == test_array[j]);\n     }\n   }\n}",
            "// TODO: Your code goes here\n\tint p,q,rank,size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tfor (int i = 0; i < size; i++) {\n\t\tif (i!= rank) {\n\t\t\tMPI_Send(image.data(), image.size(), MPI_INT, i, 0, MPI_COMM_WORLD);\n\t\t}\n\t}\n\n\tint n_pixel = image.size();\n\tint n_pixel_per_rank = n_pixel / size;\n\tint extra = n_pixel % size;\n\n\tint offset = 0;\n\tint offset_extra = 0;\n\n\tint n_pixel_rank = 0;\n\n\t#pragma omp parallel default(shared) private(q, p, n_pixel_rank)\n\t{\n\t\t#pragma omp single\n\t\t{\n\t\t\toffset = rank * n_pixel_per_rank;\n\t\t\toffset_extra = rank * extra;\n\t\t}\n\t\t#pragma omp for\n\t\tfor (q = 0; q < n_pixel_per_rank; q++) {\n\t\t\tint gray = image[offset + q];\n\t\t\tp = gray + 1;\n\t\t\tn_pixel_rank += 1;\n\t\t\tbins[gray] += 1;\n\t\t}\n\t\t#pragma omp for\n\t\tfor (q = n_pixel_per_rank; q < n_pixel_per_rank + extra; q++) {\n\t\t\tint gray = image[offset + q];\n\t\t\tp = gray + 1;\n\t\t\tn_pixel_rank += 1;\n\t\t\tbins[gray] += 1;\n\t\t}\n\t\t#pragma omp single\n\t\t{\n\t\t\tn_pixel = n_pixel_rank;\n\t\t}\n\t}\n\tint sum = 0;\n\tfor (int i = 0; i < 256; i++) {\n\t\tsum += bins[i];\n\t}\n\tint check = sum;\n\tint check1 = 0;\n\tMPI_Reduce(&check1, &check, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\tint n_pixel_per_rank_ = n_pixel / size;\n\tint extra_ = n_pixel % size;\n\tint offset_ = 0;\n\tint offset_extra_ = 0;\n\tint n_pixel_rank_ = 0;\n\tint n_pixel_local = 0;\n\t#pragma omp parallel default(shared) private(q, p, n_pixel_rank_, n_pixel_local)\n\t{\n\t\t#pragma omp single\n\t\t{\n\t\t\toffset_ = rank * n_pixel_per_rank_;\n\t\t\toffset_extra_ = rank * extra_;\n\t\t}\n\t\t#pragma omp for\n\t\tfor (q = 0; q < n_pixel_per_rank_; q++) {\n\t\t\tint gray = image[offset_ + q];\n\t\t\tp = gray + 1;\n\t\t\tn_pixel_rank_ += 1;\n\t\t\tn_pixel_local += 1;\n\t\t}\n\t\t#pragma omp for\n\t\tfor (q = n_pixel_per_rank_; q < n_pixel_per_rank_ + extra_; q++) {\n\t\t\tint gray = image[offset_ + q];\n\t\t\tp = gray + 1;\n\t\t\tn_pixel_rank_ += 1;\n\t\t\tn_pixel_local += 1;\n\t\t}\n\t\t#pragma omp single\n\t\t{\n\t\t\tif (rank == 0) {\n\t\t\t\tprintf(\"rank: %d - n_pixel_local: %d\\n\", rank, n_pixel_local);\n\t\t\t}\n\t\t}\n\t}\n\tint sum_local =",
            "int num_procs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < image.size(); i += num_procs) {\n            bins[image[i]]++;\n        }\n    } else {\n        // TODO:\n        // Split up the work using OpenMP and MPI.\n        // Each rank should count the number of pixels for its chunk of the image.\n        // The first chunk is image[i] for rank 1, image[i+1] for rank 2, etc.\n        // The result should be stored in bins[i] for rank 1, bins[i+1] for rank 2, etc.\n\n        #pragma omp parallel for\n        for (size_t i = rank; i < image.size(); i += num_procs) {\n            bins[image[i]]++;\n        }\n    }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int num_ranks;\n    MPI_Comm_size(comm, &num_ranks);\n    int rank;\n    MPI_Comm_rank(comm, &rank);\n\n    std::vector<size_t> localBins(256);\n    for (int i = 0; i < image.size(); i++)\n        localBins[image[i]]++;\n\n    if (rank == 0) {\n        for (int i = 0; i < 256; i++)\n            bins[i] = 0;\n    }\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, comm);\n    return;\n}",
            "#pragma omp parallel for\n\tfor (int i = 0; i < image.size(); i++) {\n\t\tbins[image[i]]++;\n\t}\n}",
            "//TODO\n   int const numRanks = MPI_Comm_size(MPI_COMM_WORLD);\n   int const rank = MPI_Comm_rank(MPI_COMM_WORLD);\n   int const imageSize = image.size();\n   int const imageSizePerRank = imageSize / numRanks;\n   int const numRemaining = imageSize % numRanks;\n   \n   int chunkStart = rank * imageSizePerRank;\n   int chunkSize = imageSizePerRank;\n   if (rank < numRemaining) {\n     chunkStart += rank;\n     chunkSize++;\n   }\n   else {\n     chunkStart += numRemaining;\n   }\n\n   //omp_set_num_threads(8);\n   std::vector<int> localChunk(chunkSize, 0);\n   #pragma omp parallel for\n   for(int i = 0; i < chunkSize; i++) {\n     localChunk[i] = image[chunkStart + i];\n   }\n\n   for(int i = 0; i < chunkSize; i++) {\n     bins[localChunk[i]]++;\n   }\n   MPI_Reduce(&bins, &bins, 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "// YOUR CODE HERE\n  \n     // 1. MPI_Bcast(MPI_IN_PLACE, size, MPI_DATATYPE, rank, MPI_COMM_WORLD);\n     // 2. int numPixels = image.size() / 2;\n     // 3. int numRanks = numPixels / 100;\n     // 4. int rankPixels = numPixels / numRanks;\n     // 5. int rankExtraPixels = numPixels % numRanks;\n     // 6. int rankStartPixels = rank * rankPixels;\n     // 7. int rankEndPixels = rankStartPixels + rankPixels + rankExtraPixels;\n     // 8. int rankStartPixels_local = rankStartPixels - rank * rankPixels;\n     // 9. int rankEndPixels_local = rankEndPixels - rank * rankPixels;\n     // 10. if (rank < numRanks) {\n     // 11.     for (int i = rankStartPixels_local; i < rankEndPixels_local; i++) {\n     // 12.         int intensity = image[2 * i] * 256 + image[2 * i + 1];\n     // 13.         bins[intensity]++;\n     // 14.     }\n     // 15. }\n     // 16. MPI_Barrier(MPI_COMM_WORLD);\n     // 17. MPI_Reduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  const int size = 256;\n  int rank, numRanks;\n  int numPixels = image.size() / 2;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n  int numPixels_local = numPixels / numRanks;\n  int rankStartPixels_local = rank * numPixels_local;\n  int rankEndPixels_local = rankStartPixels_local + numPixels_local;\n  if (rank < numRanks) {\n    for (int i = rankStartPixels_local; i < rankEndPixels_local; i++) {\n      int intensity = image[2 * i] * 256 + image[2 * i + 1];\n      bins[intensity]++;\n    }\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Reduce(MPI_IN_PLACE, bins.data(), size, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// Your code here\n}",
            "// TODO: implement this\n\n}",
            "int n_rank;\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &n_rank);\n    int n_per_rank = image.size() / n_rank;\n    int remainder = image.size() % n_rank;\n    int start_index = 0;\n    int end_index = n_per_rank;\n    if (rank < remainder) {\n        end_index += 1;\n        start_index = rank * n_per_rank + rank;\n    }\n    else {\n        start_index = rank * n_per_rank + remainder;\n    }\n    int block = end_index - start_index;\n    std::fill(bins.begin(), bins.end(), 0);\n    #pragma omp parallel for\n    for (int i = start_index; i < end_index; i++) {\n        bins[image[i]] += 1;\n    }\n    MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (auto x: bins) {\n            std::cout << x << std::endl;\n        }\n    }\n}",
            "/* TODO: Your code here! */\n    // Get the number of MPI ranks and the rank ID\n    int comm_size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int chunk_size = image.size() / comm_size;\n    // Set chunk_size to size of image if the image is smaller than number of MPI ranks\n    if (image.size() < comm_size) {\n        chunk_size = image.size();\n    }\n    // Create vector for storing the local counts\n    std::vector<size_t> local_bins(256, 0);\n\n    // Compute local counts\n    for (int i = rank * chunk_size; i < (rank + 1) * chunk_size; i++) {\n        local_bins[image[i]]++;\n    }\n\n    // Communicate local counts with other ranks\n    std::vector<size_t> new_bins(256, 0);\n    MPI_Allreduce(local_bins.data(), new_bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    // Copy local counts to bins vector\n    if (rank == 0) {\n        for (int i = 0; i < 256; i++) {\n            bins[i] = new_bins[i];\n        }\n    }\n}",
            "// TODO: Your code here\n\t\n\t// TODO: add an MPI reduction call to count the pixels in parallel\n\t// TODO: add an OpenMP parallel for to parallelize the reduction\n\t\n\tMPI_Reduce(image.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\t\n\tif(mpi_rank == 0){\n\t\t\n\t\t// for(size_t i=0; i < image.size(); i++){\n\t\t// \tbins[image[i]]++;\n\t\t// }\n\t}\n\t\n\t\n\treturn;\n}",
            "// TODO: Your code here\n }",
            "int n_rank, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &n_rank);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        MPI_Status status;\n\n        for (auto& count: bins) {\n            count = 0;\n        }\n\n        for (int i = 0; i < image.size(); i += n_rank) {\n            MPI_Send(&image[i], n_rank, MPI_INT, 1, i, MPI_COMM_WORLD);\n        }\n\n        for (int i = 0; i < n_rank; i++) {\n            MPI_Recv(&bins[i], 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);\n        }\n    } else {\n        MPI_Status status;\n        std::array<size_t, 256> local_bins = {0};\n        std::vector<int> local_image;\n\n        for (int i = 0; i < image.size() / n_rank; i++) {\n            local_image.push_back(image[rank * (image.size() / n_rank) + i]);\n        }\n\n        int last_index = image.size() / n_rank * (rank + 1);\n        if (last_index > image.size()) {\n            last_index = image.size();\n        }\n\n        #pragma omp parallel for\n        for (int i = rank * (image.size() / n_rank); i < last_index; i++) {\n            local_bins[local_image[i]] += 1;\n        }\n\n        MPI_Send(&local_bins[0], 256, MPI_INT, 0, rank, MPI_COMM_WORLD);\n        MPI_Recv(&bins[0], 256, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n    }\n}",
            "// FIXME: Your code here\n }",
            "// Fill bins with 0s.\n   for (int i = 0; i < 256; ++i)\n     bins[i] = 0;\n\n   // Count the number of pixels in each intensity level.\n   #pragma omp parallel for\n   for (int i = 0; i < image.size(); ++i) {\n     ++bins[image[i]];\n   }\n }",
            "size_t num_pixels = image.size();\n   size_t num_mpi_ranks = 1;\n   size_t mpi_rank = 0;\n   int mpi_result = MPI_Comm_size(MPI_COMM_WORLD, &num_mpi_ranks);\n   assert(mpi_result == MPI_SUCCESS);\n   int mpi_rank_result = MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n   assert(mpi_rank_result == MPI_SUCCESS);\n   // TODO: compute `my_num_pixels` using the number of MPI ranks and this rank's id\n   size_t my_num_pixels = 0;\n   #pragma omp parallel\n   {\n     int num_threads = omp_get_num_threads();\n     #pragma omp single\n     {\n       assert(num_threads == num_mpi_ranks);\n     }\n     size_t my_num_pixels_per_thread = num_pixels / num_threads;\n     size_t my_num_pixels_remainder = num_pixels % num_threads;\n     // TODO: set up the thread private data\n     std::array<size_t, 256> my_bins{};\n     // TODO: parallelize over the pixels and populate `my_bins`\n     for (size_t i = 0; i < my_num_pixels; ++i) {\n       ++my_bins[image[i]];\n     }\n     // TODO: combine the thread private data into `bins`\n     #pragma omp for\n     for (size_t i = 0; i < 256; ++i) {\n       bins[i] += my_bins[i];\n     }\n   }\n }",
            "int num_ranks = 1;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    int rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int num_threads = 1;\n    #pragma omp parallel\n    {\n        #pragma omp master\n        num_threads = omp_get_num_threads();\n    }\n    size_t image_size = image.size();\n    size_t num_pixels = image_size / 3;\n    int max_threads = 4;\n    int local_threads = num_threads;\n    int local_ranks = num_ranks;\n\n    size_t chunk_size = (num_pixels / local_ranks) / local_threads;\n    if (chunk_size == 0)\n        chunk_size = 1;\n\n    size_t local_offset = rank * chunk_size;\n    size_t global_offset = 0;\n\n    // TODO: Use MPI and OpenMP to compute the counts in parallel\n    //  on each rank.\n    if(local_ranks == 1){\n        #pragma omp parallel for\n        for (size_t i = local_offset; i < local_offset + chunk_size; i++){\n            bins[image[i*3]]++;\n        }\n    }\n    else{\n        int count = 0;\n        if(rank == 0){\n            #pragma omp parallel for\n            for(int i = 0; i < local_ranks; i++){\n                if(i == rank){\n                    #pragma omp parallel for\n                    for(size_t j = local_offset; j < local_offset + chunk_size; j++){\n                        bins[image[j*3]]++;\n                    }\n                }\n                else{\n                    if(local_ranks > max_threads){\n                        global_offset += (local_ranks / max_threads);\n                    }\n                    global_offset += local_offset;\n                    MPI_Status status;\n                    MPI_Recv(&image[global_offset], chunk_size, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n                    global_offset += chunk_size;\n                    #pragma omp parallel for\n                    for(size_t j = local_offset; j < local_offset + chunk_size; j++){\n                        bins[image[j*3]]++;\n                    }\n                }\n            }\n        }\n        else{\n            global_offset += rank * chunk_size;\n            MPI_Status status;\n            MPI_Recv(&image[global_offset], chunk_size, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n        }\n    }\n    // TODO: Gather the partial counts from each rank and add them on rank 0\n    //  to the final result, `bins`.\n    if(rank == 0){\n        for(int i = 1; i < local_ranks; i++){\n            MPI_Status status;\n            MPI_Recv(&bins[0], 256, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n        }\n    }\n    else{\n        MPI_Send(&bins[0], 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n    // TODO: Wait for all ranks to complete and free memory.\n}",
            "// TODO: Your code here\n    int rank, num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    size_t total_size = image.size();\n    int block_size = total_size / num_ranks;\n    int remainder = total_size % num_ranks;\n\n    // Initialize the bins\n    for (int i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n\n    // Calculate the chunk size for each rank\n    if (rank == 0) {\n        int chunk_size = block_size;\n        if (remainder > 0) {\n            chunk_size += 1;\n        }\n        bins.resize(chunk_size);\n        for (int i = 0; i < chunk_size; i++) {\n            bins[i] = 0;\n        }\n    }\n\n    // Find the start and end of the local chunk\n    int start = rank * block_size;\n    int end;\n    if (rank == num_ranks - 1) {\n        end = total_size;\n    }\n    else {\n        end = start + block_size;\n    }\n\n    // Count the number of pixels in local chunk\n    int count = 0;\n    int value;\n#pragma omp parallel for reduction(+:count)\n    for (int i = start; i < end; i++) {\n        value = image[i];\n        count += 1;\n        bins[value] += 1;\n    }\n\n    // Get the results from other ranks\n    MPI_Allreduce(bins.data(), bins.data(), bins.size(), MPI_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "int size;\n    int rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    const int perRank = image.size() / size;\n    std::vector<int> localImage(perRank);\n    std::copy(image.begin() + rank * perRank, image.begin() + (rank + 1) * perRank, localImage.begin());\n\n    std::array<size_t, 256> localBins{};\n    for(int i : localImage) {\n        localBins[i] += 1;\n    }\n\n    std::array<size_t, 256> globalBins = localBins;\n    MPI_Reduce(localBins.data(), globalBins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if(rank == 0) {\n        bins = globalBins;\n    }\n}",
            "// Your code here\n#pragma omp parallel for num_threads(32)\n    for (int i = 0; i < image.size(); ++i) {\n        ++bins[image[i]];\n    }\n}",
            "// TODO: Your code here\n   // You may assume image has at least one element.\n\n   // for (int i = 0; i < image.size(); ++i) {\n   //   std::cout << image[i] << \" \";\n   // }\n   // std::cout << std::endl;\n   std::vector<size_t> localBins(256, 0);\n\n   // if (rank == 0) {\n   //   std::cout << \"rank: \" << rank << std::endl;\n   //   std::cout << \"bins: \";\n   //   for (int i = 0; i < 256; ++i) {\n   //     std::cout << bins[i] << \" \";\n   //   }\n   //   std::cout << std::endl;\n   // }\n\n   #pragma omp parallel for\n   for (int i = 0; i < image.size(); ++i) {\n     localBins[image[i]]++;\n     // if (rank == 0) {\n     //   std::cout << \"rank: \" << rank << \" \" << image[i] << \" \" << localBins[image[i]] << std::endl;\n     // }\n   }\n\n   if (rank == 0) {\n     for (int i = 0; i < 256; ++i) {\n       bins[i] = 0;\n     }\n     for (int i = 0; i < num_procs; ++i) {\n       for (int j = 0; j < 256; ++j) {\n         bins[j] += localBins[j];\n       }\n     }\n   }\n   // std::cout << \"rank: \" << rank << \" \" << \" bins: \";\n   // for (int i = 0; i < 256; ++i) {\n   //   std::cout << bins[i] << \" \";\n   // }\n   // std::cout << std::endl;\n }",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // 1) determine number of images each process will count\n    int num_rows = 1000;\n    int num_cols = 1000;\n    int num_pixels_per_proc = num_rows * num_cols;\n    int num_pixels_to_count = image.size();\n\n    // determine number of images to be counted on this process\n    int num_images_per_proc = num_pixels_to_count / num_pixels_per_proc;\n    if (num_images_per_proc * num_pixels_per_proc < num_pixels_to_count) {\n        ++num_images_per_proc;\n    }\n\n    // determine number of pixels on this process that should be counted\n    int num_pixels_to_count_this_proc = num_images_per_proc * num_pixels_per_proc;\n\n    // determine which pixels on this process should be counted\n    std::vector<int> pixels_to_count;\n    for (int i = 0; i < num_pixels_to_count_this_proc; ++i) {\n        pixels_to_count.push_back(image[i]);\n    }\n\n    // 2) count pixels on this process\n    std::array<size_t, 256> bins_local;\n    for (int i = 0; i < 256; ++i) {\n        bins_local[i] = 0;\n    }\n\n    for (int i = 0; i < num_pixels_to_count_this_proc; ++i) {\n        ++bins_local[pixels_to_count[i]];\n    }\n\n    // 3) combine bins from all processes\n    if (rank == 0) {\n        for (int i = 0; i < 256; ++i) {\n            bins[i] = 0;\n        }\n    }\n\n    MPI_Allreduce(bins_local.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  // int n_threads = omp_get_max_threads();\n  std::vector<int> local_counts(256, 0);\n  std::vector<int> global_counts(256, 0);\n  // std::vector<int> local_counts(n_threads, 0);\n  // std::vector<int> global_counts(n_threads, 0);\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < image.size(); i++) {\n    local_counts[image[i]]++;\n  }\n\n  for (size_t i = 0; i < 256; i++) {\n    global_counts[i] = 0;\n  }\n\n  for (int i = 0; i < size; i++) {\n    for (size_t j = 0; j < 256; j++) {\n      global_counts[j] += local_counts[j];\n    }\n  }\n\n  if (rank == 0) {\n    bins = std::move(global_counts);\n  }\n}",
            "// TODO\n }",
            "bins.fill(0);\n    for (int i = 0; i < image.size(); ++i) {\n        ++bins[image[i]];\n    }\n}",
            "// get number of ranks\n    int nproc = omp_get_num_threads();\n    int rank = omp_get_thread_num();\n\n    // get size of rank\n    int nperrank = image.size()/nproc;\n\n    // copy to local image\n    std::vector<int> local_image(image.begin() + nperrank * rank, image.begin() + nperrank * rank + nperrank);\n\n    // count pixels in local image\n    for (int i = 0; i < local_image.size(); i++) {\n        bins[local_image[i]] += 1;\n    }\n\n    // gather bins from local image\n    MPI_Allgather(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, bins.data(), 256, MPI_LONG, MPI_COMM_WORLD);\n\n}",
            "int num_ranks, rank;\n     MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n     size_t bins_size = bins.size();\n     size_t image_size = image.size();\n\n     size_t mpi_buffer_size = bins_size / num_ranks;\n\n     // create a buffer to hold the results from each rank\n     std::vector<size_t> mpi_buffer(mpi_buffer_size);\n\n     size_t offset = rank * mpi_buffer_size;\n\n     // create a buffer to hold the results from each thread\n     std::vector<size_t> omp_buffer(bins_size);\n\n#pragma omp parallel\n     {\n         int tid = omp_get_thread_num();\n         int num_threads = omp_get_num_threads();\n\n         // set the local offset for this thread in the buffer\n         size_t local_offset = tid * mpi_buffer_size;\n\n         // set the local size for this thread in the buffer\n         size_t local_size = mpi_buffer_size / num_threads;\n         local_size += (mpi_buffer_size % num_threads > tid);\n\n         // set the local end for this thread in the buffer\n         size_t local_end = local_offset + local_size;\n\n         // fill the buffer with zeroes\n         std::fill(omp_buffer.begin(), omp_buffer.end(), 0);\n\n         // count the pixels\n         for (size_t i = 0; i < local_size; i++) {\n             omp_buffer[image[offset + i] - 1]++;\n         }\n\n         // send the buffer to rank 0\n         if (rank == 0) {\n             // fill the MPI buffer with the results of each thread\n             for (size_t i = 0; i < bins_size; i++) {\n                 mpi_buffer[i] = omp_buffer[i];\n             }\n         }\n         // receive the results from rank 0\n         else {\n             MPI_Recv(&mpi_buffer[0], mpi_buffer_size, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         }\n\n         // sum the results from each thread and rank 0\n         for (size_t i = local_offset; i < local_end; i++) {\n             bins[i] += mpi_buffer[i];\n         }\n     }\n }",
            "int rank, nproc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    size_t npixels = image.size();\n    size_t nperproc = npixels / nproc;\n    size_t nleft = npixels - nperproc * nproc;\n    size_t nproc2 = nproc / 2;\n    if(rank < nproc2) {\n      nperproc += 1;\n      nleft += 1;\n    } else {\n      nperproc -= 1;\n      nleft -= 1;\n    }\n\n    std::vector<int> image_local(nperproc);\n\n    for(size_t i = 0; i < nperproc; i++) {\n      image_local[i] = image[rank*nperproc + i];\n    }\n\n    std::array<size_t, 256> bins_local = {};\n    #pragma omp parallel for\n    for(size_t i = 0; i < nperproc; i++) {\n      bins_local[image_local[i]] += 1;\n    }\n\n    std::array<size_t, 256> bins_global = {};\n    MPI_Reduce(&bins_local[0], &bins_global[0], 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for(size_t i = 0; i < nleft; i++) {\n      bins_global[image[nproc*nperproc + i]] += 1;\n    }\n\n    bins = bins_global;\n}",
            "const size_t np = bins.size();\n    #pragma omp parallel for\n    for(size_t i=0; i<np; ++i) {\n        bins[i] = 0;\n    }\n    const size_t nn = image.size();\n    #pragma omp parallel for\n    for(size_t i=0; i<nn; ++i) {\n        int val = image[i];\n        #pragma omp atomic\n        bins[val] += 1;\n    }\n}",
            "// TODO: your code here\n }",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   const size_t N = image.size();\n   const size_t nPerRank = N / size;\n   const size_t remainder = N % size;\n   const size_t n = rank < remainder? nPerRank + 1 : nPerRank;\n   std::vector<int> myImage = image;\n   if (rank < remainder) {\n     myImage[rank * nPerRank] = image[rank * nPerRank + remainder];\n   }\n   const size_t offset = rank * nPerRank;\n\n   std::fill(bins.begin(), bins.end(), 0);\n   std::vector<int> localBins(bins.begin() + offset, bins.begin() + offset + n);\n\n   int nthreads;\n#pragma omp parallel\n   {\n     nthreads = omp_get_num_threads();\n   }\n   std::vector<int> counts(nthreads, 0);\n\n#pragma omp parallel for\n   for (size_t i = 0; i < n; ++i) {\n     const int idx = myImage[i];\n     ++localBins[idx];\n     ++counts[omp_get_thread_num()];\n   }\n\n#pragma omp parallel for\n   for (size_t i = 0; i < nthreads; ++i) {\n     MPI_Allreduce(&counts[i], &counts[0], 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n   }\n\n   for (size_t i = 0; i < n; ++i) {\n     bins[offset + i] += localBins[i];\n   }\n\n   if (rank == 0) {\n     std::cout << \"Number of threads: \" << nthreads << \"\\n\";\n     for (size_t i = 0; i < 256; ++i) {\n       std::cout << bins[i] << \" \";\n     }\n     std::cout << std::endl;\n   }\n\n   MPI_Barrier(MPI_COMM_WORLD);\n   if (rank == 0) {\n     std::cout << \"Summed: \" << std::accumulate(bins.begin(), bins.end(), 0) << \"\\n\";\n   }\n }",
            "bins.fill(0);\n     size_t n = image.size();\n     #pragma omp parallel for\n     for (size_t i=0; i<n; ++i) {\n         ++bins[image[i]];\n     }\n }",
            "#pragma omp parallel\n    {\n        int rank;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        if (rank == 0) {\n            int size;\n            MPI_Comm_size(MPI_COMM_WORLD, &size);\n            size_t per_rank = image.size() / size;\n            std::vector<size_t> local_bins(256);\n#pragma omp for\n            for (size_t i = 0; i < image.size(); i++) {\n                local_bins[image[i]]++;\n            }\n#pragma omp for\n            for (int i = 1; i < size; i++) {\n                MPI_Recv(bins.data() + i * per_rank, per_rank, MPI_INT, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            }\n            MPI_Reduce(local_bins.data(), bins.data(), per_rank, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n        } else {\n            std::vector<size_t> local_bins(256);\n            size_t offset = (rank - 1) * image.size() / MPI_COMM_WORLD.Size();\n            for (size_t i = offset; i < offset + image.size() / MPI_COMM_WORLD.Size(); i++) {\n                local_bins[image[i]]++;\n            }\n            MPI_Send(local_bins.data(), image.size() / MPI_COMM_WORLD.Size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n        }\n    }\n}",
            "size_t global_image_size;\n\tint rank, num_procs;\n\tMPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\t// find the number of pixels in the image\n\tglobal_image_size = image.size();\n\t\n\t// calculate the number of pixels each process will be dealing with\n\t// this is the number of pixels in the image divided by the number of ranks\n\tsize_t local_image_size = (global_image_size / num_procs) + (rank < (global_image_size % num_procs)? 1 : 0);\n\t\n\t// calculate the starting index for this process's image data\n\t// the starting index is the number of pixels that have been counted so far (rank * local_image_size)\n\tsize_t start_idx = (rank * local_image_size);\n\t\n\t// calculate the ending index for this process's image data\n\t// this is start_idx + local_image_size\n\tsize_t end_idx = (start_idx + local_image_size) - 1;\n\t\n\t// the image data that this rank will be responsible for counting\n\tstd::vector<int> local_image(image.begin() + start_idx, image.begin() + end_idx + 1);\n\t\n\t// initialize bins to zeros\n\tfor (int i = 0; i < 256; i++) {\n\t\tbins[i] = 0;\n\t}\n\n\t// count the number of pixels with each intensity\n\tfor (int i = 0; i < local_image.size(); i++) {\n\t\tbins[local_image[i]]++;\n\t}\n\n\t// sum the counts in bins\n\tfor (int i = 0; i < num_procs; i++) {\n\t\tif (i == rank) {\n\t\t\tcontinue;\n\t\t}\n\t\tint buffer[256];\n\t\tMPI_Recv(buffer, 256, MPI_INT, i, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\tfor (int j = 0; j < 256; j++) {\n\t\t\tbins[j] += buffer[j];\n\t\t}\n\t}\n\n\t// send the bins array to rank 0\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < num_procs; i++) {\n\t\t\tMPI_Send(bins.data(), 256, MPI_INT, i, i, MPI_COMM_WORLD);\n\t\t}\n\t} else {\n\t\tMPI_Send(bins.data(), 256, MPI_INT, 0, rank, MPI_COMM_WORLD);\n\t}\n\n\t// only rank 0 should have the final result\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < 256; i++) {\n\t\t\tstd::cout << i << \": \" << bins[i] << \"\\n\";\n\t\t}\n\t}\n\t//return;\n}",
            "// TODO\n    // Your code goes here\n    \n    if (image.empty()) {\n        for (int i = 0; i < 256; ++i) {\n            bins[i] = 0;\n        }\n        return;\n    }\n    \n    int world_rank = 0;\n    int world_size = 1;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    int chunk_size = image.size() / world_size;\n    if (world_rank == world_size - 1) {\n        chunk_size += image.size() % world_size;\n    }\n    \n    int i, j;\n    MPI_Request req[world_size];\n    MPI_Status stat[world_size];\n    std::vector<int> buf(chunk_size);\n    \n    if (world_rank == 0) {\n        for (j = 1; j < world_size; ++j) {\n            MPI_Irecv(&buf[0], chunk_size, MPI_INT, j, 1, MPI_COMM_WORLD, &req[j]);\n        }\n    } else {\n        MPI_Send(&image[0] + world_rank * chunk_size, chunk_size, MPI_INT, 0, 1, MPI_COMM_WORLD);\n    }\n    \n    MPI_Barrier(MPI_COMM_WORLD);\n    \n    for (i = 0; i < image.size(); ++i) {\n        if (world_rank == 0) {\n            ++bins[image[i]];\n            if (i % chunk_size == 0 && i!= 0) {\n                ++j;\n                MPI_Wait(&req[j], &stat[j]);\n                if (j < world_size - 1) {\n                    MPI_Irecv(&buf[0], chunk_size, MPI_INT, j + 1, 1, MPI_COMM_WORLD, &req[j + 1]);\n                }\n            }\n        } else {\n            MPI_Send(&image[0] + world_rank * chunk_size, chunk_size, MPI_INT, 0, 1, MPI_COMM_WORLD);\n        }\n    }\n    \n    MPI_Barrier(MPI_COMM_WORLD);\n    \n    if (world_rank!= 0) {\n        MPI_Send(buf.data(), buf.size(), MPI_INT, 0, 1, MPI_COMM_WORLD);\n    } else {\n        for (j = 1; j < world_size; ++j) {\n            MPI_Wait(&req[j], &stat[j]);\n        }\n    }\n    \n    MPI_Barrier(MPI_COMM_WORLD);\n    \n    if (world_rank == 0) {\n        for (i = 1; i < world_size; ++i) {\n            for (j = 0; j < chunk_size; ++j) {\n                bins[buf[j]] += bins[buf[j]];\n            }\n        }\n    }\n    \n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "const size_t num_workers = omp_get_num_threads();\n    const size_t worker_id = omp_get_thread_num();\n    int rank, num_procs;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size_t offset = num_workers * rank * image.size() / num_procs;\n    size_t num_pixels = image.size();\n\n    // OpenMP parallel section\n    #pragma omp parallel\n    {\n        std::array<size_t, 256> local_bins{};\n\n        #pragma omp for\n        for (size_t i = offset; i < offset + num_pixels; i++) {\n            local_bins[image[i]] += 1;\n        }\n        // OpenMP reduction (only on rank 0)\n        #pragma omp critical\n        if (rank == 0) {\n            for (size_t i = 0; i < 256; i++) {\n                bins[i] += local_bins[i];\n            }\n        }\n    }\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n   // #pragma omp parallel for\n   for (int i = 0; i < image.size(); i++) {\n      bins[image[i]]++;\n   }\n\n   if (world_rank == 0) {\n      // MPI_Reduce(MPI_IN_PLACE, bins, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n      MPI_Reduce(MPI_IN_PLACE, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n   } else {\n      MPI_Reduce(bins.data(), bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n}",
            "/* Start your code here */\n\n    /* \n    Your code goes here\n    */\n\n    /* End your code here */\n}",
            "// TODO: Your code here\n}",
            "const size_t rows = image.size() / 256;\n    const int nrows = omp_get_max_threads();\n    const size_t chunk = 256 / nrows;\n    const int rank = MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    const int nprocs = MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    // Compute the number of pixels for each intensity \n    // with the help of openmp\n    #pragma omp parallel for\n    for (int i = 0; i < 256; i++) {\n        int counter = 0;\n        for (int j = 0; j < rows; j++) {\n            counter += image[j * 256 + i];\n        }\n        bins[i] = counter;\n    }\n\n    // Gather the results\n    MPI_Allgather(&bins, 256, MPI_UNSIGNED_LONG_LONG, &bins, 256, MPI_UNSIGNED_LONG_LONG, MPI_COMM_WORLD);\n\n}",
            "int nRanks;\n   MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n\n   int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // initialize bins\n   for (int i = 0; i < 256; ++i) {\n     bins[i] = 0;\n   }\n\n   // do work\n   int chunkSize = (int)image.size() / nRanks;\n   int offset = rank * chunkSize;\n   int limit = std::min(offset + chunkSize, (int)image.size());\n\n   for (int i = offset; i < limit; ++i) {\n     bins[image[i]] += 1;\n   }\n\n   // gather\n   size_t *binsTotals = new size_t[256];\n   MPI_Allgather(bins.data(), 256, MPI_UNSIGNED_LONG, binsTotals, 256, MPI_UNSIGNED_LONG, MPI_COMM_WORLD);\n\n   // output\n   if (rank == 0) {\n     for (int i = 0; i < 256; ++i) {\n       bins[i] = binsTotals[i];\n     }\n   }\n\n   delete[] binsTotals;\n }",
            "/* TODO: Your code here */\n     int mpi_rank, mpi_size;\n     MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n     MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n     size_t total_pixels = image.size();\n     size_t per_rank = total_pixels / mpi_size;\n     size_t per_rank_leftover = total_pixels - per_rank * mpi_size;\n     size_t current_rank_leftover = 0;\n\n     std::vector<int> local_image;\n     if (mpi_rank == 0) {\n         current_rank_leftover = per_rank_leftover;\n     }\n     if (current_rank_leftover > 0) {\n         for (size_t i = 0; i < per_rank + 1; i++) {\n             if (mpi_rank == 0 && i < per_rank_leftover) {\n                 local_image.push_back(image[i]);\n             } else if (mpi_rank!= 0 && i < per_rank) {\n                 local_image.push_back(image[i]);\n             }\n         }\n     } else {\n         for (size_t i = 0; i < per_rank; i++) {\n             local_image.push_back(image[i]);\n         }\n     }\n\n     std::vector<std::array<size_t, 256>> local_bins(omp_get_max_threads());\n     #pragma omp parallel num_threads(omp_get_max_threads())\n     {\n         std::array<size_t, 256> tmp_bins = { 0 };\n         for (size_t i = 0; i < local_image.size(); i++) {\n             tmp_bins[local_image[i]]++;\n         }\n         local_bins[omp_get_thread_num()] = tmp_bins;\n     }\n\n     std::array<size_t, 256> total_bins = { 0 };\n     #pragma omp parallel for num_threads(omp_get_max_threads())\n     for (size_t i = 0; i < 256; i++) {\n         for (int j = 0; j < omp_get_max_threads(); j++) {\n             total_bins[i] += local_bins[j][i];\n         }\n     }\n     if (mpi_rank == 0) {\n         for (int i = 0; i < 256; i++) {\n             bins[i] = total_bins[i];\n         }\n     }\n     MPI_Bcast(&bins, 256, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n }",
            "size_t n_rows = image.size() / 256;\n    size_t n_cols = 256;\n    #pragma omp parallel for collapse(2)\n    for (size_t i = 0; i < n_rows; ++i) {\n        for (size_t j = 0; j < n_cols; ++j) {\n            bins[j] += image[i * n_cols + j];\n        }\n    }\n}",
            "// Your code here\n#ifdef TESTING\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   if (rank == 0) {\n       printf(\"TEST: image.size=%lu\\n\", image.size());\n   }\n#endif\n   int num_procs;\n   int rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int chunk_size = image.size() / num_procs;\n   std::vector<int> local_image;\n   int last_idx = rank * chunk_size;\n   int first_idx = last_idx + 1;\n   last_idx = (rank + 1) * chunk_size;\n   if (rank == num_procs - 1) {\n       last_idx = image.size();\n   }\n   for (int i = first_idx; i < last_idx; i++) {\n       local_image.push_back(image[i]);\n   }\n   bins.fill(0);\n   #pragma omp parallel for\n   for (int i = 0; i < local_image.size(); i++) {\n       bins[local_image[i]] += 1;\n   }\n\n   std::array<size_t, 256> global_bins = bins;\n\n   MPI_Reduce(bins.data(), global_bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   bins = global_bins;\n\n }",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int imageSize = image.size();\n    if (imageSize % size!= 0) {\n        std::cerr << \"The number of pixels in the image is not divisible by the number of ranks\" << std::endl;\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n    int pixelsPerRank = imageSize / size;\n\n#pragma omp parallel\n    {\n        int threadId = omp_get_thread_num();\n        int threadCount = omp_get_num_threads();\n        int start = rank * pixelsPerRank;\n        int end = start + pixelsPerRank;\n        int increment = ceil((double)pixelsPerRank / threadCount);\n        int threadStart = start + threadId * increment;\n        int threadEnd = (threadId + 1) * increment < pixelsPerRank?\n                         (threadId + 1) * increment : pixelsPerRank;\n        std::vector<int> localCounts(256, 0);\n\n#pragma omp for\n        for (int i = threadStart; i < threadEnd; i++) {\n            int index = i;\n            if (i >= end) {\n                int remainder = i % pixelsPerRank;\n                int imageSize = image.size();\n                index = remainder + rank * pixelsPerRank;\n            }\n            int value = image[index];\n            localCounts[value]++;\n        }\n\n        MPI_Allreduce(MPI_IN_PLACE, &localCounts[0], 256, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n        memcpy(&bins[0], &localCounts[0], 256 * sizeof(size_t));\n    }\n}",
            "const int n = image.size();\n    int nthreads;\n    MPI_Comm_size(MPI_COMM_WORLD, &nthreads);\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        int nthreads = omp_get_num_threads();\n        int nimages = n/nthreads;\n        int istart = nimages*tid;\n        int iend = istart+nimages;\n        int ii=0;\n        int pixel;\n        std::array<size_t, 256> localbins;\n        for(int i=istart; i<iend; i++) {\n            pixel = image[i];\n            ii++;\n            localbins[pixel]++;\n        }\n        if (tid == 0) {\n            for (int ii=0; ii<nthreads; ii++) {\n                for (int ii=0; ii<256; ii++) {\n                    bins[ii] += localbins[ii];\n                }\n            }\n        }\n    }\n}",
            "// TODO: implement\n   int rank, n_ranks;\n   MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   const int chunk_size = image.size() / n_ranks;\n   int start_pos = chunk_size * rank;\n   int end_pos = chunk_size * (rank + 1);\n   if (rank == n_ranks - 1)\n     end_pos = image.size();\n\n   for (int i = start_pos; i < end_pos; ++i) {\n     bins[image[i]] += 1;\n   }\n\n   // MPI_Reduce(image.data(), bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n     MPI_Reduce(MPI_IN_PLACE, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n   else {\n     MPI_Reduce(bins.data(), bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n }",
            "size_t rank = 0, size = 1;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t start_index = rank * image.size() / size;\n   size_t end_index = (rank+1) * image.size() / size;\n\n   std::array<size_t, 256> local_bins{};\n   for (size_t i = start_index; i < end_index; ++i) {\n     ++local_bins[image[i]];\n   }\n\n   std::array<size_t, 256> bins_local{};\n   MPI_Gather(local_bins.data(), 256, MPI_UNSIGNED_LONG_LONG, bins_local.data(), 256, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n     bins.fill(0);\n     for (int i = 0; i < size; ++i) {\n       for (int j = 0; j < 256; ++j) {\n         bins[j] += bins_local[i][j];\n       }\n     }\n   }\n}",
            "// TODO\n}",
            "int world_size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int nrows = (int)image.size()/256;\n\n  if (rank == 0) {\n    std::fill(bins.begin(), bins.end(), 0);\n    #pragma omp parallel for collapse(2)\n    for (int row = 0; row < nrows; row++) {\n      for (int col = 0; col < 256; col++) {\n        bins[col] += image[row*256+col];\n      }\n    }\n  }\n  else {\n    #pragma omp parallel for collapse(2)\n    for (int row = 0; row < nrows; row++) {\n      for (int col = 0; col < 256; col++) {\n        bins[col] += image[row*256+col];\n      }\n    }\n  }\n\n}",
            "int rank, num_proc;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n\n  const int num_elements = image.size();\n  const int local_num_elements = num_elements / num_proc;\n  const int local_min = local_num_elements * rank;\n  const int local_max = local_min + local_num_elements;\n\n  std::vector<int> local_image;\n  local_image.reserve(local_num_elements);\n  for(int i=0;i<local_num_elements;i++){\n    local_image.push_back(image[local_min + i]);\n  }\n\n  #pragma omp parallel for\n  for (int i=0;i<local_num_elements;i++){\n    bins[local_image[i]]++;\n  }\n\n  if(rank == 0){\n    for(int i=0;i<256;i++){\n      for(int j=1;j<num_proc;j++){\n        bins[i] += bins[i] + j;\n      }\n    }\n    for(int j=0;j<num_proc;j++){\n      if(j!= 0){\n        MPI_Send(bins.data() + j * 256, 256, MPI_UNSIGNED_LONG, j, 0, MPI_COMM_WORLD);\n      }\n    }\n    for(int j=1;j<num_proc;j++){\n      std::array<size_t, 256> recv_array;\n      MPI_Recv(recv_array.data(), 256, MPI_UNSIGNED_LONG, j, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for(int i=0;i<256;i++){\n        bins[i] += recv_array[i];\n      }\n    }\n  }\n  else{\n    MPI_Recv(bins.data(), 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n}",
            "//TODO: replace with your solution\n     return;\n }",
            "int rank, num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    int chunk_size = (image.size()+num_ranks-1)/num_ranks;\n    std::vector<int> chunk(image.begin()+rank*chunk_size, image.begin()+(rank+1)*chunk_size);\n#pragma omp parallel for schedule(dynamic, 1)\n    for (int i = 0; i < chunk.size(); ++i) {\n        bins[chunk[i]]++;\n    }\n    std::array<size_t, 256> local_bins;\n    MPI_Allreduce(bins.data(), local_bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n    bins = local_bins;\n}",
            "int mpi_size = 1;\n    int mpi_rank = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    if (mpi_rank == 0) {\n        std::fill(bins.begin(), bins.end(), 0);\n    }\n    const int pixel_size = 3;\n    const size_t pixel_count = image.size() / pixel_size;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < pixel_count; i++) {\n        int intensity = image[i * pixel_size];\n        #pragma omp atomic\n        bins[intensity] += 1;\n    }\n\n    std::vector<size_t> sums(256);\n    MPI_Allreduce(bins.data(), sums.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < 256; i++) {\n        bins[i] = sums[i];\n    }\n }",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int num_of_chunks = size * OMP_NUM_THREADS;\n   int chunk_size = image.size() / num_of_chunks;\n\n#pragma omp parallel for\n   for (int i = 0; i < num_of_chunks; i++) {\n      int start = (chunk_size * i) + (chunk_size * rank);\n      int end = start + chunk_size;\n      if (end > image.size()) {\n         end = image.size();\n      }\n      std::vector<int> copy = std::vector<int>(image.begin() + start, image.begin() + end);\n      for (int j = 0; j < copy.size(); j++) {\n         bins[copy[j]]++;\n      }\n   }\n}",
            "int numProcs, procRank;\n   MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &procRank);\n\n   const int blockSize = 256/numProcs;\n   std::vector<int> partialCounts(blockSize);\n   std::fill(partialCounts.begin(), partialCounts.end(), 0);\n\n   for (auto pixel : image) {\n     int index = pixel/blockSize;\n     int offset = pixel%blockSize;\n     partialCounts[offset] += index;\n   }\n\n   std::vector<int> allCounts(256);\n   MPI_Allreduce(partialCounts.data(), allCounts.data(), 256, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n   // Convert to size_t\n   for (int i = 0; i < 256; ++i) {\n     bins[i] = allCounts[i];\n   }\n\n }",
            "#pragma omp parallel for schedule(static)\n     for (size_t j = 0; j < image.size(); j++) {\n         bins[image[j]]++;\n     }\n }",
            "// YOUR CODE HERE\n }",
            "int proc_num, proc_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &proc_num);\n    MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);\n\n    int width = 8;\n    int height = 4;\n\n    int chunk_num = (height + proc_num - 1) / proc_num;\n\n    int start_row = proc_rank * chunk_num;\n    int end_row = std::min(start_row + chunk_num, height);\n    int chunk_height = end_row - start_row;\n\n    size_t local_pixel_num = width * chunk_height;\n\n    size_t chunk_size = local_pixel_num / proc_num;\n    size_t remainder = local_pixel_num % proc_num;\n\n    size_t offset = proc_rank * chunk_size;\n\n    offset = offset + std::min(proc_rank, remainder);\n\n#pragma omp parallel for\n    for (size_t i = offset; i < offset + chunk_size; i++) {\n        bins[image[i]]++;\n    }\n\n    int prev = 0;\n    int curr = 0;\n    int next = 0;\n\n    // std::array<size_t, 256> send_bins;\n    // std::array<size_t, 256> recv_bins;\n\n    if (proc_rank > 0) {\n        MPI_Status status;\n        MPI_Recv(&prev, 256, MPI_UNSIGNED_LONG, proc_rank - 1, 0, MPI_COMM_WORLD, &status);\n        curr = bins[0];\n        bins[0] = prev + curr;\n\n        for (int i = 1; i < 256; i++) {\n            next = bins[i];\n            bins[i] = prev + next;\n            prev = curr;\n            curr = next;\n        }\n    }\n\n    if (proc_rank < proc_num - 1) {\n        MPI_Send(bins.data() + 256 - 1, 256, MPI_UNSIGNED_LONG, proc_rank + 1, 0, MPI_COMM_WORLD);\n    }\n\n    if (proc_rank > 0) {\n        MPI_Send(bins.data(), 256, MPI_UNSIGNED_LONG, proc_rank - 1, 0, MPI_COMM_WORLD);\n    }\n}",
            "int rank, nproc;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  size_t npx = image.size();\n\n  size_t px_per_proc = npx / nproc;\n  size_t my_start = rank * px_per_proc;\n  size_t my_end = my_start + px_per_proc;\n\n  for (size_t i = my_start; i < my_end; i++) {\n    if (image[i] > 0) {\n      #pragma omp atomic update\n      bins[image[i]] += 1;\n    }\n  }\n\n  if (nproc > 1) {\n    size_t px_to_send = 0;\n    if (rank!= 0) {\n      // send px_per_proc / 2 to rank - 1\n      MPI_Send(image.data() + my_start, px_per_proc / 2, MPI_INT, rank - 1, 0, MPI_COMM_WORLD);\n      px_to_send = px_per_proc / 2;\n    } else {\n      px_to_send = px_per_proc;\n    }\n    if (rank!= nproc - 1) {\n      // send px_per_proc / 2 to rank + 1\n      MPI_Send(image.data() + my_end - px_per_proc / 2, px_per_proc / 2, MPI_INT, rank + 1, 0, MPI_COMM_WORLD);\n      px_to_send += px_per_proc / 2;\n    }\n\n    if (rank == 0) {\n      // receive from rank 1\n      int recvcount;\n      MPI_Status status;\n      MPI_Probe(rank + 1, 0, MPI_COMM_WORLD, &status);\n      MPI_Get_count(&status, MPI_INT, &recvcount);\n      std::vector<int> px_to_recv(recvcount);\n      MPI_Recv(px_to_recv.data(), recvcount, MPI_INT, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t i = 0; i < px_to_recv.size(); i++) {\n        if (px_to_recv[i] > 0) {\n          #pragma omp atomic update\n          bins[px_to_recv[i]] += 1;\n        }\n      }\n\n      // receive from rank nproc - 1\n      MPI_Probe(rank - 1, 0, MPI_COMM_WORLD, &status);\n      MPI_Get_count(&status, MPI_INT, &recvcount);\n      px_to_recv.resize(recvcount);\n      MPI_Recv(px_to_recv.data(), recvcount, MPI_INT, rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t i = 0; i < px_to_recv.size(); i++) {\n        if (px_to_recv[i] > 0) {\n          #pragma omp atomic update\n          bins[px_to_recv[i]] += 1;\n        }\n      }\n    }\n  }\n}",
            "int rank, nproc;\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n#pragma omp parallel\n     {\n         // TODO: each thread computes bins[0]...bins[7]\n         std::array<size_t, 256> local_bins;\n         for (int i = 0; i < 256; i++) {\n             local_bins[i] = 0;\n         }\n#pragma omp for\n         for (int i = 0; i < image.size(); i++) {\n             local_bins[image[i]] += 1;\n         }\n#pragma omp critical\n         for (int i = 0; i < 256; i++) {\n             bins[i] += local_bins[i];\n         }\n     }\n\n     return;\n }",
            "#pragma omp parallel for num_threads(2)\nfor(int i = 0; i < image.size(); i++){\n\n\n    bins[image[i]]++;\n}\n\n\n\n }",
            "// TO DO: add code\n   int nprocs;\n   int myrank;\n   MPI_Comm_size(MPI_COMM_WORLD,&nprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD,&myrank);\n\n   int chunkSize = image.size()/nprocs;\n   int extra = image.size()%nprocs;\n\n   std::vector<int> chunk;\n   int i = myrank;\n   int j = 0;\n   for(int k = 0; k < nprocs; k++){\n     if(i<nprocs){\n       chunk.resize(chunkSize);\n       if(i < extra){\n         chunkSize += 1;\n       }\n       for(int z = 0; z < chunkSize; z++){\n         chunk[z] = image[j];\n         j++;\n       }\n       #pragma omp parallel for\n       for(int x = 0; x < chunk.size(); x++){\n         bins[chunk[x]]++;\n       }\n     }\n     i++;\n   }\n   MPI_Reduce(&bins[0], &bins[0], 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        bins[image[i]]++;\n    }\n}",
            "int nprocs, myrank, rc;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n    int blockSize = image.size() / nprocs;\n    int reminder = image.size() % nprocs;\n    int start = myrank * blockSize;\n    int end = start + blockSize;\n    if (myrank == nprocs - 1)\n    {\n        end += reminder;\n    }\n    // OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < 256; i++)\n    {\n        bins[i] = 0;\n    }\n    #pragma omp parallel for\n    for (int i = start; i < end; i++)\n    {\n        bins[image[i]]++;\n    }\n}",
            "// your code here\n}",
            "// TODO\n     int size, rank;\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     int n_pixels = image.size() / size;\n     int start = n_pixels * rank;\n     int end = start + n_pixels;\n     std::vector<int> image_block(image.begin() + start, image.begin() + end);\n     int sum = 0;\n     for (int i = 0; i < n_pixels; i++) {\n         sum += image_block[i];\n     }\n#pragma omp parallel for\n     for (int i = 0; i < 256; i++) {\n         for (int j = 0; j < size; j++) {\n             if (image_block[i] == i) {\n                 bins[i] += 1;\n             }\n         }\n     }\n     MPI_Reduce(MPI_IN_PLACE, bins.data(), 256, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n     MPI_Barrier(MPI_COMM_WORLD);\n }",
            "int rank = 0;\n     int size = 1;\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n     bins.fill(0);\n     int pixelCount = (int) image.size();\n     int chunkSize = pixelCount / size;\n     int remainPixel = pixelCount % size;\n     int startIndex = rank * chunkSize;\n     int endIndex = startIndex + chunkSize;\n     if(rank < remainPixel)\n     {\n         endIndex++;\n     }\n     for(int i = startIndex; i < endIndex; i++)\n     {\n         bins[image[i]]++;\n     }\n\n     MPI_Reduce(bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "// TODO\n   // MPI_Init();\n   // int nproc, rank;\n   // MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n   // MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // // MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n   // // MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // // MPI_Barrier(MPI_COMM_WORLD);\n\n   // // int nproc, rank;\n   // // MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n   // // MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   // std::cout<<\"I'm rank \"<<rank<<std::endl;\n   // int chunk=image.size()/nproc;\n   // std::vector<int> img_chunk(chunk);\n\n   // // printf(\"I am %d and my chunk is %d\\n\", rank, chunk);\n   // // for(int i=0; i<chunk; i++){\n   // //   img_chunk[i]=image[i+rank*chunk];\n   // // }\n\n   // MPI_Status status;\n   // MPI_Send(&image[rank*chunk], chunk, MPI_INT, 0, rank, MPI_COMM_WORLD);\n   // MPI_Recv(&img_chunk[0], chunk, MPI_INT, 0, MPI_ANY_SOURCE, MPI_COMM_WORLD, &status);\n   // int recv_rank=status.MPI_SOURCE;\n\n   // // std::vector<int> img_chunk=image[rank*chunk];\n   // // std::cout<<img_chunk[0]<<std::endl;\n   // // std::cout<<image[rank*chunk]<<std::endl;\n   // // std::cout<<img_chunk[0]<<std::endl;\n\n   // for(int i=0; i<chunk; i++){\n   //   bins[img_chunk[i]]++;\n   // }\n\n   // // std::cout<<img_chunk[0]<<std::endl;\n   // // std::cout<<bins[img_chunk[0]]<<std::endl;\n\n   // MPI_Finalize();\n\n\n   // int nproc, rank;\n   // MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n   // MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // // std::cout<<\"I'm rank \"<<rank<<std::endl;\n\n   // int chunk=image.size()/nproc;\n   // std::vector<int> img_chunk(chunk);\n   // // printf(\"I am %d and my chunk is %d\\n\", rank, chunk);\n   // // for(int i=0; i<chunk; i++){\n   // //   img_chunk[i]=image[i+rank*chunk];\n   // // }\n\n   // // std::cout<<img_chunk[0]<<std::endl;\n   // // std::cout<<image[rank*chunk]<<std::endl;\n   // // std::cout<<img_chunk[0]<<std::endl;\n\n   // MPI_Status status;\n   // MPI_Send(&image[rank*chunk], chunk, MPI_INT, 0, rank, MPI_COMM_WORLD);\n   // MPI_Recv(&img_chunk[0], chunk, MPI_INT, 0, MPI_ANY_SOURCE, MPI_COMM_WORLD, &status);\n   // int recv_rank=status.MPI_SOURCE;\n\n   // // std::vector<int> img_chunk=image[rank*chunk];\n   // // std::cout<<img_chunk[0]<<std::endl;\n   // // std::cout<<image[rank*chunk]<<std::endl;\n   // // std::cout<<img_chunk[0]<<std::endl;\n\n   // for(int i=0; i<chunk; i++){\n   //   bins[img_chunk[i",
            "// TODO: Your code here\n#pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n\n}",
            "bins.fill(0);\n     for (auto& i : image)\n         bins[i]++;\n\n     // parallel_for\n     // #pragma omp parallel for\n     // for (int i = 0; i < bins.size(); i++)\n     //     bins[i]++;\n\n     // MPI\n     // int rank, size;\n     // MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     // MPI_Comm_size(MPI_COMM_WORLD, &size);\n     // for (int i = rank; i < bins.size(); i += size)\n     //     bins[i]++;\n }",
            "int num_proc = 4;\n   int num_thrd = 8;\n   int rank = 0;\n   int size = 0;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // Split image into num_proc chunks\n   int proc_id = rank;\n   int proc_num = num_proc;\n   int proc_size = image.size() / proc_num;\n   if (image.size() % proc_num!= 0) proc_size++;\n\n   // Split image into chunks and count pixels in each chunk\n   int chunk_start = proc_size * proc_id;\n   int chunk_end = proc_size * (proc_id + 1);\n   if (proc_id == proc_num - 1) {\n     chunk_end = image.size();\n   }\n   int chunk_size = chunk_end - chunk_start;\n   std::array<size_t, 256> my_bins = {};\n   for (int i = chunk_start; i < chunk_end; i++) {\n     my_bins[image[i]]++;\n   }\n   std::vector<size_t> bins_all(256, 0);\n\n#pragma omp parallel num_threads(num_thrd)\n   {\n     // Compute pixel counts in parallel\n     int tid = omp_get_thread_num();\n     int num_thrd = omp_get_num_threads();\n\n     std::array<size_t, 256> bins_thrd = {};\n     for (int i = chunk_start; i < chunk_end; i++) {\n       bins_thrd[image[i]]++;\n     }\n     // Sum results from all threads to get the final results\n     for (int i = 0; i < 256; i++) {\n       bins_all[i] += bins_thrd[i];\n     }\n   }\n   // Sum the results across all ranks\n   for (int i = 0; i < 256; i++) {\n     if (rank == 0) {\n       bins[i] = bins_all[i];\n     } else {\n       MPI_Reduce(&bins_all[i], &bins[i], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n     }\n   }\n }",
            "bins.fill(0);\n     int rank, size;\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n     int local_size = image.size();\n     int local_bins[256];\n     std::fill(local_bins, local_bins+256, 0);\n     for (int i=0; i<local_size; i++) {\n        local_bins[image[i]]++;\n     }\n\n     MPI_Reduce(local_bins, bins.data(), 256, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "//TODO: Your code here\n   //...\n }",
            "std::array<size_t, 256> localBins;\n   localBins.fill(0);\n   for (int i = 0; i < image.size(); i++) {\n     localBins[image[i]]++;\n   }\n   int nprocs, myrank;\n   MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n   if (myrank == 0) {\n     for (int i = 1; i < nprocs; i++) {\n       MPI_Recv(&(localBins[i]), 256, MPI_LONG_LONG_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n     }\n   } else {\n     MPI_Send(&(localBins[0]), 256, MPI_LONG_LONG_INT, 0, 0, MPI_COMM_WORLD);\n   }\n   bins = localBins;\n }",
            "// TODO: Your code here\n\n }",
            "// TODO\n    int rank, nprocs, imageSize;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    imageSize = image.size();\n    if(rank==0){\n        for(int i=0; i<256; i++){\n            bins[i] = 0;\n        }\n        int step = imageSize/nprocs;\n        int last = imageSize%nprocs;\n        int current = 0;\n        for(int i=0; i<nprocs; i++){\n            int count = 0;\n            if(i==nprocs-1){\n                for(int j=current; j<current+last; j++){\n                    bins[image[j]] += 1;\n                }\n            }else{\n                for(int j=current; j<current+step; j++){\n                    bins[image[j]] += 1;\n                }\n            }\n            current += step;\n        }\n    }\n}",
            "size_t N_pixels=image.size();\n\t size_t N_pixels_per_proc=N_pixels/size;\n\t size_t N_left_over=N_pixels%size;\n\t int rank=MPI_Comm_rank(MPI_COMM_WORLD);\n\t size_t start_index=N_pixels_per_proc*rank+std::min(rank,N_left_over);\n\t size_t end_index=std::min(start_index+N_pixels_per_proc,N_pixels);\n\t size_t index=start_index;\n\t while(index<end_index)\n\t {\n\t\t bins[image[index]]++;\n\t\t index++;\n\t }\n\t MPI_Gatherv(bins.data(),bins.size(),MPI_INT,bins.data(),bins.size(),MPI_INT,0,MPI_COMM_WORLD);\n\t return;\n }",
            "#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel num_threads(4)\n  {\n      int tid = omp_get_thread_num();\n\n      //MPI stuff\n      int size, rank;\n      MPI_Comm_size(MPI_COMM_WORLD, &size);\n      MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n      //OMP stuff\n      int num_threads = omp_get_num_threads();\n      int thread_id = omp_get_thread_num();\n\n      size_t pixels_per_thread = image.size()/num_threads;\n      size_t start = thread_id * pixels_per_thread;\n      size_t end = (thread_id+1) * pixels_per_thread;\n\n      for(size_t i = start; i < end; i++) {\n          bins[image[i]]++;\n      }\n\n      if(thread_id == (num_threads-1)) {\n          for(size_t i = end; i < image.size(); i++) {\n              bins[image[i]]++;\n          }\n      }\n\n      //barrier for MPI\n      MPI_Barrier(MPI_COMM_WORLD);\n\n      //Gather results to rank 0\n      if(rank == 0) {\n          size_t *bins_ptr = bins.data();\n          size_t *bins_array = new size_t[256*num_threads];\n          MPI_Gather(bins_ptr, 256, MPI_LONG_LONG, bins_array, 256, MPI_LONG_LONG, 0, MPI_COMM_WORLD);\n          for(int i = 0; i < 256*num_threads; i++) {\n              bins[i] += bins_array[i];\n          }\n      } else {\n          MPI_Gather(bins.data(), 256, MPI_LONG_LONG, NULL, 256, MPI_LONG_LONG, 0, MPI_COMM_WORLD);\n      }\n\n      //barrier for MPI\n      MPI_Barrier(MPI_COMM_WORLD);\n\n      //Sort\n      std::sort(bins.begin(), bins.end());\n  }\n }",
            "#pragma omp parallel\n   {\n     int rank, nprocs;\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n     int size = image.size();\n     int chunk = size / nprocs;\n     int remainder = size % nprocs;\n     int start = chunk * rank;\n\n     if (rank == 0) {\n       start = 0;\n     }\n     if (rank == nprocs - 1) {\n       start += remainder;\n     }\n\n     // std::vector<int> vec_to_distribute = image;\n     std::vector<int> vec_to_distribute(chunk);\n     for (int i = 0; i < vec_to_distribute.size(); i++) {\n       vec_to_distribute[i] = image[i + start];\n     }\n\n     #pragma omp for schedule(static)\n     for (int i = 0; i < chunk; i++) {\n       int intensity = vec_to_distribute[i];\n       bins[intensity] += 1;\n     }\n\n     #pragma omp barrier\n\n     #pragma omp master\n     {\n       if (rank == 0) {\n         for (int i = 0; i < vec_to_distribute.size(); i++) {\n           bins[vec_to_distribute[i]] += 1;\n         }\n       }\n     }\n   }\n\n }",
            "// Write your code here\n\n}",
            "//TODO: add your code here\n    int numOfRanks;\n    int myRank;\n    MPI_Comm_size(MPI_COMM_WORLD, &numOfRanks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n\n    //each rank gets a number of images\n    int image_per_rank = image.size() / numOfRanks;\n    int extra_images = image.size() - (image_per_rank * numOfRanks);\n\n    //each rank gets an offset\n    int offset = (myRank * image_per_rank) + std::min(myRank, extra_images);\n\n    //each rank gets its own image\n    std::vector<int> my_image(image.begin() + offset, image.begin() + offset + image_per_rank);\n    \n    //loop through all images, count pixels for each intensity\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (int i = 0; i < image_per_rank; i++) {\n            //if the intensity is out of bounds, ignore it\n            if (my_image[i] >= 256) {\n                continue;\n            }\n            //increase pixel count\n            #pragma omp atomic\n            bins[my_image[i]]++;\n        }\n    }\n    \n    //check to see if there are more ranks to receive from\n    int recv_flag = 0;\n    //send total number of images for current rank\n    MPI_Send(&myRank, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    //receive from all other ranks\n    for (int i = 1; i < numOfRanks; i++) {\n        //check if there are more ranks to receive from\n        MPI_Recv(&recv_flag, 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        if (recv_flag) {\n            //get the rank to receive from\n            MPI_Recv(&recv_flag, 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            //get number of images to receive\n            MPI_Recv(&image_per_rank, 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            //get offset\n            MPI_Recv(&offset, 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            //get the image\n            MPI_Recv(&my_image, image_per_rank, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            //loop through all images, count pixels for each intensity\n            #pragma omp parallel\n            {\n                #pragma omp for\n                for (int i = 0; i < image_per_rank; i++) {\n                    //if the intensity is out of bounds, ignore it\n                    if (my_image[i] >= 256) {\n                        continue;\n                    }\n                    //increase pixel count\n                    #pragma omp atomic\n                    bins[my_image[i]]++;\n                }\n            }\n        }\n    }\n\n\n}",
            "// TODO: your code here\n   \n   \n   int rank = 0;\n   int size = 0;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // determine how much work each thread will do\n   int workPerThread = image.size() / size;\n   int workRemaining = image.size() - workPerThread * size;\n   \n   // calculate the bounds of the portion of the array that this thread will process\n   int begin = rank * workPerThread;\n   int end = begin + workPerThread;\n   if (rank < workRemaining) {\n      end++;\n   }\n\n   // loop over the portion of the array that this thread is responsible for and sum the elements that match the current thread's ID\n   size_t threadSum = 0;\n   for (int i = begin; i < end; i++) {\n      threadSum += image[i];\n   }\n\n   // create a vector to store thread sums, one for each thread\n   std::vector<size_t> threadSumVec(size);\n\n   // have each thread store its thread sum in the correct place\n   threadSumVec[rank] = threadSum;\n\n   // have each thread wait for the previous thread to complete\n   if (rank > 0) {\n      MPI_Recv(&threadSumVec[rank-1], 1, MPI_UNSIGNED_LONG, rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n   }\n\n   // have each thread send its thread sum to the next thread\n   if (rank < size-1) {\n      MPI_Send(&threadSumVec[rank], 1, MPI_UNSIGNED_LONG, rank + 1, 0, MPI_COMM_WORLD);\n   }\n\n   // store the final thread sum in the bins\n   if (rank == 0) {\n      size_t totalSum = 0;\n      for (int i = 0; i < threadSumVec.size(); i++) {\n         totalSum += threadSumVec[i];\n      }\n      bins[0] = totalSum;\n   }\n   \n   // store the thread sums in the bins\n   if (rank > 0) {\n      bins[threadSumVec[rank - 1]]++;\n   }\n\n   // have rank 0 sum the elements of the bins\n   if (rank == 0) {\n      for (int i = 1; i < 256; i++) {\n         bins[i] += bins[i-1];\n      }\n   }\n\n   // have each thread wait for the final sum to complete\n   if (rank < size-1) {\n      MPI_Recv(&bins[255], 1, MPI_UNSIGNED_LONG, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n   }\n\n   // have rank 0 send the final sum to the correct position in the bins\n   if (rank == 0) {\n      MPI_Send(&bins[255], 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n   }\n\n   // store the final thread sum in the bins\n   if (rank > 0) {\n      MPI_Recv(&bins[255], 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n   }\n\n   // sum the elements in the bins\n   for (int i = 1; i < 256; i++) {\n      bins[i] += bins[i-1];\n   }\n }",
            "}",
            "// Fill bins with 0\n    bins.fill(0);\n\n    // TODO: Replace this code with MPI/OpenMP\n    // #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n\n    // TODO: Use MPI to distribute image equally among ranks.\n    // TODO: Use MPI to sum the counts in bins on each rank.\n    // TODO: Store the summed counts in bins on rank 0.\n}",
            "/*\n   Your code here.\n   */\n    bins.fill(0);\n    int nrows = image.size() / 256;\n    omp_set_num_threads(4);\n    #pragma omp parallel for\n    for (int i = 0; i < nrows; ++i){\n        for(int j = 0; j < 256; ++j){\n            if(image[i*256+j] == j){\n                bins[j]++;\n            }\n        }\n    }\n}",
            "int num_pixels = image.size();\n   size_t num_ranks = size_t(MPI_Comm_size(MPI_COMM_WORLD));\n   int rank = MPI_Comm_rank(MPI_COMM_WORLD);\n   int chunk_size = num_pixels/num_ranks;\n   size_t chunk_offset = rank * chunk_size;\n   int chunk_remainder = num_pixels % num_ranks;\n\n   // Count the number of pixels in this image chunk\n   int thread_count = omp_get_max_threads();\n   std::array<int, 256> counts;\n   counts.fill(0);\n#pragma omp parallel for schedule(static)\n   for (int i = 0; i < chunk_size; i++) {\n     int gray_value = image[chunk_offset + i];\n     int pixel_count = 1;\n     // This value is not used, it is just to have a value for the pixel in counts\n     if (gray_value > 255) {\n       pixel_count = 0;\n     }\n     counts[gray_value] += pixel_count;\n   }\n   if (rank < chunk_remainder) {\n     int gray_value = image[chunk_offset + chunk_size + rank];\n     int pixel_count = 1;\n     if (gray_value > 255) {\n       pixel_count = 0;\n     }\n     counts[gray_value] += pixel_count;\n   }\n\n   // Reduce the counts from each rank\n   std::vector<size_t> my_counts(256);\n   my_counts.assign(counts.begin(), counts.begin() + 256);\n   std::vector<size_t> global_counts(256);\n   MPI_Allreduce(&my_counts[0], &global_counts[0], 256, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n   bins = global_counts;\n }",
            "int rank = 0;\n   int size = 0;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int const totalPixels = image.size();\n   size_t const blockSize = (totalPixels + size - 1) / size;\n   size_t const startPixel = rank * blockSize;\n   size_t const endPixel = std::min(startPixel + blockSize, (size_t) totalPixels);\n\n   for (size_t pix = startPixel; pix < endPixel; ++pix) {\n     ++bins[image[pix]];\n   }\n\n   // combine histograms\n   std::vector<size_t> all_bins(bins.begin(), bins.end());\n\n   MPI_Allreduce(MPI_IN_PLACE, all_bins.data(), all_bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n     for (size_t i = 0; i < all_bins.size(); ++i) {\n       bins[i] = all_bins[i];\n     }\n   }\n }",
            "#ifdef HAVE_MPI\n   int rank, size;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int total;\n   MPI_Allreduce(&image.size(), &total, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n   int local_total = image.size() / size;\n   int leftovers = image.size() - local_total * size;\n\n   int start = (rank * local_total) + std::min(leftovers, rank);\n   int end = start + local_total;\n   if (rank < leftovers) {\n     end += 1;\n   }\n\n   int left = start;\n   int right = start;\n   if (rank > 0) {\n     left = (rank - 1) * local_total + std::min(leftovers, rank - 1);\n   }\n   if (rank < (size - 1)) {\n     right = (rank + 1) * local_total + std::min(leftovers, rank + 1);\n   }\n\n   int left_pixel = image[left];\n   int right_pixel = image[right];\n\n   if (rank == 0) {\n     bins.fill(0);\n   }\n\n#pragma omp parallel\n   {\n     std::array<size_t, 256> local_bins;\n     local_bins.fill(0);\n\n#pragma omp for\n     for (int i = start; i < end; i++) {\n       int pixel = image[i];\n       local_bins[pixel] += 1;\n     }\n\n#pragma omp critical\n     {\n       for (int i = 0; i < 256; i++) {\n         bins[i] += local_bins[i];\n       }\n     }\n   }\n\n   if (rank == 0) {\n     MPI_Send(bins.data(), 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n   } else {\n     MPI_Recv(bins.data(), 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n   }\n#else\n   bins.fill(0);\n   int left = -1;\n   int right = image.size();\n\n   for (int i = 0; i < image.size(); i++) {\n     int pixel = image[i];\n     bins[pixel] += 1;\n   }\n#endif\n }",
            "#pragma omp parallel\n   {\n     int rank, size;\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n     std::array<size_t, 256> localBins;\n     for (int i = 0; i < 256; ++i) {\n       localBins[i] = 0;\n     }\n     int nRows = image.size() / 256;\n     int n = 0;\n     for (int row = rank; row < nRows; row += size) {\n       for (int i = 0; i < 256; ++i) {\n         localBins[i] += (image[row * 256 + i] == i);\n       }\n     }\n\n     //Gather all counts\n     MPI_Allreduce(localBins.data(), bins.data(), 256, MPI_LONG, MPI_SUM, MPI_COMM_WORLD);\n   }\n }",
            "// Compute the number of pixels in each range\n\tint num_pixels = image.size();\n\n\t// create thread for each range\n\t#pragma omp parallel for\n\tfor (int i = 0; i < 256; i++) {\n\t\t// create thread for each pixel\n\t\tfor (int j = 0; j < num_pixels; j++) {\n\t\t\tif (image[j] == i) {\n\t\t\t\t// increment the number of pixels\n\t\t\t\t#pragma omp atomic\n\t\t\t\tbins[i]++;\n\t\t\t}\n\t\t}\n\t}\n}",
            "int numRanks, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int size = image.size();\n\n  int blocksPerRank = size/numRanks;\n  int remainder = size%numRanks;\n  int start = 0;\n  int end = 0;\n\n  for(int i = 0; i < numRanks; i++) {\n    if(i<remainder) {\n      end += blocksPerRank+1;\n    }\n    else {\n      end += blocksPerRank;\n    }\n  }\n\n  if(rank == 0) {\n    std::vector<int> buffer(256, 0);\n    for(int i = start; i < end; i++) {\n      buffer[image[i]]++;\n    }\n    for(int i = 0; i < 256; i++) {\n      bins[i] = buffer[i];\n    }\n  }\n  else {\n    std::vector<int> buffer(256, 0);\n    for(int i = start; i < end; i++) {\n      buffer[image[i]]++;\n    }\n    MPI_Send(buffer.data(), 256, MPI_INT, 0, 100, MPI_COMM_WORLD);\n  }\n\n}",
            "int rank, nranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n    // Use OpenMP to parallelize loop over pixels in image.\n    // Store counts in bins.\n    #pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]] += 1;\n    }\n\n    // Reduce counts into bins on rank 0.\n    // Use MPI_SUM.\n    int total_ranks = nranks;\n    if (rank == 0) {\n        for (int i = 1; i < total_ranks; i++) {\n            MPI_Status status;\n            MPI_Recv(&bins, 256, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n        }\n    } else {\n        MPI_Send(&bins, 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    int image_size = image.size();\n    int blocks_per_rank = image_size / world_size;\n    int remainder = image_size % world_size;\n    int blocks_this_rank = blocks_per_rank + (world_rank < remainder? 1 : 0);\n    int block_start = blocks_per_rank * world_rank + std::min(world_rank, remainder);\n    int block_end = block_start + blocks_this_rank;\n    int threads_per_block = 128;\n    int blocks_per_thread = (blocks_this_rank + threads_per_block - 1) / threads_per_block;\n    std::array<size_t, 256> local_bins{};\n\n#pragma omp parallel for\n    for (int i = block_start; i < block_end; i += blocks_per_thread) {\n        int image_start = i * 256;\n        int image_end = (i + 1) * 256;\n        for (int j = image_start; j < image_end; j++) {\n            local_bins[image[j]]++;\n        }\n    }\n\n    MPI_Reduce(MPI_IN_PLACE, local_bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (world_rank == 0) {\n        bins = local_bins;\n    }\n}",
            "int world_size = 0;\n     int world_rank = 0;\n     MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n     MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n     const size_t width = image.size() / world_size;\n     const size_t start = world_rank * width;\n\n     std::vector<size_t> counts(256, 0);\n     #pragma omp parallel for\n     for (size_t i = 0; i < width; i++) {\n         counts[image[start + i]]++;\n     }\n     // Reduction over all ranks\n     MPI_Allreduce(counts.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "if (bins.size()!= 256) {\n        std::cout << \"Input array is the wrong size: \" << bins.size() << '\\n';\n        return;\n    }\n    int const nRanks = omp_get_max_threads();\n\n#pragma omp parallel num_threads(nRanks)\n    {\n        int rank = omp_get_thread_num();\n        int size = omp_get_num_threads();\n        if (rank == 0)\n            std::fill(bins.begin(), bins.end(), 0);\n\n        int chunkSize = image.size() / size;\n        int localOffset = rank * chunkSize;\n        int localSize = chunkSize;\n        if (rank == size - 1)\n            localSize = image.size() - localOffset;\n        if (rank == size - 1)\n            localSize = image.size() - localOffset;\n\n        // count local pixels\n        for (int i = 0; i < localSize; i++)\n            bins[image[localOffset + i]]++;\n\n        if (rank == 0) {\n            // add counts to global sum\n            for (int i = 1; i < size; i++) {\n                MPI_Recv(&bins[0], 256, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD,\n                         MPI_STATUS_IGNORE);\n            }\n        } else {\n            MPI_Send(&bins[0], 256, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n        }\n    }\n}",
            "int num_ranks, rank;\n     MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n     int block_size = image.size()/num_ranks;\n\n     int blocks = block_size/256;\n     if (block_size%256) blocks++;\n\n     int remainder = block_size%256;\n\n     std::vector<size_t> bins_local(256, 0);\n\n     if (rank == 0) {\n         for (int i = 0; i < blocks; i++) {\n             #pragma omp parallel for schedule(static)\n             for (int j = 0; j < 256; j++) {\n                 bins[j] += bins_local[j];\n             }\n         }\n\n         if (remainder) {\n             #pragma omp parallel for schedule(static)\n             for (int j = 0; j < 256; j++) {\n                 bins[j] += bins_local[j];\n             }\n         }\n     } else {\n         int start = rank * block_size;\n         int end = start + block_size;\n\n         if (remainder) {\n             end = start + remainder;\n         }\n         end = std::min(end, image.size());\n\n         #pragma omp parallel for schedule(static)\n         for (int i = start; i < end; i++) {\n             bins_local[image[i]]++;\n         }\n\n         MPI_Reduce(MPI_IN_PLACE, bins_local.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n     }\n }",
            "bins.fill(0);\n    #pragma omp parallel for\n    for(int i=0; i<image.size(); i++) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n   for (auto i = 0u; i < image.size(); ++i)\n     ++bins[image[i]];\n }",
            "int mpi_size, mpi_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n  std::vector<int> local_image;\n  int local_size = image.size() / mpi_size;\n  int global_offset = mpi_rank * local_size;\n  int local_offset = global_offset;\n  for (int i = 0; i < local_size; ++i) {\n    local_image.push_back(image[local_offset++]);\n  }\n\n  std::array<size_t, 256> local_bins;\n  std::fill(local_bins.begin(), local_bins.end(), 0);\n#pragma omp parallel\n  {\n    int i = omp_get_thread_num();\n    int j = omp_get_num_threads();\n    local_bins[local_image[i]]++;\n  }\n  MPI_Allreduce(MPI_IN_PLACE, local_bins.data(), local_bins.size(), MPI_LONG_LONG_INT, MPI_SUM, MPI_COMM_WORLD);\n  for (int i = 0; i < 256; ++i) {\n    bins[i] += local_bins[i];\n  }\n  return;\n}",
            "#pragma omp parallel \n   {\n     // This is the OpenMP part.\n     // Each thread has to have its own array, but we can share the result array bins.\n     std::array<size_t, 256> local_bins = {0};\n     // The array holding the image is also private per thread.\n     std::vector<int> local_image(image);\n\n     // For each pixel in the image\n#pragma omp for\n     for (size_t i = 0; i < image.size(); ++i) {\n       // Increment the pixel count for its grayscale value\n       local_bins[local_image[i]]++;\n     }\n     // Write our local results into the results array `bins`\n#pragma omp critical\n     for (size_t i = 0; i < 256; ++i) {\n       bins[i] += local_bins[i];\n     }\n   }\n }",
            "MPI_Comm_rank(MPI_COMM_WORLD, &worldRank);\n     MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n\n     int n_x = worldSize;\n     int n_y = 1;\n     int x_start = 0;\n     int y_start = 0;\n\n     if (worldRank < worldSize / 2) {\n         n_x = worldSize / 2;\n         x_start = 0;\n         y_start = 0;\n     } else {\n         n_x = worldSize - worldSize / 2;\n         x_start = worldSize / 2;\n         y_start = worldSize / 2;\n     }\n\n     int x_length = image.size() / n_y / n_x;\n\n     std::array<size_t, 256> localBins;\n\n     #pragma omp parallel\n     {\n         size_t nthreads = omp_get_num_threads();\n         size_t threadNum = omp_get_thread_num();\n\n         int y_length = x_length;\n\n         int x_start = x_start + x_length * (threadNum % nthreads);\n         int x_end = x_start + x_length;\n         int y_start = y_start + y_length * (threadNum / nthreads);\n         int y_end = y_start + y_length;\n\n         for (int i = x_start; i < x_end; i++) {\n             for (int j = y_start; j < y_end; j++) {\n                 int gray = image[i * n_x + j * n_y];\n                 localBins[gray]++;\n             }\n         }\n     }\n\n     if (worldRank!= 0) {\n         MPI_Send(&localBins, 256, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n     } else {\n         std::fill(bins.begin(), bins.end(), 0);\n         for (int i = 1; i < worldSize; i++) {\n             std::array<size_t, 256> localBins;\n             MPI_Status status;\n             MPI_Recv(&localBins, 256, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, &status);\n             for (int j = 0; j < 256; j++) {\n                 bins[j] += localBins[j];\n             }\n         }\n     }\n }",
            "// TODO: implement me\n }",
            "if(bins.size()!= 256) {\n     throw std::runtime_error(\"The size of the bins must be 256\");\n   }\n   int nranks, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   // compute the number of workloads per rank\n   int const n = (int)image.size() / nranks;\n   // compute the remainders (number of workloads not divisible by the number of ranks)\n   int const r = image.size() % nranks;\n   // compute the starting and ending indexes of each rank\n   int const start = n * rank + std::min(rank, r);\n   int const end = start + n + (rank < r);\n   std::array<size_t, 256> local_bins;\n   // loop over the range of workloads assigned to this rank\n   for(int i = start; i < end; i++) {\n     // loop over the pixels in each workload\n     for(int j = 0; j < n; j++) {\n       // increment the number of pixels with grayscale value `image[i + j]`\n       ++local_bins[image[i + j]];\n     }\n   }\n   // update the `bins` vector\n   if(rank == 0) {\n     for(size_t i = 0; i < local_bins.size(); i++) {\n       bins[i] += local_bins[i];\n     }\n   }\n   // synchronize all ranks\n   MPI_Barrier(MPI_COMM_WORLD);\n }",
            "// Write your code here\n    size_t len = image.size();\n    int p = omp_get_max_threads();\n    size_t offset = 0;\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // MPI\n    MPI_Status status;\n    std::vector<int> sendbuf;\n    std::vector<int> recvbuf;\n    sendbuf.resize(len/p);\n    recvbuf.resize(len/p);\n\n    if (rank == 0) {\n        // first rank\n        for (int i = 1; i < p; i++) {\n            MPI_Send(image.data() + offset, len/p, MPI_INT, i, 0, MPI_COMM_WORLD);\n            offset += len/p;\n        }\n    } else {\n        MPI_Recv(sendbuf.data(), len/p, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n    }\n\n    // OpenMP\n    std::vector<size_t> counts(256);\n\n    #pragma omp parallel for num_threads(p)\n    for (size_t i = 0; i < len/p; i++) {\n        int val = sendbuf[i];\n        counts[val]++;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < p; i++) {\n            MPI_Recv(recvbuf.data(), len/p, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n            for (size_t j = 0; j < len/p; j++) {\n                counts[recvbuf[j]]++;\n            }\n        }\n    } else {\n        MPI_Send(counts.data(), len/p, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n    bins = counts;\n}",
            "int num_pixels = image.size();\n    int my_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    int n_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n\n    int n_rows = n_ranks * 4;\n    int n_cols = 4;\n    int n_pixels = n_rows * n_cols;\n\n    std::vector<int> counts(256);\n    std::vector<int> my_counts(256);\n\n    #pragma omp parallel for \n    for (int i = 0; i < num_pixels; i++) {\n        counts[image[i]]++;\n    }\n\n    for (int i = 0; i < 256; i++) {\n        my_counts[i] = counts[i];\n    }\n\n    int *bins_ptr = &(bins[0]);\n    MPI_Allreduce(my_counts.data(), bins_ptr, 256, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n}",
            "//TODO\n     // You may want to set bins[i] to 0 first for all i.\n     // You may want to initialize your MPI communicator with MPI_COMM_WORLD\n     MPI_Comm comm = MPI_COMM_WORLD;\n     int rank, size;\n     MPI_Comm_size(comm, &size);\n     MPI_Comm_rank(comm, &rank);\n     \n     std::array<size_t, 256> local_bins;\n     for(int i=0;i<256;i++) local_bins[i] = 0;\n     \n     std::vector<int> local_image;\n     local_image.assign(image.begin() + rank, image.begin() + (rank + 1)*image.size()/size);\n\n     for(auto i : local_image) local_bins[i]++;\n     \n     std::array<size_t, 256> global_bins;\n     MPI_Reduce(&local_bins[0], &global_bins[0], 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, comm);\n     \n     if(rank==0) {\n         for(int i=0;i<256;i++) bins[i] = global_bins[i];\n     }\n }",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tomp_set_num_threads(16);\n\n\tint N = image.size() / size;\n\tint remainder = image.size() % size;\n\tint offset = rank * N;\n\tif (rank < remainder) {\n\t\toffset += rank;\n\t}\n\telse {\n\t\toffset += remainder;\n\t}\n\n\t// MPI_Bcast(&bins, 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\tfor (int i = offset; i < offset + N; i++) {\n\t\tbins[image[i]]++;\n\t}\n\n\tMPI_Allreduce(MPI_IN_PLACE, bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\t// MPI_Bcast(&bins, 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n\treturn;\n}",
            "#pragma omp parallel\n  {\n      const int rank = omp_get_thread_num();\n      const int numThreads = omp_get_num_threads();\n      const int numProcesses = omp_get_num_procs();\n      std::array<size_t, 256> localBins{};\n      for(int i = rank; i < image.size(); i += numThreads) {\n          localBins[image[i]]++;\n      }\n      MPI_Reduce(MPI_IN_PLACE, localBins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n      if (rank == 0) {\n          std::copy(localBins.begin(), localBins.end(), bins.begin());\n      }\n  }\n}",
            "#pragma omp parallel\n   {\n      int rank;\n      MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n      size_t myBin[256];\n      // Fill the bins for this rank with 0s\n      for (size_t i = 0; i < 256; i++) {\n         myBin[i] = 0;\n      }\n      #pragma omp for\n      for (size_t i = 0; i < image.size(); i++) {\n         myBin[image[i]] += 1;\n      }\n      // Counts the number of elements in myBin\n      int mpi_size;\n      MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n      int block = 256/mpi_size;\n      int left_count[mpi_size];\n      // Sends the number of elements in each block from each rank to each other rank\n      for (int i = 0; i < mpi_size; i++) {\n         MPI_Send(&myBin[i*block], block, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD);\n         MPI_Recv(&left_count[i], block, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n      // Adds each number of elements in each block to get the total number of elements in each bin\n      #pragma omp for\n      for (int i = 0; i < mpi_size; i++) {\n         for (int j = 0; j < block; j++) {\n            myBin[j] += left_count[i];\n         }\n      }\n      // Copies myBin to bins on rank 0\n      if (rank == 0) {\n         for (int i = 0; i < 256; i++) {\n            bins[i] = myBin[i];\n         }\n      }\n   }\n   return;\n}",
            "int mpi_size, mpi_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n   int bins_per_rank = 256 / mpi_size;\n   int remainder = 256 % mpi_size;\n\n   size_t chunk_size = bins_per_rank;\n   if (mpi_rank < remainder) {\n     chunk_size++;\n   }\n\n   std::vector<size_t> bins_per_rank_vec(mpi_size);\n   MPI_Allgather(&chunk_size, 1, MPI_INT, &bins_per_rank_vec[0], 1, MPI_INT, MPI_COMM_WORLD);\n\n   int start_index = 0;\n   for (int i = 0; i < mpi_rank; i++) {\n     start_index += bins_per_rank_vec[i];\n   }\n   std::vector<size_t> local_bins(chunk_size);\n\n#pragma omp parallel for\n   for (size_t i = 0; i < image.size(); i++) {\n     int rank = image[i] / bins_per_rank_vec[mpi_rank];\n     int offset = image[i] % bins_per_rank_vec[mpi_rank];\n     local_bins[offset]++;\n   }\n\n   std::vector<size_t> global_bins(256);\n   MPI_Gather(&local_bins[0], chunk_size, MPI_LONG, &global_bins[0], chunk_size, MPI_LONG, 0, MPI_COMM_WORLD);\n\n   if (mpi_rank == 0) {\n     for (int i = 0; i < 256; i++) {\n       bins[i] = 0;\n     }\n     for (int i = 0; i < mpi_size; i++) {\n       for (int j = 0; j < bins_per_rank; j++) {\n         bins[i * bins_per_rank + j] = global_bins[i * bins_per_rank + j];\n       }\n     }\n   }\n\n   MPI_Barrier(MPI_COMM_WORLD);\n }",
            "/* TODO: Your code here. */\n    // TODO: Your code here.\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int world_rank;\n    int world_size;\n    MPI_Comm_rank(comm, &world_rank);\n    MPI_Comm_size(comm, &world_size);\n    int rows = image.size()/world_size;\n    int extra = image.size()%world_size;\n\n    std::vector<int> temp(rows);\n    if (world_rank == 0) {\n        for (int i = 0; i < rows; i++) {\n            temp[i] = image[i + rows * world_rank];\n        }\n    }\n    if (world_rank!= 0) {\n        MPI_Recv(&temp[0], rows, MPI_INT, 0, 0, comm, MPI_STATUS_IGNORE);\n    }\n\n    if (world_rank == 0) {\n        for (int i = 0; i < temp.size(); i++) {\n            bins[temp[i]]++;\n        }\n    }\n\n    int min_size = 10000;\n    int max_size = 0;\n    for (int i = 0; i < rows; i++) {\n        if (temp[i] > max_size) {\n            max_size = temp[i];\n        }\n        if (temp[i] < min_size) {\n            min_size = temp[i];\n        }\n    }\n\n\n    int* temp_send = new int[rows];\n    int* temp_recv = new int[rows];\n    for (int i = 0; i < rows; i++) {\n        temp_send[i] = temp[i];\n    }\n\n\n    if (world_rank == 0) {\n        for (int i = 1; i < world_size; i++) {\n            MPI_Send(&temp_send[0], rows, MPI_INT, i, 0, comm);\n        }\n    }\n\n    if (world_rank!= 0) {\n        MPI_Recv(&temp_recv[0], rows, MPI_INT, 0, 0, comm, MPI_STATUS_IGNORE);\n    }\n\n    if (world_rank == 0) {\n        for (int i = 0; i < temp_recv.size(); i++) {\n            bins[temp_recv[i]]++;\n        }\n    }\n    delete []temp_send;\n    delete []temp_recv;\n\n    if (world_rank == 0) {\n        for (int i = min_size; i <= max_size; i++) {\n            std::cout << bins[i] << \" \";\n        }\n        std::cout << std::endl;\n    }\n\n\n}",
            "const int n_bins = 256;\n  const int n_threads = omp_get_max_threads();\n  const int n_ranks = MPI_Comm_size(MPI_COMM_WORLD);\n  int rank;\n  int proc_num_bins;\n  int proc_image_size;\n  int proc_count_offset;\n  int global_count_offset;\n  int proc_bins_start;\n  int proc_bins_end;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  proc_num_bins = n_bins/n_ranks;\n  proc_image_size = image.size()/n_ranks;\n  proc_count_offset = rank*proc_image_size;\n  global_count_offset = rank*proc_num_bins;\n  proc_bins_start = rank*proc_num_bins;\n  proc_bins_end = (rank+1)*proc_num_bins;\n\n  #pragma omp parallel for num_threads(n_threads)\n  for (int i=0; i<proc_image_size; i++) {\n    int index = proc_count_offset + i;\n    int intensity = image[index];\n    int bin_index = global_count_offset + intensity;\n    bins[bin_index] += 1;\n  }\n\n}",
            "size_t size = image.size();\n    int nthr = omp_get_max_threads();\n    //TODO: Implement pixelCounts using MPI and OpenMP\n    bins = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,",
            "int my_rank;\n \tMPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n \tint n_ranks;\n \tMPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n\n \tint blocks_per_rank = image.size() / n_ranks;\n \tint remainder = image.size() % n_ranks;\n\n \tint n_threads = omp_get_max_threads();\n \tint blocks_per_thread = blocks_per_rank / n_threads;\n \tint remainder_per_thread = blocks_per_rank % n_threads;\n\n \tif (my_rank == 0) {\n \t\tfor (int i = 0; i < 256; i++) {\n \t\t\tbins[i] = 0;\n \t\t}\n \t}\n \tMPI_Bcast(&bins[0], 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n \tint offset = 0;\n \tif (my_rank == 0) {\n \t\toffset = remainder;\n \t}\n \toffset += blocks_per_rank * my_rank;\n\n \tfor (int i = 0; i < blocks_per_thread; i++) {\n \t\tif (i < remainder_per_thread) {\n \t\t\toffset += 1;\n \t\t}\n \t\tomp_set_num_threads(n_threads);\n#pragma omp parallel for\n \t\tfor (int j = offset; j < offset + blocks_per_thread; j++) {\n \t\t\tbins[image[j]] += 1;\n \t\t}\n \t\toffset += blocks_per_thread;\n \t}\n\n \tMPI_Barrier(MPI_COMM_WORLD);\n \tMPI_Reduce(&bins[0], &bins[0], 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n }",
            "int num_pixels_per_rank = image.size()/size;\n     size_t start = num_pixels_per_rank * rank;\n     size_t end = (rank == size - 1)? image.size() : start + num_pixels_per_rank;\n     int offset = image[0];\n     #pragma omp parallel for\n     for (int i = start; i < end; i++) {\n         if (image[i] < 256) {\n             bins[image[i] - offset] += 1;\n         }\n     }\n\n     if (rank == 0) {\n         size_t *bin_sum = new size_t[256];\n         MPI_Reduce(bins.data(), bin_sum, 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n         for (int i = 0; i < 256; i++) {\n             bins[i] = bin_sum[i];\n         }\n     } else {\n         MPI_Reduce(bins.data(), nullptr, 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n     }\n }",
            "// TODO: Your code here\n }",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int num = image.size() / size;\n    int rem = image.size() % size;\n    if (rank == 0)\n        for (int i = 0; i < 256; i++)\n            bins[i] = 0;\n    std::vector<int> temp;\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        int offset = num * tid;\n        int len = num;\n        if (tid == size - 1) {\n            len += rem;\n            if (len == 0)\n                offset = 0;\n        }\n        if (offset < image.size()) {\n            temp = std::vector<int>(image.begin() + offset, image.begin() + offset + len);\n        }\n        else\n            temp.resize(0);\n        #pragma omp for schedule(static, 1) nowait\n        for (int i = 0; i < temp.size(); i++)\n            bins[temp[i]]++;\n    }\n\n    if (rank == 0) {\n        int j = 0;\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&bins[0], 256, MPI_UNSIGNED_LONG, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n    else {\n        MPI_Send(&bins[0], 256, MPI_UNSIGNED_LONG, 0, 1, MPI_COMM_WORLD);\n    }\n}",
            "int rank;\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int size = 256 / world_size;\n    int remain = 256 % world_size;\n    int start = rank * size + (rank < remain? rank : remain);\n    int end = start + size + (rank < remain? 1 : 0);\n    std::array<size_t, 256> localBins = {0};\n\n    #pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        localBins[image[i]]++;\n    }\n\n    std::vector<size_t> globalBins(256);\n    if (rank == 0) {\n        for (int i = 0; i < world_size; i++) {\n            MPI_Status status;\n            MPI_Recv(&globalBins[i * size], size, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n        }\n    } else {\n        MPI_Send(&localBins, size, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < 256; i++) {\n            for (int j = 0; j < world_size; j++) {\n                globalBins[i] += globalBins[j * size + i];\n            }\n        }\n        bins = globalBins;\n    }\n}",
            "#pragma omp parallel\n    {\n        const int id = omp_get_thread_num();\n        const int nthreads = omp_get_num_threads();\n        const int size = image.size();\n        int chunk = (size - 1) / nthreads;\n        int from = id * chunk;\n        int to = from + chunk;\n        to = std::min(to, size - 1);\n\n        std::vector<int> my_bins(256);\n        for (int i = from; i < to; i++) {\n            ++my_bins[image[i]];\n        }\n        #pragma omp critical\n        {\n            for (int i = 0; i < 256; i++) {\n                bins[i] += my_bins[i];\n            }\n        }\n    }\n }",
            "size_t const n = image.size();\n\n  // Initialize bins\n  for (int i = 0; i < 256; i++) {\n    bins[i] = 0;\n  }\n\n  // Count pixels\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    bins[image[i]] += 1;\n  }\n\n  return;\n}",
            "int mpi_rank;\n    int mpi_size;\n    int x_dim;\n    int y_dim;\n    int x_rank_dim;\n    int y_rank_dim;\n\n    int x_dim_per_proc;\n    int y_dim_per_proc;\n    int x_left;\n    int x_right;\n    int y_top;\n    int y_bottom;\n\n    int x_pixel;\n    int y_pixel;\n\n    int num_pixels;\n\n    int pixel_count;\n\n    int x_pixel_coord;\n    int y_pixel_coord;\n\n    int x_pixel_index;\n    int y_pixel_index;\n\n    int x_pixel_offset;\n    int y_pixel_offset;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    x_dim = image.size();\n    y_dim = x_dim / 256;\n\n    //determine the dimension of the pixels in each process\n    x_dim_per_proc = x_dim / mpi_size;\n    x_left = mpi_rank * x_dim_per_proc;\n    x_right = (mpi_rank + 1) * x_dim_per_proc;\n    y_top = 0;\n    y_bottom = y_dim - 1;\n\n    num_pixels = 0;\n\n    for (y_pixel = y_top; y_pixel <= y_bottom; y_pixel++) {\n        for (x_pixel = x_left; x_pixel <= x_right; x_pixel++) {\n            num_pixels++;\n        }\n    }\n\n    //initialise the bins vector to be all zeros\n    bins.fill(0);\n\n    //calculate the number of pixels in each intensity for each process\n    #pragma omp parallel for\n    for (y_pixel = y_top; y_pixel <= y_bottom; y_pixel++) {\n        for (x_pixel = x_left; x_pixel <= x_right; x_pixel++) {\n\n            //get the index of the pixel\n            x_pixel_index = x_pixel / 256;\n            y_pixel_index = y_pixel * 256;\n            x_pixel_offset = x_pixel % 256;\n            y_pixel_offset = y_pixel * 256;\n\n            //get the pixel value\n            pixel_count = image.at(x_pixel_index + y_pixel_offset + x_pixel_offset);\n\n            //increase the intensity in the bin by the number of pixels of that intensity\n            bins[pixel_count]++;\n        }\n    }\n\n    //print out the bin values\n    for (int i = 0; i < 256; i++) {\n        std::cout << i << \" \" << bins[i] << std::endl;\n    }\n\n    //gather bins data in rank 0 to print out results\n    if (mpi_rank == 0) {\n        std::cout << std::endl;\n        std::cout << \"Number of pixels:\" << std::endl;\n        for (int i = 0; i < 256; i++) {\n            std::cout << i << \" \" << bins[i] << std::endl;\n        }\n    }\n\n    //calculate the total number of pixels in image\n    int total_num_pixels = 0;\n\n    //initialise the bins vector to be all zeros\n    bins.fill(0);\n\n    //determine the dimension of the pixels in each process\n    x_dim_per_proc = x_dim / mpi_size;\n    x_left = mpi_rank * x_dim_per_proc;\n    x_right = (mpi_rank + 1) * x_dim_per_proc;\n    y_top = 0;\n    y_bottom = y_dim -",
            "// TODO\n}",
            "#pragma omp parallel for\n     for (int i = 0; i < image.size(); i++)\n         bins[image[i]]++;\n\n     for(int i=0; i<256; i++){\n         #pragma omp critical\n         bins[i] += bins[i-1];\n     }\n}",
            "//TODO: Your code here\n  //use mpi send and recv to distribute the work\n  //use openmp to parallelize the process\n}",
            "#pragma omp parallel for num_threads(4)\n  for(int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "int nprocs;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int chunk_size = image.size() / nprocs;\n    int offset = rank * chunk_size;\n    int local_size = 0;\n    if(rank!= nprocs - 1)\n      local_size = chunk_size + 1;\n    else\n      local_size = image.size() - (nprocs - 1) * chunk_size;\n\n    std::vector<int> local_image;\n    local_image.reserve(local_size);\n    for (int i = 0; i < local_size; ++i)\n      local_image.emplace_back(image[i + offset]);\n\n    std::array<size_t, 256> local_bins;\n#pragma omp parallel for\n    for (int i = 0; i < local_size; ++i) {\n      ++local_bins[local_image[i]];\n    }\n    for (int i = 0; i < 256; ++i) {\n      MPI_Reduce(&local_bins[i], &bins[i], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n  }",
            "int MPI_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &MPI_rank);\n    int MPI_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &MPI_size);\n\n    int chunkSize = image.size() / MPI_size;\n    int remaining = image.size() % MPI_size;\n\n    int imageSize = image.size();\n    std::vector<int> subImage;\n\n    if (MPI_rank == 0) {\n        for (int rank = 1; rank < MPI_size; ++rank) {\n            MPI_Send(&image[rank * chunkSize], chunkSize, MPI_INT, rank, 1, MPI_COMM_WORLD);\n        }\n        if (remaining > 0) {\n            MPI_Send(&image[MPI_size * chunkSize], remaining, MPI_INT, MPI_size - 1, 1, MPI_COMM_WORLD);\n        }\n    } else if (MPI_rank == MPI_size - 1) {\n        MPI_Status status;\n        MPI_Recv(&subImage, remaining, MPI_INT, MPI_size - 1, 1, MPI_COMM_WORLD, &status);\n        subImage.resize(chunkSize + remaining);\n        MPI_Recv(&subImage[0], chunkSize, MPI_INT, 0, 1, MPI_COMM_WORLD, &status);\n    } else {\n        MPI_Status status;\n        MPI_Recv(&subImage, chunkSize, MPI_INT, 0, 1, MPI_COMM_WORLD, &status);\n    }\n\n    if (MPI_rank == 0) {\n        for (int i = 0; i < 256; ++i)\n            bins[i] = 0;\n\n        int count = 0;\n        for (auto x : subImage) {\n            ++bins[x];\n            ++count;\n        }\n\n        for (int rank = 1; rank < MPI_size; ++rank) {\n            MPI_Status status;\n            std::vector<int> tempBins;\n            MPI_Recv(&tempBins, 256, MPI_INT, rank, 1, MPI_COMM_WORLD, &status);\n            for (int i = 0; i < 256; ++i) {\n                bins[i] += tempBins[i];\n            }\n        }\n    } else {\n        std::array<size_t, 256> localBins = {0};\n        int count = 0;\n        for (auto x : subImage) {\n            ++localBins[x];\n            ++count;\n        }\n        MPI_Send(&localBins[0], 256, MPI_INT, 0, 1, MPI_COMM_WORLD);\n    }\n}",
            "if (omp_get_thread_num() == 0) {\n    bins.fill(0);\n  }\n  #pragma omp parallel for\n  for (int i = 0; i < image.size(); i++) {\n    int i_global = i + omp_get_thread_num() * image.size() / omp_get_num_threads();\n    bins[image[i_global]]++;\n  }\n}",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (rank == 0) {\n        for (int r=0; r<size; r++) {\n            for (int x=0; x<bins.size(); x++) {\n                if (r < size / 2) {\n                    bins[x] += image[x+r*256];\n                } else {\n                    bins[x] += image[x+(r-size/2)*256];\n                }\n            }\n        }\n    } else if (rank < size/2) {\n        int start = rank*256;\n        int end = start + 256;\n        for (int x=0; x<bins.size(); x++) {\n            bins[x] = image[x+start];\n        }\n    } else {\n        int start = (rank-size/2)*256;\n        int end = start + 256;\n        for (int x=0; x<bins.size(); x++) {\n            bins[x] = image[x+start];\n        }\n    }\n}",
            "int mpi_rank, mpi_size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\n  size_t image_size = image.size();\n  size_t chunk_size = (image_size / mpi_size);\n  int remainder = image_size % mpi_size;\n  if (mpi_rank == mpi_size - 1) {\n    chunk_size += remainder;\n  }\n  std::vector<int> sub_image(chunk_size);\n  std::copy_n(image.begin() + chunk_size * mpi_rank, chunk_size, sub_image.begin());\n\n  int max_threads = 1;\n  #pragma omp parallel\n  {\n    max_threads = omp_get_max_threads();\n  }\n  int chunk_size_per_thread = (chunk_size + max_threads - 1) / max_threads;\n  std::vector<int> sub_bins(256 * max_threads);\n\n  #pragma omp parallel\n  {\n    int thread_num = omp_get_thread_num();\n    int chunk_start = chunk_size_per_thread * thread_num;\n    int chunk_end = chunk_start + chunk_size_per_thread;\n    if (mpi_rank == mpi_size - 1) {\n      chunk_end = chunk_end + remainder;\n    }\n    if (mpi_rank == 0 && thread_num == 0) {\n      chunk_start = 0;\n    }\n    std::array<size_t, 256> thread_bins{};\n    for (int i = chunk_start; i < chunk_end; i++) {\n      thread_bins[sub_image[i]]++;\n    }\n\n    #pragma omp critical\n    {\n      for (int i = 0; i < 256; i++) {\n        sub_bins[i + thread_num * 256] = thread_bins[i];\n      }\n    }\n  }\n  if (mpi_rank == 0) {\n    for (int i = 0; i < 256; i++) {\n      for (int j = 0; j < max_threads; j++) {\n        bins[i] += sub_bins[i + j * 256];\n      }\n    }\n  }\n  else {\n    MPI_Send(sub_bins.data(), 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "// TODO\n    // Get the number of ranks, rank number, and the number of threads\n    int nProcs, rank, nThreads;\n    MPI_Comm_size(MPI_COMM_WORLD, &nProcs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    nThreads = omp_get_max_threads();\n\n    // Get the length of the image\n    int len = image.size();\n\n    // Split the image into chunks for each rank\n    std::vector<int> image_rank;\n    if(rank == 0) {\n        // Get the start and end index for each rank\n        std::vector<int> start_end = get_start_end(len, nProcs);\n        std::vector<int> start_index = start_end[0];\n        std::vector<int> end_index = start_end[1];\n\n        // Get the total number of pixels for each rank\n        size_t n_pixels = 0;\n        for(int i = 0; i < nProcs; i++) {\n            n_pixels += end_index[i] - start_index[i];\n        }\n\n        // Allocate the vector for the ranks\n        image_rank.resize(n_pixels);\n\n        // Copy the image to the vector\n        for(int i = 0; i < nProcs; i++) {\n            for(int j = start_index[i]; j < end_index[i]; j++) {\n                image_rank[j - start_index[i]] = image[j];\n            }\n        }\n    }\n\n    // Broadcast the image vector to all ranks\n    MPI_Bcast(image_rank.data(), image_rank.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n    // Set the starting and ending index for each rank\n    int start_index = rank * len / nProcs;\n    int end_index = (rank + 1) * len / nProcs;\n\n    // Count the number of pixels for each rank\n    std::vector<int> counts(256);\n    for(int i = start_index; i < end_index; i++) {\n        counts[image_rank[i - start_index]]++;\n    }\n\n    // Get the maximum and minimum counts for each rank\n    int max_count, min_count;\n    MPI_Allreduce(&counts[0], &min_count, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    MPI_Allreduce(&counts[0], &max_count, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);\n\n    // Get the min and max counts for all ranks\n    int all_min_count, all_max_count;\n    MPI_Allreduce(&min_count, &all_min_count, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    MPI_Allreduce(&max_count, &all_max_count, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);\n\n    // Allocate the vector for storing the counts\n    std::vector<int> counts_rank(256);\n\n    // Copy the counts of the local rank to the global rank\n    for(int i = 0; i < 256; i++) {\n        counts_rank[i] = counts[i] - all_min_count;\n    }\n\n    // Compute the min and max counts for the global rank\n    int rank_min_count, rank_max_count;\n    MPI_Allreduce(&counts_rank[0], &rank_min_count, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    MPI_Allreduce(&counts_rank[0], &rank_max_count, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);\n\n    // Compute the starting and ending index for each rank\n    std::vector<int> start_index_rank(nProcs), end_index_rank(nProcs);\n    for(int i",
            "// YOUR CODE HERE\n    \n    int nbins=256;\n    \n    //initialize bin array\n    for (int i=0; i<nbins; i++) {\n        bins[i]=0;\n    }\n    \n    //count each pixel\n    for (int i=0; i<image.size(); i++) {\n        bins[image[i]]++;\n    }\n    \n    \n}",
            "}",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int chunk = image.size()/size;\n\n    if (rank == 0) {\n        std::fill(bins.begin(), bins.end(), 0);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < chunk; i++) {\n        bins[image[rank * chunk + i]] += 1;\n    }\n}",
            "size_t rank, nranks, bins_per_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n    bins_per_rank = 256 / nranks;\n    if(rank == 0) {\n        bins.fill(0);\n    }\n    for(size_t i = rank * bins_per_rank; i < (rank + 1) * bins_per_rank; i++) {\n        size_t index = image[i];\n        #pragma omp atomic\n        bins[index]++;\n    }\n    if(rank == 0) {\n        for(int i = 1; i < nranks; i++) {\n            MPI_Recv(&bins[0] + i * bins_per_rank, bins_per_rank, MPI_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n    else {\n        MPI_Send(&bins[0], bins_per_rank, MPI_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int nproc;\n   MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n   int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // Get the global image size\n   size_t width = image.size() / nproc;\n   // Get the number of pixels per rank\n   size_t local_width = width / nproc;\n   size_t local_width_rem = width % nproc;\n\n   // Divide pixels in local chunks\n   size_t local_start = rank * local_width;\n   size_t local_end = local_start + local_width;\n   if (rank < local_width_rem) {\n     local_end += 1;\n   }\n   // Initialize the local image\n   std::vector<int> local_image(local_end - local_start);\n   for (size_t i = 0; i < local_end - local_start; i++) {\n     local_image[i] = image[local_start + i];\n   }\n   // Count the number of pixels with each intensity\n   std::array<size_t, 256> local_bins;\n   #pragma omp parallel for\n   for (size_t i = 0; i < 256; i++) {\n     local_bins[i] = 0;\n   }\n   for (size_t i = 0; i < local_end - local_start; i++) {\n     local_bins[local_image[i]] += 1;\n   }\n   // Reduce the results to the root\n   if (rank == 0) {\n     for (size_t i = 0; i < 256; i++) {\n       bins[i] = local_bins[i];\n     }\n   }\n   MPI_Reduce(MPI_IN_PLACE, &bins[0], 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "// Compute the number of pixels with each intensity value\n   #pragma omp parallel for\n   for (int i = 0; i < image.size(); ++i) {\n     // OpenMP parallelization of the intensity values\n     bins[image[i]] += 1;\n   }\n\n   // Sum the pixel counts across all processes\n   #pragma omp parallel\n   {\n      int rank, nprocs;\n      MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n      MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n      int* rbins = new int[256];\n\n      // Copy the local bins array to an array on rank 0\n      if(rank == 0)\n      {\n        for(int i = 0; i < 256; i++)\n        {\n          rbins[i] = bins[i];\n        }\n      }\n\n      // Gather bins from all ranks to rank 0\n      MPI_Gather(rbins, 256, MPI_INT, bins.data(), 256, MPI_INT, 0, MPI_COMM_WORLD);\n\n      if(rank == 0)\n      {\n        // Rank 0 sums the pixel counts of all the ranks\n        for(int i = 0; i < 256; i++)\n        {\n          for(int j = 1; j < nprocs; j++)\n          {\n            bins[i] += bins[j * 256 + i];\n          }\n        }\n      }\n\n      if(rbins)\n      {\n        delete rbins;\n      }\n   }\n\n}",
            "// YOUR CODE HERE\n}",
            "bins.fill(0);\n    if (image.size() == 0) return;\n    int rank, numProcs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n    int chunkSize = image.size() / numProcs;\n    int remainder = image.size() % numProcs;\n\n    // count pixels on each rank\n    std::vector<int> counts(256);\n#pragma omp parallel\n    {\n        std::fill(counts.begin(), counts.end(), 0);\n        size_t start = chunkSize * rank;\n        if (rank < remainder) start += rank;\n        size_t end = start + chunkSize;\n        if (rank < remainder) end += 1;\n        for (size_t i = start; i < end; i++) {\n            if (image[i] > 255) {\n                std::cout << \"rank \" << rank << \" pixel \" << i << \" is out of bounds of the image\" << std::endl;\n                MPI_Abort(MPI_COMM_WORLD, 1);\n            }\n            counts[image[i]] += 1;\n        }\n    }\n\n    // combine counts across all ranks\n    MPI_Reduce(&counts[0], &bins[0], 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n\n }",
            "int num_rank = omp_get_num_threads();\n\tstd::vector<int> local_image(image);\n\tstd::vector<size_t> local_bins(256, 0);\n\n\tomp_set_num_threads(num_rank);\n\t#pragma omp parallel for\n\tfor (int i = 0; i < image.size(); i++) {\n\t\tlocal_bins[image[i]]++;\n\t}\n\n\tMPI_Allreduce(&local_bins[0], &bins[0], 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n}",
            "// TODO\n }",
            "int count = image.size();\n\t int rank, size;\n\t MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\t MPI_Comm_size(MPI_COMM_WORLD, &size);\n\t int chunk_size = count / size;\n\t int extra = count % size;\n\t std::vector<int> local_image;\n\t std::vector<int> local_bins(256);\n\t for (int i = rank * chunk_size; i < (rank == size - 1? count : (rank * chunk_size + chunk_size)); i++) {\n\t\t local_image.push_back(image[i]);\n\t }\n\t #pragma omp parallel for\n\t for (int i = 0; i < local_image.size(); i++) {\n\t\t local_bins[local_image[i]]++;\n\t }\n\t MPI_Gather(local_bins.data(), 256, MPI_UNSIGNED, bins.data(), 256, MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n }",
            "//TODO\n }",
            "size_t image_size=image.size();\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int bins_per_rank=image_size/size;\n    int bins_left=image_size-bins_per_rank*size;\n    std::vector<int> bins_local(bins_per_rank);\n\n    #pragma omp parallel for\n    for (int i=0; i<bins_per_rank; i++)\n    {\n        bins_local[i]=image[i*size+rank];\n    }\n\n    int bins_to_receive=0;\n    if (rank<bins_left)\n        bins_to_receive=1;\n\n    std::vector<int> bins_received(bins_to_receive);\n\n    if (rank<bins_left)\n    {\n        int recv_size=bins_local[bins_per_rank-1];\n        MPI_Status status;\n        MPI_Recv(&bins_received[0], recv_size, MPI_INT, rank+size, 1, MPI_COMM_WORLD, &status);\n    }\n\n    if (rank<bins_left)\n    {\n        #pragma omp parallel for\n        for (int i=0; i<bins_left; i++)\n        {\n            bins_local[bins_per_rank-1]+=bins_received[i];\n        }\n    }\n\n    std::array<int, 256> bins_local_array;\n    std::copy(bins_local.begin(), bins_local.end(), bins_local_array.begin());\n    std::copy(bins_local_array.begin(), bins_local_array.end(), bins.begin()+rank*256);\n\n    #pragma omp parallel for\n    for (int i=0; i<bins_per_rank; i++)\n    {\n        for (int j=0; j<256; j++)\n        {\n            if (bins_local[i]==j)\n                bins[256*rank+j]++;\n        }\n    }\n\n    #pragma omp parallel for\n    for (int i=0; i<bins_to_receive; i++)\n    {\n        for (int j=0; j<256; j++)\n        {\n            if (bins_received[i]==j)\n                bins[256*(rank+size)+j]++;\n        }\n    }\n}",
            "// TODO: Your code here\n    size_t numRanks = 0;\n    size_t rank = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if(rank == 0) {\n        for(size_t i = 0; i < image.size(); i++) {\n            bins[image[i]]++;\n        }\n    }\n\n    // TODO: Your code here\n    // Synchronize all ranks\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    // Perform OpenMP parallel reduction\n    #pragma omp parallel for\n    for(size_t i = 0; i < image.size(); i++) {\n        bins[image[i]] += 1;\n    }\n\n    // Synchronize all ranks\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "#pragma omp parallel\n    {\n        #pragma omp single\n        {\n            int rank, world_size;\n            MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n            MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n            size_t local_bins[256];\n            #pragma omp for schedule(static)\n            for(int i = 0; i < 256; i++) local_bins[i] = 0;\n            int chunk = image.size() / world_size;\n            #pragma omp for schedule(static)\n            for(int i = rank * chunk; i < (rank + 1) * chunk; i++) local_bins[image[i]]++;\n            #pragma omp barrier\n            #pragma omp for schedule(static)\n            for(int i = 0; i < 256; i++) bins[i] = 0;\n            #pragma omp for schedule(static)\n            for(int i = 0; i < 256; i++) bins[i] += local_bins[i];\n        }\n    }\n}",
            "int rank, nRanks;\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n\n     size_t const N = image.size();\n     std::array<size_t, 256> count_per_rank{};\n     for (size_t i = 0; i < N; i++) {\n         count_per_rank[image[i]]++;\n     }\n\n#pragma omp parallel for\n     for (size_t i = 0; i < 256; i++) {\n         if (rank == 0) {\n             bins[i] = 0;\n         }\n         int temp = count_per_rank[i];\n         MPI_Reduce(&temp, &bins[i], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n     }\n }",
            "// TODO: Implement\n}",
            "// your code goes here\n\n\n    int num_images = image.size();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int chunk_size = num_images / size;\n\n    int start = rank * chunk_size;\n    int end = (rank + 1) * chunk_size;\n    std::vector<int> local_image(image.begin() + start, image.begin() + end);\n\n    std::array<size_t, 256> local_bins;\n\n#pragma omp parallel for\n    for(int i = 0; i < 256; i++)\n    {\n        local_bins[i] = 0;\n    }\n\n    for(int i = 0; i < local_image.size(); i++)\n    {\n        local_bins[local_image[i]] += 1;\n    }\n\n    bins = local_bins;\n\n    //int local_bins[256];\n\n    //int local_bins[256];\n\n    //#pragma omp parallel for\n    //for (int i = 0; i < 256; i++)\n    //{\n    //    local_bins[i] = 0;\n    //}\n\n    //for(int i = 0; i < local_image.size(); i++)\n    //{\n    //    local_bins[local_image[i]] += 1;\n    //}\n\n    //std::array<size_t, 256> global_bins;\n\n    //MPI_Allreduce(&local_bins, &global_bins, 256, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    //bins = global_bins;\n}",
            "#pragma omp parallel\n    {\n        int thread_num = omp_get_thread_num();\n        int num_threads = omp_get_num_threads();\n        int rank = omp_get_thread_num();\n        int size = omp_get_num_threads();\n        if (rank == 0)\n            std::fill(bins.begin(), bins.end(), 0);\n        //bins[image[i]]++;\n        for (int i = rank; i < image.size(); i += size)\n            bins[image[i]]++;\n    }\n\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(Kokkos::View<const char**> const& s, Kokkos::View<size_t[26]> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO 1: Your code here\n\n  Kokkos::parallel_for(\"first_letter_counts\", Kokkos::RangePolicy<>(0, s.extent(0)),\n                       KOKKOS_LAMBDA(int i) {\n                         bins[s(i)[0] - 'a']++;\n                       });\n\n  Kokkos::fence();\n}",
            "//TODO: Your code here\n\n}",
            "Kokkos::parallel_for(\"firstLetterCounts\", s.size(), [=] (int i) {\n    const int letter = s(i)[0] - 'a';\n    bins(letter)++;\n  });\n}",
            "Kokkos::parallel_for(\"first letter counts\", s.size(),\n\t\t\t\t\t\t KOKKOS_LAMBDA (int i) {\n                             ++bins[s(i)[0] - 'a'];\n                         });\n}",
            "// Your code here\n}",
            "// TODO\n\n    Kokkos::parallel_for(\"firstLetterCounts\", Kokkos::RangePolicy<Kokkos::Serial>(0, bins.size()), [=](int i) {\n\n        for (auto j = 0; j < s.size(); ++j) {\n            if (s(j)[0] == i + 'a') {\n                ++bins[i];\n            }\n        }\n\n    });\n\n}",
            "// TODO: Fill in this function\n}",
            "// TODO: Implement this function\n  // Hint: Use `std::string::front`\n\n}",
            "auto numWords = s.size();\n\n  // for each word in the string, count the number of strings in the vector s that start with that letter.\n  // For this, we'll use the Kokkos lambda functions.\n  // We'll set the lambda to run on each word in the vector.\n  // The lambda will also need to use a shared view to store the count for each character.\n  // The lambda will also need to use a team policy to distribute the work in parallel\n\n  // create a view that stores the counts for each character\n  // Kokkos::View<size_t[26]> bins(\"bins\", 26);\n\n  // create a view that stores the counts for each character\n  // Kokkos::View<size_t[26]> bins(\"bins\", 26);\n\n  // initialize the bins to zero\n  //Kokkos::deep_copy(bins, 0);\n\n  // create a lambda function to count the number of strings that start with a particular letter\n  //Kokkos::parallel_for(\"count\", Kokkos::TeamPolicy(numWords, 26), [=] (const Kokkos::TeamMember &member) {\n  Kokkos::parallel_for(\"count\", Kokkos::TeamPolicy(26, numWords), [=] (const Kokkos::TeamMember &member) {\n    Kokkos::parallel_for(member.league_range(0, numWords), [=] (int i) {\n      size_t offset = i * 26;\n      const char* word = s(i);\n      char ch = word[0];\n      if (ch >= 'a' && ch <= 'z') {\n        size_t index = ch - 'a';\n        //bins(index)++;\n        bins(offset + index)++;\n      }\n    });\n  });\n\n  //Kokkos::deep_copy(bins, 0);\n}\n\n\n/* For each letter in the alphabet, count the number of strings in the vector s that end with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n*/\nvoid lastLetterCounts(Kokkos::View<const char**> const& s, Kokkos::View<size_t[26]> &bins) {\n  auto numWords = s.size();\n\n  // for each word in the string, count the number of strings in the vector s that start with that letter.\n  // For this, we'll use the Kokkos lambda functions.\n  // We'll set the lambda to run on each word in the vector.\n  // The lambda will also need to use a shared view to store the count for each character.\n  // The lambda will also need to use a team policy to distribute the work in parallel\n\n  // create a view that stores the counts for each character\n  //Kokkos::View<size_t[26]> bins(\"bins\", 26);\n\n  // initialize the bins to zero\n  //Kokkos::deep_copy(bins, 0);\n\n  // create a lambda function to count the number of strings that start with a particular letter\n  //Kokkos::parallel_for(\"count\", Kokkos::TeamPolicy(numWords, 26), [=] (const Kokkos::TeamMember &member) {\n  Kokkos::parallel_for(\"count\", Kokkos::TeamPolicy(26, numWords), [=] (const Kokkos::TeamMember &member) {\n    Kokkos::parallel_for(member.league_range(0, numWords), [",
            "// TODO\n}",
            "const int N = s.size();\n    const size_t numThreads = Kokkos::hwloc::get_available_numa_count();\n    const size_t numThreadsPerTeam = 2;\n\n    // Fill bins with zeros\n    Kokkos::deep_copy(bins, 0);\n\n    // Get the team policy\n    Kokkos::TeamPolicy<Kokkos::Schedule<Kokkos::Dynamic> > policy(N, numThreads, numThreadsPerTeam);\n\n    // Create a team policy for parallel_for\n    Kokkos::parallel_for(policy, [=](Kokkos::TeamPolicy<Kokkos::Schedule<Kokkos::Dynamic> >::member_type teamMember) {\n        const int teamId = teamMember.league_rank();\n        const int threadId = teamMember.team_rank();\n\n        // Get the string for the current thread\n        const char* sThread = s(teamId);\n        const char c = sThread[0];\n\n        // Find the position of the letter in the alphabet\n        const int cOffset = c - 'a';\n        const int cBin = cOffset + threadId;\n\n        // Increment the corresponding bin\n        bins(cBin)++;\n    });\n}",
            "// Your code goes here\n\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Serial>(0, s.size()),\n                       KOKKOS_LAMBDA(int i) {\n\n                       if (s[i][0] >= 'a' && s[i][0] <= 'z')\n                         bins[s[i][0] - 'a'] += 1;\n\n                       });\n\n}",
            "const size_t n = s.size();\n  constexpr size_t max_size = 26;\n  Kokkos::View<size_t[max_size], Kokkos::HostSpace> host_bins(\"host_bins\");\n  // Initialize the bins to zero\n  Kokkos::deep_copy(host_bins, 0);\n  Kokkos::deep_copy(bins, host_bins);\n\n  // TODO: Fill in the code that computes bins\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Serial, size_t>(0, n),\n                       [=](size_t i) {\n    host_bins[static_cast<size_t>(s[i][0]) - 97] += 1;\n  });\n\n  Kokkos::deep_copy(bins, host_bins);\n}",
            "// TODO: Your code here\n}",
            "// Your code goes here.\n}",
            "const int n = s.extent(0);\n\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, n), KOKKOS_LAMBDA(const int i) {\n        int letter = (s(i)[0] - 'a');\n        bins(letter)++;\n    });\n}",
            "auto count_functor = KOKKOS_LAMBDA (const size_t &i) {\n    auto word = s(i);\n    auto first_letter = *word;\n    if (first_letter >= 'a' && first_letter <= 'z') {\n      bins(first_letter - 'a') += 1;\n    }\n  };\n\n  Kokkos::parallel_for(\"first_letter_counts\", s.extent(0), count_functor);\n}",
            "Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(int i) {\n      bins(s(i, 0) - 'a')++;\n  });\n}",
            "Kokkos::parallel_for(\"firstlettercounts\", s.extent(0), [=](int i) {\n    char c = s(i, 0);\n    if (c < 'a') {\n      c = 'z';\n    }\n    bins(c - 'a')++;\n  });\n}",
            "// TODO: Implement your solution here\n\n}",
            "Kokkos::parallel_for(\"firstLetterCounts\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 26), [&] (const int& i) {\n        int count = 0;\n        for (int j = 0; j < s.extent(0); ++j) {\n            if (s(j)[0] == 'a' + i) {\n                ++count;\n            }\n        }\n        bins(i) = count;\n    });\n}",
            "Kokkos::parallel_for(\"first_letter_counts\", Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, s.size()),\n        KOKKOS_LAMBDA (int i) {\n            size_t bin = 0;\n            for (char c : s[i]) {\n                if (c >= 'a' && c <= 'z') {\n                    bin = c - 'a';\n                    break;\n                }\n            }\n            ++bins[bin];\n        });\n}",
            "// Your code goes here.\n}",
            "int n = s.size();\n    for (int i = 0; i < n; i++) {\n        int c = (int)s(i, 0);\n        if (c >= 'a' && c <= 'z')\n            bins(c - 'a') += 1;\n    }\n}",
            "}",
            "}",
            "// TO DO: implement this function\n}",
            "// TODO\n}",
            "// TODO\n    // Your solution here\n}",
            "// Your code here\n}",
            "//TODO: implement this function\n    //hint: use Kokkos::parallel_for\n}",
            "// TODO: Implement this function.\n}",
            "}",
            "int n = s.size();\n    Kokkos::parallel_for(\"First Letter Counts\", Kokkos::RangePolicy<>(0, n), [=] (int i) {\n        // For each string in s, count the number of strings that start with the letter of the string's first character.\n        // Update the corresponding bin in the bins array.\n        // Example:\n        //\n        // For i = 0, s[0] = \"dog\". Increment the bin at position 'd' (which is 4) in the bins array.\n        // For i = 1, s[1] = \"cat\". Increment the bin at position 'c' (which is 2) in the bins array.\n        // For i = 2, s[2] = \"xray\". Increment the bin at position 'x' (which is 24) in the bins array.\n        // For i = 3, s[3] = \"cow\". Increment the bin at position 'c' (which is 2) in the bins array.\n        // For i = 4, s[4] = \"code\". Increment the bin at position 'c' (which is 2) in the bins array.\n        // For i = 5, s[5] = \"type\". Increment the bin at position 't' (which is 19) in the bins array.\n        // For i = 6, s[6] = \"flower\". Increment the bin at position 'f' (which is 6) in the bins array.\n\n        // Remember that lower case letters are from 'a' to 'z', so the ASCII value of 'a' is 97.\n        // Increment the bin at position 97 + (ASCII value of first character of string s[i]) in the bins array.\n        // Note that the ASCII value of a string's first character can be obtained by casting the string to a char *.\n\n        // Here is an example of how to obtain the ASCII value of the first character of string s[i]:\n        // char firstChar = s[i][0]; // char is a built-in type in C++.\n        // int asciiValOfFirstChar = firstChar; // implicit conversion of char to int\n    });\n}",
            "constexpr size_t numChars = 26;\n\n  // Create a lambda function to count the number of occurrences of each character.\n  // Note: it is important to use Kokkos::RangePolicy, not Kokkos::RangePolicy<KokkosExecSpace>.\n  Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace> policy(0, numChars);\n  Kokkos::parallel_for(\"count_chars\", policy, [=](int i) {\n    for (int j = 0; j < s.extent(0); j++) {\n      bins[i] += (s[j][0] - 'a' == i);\n    }\n  });\n\n}",
            "}",
            "Kokkos::parallel_for(\"firstLetterCounts\", Kokkos::RangePolicy<>(0, s.extent(0)),\n                         KOKKOS_LAMBDA(const int& i) {\n        char c = tolower(s(i, 0));\n        //if (c < 'a' || c > 'z') return; // this is not necessary since we assume all lower case strings.\n        bins(c - 'a')++;\n    });\n    Kokkos::fence();\n}",
            "// This is a hint to Kokkos that this loop will be executed 26 times\n  Kokkos::parallel_for(\"FirstLetterCounts\", Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 26),\n    [=](int letter) {\n      for (int i = 0; i < s.extent(0); ++i) {\n        if (s(i)[0] == 'a' + letter) {\n          ++bins[letter];\n        }\n      }\n    });\n\n}",
            "// TODO: Your code here\n}",
            "// your code here\n}",
            "auto f = KOKKOS_LAMBDA(const size_t i) {\n      const auto c = s[i][0] - 'a';\n      bins[c] += 1;\n   };\n   Kokkos::parallel_for(\"firstLetterCounts\", Kokkos::RangePolicy<>(0, s.size()), f);\n   Kokkos::fence();\n}",
            "const size_t N = s.size();\n  auto sHost = Kokkos::create_mirror_view_and_copy(Kokkos::HostSpace(), s);\n  Kokkos::parallel_for(\"First letter counts\", Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, N),\n    KOKKOS_LAMBDA(const int& i) {\n      char c = sHost(i)[0];\n      if (c < 'a' || c > 'z') {\n        throw std::runtime_error(\"Input string is not lower case.\");\n      }\n      bins(c-'a') += 1;\n  });\n  Kokkos::deep_copy(bins, bins_host);\n}",
            "}",
            "// TODO: fill in the code to implement this function\n\n}",
            "// TODO: Fill in this function\n    int num_strings = s.size();\n    Kokkos::parallel_for(\"firstLetterCounts\", Kokkos::RangePolicy<>(0, num_strings),\n        KOKKOS_LAMBDA (int i) {\n            bins(s(i)[0]-'a') += 1;\n        });\n}",
            "// Kokkos::RangePolicy does not work, since it is not Kokkos::View-like\n    // Kokkos::RangePolicy<> policy(0, 26);\n    Kokkos::parallel_for(\"firstLetterCounts\", Kokkos::RangePolicy<>(0, 26),\n        KOKKOS_LAMBDA(const size_t i) {\n            const char c = 'a' + i;\n            size_t count = 0;\n            for (size_t j = 0; j < s.extent(0); ++j) {\n                const char *s_j = s(j);\n                if (*s_j == c) ++count;\n            }\n            bins(i) = count;\n        }\n    );\n}",
            "auto policy = Kokkos::RangePolicy<>(0, s.size());\n\n    Kokkos::parallel_for(\"firstLetterCounts\", policy,\n        KOKKOS_LAMBDA(const int i) {\n            bins(s[i][0] - 'a')++;\n        }\n    );\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<>(0, s.size()),\n        [=](int i) {\n            bins[s[i][0]-'a']++;\n        });\n}",
            "// TODO: fill in this function\n\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<>(0, s.extent(0)), [&s, &bins](int i) {\n    bins[s(i)[0] - 'a']++;\n  });\n}",
            "// TODO\n\n}",
            "//TODO\n}",
            "// Use Kokkos to execute code in parallel\n  //  1. Write the C++ code that will calculate the counts in the 'bins' array\n  //  2. Write the Kokkos code that will call the C++ code\n  //  3. Ensure the code runs correctly.\n\n}",
            "// YOUR CODE HERE\n}",
            "auto num_strings = s.size();\n    auto num_threads = 26;\n    Kokkos::parallel_for(\"firstLetterCounts\", num_threads, KOKKOS_LAMBDA(int i) {\n        bins[i] = 0;\n    });\n    Kokkos::parallel_for(\"firstLetterCounts\", num_strings, KOKKOS_LAMBDA(int i) {\n        auto firstLetter = tolower(s[i][0]) - 'a';\n        Kokkos::atomic_add(&bins[firstLetter], 1);\n    });\n}",
            "// TODO: Fill this in\n\n}",
            "auto f = KOKKOS_LAMBDA (size_t idx) {\n    char firstChar = s(idx)[0];\n    if (firstChar >= 'a' && firstChar <= 'z') {\n      bins(firstChar-'a') += 1;\n    }\n  };\n  Kokkos::RangePolicy<Kokkos::Serial> policy(0, s.size());\n  Kokkos::parallel_for(\"firstLetterCounts\", policy, f);\n}",
            "// Kokkos kernels must be executed on a device\n  Kokkos::View<char**> sView(\"sView\", s.size());\n  Kokkos::deep_copy(sView, s);\n\n  // Kokkos::View<char**> sView(\"sView\", s.size());\n  // Kokkos::deep_copy(sView, s);\n\n  // Execute the kernel\n  Kokkos::parallel_for(\"firstLetterCounts\", Kokkos::RangePolicy<>(0, 26), [=] (int i) {\n    // Allocate a variable to store the number of strings in bin i\n    int n = 0;\n    // Loop over each string in the input vector\n    for (int j = 0; j < sView.size(); j++) {\n      // Check if the string starts with the ith letter of the alphabet\n      if (sView(j, 0) == i + 'a') {\n        n++;\n      }\n    }\n    // Store the number of strings in bin i in the output array\n    bins(i) = n;\n  });\n\n  // Synchronize to ensure all threads have finished\n  Kokkos::finalize();\n}",
            "// NOTE: The following code assumes that the input has at least 1 string and that all strings are <= 26 characters.\n  //       The output array is not initialized to 0 before the code is executed.\n\n  const int N = s.size();\n  Kokkos::View<const char**> s_host(\"s_host\", N);\n  Kokkos::deep_copy(s_host, s);\n\n  // Kokkos::RangePolicy<> range(0, N);\n  // Kokkos::parallel_for(\"first_letter_count\", range, [=] (int i) {\n    int j = 0;\n    for (int i = 0; i < N; i++) {\n      if (s_host(i)[0] == 'a') { bins(j++); }\n      if (s_host(i)[0] == 'b') { bins(j++); }\n      if (s_host(i)[0] == 'c') { bins(j++); }\n      if (s_host(i)[0] == 'd') { bins(j++); }\n      if (s_host(i)[0] == 'e') { bins(j++); }\n      if (s_host(i)[0] == 'f') { bins(j++); }\n      if (s_host(i)[0] == 'g') { bins(j++); }\n      if (s_host(i)[0] == 'h') { bins(j++); }\n      if (s_host(i)[0] == 'i') { bins(j++); }\n      if (s_host(i)[0] == 'j') { bins(j++); }\n      if (s_host(i)[0] == 'k') { bins(j++); }\n      if (s_host(i)[0] == 'l') { bins(j++); }\n      if (s_host(i)[0] =='m') { bins(j++); }\n      if (s_host(i)[0] == 'n') { bins(j++); }\n      if (s_host(i)[0] == 'o') { bins(j++); }\n      if (s_host(i)[0] == 'p') { bins(j++); }\n      if (s_host(i)[0] == 'q') { bins(j++); }\n      if (s_host(i)[0] == 'r') { bins(j++); }\n      if (s_host(i)[0] =='s') { bins(j++); }\n      if (s_host(i)[0] == 't') { bins(j++); }\n      if (s_host(i)[0] == 'u') { bins(j++); }\n      if (s_host(i)[0] == 'v') { bins(j++); }\n      if (s_host(i)[0] == 'w') { bins(j++); }\n      if (s_host(i)[0] == 'x') { bins(j++); }\n      if (s_host(i)[0] == 'y') { bins(j++); }\n      if (s_host(i)[0] == 'z') { bins(j++); }\n    }\n  // });\n}",
            "// Your code here\n}",
            "}",
            "// 0) Copy data to host\n  // This will be required to call Kokkos functions\n  // Kokkos::View<const char**> h_s(\"h_s\", s.size());\n  // Kokkos::deep_copy(h_s, s);\n\n  // 1) Create an index array\n  // This is required to get the value of the first letter for each string\n  // Kokkos::View<size_t*> inds(\"inds\", s.size());\n\n  // 2) Create a View for the alphabet\n  // Kokkos::View<size_t[26]> alphabet(\"alphabet\");\n  // Kokkos::deep_copy(alphabet, Kokkos::View<size_t*>(\"abcdefghijklmnopqrstuvwxyz\", 26));\n\n  // 3) Create the result View\n  // Kokkos::View<size_t[26]> counts(\"counts\");\n\n  // 4) Create the parallel_for lambda function\n  // [parallel_for lambda]\n\n  // 5) Create the parallel_reduce lambda function\n  // [parallel_reduce lambda]\n\n  // 6) Create the team_reduce lambda function\n  // [team_reduce lambda]\n\n  // 7) Execute team_reduce\n  // Kokkos::parallel_for(Kokkos::TeamPolicy<>(), [parallel_for lambda]);\n  // Kokkos::parallel_reduce(Kokkos::TeamPolicy<>(), [parallel_reduce lambda], [team_reduce lambda]);\n\n  // 8) Copy the result back to host\n  // Kokkos::deep_copy(bins, counts);\n}",
            "// TODO: fill in your implementation here\n}",
            "Kokkos::parallel_for(\n        \"firstLetterCounts\",\n        s.size(),\n        KOKKOS_LAMBDA(size_t i) {\n            auto str = s(i);\n            constexpr size_t letters = 26;\n            int letter = str[0] - 'a';\n            assert(letter >= 0);\n            assert(letter < letters);\n            bins(letter)++;\n        }\n    );\n}",
            "size_t size = s.size();\n  Kokkos::parallel_for(\n    \"firstLetterCounts\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, size),\n    KOKKOS_LAMBDA(const int i) {\n      bins[s(i)[0] - 'a'] += 1;\n    }\n  );\n}",
            "Kokkos::RangePolicy<Kokkos::Serial> policy(0, s.size());\n  Kokkos::parallel_for(policy, [&] (const int& i) {\n    bins[s[i][0] - 'a']++;\n  });\n}",
            "for (size_t i=0; i<s.size(); i++) {\n        bins(s(i)[0]-'a')++;\n    }\n}",
            "int numElements = s.size();\n\n    // TODO:\n    // 1) Create a view of `bins` called `binsView`.\n    // 2) Loop over each string and count the number of times each letter appears in each string.\n    // 3) Set the corresponding element in `bins` equal to the number of times that letter appears.\n    // 4) The first string starts at index 0. The second string starts at index 1. And so on.\n\n    // TODO:\n    // 5) Print the values in `binsView` using Kokkos::deep_copy.\n}",
            "// Your code here\n  return;\n}",
            "// TODO: Your code here\n\n}",
            "Kokkos::parallel_for(s.size(), KOKKOS_LAMBDA(const int i) {\n        const char first_letter = s(i)[0];\n        const int index = first_letter - 'a';\n        bins(index)++;\n    });\n}",
            "const size_t n = s.extent_int(0);\n\n    // TODO: Your code goes here\n\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: Your code here\n}",
            "}",
            "}",
            "// Hint: for_each, reduction\n\n  // Implement code here\n\n}",
            "Kokkos::parallel_for(s.size(), KOKKOS_LAMBDA(const int i) {\n    int index = int(s(i)[0]) - 97;\n    bins(index)++;\n  });\n}",
            "Kokkos::RangePolicy<Kokkos::Serial> policy(0, s.extent_int(0));\n  Kokkos::parallel_for(policy,\n    KOKKOS_LAMBDA (const int i) {\n      bins(s(i)[0]-'a')++;\n    });\n}",
            "// YOUR CODE HERE\n    auto policy = Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Dynamic>>(0, 26);\n    Kokkos::parallel_for(\"par_for_first_letter_counts\", policy, [&] (int i) {\n        for (int j = 0; j < s.extent(0); j++) {\n            if (s(j)[0] == 'a' + i) {\n                bins(i)++;\n            }\n        }\n    });\n    Kokkos::fence();\n}",
            "for (auto i : Kokkos::RangePolicy<>(0, s.extent(0))) {\n        auto &s_i = s(i);\n        bins(s_i[0] - 'a')++;\n    }\n}",
            "// YOUR CODE HERE\n    // NOTE: the strings in s are in lower case\n\n    // hint: use an integer to keep track of the current character\n    // hint: use Kokkos::parallel_for to iterate over all strings in s\n    // hint: use Kokkos::atomic_fetch_add to update the bins\n\n}",
            "int N = s.size();\n    for (int i=0; i<N; ++i) {\n        bins[s(i,0)-'a']++;\n    }\n}",
            "using namespace Kokkos;\n\n    /* START SUBMIT YOUR CODE HERE */\n\n    // TODO: Replace the above line with your code\n\n    /* END SUBMIT YOUR CODE HERE */\n}",
            "// TODO: Your code here\n\n}",
            "Kokkos::parallel_for(\"firstLetterCounts\", s.size(), KOKKOS_LAMBDA (int i) {\n    bins(s(i)[0] - 'a') += 1;\n  });\n}",
            "Kokkos::parallel_for(\"counts\", s.extent(0), KOKKOS_LAMBDA(const int& i) {\n        char ch = s(i, 0);\n        bins(ch - 'a')++;\n    });\n}",
            "auto host_bins = Kokkos::create_mirror_view(bins);\n\n    Kokkos::parallel_for(\n        \"firstLetterCounts\",\n        Kokkos::RangePolicy<Kokkos::Serial>(0, 26),\n        KOKKOS_LAMBDA(const int letter_index) {\n            auto letter = 'a' + letter_index;\n            size_t count = 0;\n            for (int string_index = 0; string_index < s.extent(0); ++string_index) {\n                if (s(string_index)[0] == letter) {\n                    ++count;\n                }\n            }\n            host_bins(letter_index) = count;\n        }\n    );\n\n    Kokkos::deep_copy(bins, host_bins);\n}",
            "//...\n}",
            "// TODO: Your code here\n    int s_size = s.extent(0);\n    int i = 0;\n    // Kokkos::RangePolicy rp(0, s_size);\n    // Kokkos::parallel_for(rp, [&] (const int& i) {\n    //     if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n    //         bins[s[i][0] - 'a']++;\n    //     }\n    // });\n    // Kokkos::parallel_reduce(\"first_letter_counts\", rp, 0, [&] (int, int v) { return v; }, [&] (int& v, int i) {\n    //     if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n    //         v++;\n    //     }\n    // }, i);\n\n    Kokkos::parallel_reduce(\"first_letter_counts\", s_size, 0, [&] (int i, int v) {\n        if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n            return v + 1;\n        }\n        return v;\n    }, [&] (int v, int i) {\n        if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n            bins[s[i][0] - 'a']++;\n        }\n    });\n}",
            "// Your code goes here\n}",
            "// TODO\n}",
            "Kokkos::RangePolicy<Kokkos::Serial> range_policy(0, s.extent(0));\n    Kokkos::parallel_for(\"first-letter-counts\", range_policy, KOKKOS_LAMBDA(size_t i) {\n        char letter = s(i)[0];\n        letter += 'a';\n        bins(letter - 'a')++;\n    });\n}",
            "// Compute the counts using Kokkos here.\n\n\n    /*\n    This code is the correct solution. You can use it to see the correct output.\n\n    // Count the number of strings in the vector s that start with a given letter.\n    // Return 0 if the letter is not in the alphabet.\n    // Return -1 if the letter is not in the alphabet, and return -1 to all other letters as well.\n    auto f = [](const int i, char letter) {\n        if (letter < 'a' || letter > 'z') return -1;\n        return (i < s.size())? s(i)[0] == letter? 1 : 0 : 0;\n    };\n    // Compute the count of the first letters of the words in the vector s.\n    Kokkos::parallel_for(\"counts\", Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 26), [&](const int i) {\n        bins(i) = Kokkos::sum(Kokkos::make_pair_range(f, 'a' + i));\n    });\n    */\n}",
            "// your code goes here\n    size_t n = s.size();\n    for (size_t i = 0; i < n; i++) {\n        bins(s(i)(0) - 'a')++;\n    }\n}",
            "// Your code here\n}",
            "// Hint: use Kokkos::parallel_for\n  // Hint: you may need to create another View to store the letter of each string\n  // Hint: you may need to create an index View for the letter of each string\n  // Hint: you may need to create a functor\n}",
            "// Hint: use Kokkos::TeamPolicy to parallelize over the elements of the vector s.\n  // Hint: each team is a parallel for loop, where the team size is the number of elements in the vector s.\n  // Hint: inside the team parallel for, use the Kokkos::team_thread_range to parallelize over the alphabet.\n  // Hint: Kokkos::team_thread_range starts at 0 and the loop executes num_threads times, where num_threads is the number of elements in the team.\n  // Hint: in the loop, check whether the string in the team parallel for starts with the letter of the alphabet.\n  // Hint: if the string starts with the letter, increment the corresponding bin.\n  Kokkos::parallel_for(Kokkos::TeamPolicy<>(26), [&] (const Kokkos::TeamPolicy<>::member_type& team) {\n    size_t team_size = team.league_size();\n    size_t team_idx = team.team_rank();\n    for(size_t thread_idx = 0; thread_idx < team_size; thread_idx++) {\n      char *str = s(thread_idx);\n      char first_letter = str[0];\n      if(first_letter >= 'a' && first_letter <= 'z') {\n        size_t letter_idx = first_letter - 'a';\n        size_t bin_idx = team_idx * 26 + letter_idx;\n        bins(bin_idx)++;\n      }\n    }\n  });\n}",
            "auto num_strings = s.size();\n\n  Kokkos::parallel_for(\"firstLetterCounts\", Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, num_strings),\n                       KOKKOS_LAMBDA(int i) {\n                         bins(s(i)[0] - 'a') += 1;\n                       });\n}",
            "// TODO 1: Compute the counts for each letter in parallel\n}",
            "// TODO: Your code here.\n    // Hint: the first letter of a string is at `s[0]`.\n    // Hint: `s` is a \"multi-view\", meaning it has multiple subviews.\n}",
            "// Write your code here\n}",
            "// Your code here\n  // Hint: Kokkos has a parallel_reduce function that is like a parallel for-loop with a reduction\n\n\n  // This is a Kokkos-style parallel for loop\n  // You can replace this with your code\n  Kokkos::parallel_for(\n      \"ParallelForExample\",\n      Kokkos::RangePolicy<Kokkos::Serial>(0, 26),\n      KOKKOS_LAMBDA(int i) {\n    bins[i] = 0;\n    for (size_t j = 0; j < s.size(); ++j) {\n      if (s(j)[0] == i + 97)\n        ++bins[i];\n    }\n  });\n\n  // This is a Kokkos-style parallel reduce\n  // You can replace this with your code\n  Kokkos::parallel_reduce(\n      \"ParallelReduceExample\",\n      Kokkos::RangePolicy<Kokkos::Serial>(0, 26),\n      KOKKOS_LAMBDA(int i, size_t& l) {\n    size_t tmp = 0;\n    for (size_t j = 0; j < s.size(); ++j) {\n      if (s(j)[0] == i + 97)\n        ++tmp;\n    }\n    l += tmp;\n  },\n      Kokkos::Sum<size_t>(bins));\n}",
            "constexpr int num_strings = 4;\n\n  // Your code here: initialize bins\n  // Your code here: loop over each string in the vector s\n  // Your code here: increment the correct bin\n\n  // Your code here: copy the contents of the `bins` array to `bins_host`\n  Kokkos::Host",
            "//TODO: implement using Kokkos\n}",
            "// TODO: your code here\n  const int N = s.size();\n\n  for(int i=0; i<N; i++){\n\n    for(int j=0; j<26; j++){\n\n      if(s(i,0) == 'a'+j){\n\n        bins(j)++;\n\n        }\n\n    }\n  }\n\n\n}",
            "const size_t num_strings = s.size();\n  constexpr size_t num_characters = 26;\n\n  Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace> range_policy(0, num_strings);\n\n  // TODO: Use a lambda to define the functor, and a kokkos parallel_for to apply the functor to each element of the range policy\n  // Hint:\n  //   Kokkos::parallel_for(range_policy, functor)\n\n}",
            "Kokkos::parallel_for(\"first_letter_counts\", s.size(), KOKKOS_LAMBDA (const size_t& i) {\n    ++bins[s(i)[0] - 'a'];\n  });\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: write your solution here\n}",
            "// TODO: implement\n}",
            "// TODO\n}",
            "// TODO\n}",
            "auto device = Kokkos::DefaultExecutionSpace::execution_space();\n\n  // TODO: Implement me\n\n\n\n  // TODO: Add barriers to make sure the results are computed before they are printed.\n}",
            "// TODO:\n}",
            "// TODO\n}",
            "size_t n = s.extent(0);\n    Kokkos::parallel_for(n, KOKKOS_LAMBDA(const size_t i) {\n            bins[s[i][0] - 'a']++;\n        });\n}",
            "// Start by declaring the type of the index set for the parallel_for loop.\n    // You might need to look up documentation for Kokkos::RangePolicy for more information.\n    using policy_type = Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>;\n    using view_type = Kokkos::View<size_t[26], Kokkos::DefaultExecutionSpace>;\n\n    // Declare a 26 element array on the device, initialized to 0.\n    view_type count(\"count\", 26);\n\n    // Now do the parallel_for loop.\n    // You might need to look up documentation for the Kokkos::Experimental::HPX::parallel_reduce to understand this code.\n    // You might want to try the code without the parallel_reduce call first.\n    Kokkos::Experimental::HPX::parallel_reduce(\n        policy_type(0, s.size()),\n        KOKKOS_LAMBDA(const int& i, view_type& update) {\n            char c = tolower(s(i, 0));\n            update(c - 'a')++;\n        },\n        count);\n\n    // Copy the results from the temporary array `count` to `bins`.\n    Kokkos::deep_copy(bins, count);\n}",
            "// Your code goes here\n\n}",
            "// TODO\n}",
            "// Your code goes here\n}",
            "// Your code here.\n  size_t nStr = s.extent(0);\n  // for (int i=0; i<nStr; i++) {\n  //   printf(\"s[%d] = %s\\n\", i, s(i));\n  // }\n\n  for (int i=0; i<26; i++) {\n    for (int j=0; j<nStr; j++) {\n      if (s(j)[0] == 'a'+i) bins(i) += 1;\n    }\n  }\n\n  // for (int i=0; i<26; i++) {\n  //   printf(\"bins[%d] = %ld\\n\", i, bins(i));\n  // }\n}",
            "int alphabetSize = 26;\n\n  /* This is the \"base\" implementation which does not use Kokkos.\n     It is here so that you can use it to debug your solution. */\n\n  // Compute the number of strings starting with each letter of the alphabet.\n  //\n  // This implementation loops over all the strings and increments the corresponding\n  // counter in the bins array.\n  //\n  // You may write the \"parallel\" version of this loop using Kokkos.\n  //\n  // The code inside the for loop will look something like:\n  //\n  // Kokkos::parallel_for(s.size(), KOKKOS_LAMBDA(const int i) {\n  //   const char letter = s(i)[0];\n  //   bins(letter - 'a')++;\n  // });\n  //\n  // NOTE: This is the most efficient implementation.\n\n  bins = 0;\n  for (size_t i = 0; i < s.size(); i++) {\n    const char letter = s(i)[0];\n    bins(letter - 'a')++;\n  }\n}",
            "// Your code here\n}",
            "Kokkos::parallel_for(\"firstLetterCounts\", Kokkos::RangePolicy<>(0, s.extent(0)),\n    KOKKOS_LAMBDA (int i) {\n        bins(s[i][0]-'a')++;\n    });\n}",
            "const size_t N = s.size();\n  const size_t B = 512;\n  Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Dynamic> > range_policy(0, N);\n  Kokkos::parallel_for(\"firstLetterCounts\", range_policy, KOKKOS_LAMBDA(const size_t i) {\n    const char firstLetter = tolower(s(i, 0));\n    const int bin = firstLetter - 'a';\n    Kokkos::atomic_fetch_add(&bins[bin], 1);\n  });\n}",
            "// Your code goes here.\n}",
            "Kokkos::parallel_for(s.size(), KOKKOS_LAMBDA(int i) {\n        bins(s(i)[0] - 'a')++;\n    });\n}",
            "/*\n\t// NOTE: This is the serial version of the code. It is used for testing.\n\t//       Uncomment the following lines and comment out the Kokkos version below to\n\t//       run this code.\n\t*/\n\t//for(size_t i = 0; i < s.size(); i++)\n\t//\tbins[s[i][0]-'a']++;\n\n\t// NOTE: The following is the Kokkos version of the code.\n\tsize_t i = 0;\n\tKokkos::parallel_reduce(Kokkos::RangePolicy<>(0, s.size()),\n\t\tKOKKOS_LAMBDA(const size_t& i_r, size_t& i_l) {\n\t\t\ti_l += (s[i_r][0]-'a');\n\t\t}, i);\n\n}",
            "Kokkos::parallel_for(\"firstLetterCounts\", Kokkos::RangePolicy<>(0, s.size()),\n        KOKKOS_LAMBDA(const int i) {\n        const int c = s(i)[0];\n        bins(c-'a')++;\n    });\n    Kokkos::deep_copy(bins, bins);\n}",
            "// TODO: Implement this function.\n}",
            "// TODO: Your code here\n}",
            "// TODO: YOUR CODE HERE\n}",
            "// TODO\n    Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Dynamic> > range_policy(0, s.size());\n    Kokkos::parallel_for(range_policy, KOKKOS_LAMBDA (const int& i) {\n        int sum = 0;\n        for(int j = 0; j < s[i].size(); j++)\n        {\n            if(s[i][j] == 'a' || s[i][j] == 'b' || s[i][j] == 'c')\n                sum++;\n            else if(s[i][j] == 'd' || s[i][j] == 'e' || s[i][j] == 'f')\n                sum++;\n            else if(s[i][j] == 'g' || s[i][j] == 'h' || s[i][j] == 'i')\n                sum++;\n            else if(s[i][j] == 'j' || s[i][j] == 'k' || s[i][j] == 'l')\n                sum++;\n            else if(s[i][j] =='m' || s[i][j] == 'n' || s[i][j] == 'o')\n                sum++;\n            else if(s[i][j] == 'p' || s[i][j] == 'q' || s[i][j] == 'r')\n                sum++;\n            else if(s[i][j] =='s' || s[i][j] == 't' || s[i][j] == 'u')\n                sum++;\n            else if(s[i][j] == 'v' || s[i][j] == 'w' || s[i][j] == 'x')\n                sum++;\n            else if(s[i][j] == 'y' || s[i][j] == 'z')\n                sum++;\n            else if(s[i][j] =='')\n                sum++;\n        }\n        bins[sum]++;\n    });\n}",
            "const int numWords = s.size();\n  const int vectorSize = s.extent(1);\n\n  // parallel_for will run on all work items\n  Kokkos::parallel_for(numWords, [&s, &bins, vectorSize](const int i) {\n    const char* string = s(i);\n\n    // compute the index of the first letter in the word\n    int letterIndex = tolower(string[0]) - 'a';\n\n    // increment the bin for that letter\n    bins[letterIndex] += 1;\n  });\n}",
            "auto f = KOKKOS_LAMBDA(const size_t i) {\n        bins[s[i][0] - 'a']++;\n    };\n    Kokkos::parallel_for(s.extent(0), f);\n}",
            "// Your code here\n}",
            "Kokkos::parallel_for(\"firstLetterCounts\", Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Dynamic>>(0, bins.size()), KOKKOS_LAMBDA(const int& i) {\n        bins(i) = 0;\n    });\n    Kokkos::parallel_for(\"firstLetterCounts\", Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Dynamic>>(0, s.size()), KOKKOS_LAMBDA(const int& i) {\n        size_t& bin = bins(s(i)[0] - 'a');\n        bin++;\n    });\n}",
            "Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA (const int i) {\n    const int offset = s(i)[0] - 'a';\n    bins(offset) += 1;\n  });\n}",
            "// TODO: Your code here\n}",
            "}",
            "// TODO: Implement your code here\n}",
            "// fill in your code here\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Serial>(0, s.size()),\n                         [=](const int i) {\n                             bins[s(i)[0] - 'a']++;\n                         });\n}",
            "auto policy = Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, s.size());\n\n    Kokkos::parallel_for(\"firstLetterCounts\", policy, [=](int i) {\n        auto letter = s(i)(0);\n        bins(letter - 'a') += 1;\n    });\n}",
            "// TODO: implement me!\n}",
            "// TODO\n    //...\n}",
            "// Your code goes here\n    // You can use a loop or Kokkos algorithms\n    auto b = s.size();\n    Kokkos::parallel_for(Kokkos::RangePolicy<>(0,b),KOKKOS_LAMBDA(const int& i){\n        const char c = s(i)[0];\n        // for(auto j=0;j<26;j++)\n        //     if (bins(j)!=0)\n        //         std::cout << \"bins(\" << j << \") = \" << bins(j) << \"\\n\";\n        // std::cout << \"c = \" << c << \" \" << int(c) << \"\\n\";\n        bins(int(c)-97) += 1;\n    });\n    // Kokkos::parallel_reduce(Kokkos::RangePolicy<>(0,26),KOKKOS_LAMBDA(const int& i, size_t& count) {\n    //     count += bins(i);\n    // }, b);\n}",
            "}",
            "Kokkos::parallel_for(\"count_first_letter\", Kokkos::RangePolicy<>(0, s.size()), KOKKOS_LAMBDA(const int i) {\n    bins[s(i)[0] - 'a']++;\n  });\n}",
            "Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace> policy(0, s.size());\n    Kokkos::parallel_for(\"firstLetterCounts\", policy, [=](int i) {\n        char c = tolower(s(i, 0));\n        bins(c - 'a')++;\n    });\n}",
            "// Fill in your code here\n  Kokkos::parallel_for(\"firstLetterCounts\", Kokkos::RangePolicy<>(0, s.size()), KOKKOS_LAMBDA(int i) {\n    bins[s(i)[0]-'a']++;\n  });\n}",
            "// TODO\n}",
            "//...\n}",
            "Kokkos::parallel_for(\"First Letter Count\", Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, s.size()),\n                         [s, bins](int i) {\n                             char first_letter = s(i)[0];\n                             bins(first_letter - 'a') += 1;\n                         });\n}",
            "Kokkos::parallel_for(\"firstLetterCounts\", Kokkos::RangePolicy<Kokkos::Serial>(0, s.extent_int(0)), [&] (const int i) {\n        const char* str = s(i);\n        const size_t idx = str[0] - 'a';\n        bins(idx)++;\n    });\n}",
            "Kokkos::parallel_for(\"firstLetterCounts\", s.size(), KOKKOS_LAMBDA (const int i) {\n        bins[s(i)[0] - 'a']++;\n    });\n}",
            "// TODO\n\n}",
            "// your code here\n  Kokkos::RangePolicy policy(0, s.extent(0));\n  Kokkos::parallel_for(policy, [&] (int i) {\n    bins[s(i,0)-'a'] += 1;\n  });\n}",
            "// TODO\n}",
            "const int NUM_STRINGS = s.extent(0);\n    auto str = Kokkos::create_mirror_view(s);\n    Kokkos::deep_copy(str, s);\n\n    for (int i = 0; i < NUM_STRINGS; i++) {\n        bins[str(i, 0) - 'a']++;\n    }\n}",
            "Kokkos::parallel_for(\"\", Kokkos::RangePolicy<>(0, s.size()),\n        KOKKOS_LAMBDA (const int &i) {\n            const char letter = tolower(s[i][0]);\n            bins(letter - 'a')++;\n        }\n    );\n}",
            "//...\n}",
            "}",
            "auto kokkosScope = Kokkos::DefaultExecutionSpace();\n  auto numStrings = s.size();\n  Kokkos::parallel_for(\"kokkos-experiments-first-letter-counts\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, numStrings), [&] (int i) {\n    bins(s(i)[0] - 'a') += 1;\n  });\n  Kokkos::fence();\n}",
            "// Fill in your code here.\n    // Call Kokkos::deep_copy(bins, 0) to initialize the array to 0s.\n    // This is the easiest way to do that.\n    for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n    size_t* bins_ptr = bins.data();\n    int num_strings = s.size();\n    for (int i = 0; i < num_strings; i++) {\n        char first = s(i)[0];\n        if (first >= 'a' && first <= 'z')\n            bins_ptr[first - 'a']++;\n    }\n}",
            "// TODO: YOUR CODE HERE\n  // Hint: Use Kokkos::parallel_for and a lambda to iterate over the letters.\n  // Hint: You need to write a parallel reduction that counts the number of strings that start with each letter.\n  // Hint: You'll want to use Kokkos::atomic_fetch_add (or Kokkos::atomic_add)\n\n}",
            "// TODO: Implement\n\n}",
            "// Write your solution here\n}",
            "// NOTE: 1. We assume all strings are in lower case\n  // 2. We assume the alphabet starts with letter 'a'\n\n  //...\n}",
            "int numThreads = 1; // change to 1000 to speed up\n\n  // Compute first letter counts\n  Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace, int> range(0, numThreads);\n  Kokkos::parallel_for(\"firstLetterCounts\", range, [&] (int i) {\n    size_t offset = i * 26;\n    for (int j = 0; j < 26; j++)\n      bins[j] = 0;\n\n    for (int k = 0; k < s.size(); k++) {\n      bins[s[k][0] - 'a'] += 1;\n    }\n\n    for (int j = 0; j < 26; j++)\n      bins[j] += bins[j+1];\n\n    for (int j = 25; j >= 0; j--)\n      bins[j+1] = bins[j];\n\n    bins[0] = 0;\n  });\n}",
            "// TODO: Implement this function\n}",
            "// Fill in this function\n}",
            "Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static> > policy(0, 26);\n    Kokkos::parallel_for(policy, [&](int j) {\n      constexpr size_t N = 26;\n      char first = j + 'a';\n      size_t cnt = 0;\n      for (size_t i = 0; i < s.extent(0); i++) {\n        if (s(i)[0] == first)\n          cnt++;\n      }\n      bins[j] = cnt;\n    });\n  }",
            "// First initialize bins to all zeros.\n  // Use a loop over all the 26 letters.\n  //\n  // Use parallel_for to compute the counts in parallel.\n  // Each thread should read one element from s and count the letters.\n  // At the end of the thread, add the counts to the appropriate bin.\n  //\n  // Use parallel_reduce to compute the total number of elements in each bin.\n  // In parallel_reduce, each thread should read the values in the bin array.\n  // The reduction should add these numbers together.\n  // The final result should be stored in the bins array.\n}",
            "using namespace Kokkos;\n  // TODO: Fill in this function.\n\n  // Get length of the longest string in the vector s.\n  int maxLen = 0;\n  size_t n = s.size();\n  for (int i=0; i<n; i++) {\n    int len = 0;\n    while(s(i)[len]!= '\\0') {\n      len++;\n    }\n    maxLen = maxLen < len? len : maxLen;\n  }\n  // Initialize bins.\n  // TODO: Fill in the initialization statement.\n\n  // Loop over strings in the vector s.\n  // TODO: Fill in the loop to count the number of strings in the vector s that start with each letter.\n\n  // Kokkos::fence();\n  // Kokkos::deep_copy(bins, bins_temp);\n}",
            "Kokkos::parallel_for(\"counts\", s.extent(0), KOKKOS_LAMBDA(const int i) {\n        bins(s(i)[0] - 'a') += 1;\n    });\n}",
            "using namespace Kokkos;\n    auto p = Threads(\"simple:firstLetterCounts\");\n    Kokkos::parallel_for(s.extent(0), p, [=](int i) {\n        bins(s(i, 0) - 'a')++;\n    });\n    Kokkos::deep_copy(bins, bins);\n}",
            "}",
            "// TODO:\n}",
            "// TODO\n\n}",
            "// TODO: Your code here\n  using namespace std;\n  using namespace Kokkos;\n\n  //Kokkos::parallel_for(\"First letter count\", 0, 25, [=](int k) {\n  //  bins[k] = 0;\n  //});\n  int n = s.extent(0);\n  Kokkos::RangePolicy<HostSpace> policy(0, n);\n  Kokkos::parallel_for(\"First letter count\", policy, [=](int k) {\n    bins[s(k)[0] - 'a'] += 1;\n  });\n  Kokkos::fence();\n}",
            "// implement this function\n}",
            "// TODO: Fill in this function.\n  // Hint: First, use Kokkos to parallelize the for-loop over all letters of the alphabet.\n  // Hint: Second, use Kokkos to parallelize the for-loop over the number of strings in the vector.\n  // Hint: Third, use Kokkos to parallelize the for-loop over the length of the string.\n  // Hint: Fourth, use Kokkos to parallelize the for-loop over the alphabet.\n  // Hint: Fifth, use Kokkos to parallelize the if-statement.\n  // Hint: Sixth, use Kokkos to parallelize the for-loop over the alphabet.\n  // Hint: Seventh, use Kokkos to parallelize the if-statement.\n  // Hint: Eighth, use Kokkos to parallelize the if-statement.\n  // Hint: Ninth, use Kokkos to parallelize the if-statement.\n  // Hint: Tenth, use Kokkos to parallelize the for-loop over the alphabet.\n  // Hint: Eleventh, use Kokkos to parallelize the for-loop over the alphabet.\n  // Hint: Twelfth, use Kokkos to parallelize the for-loop over the alphabet.\n  // Hint: Thirteenth, use Kokkos to parallelize the for-loop over the alphabet.\n  // Hint: Fourteenth, use Kokkos to parallelize the for-loop over the alphabet.\n  // Hint: Fifteenth, use Kokkos to parallelize the for-loop over the alphabet.\n  // Hint: Sixteenth, use Kokkos to parallelize the if-statement.\n  // Hint: Seventeenth, use Kokkos to parallelize the if-statement.\n  // Hint: Eighteenth, use Kokkos to parallelize the if-statement.\n}",
            "// TODO fill in this function\n}",
            "//TODO\n}",
            "Kokkos::parallel_for(\"first_letter_counts\", Kokkos::RangePolicy<>(0, s.extent(0)),\n                       [&](const size_t i) {\n                         bins[s(i, 0) - 'a'] += 1;\n                       });\n}",
            "// Fill in the following\n}",
            "// Your code here\n    auto b = bins.data();\n    auto ss = s.data();\n    int N = s.size();\n    for (int i = 0; i < N; i++) {\n        b[ss[i][0] - 97]++;\n    }\n}",
            "int N = s.size();\n  for(int i = 0; i < N; i++) {\n    // bins[s[i][0]-'a'] += 1;\n    bins(s(i)[0]-'a') += 1;\n  }\n}",
            "Kokkos::RangePolicy<> policy(0, s.extent(0));\n\n  Kokkos::parallel_for(policy, [=](const int i) {\n      bins[s(i, 0) - 'a'] += 1;\n  });\n}",
            "// Your code goes here!\n}",
            "const auto& h_s = Kokkos::create_mirror_view_and_copy(Kokkos::HostSpace(), s);\n    auto host_bins = Kokkos::create_mirror_view(bins);\n    for (auto i = 0; i < 26; i++) host_bins[i] = 0;\n\n    for (auto i = 0; i < s.extent_int(0); i++) {\n        char c = h_s[i][0];\n        if (c >= 'a' && c <= 'z') {\n            auto idx = (int) c - 'a';\n            host_bins(idx) += 1;\n        }\n    }\n    Kokkos::deep_copy(bins, host_bins);\n}",
            "// TODO: Your code here\n    //bins[i] = 0;\n}",
            "// YOUR CODE HERE\n}",
            "using Kokkos::RangePolicy;\n    using Kokkos::TeamPolicy;\n\n    const size_t numStrings = s.extent_int(0);\n    const size_t length = s.extent_int(1);\n\n    // TODO: implement firstLetterCounts\n\n}",
            "// Your code here.\n}",
            "// Your code here\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "#pragma omp parallel for\n    for (size_t i=0; i < s.size(); i++) {\n        for (size_t j=0; j < s[i].size(); j++) {\n            if (s[i][j] >= 'a' && s[i][j] <= 'z') {\n                bins[s[i][j]-'a']++;\n            }\n        }\n    }\n}",
            "// TODO: complete this function.\n  #pragma omp parallel for\n  for (int i = 0; i < 26; i++)\n  {\n    for (size_t j = 0; j < s.size(); j++)\n    {\n      bins[i] += s[j][0] - 'a';\n    }\n  }\n}",
            "int n_threads = 1;\n#pragma omp parallel\n  {\n#pragma omp single\n    n_threads = omp_get_num_threads();\n  }\n  std::array<std::array<size_t, 26>, 16> bins_local;\n\n  for (auto &str : s) {\n    size_t index = str[0] - 'a';\n    bins_local[index][str[0] - 'a']++;\n  }\n\n  for (auto &i : bins_local) {\n    for (int j = 0; j < 26; j++) {\n      bins[j] += i[j];\n    }\n  }\n\n}",
            "// TO BE COMPLETED\n}",
            "// Your code here\n    #pragma omp parallel for shared(bins)\n    for (int i = 0; i < 26; i++){\n        int letter_count = 0;\n        for (auto const& word : s)\n        {\n            if (word[0] == 'a' + i)\n            {\n                letter_count++;\n            }\n        }\n        bins[i] = letter_count;\n    }\n}",
            "// TODO\n  #pragma omp parallel for\n  for (int i = 0; i < 26; i++){\n    bins[i] = 0;\n  }\n  for(auto x : s) {\n    #pragma omp parallel for\n    for (int i = 0; i < 26; i++){\n      if(x[0] == ('a' + i)){\n        bins[i]++;\n      }\n    }\n  }\n}",
            "// TODO\n\tfor(int i = 0; i < 26; i++) {\n\t\tbins[i] = 0;\n\t}\n\tfor(int i = 0; i < s.size(); i++) {\n\t\tif(isalpha(s[i][0])) {\n\t\t\tint letter = (int)tolower(s[i][0]);\n\t\t\tbins[letter - 97] += 1;\n\t\t}\n\t}\n}",
            "#pragma omp parallel for shared(bins)\n  for(auto i = 0; i < s.size(); i++) {\n    char first_letter = tolower(s[i][0]);\n    bins[first_letter - 'a']++;\n  }\n}",
            "#pragma omp parallel for\n  for (int i=0; i<s.size(); i++){\n    char c = s[i][0];\n    int letter = (int)c - 97;\n    bins[letter]++;\n  }\n\n}",
            "#pragma omp parallel\n  {\n    #pragma omp for\n    for (int i = 0; i < s.size(); ++i) {\n      char c = s[i][0];\n      #pragma omp atomic\n      ++bins[c-'a'];\n    }\n  }\n}",
            "std::array<size_t, 26> counts{};\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        char firstLetter = tolower(s[i][0]);\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            counts[firstLetter - 'a']++;\n        }\n    }\n    bins = counts;\n}",
            "#pragma omp parallel for\n  for (char i = 'a'; i < 'a' + 26; i++) {\n    bins[i - 'a'] = 0;\n  }\n  #pragma omp parallel for\n  for (size_t i = 0; i < s.size(); i++) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "// Initialize bins with zeros\n  for (size_t i = 0; i < 26; ++i) {\n    bins[i] = 0;\n  }\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < s.size(); i++) {\n    char c = s[i][0];\n    c = std::tolower(c);\n    size_t index = (size_t)c - (size_t)'a';\n    bins[index]++;\n  }\n}",
            "size_t max_threads = 4;\n\n#pragma omp parallel num_threads(max_threads)\n    {\n#pragma omp for\n        for(size_t i = 0; i < s.size(); ++i){\n            bins[s[i][0] - 'a']++;\n        }\n    }\n    \n}",
            "for (std::string i : s) {\n        bins[i[0] - 'a']++;\n    }\n}",
            "#pragma omp parallel for schedule(dynamic)\n  for (int i=0; i<bins.size(); i++)\n  {\n    for (auto e : s)\n    {\n      if (e[0] == (i+97))\n      {\n        bins[i]++;\n      }\n    }\n  }\n}",
            "for(size_t i=0; i<s.size(); i++) {\n    size_t pos = s[i][0] - 'a';\n    #pragma omp atomic\n    bins[pos]++;\n  }\n}",
            "// TODO: Your code here\n}",
            "#pragma omp parallel for\n    for (int i=0; i < 26; i++){\n        for(auto &x: s){\n            if(x[0] == (char)('a' + i)){\n                bins[i] += 1;\n            }\n        }\n    }\n\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; ++i)\n        bins[i] = 0;\n\n    for (std::string word: s) {\n        int ind = word[0] - 'a';\n        bins[ind]++;\n    }\n}",
            "for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        bins[s[i].front() - 'a']++;\n    }\n}",
            "#pragma omp parallel for schedule(static) \n  for(int i = 0; i < 26; ++i)\n    bins[i] = 0;\n\n  #pragma omp parallel for schedule(static)\n  for (int i = 0; i < s.size(); ++i) {\n    int k = (int)s[i][0] - (int)'a';\n    bins[k]++;\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        for (std::string string : s) {\n            if (string[0] == 'a' + i) {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "// TODO\n\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < 26; i++) {\n    bins[i] = 0;\n  }\n\n  for (auto &x : s) {\n    #pragma omp parallel for\n    for (int j = 0; j < 26; j++) {\n      if(x[0] == 'a' + j) {\n        bins[j]++;\n      }\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n        for (int j = 0; j < s.size(); j++) {\n            if (s[j][0] == 'a' + i) {\n                bins[i] += 1;\n            }\n        }\n    }\n}",
            "// TODO: implement the function using OpenMP\n#pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        int index = s[i][0] - 'a';\n        bins[index]++;\n    }\n\n}",
            "int i;\n    // code here\n    //bins[0] = 0;\n    //bins[1] = 0;\n    for(i = 0; i < s.size(); i++){\n        int k = 0;\n        for (int j = 0; j < s[i].size(); j++){\n            if (s[i][j] < 97 || s[i][j] > 122){\n                k = 1;\n                break;\n            }\n            if (k == 0){\n                bins[(int)(s[i][j] - 97)] += 1;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for schedule(static, 1)\n    for (size_t i = 0; i < 26; ++i) {\n        bins[i] = 0;\n    }\n    for (auto &it : s) {\n        ++bins[it[0] - 'a'];\n    }\n}",
            "// TODO: Your code here\n\n    int i,j;\n    for (i=0; i<s.size(); i++) {\n        j=s[i][0]-'a';\n        bins[j]++;\n    }\n}",
            "bins = {0};\n\n#pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n\n        char letter = tolower(s[i][0]);\n        char letter_index = letter - 97;\n\n        bins[letter_index] += 1;\n\n    }\n\n}",
            "std::vector<std::string> s_copy;\n    for (auto const& str : s) {\n        s_copy.push_back(str);\n    }\n\n#pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        auto &x = s_copy[i];\n        bins[x[0] - 'a']++;\n    }\n\n    // copy the result back\n    for (int i = 0; i < 26; i++) {\n        bins[i] = s_copy[i];\n    }\n\n}",
            "// TODO\n\n    // size_t number_threads = 4;\n    // size_t number_of_strings = s.size();\n    // size_t number_of_bins = 26;\n\n    // int bins[26] = {0};\n\n    // int letter = s[0].at(0);\n    // int start = 0;\n\n    // int threads_per_bin = number_of_strings / number_of_bins;\n    // int remainder = number_of_strings % number_of_bins;\n\n    // int end = threads_per_bin;\n\n    // if(end % 2 == 0)\n    //     end += 1;\n\n    // #pragma omp parallel for num_threads(number_threads)\n    // for(int i = 0; i < number_of_bins; i++) {\n    //     for(int j = start; j < end; j++) {\n    //         if(s[j].at(0) == letter) {\n    //             bins[i] += 1;\n    //         }\n    //     }\n    //     start = end;\n    //     end += threads_per_bin;\n    //     if(end > number_of_strings) {\n    //         end = number_of_strings;\n    //     }\n    //     letter++;\n    // }\n\n    // for(int k = 0; k < number_of_bins; k++) {\n    //     std::cout << bins[k] << \" \";\n    // }\n    // std::cout << std::endl;\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; i++)\n    {\n        bins[i] = 0;\n    }\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++)\n    {\n        bins[s[i][0] - 97]++;\n    }\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for\n        for(size_t i = 0; i < s.size(); i++) {\n            int j = s[i][0] - 'a';\n            bins[j]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "//TODO\n    //int num_of_strings = s.size();\n    #pragma omp parallel for schedule(guided)\n    for (int i = 0; i < 26; i++){\n        bins[i] = 0;\n    }\n    //#pragma omp parallel for schedule(guided)\n    for (int i = 0; i < s.size(); i++){\n        int count = 0;\n        for (int j = 0; j < 26; j++){\n            if (s[i][0] == 'a' + j){\n                count++;\n            }\n        }\n        bins[count]++;\n    }\n}",
            "for(auto& str : s){\n        bins[str[0]-'a']++;\n    }\n}",
            "for (size_t i = 0; i < 26; ++i) {\n    bins[i] = 0;\n  }\n\n  for (auto str : s) {\n    if (str.empty()) {\n      continue;\n    }\n    char first_char = str[0];\n    if (first_char >= 'a' && first_char <= 'z') {\n      bins[first_char - 'a']++;\n    }\n  }\n}",
            "#pragma omp parallel for\n  for(int i=0;i<26;i++) {\n    bins[i] = 0;\n  }\n  \n  for (std::string const& str : s) {\n    char c = str[0];\n    size_t bin = c - 'a';\n    bins[bin]++;\n  }\n  \n}",
            "// Fill in your code here\n  #pragma omp parallel for\n  for (int i=0; i<bins.size(); i++) {\n    for (int j=0; j<s.size(); j++) {\n      if(s[j].size() >= i+1) {\n        if(s[j][0] == 'a' + i) {\n          bins[i]++;\n        }\n      }\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (char ch = 'a'; ch <= 'z'; ++ch) {\n        auto it = s.begin();\n        while (it!= s.end()) {\n            if (*it == ch) {\n                ++bins[ch - 'a'];\n                ++it;\n            } else {\n                it = s.erase(it);\n            }\n        }\n    }\n}",
            "// your code here\n    #pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n    for (std::string i : s) {\n        int j = i[0];\n        j = j - 'a';\n        bins[j]++;\n    }\n\n}",
            "int nthreads = omp_get_num_threads();\n    int thread_id = omp_get_thread_num();\n\n    for (int i = 0; i < 26; i++) {\n        int temp = 0;\n        for (int j = thread_id; j < s.size(); j+= nthreads) {\n            if (s[j][0] == i + 97) {\n                temp += 1;\n            }\n        }\n        bins[i] = temp;\n    }\n}",
            "for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n\n    #pragma omp parallel for shared(s, bins)\n    for (int i = 0; i < s.size(); i++) {\n        char temp = s[i][0];\n        temp = tolower(temp);\n        bins[temp - 'a']++;\n    }\n}",
            "// Fill in the code here\n    int len = s.size();\n    #pragma omp parallel for\n    for(int i = 0; i < len; i++){\n        int l = s[i][0]-'a';\n        bins[l]++;\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < 26; i++) {\n    bins[i] = 0;\n  }\n  for (int i = 0; i < s.size(); i++) {\n    bins[s[i][0] - 'a'] += 1;\n  }\n}",
            "std::array<size_t, 26> array;\n    #pragma omp parallel for\n    for(size_t i = 0; i < 26; i++){\n        for(size_t j = 0; j < s.size(); j++){\n            if(s.at(j).at(0) == 'a' + i){\n                array.at(i)++;\n            }\n        }\n    }\n    for(size_t i = 0; i < 26; i++){\n        bins.at(i) = array.at(i);\n    }\n}",
            "#pragma omp parallel for\n  for(int i = 0; i < 26; i++){\n    for(int j = 0; j < s.size(); j++){\n      if(i == s[j][0] - 'a'){\n        bins[i]++;\n      }\n    }\n  }\n\n}",
            "// Your code here.\n\n\tomp_set_num_threads(omp_get_max_threads());\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < s.size(); i++) {\n\t\tchar first = s[i][0];\n\t\tif (first >= 'a' && first <= 'z') {\n\t\t\tbins[first - 'a']++;\n\t\t}\n\t}\n}",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < 26; i++)\n\t\tbins[i] = 0;\n\n\tfor (size_t i = 0; i < s.size(); i++) {\n\t\tchar c = s[i][0];\n\t\tint index = c - 'a';\n\t\tbins[index] += 1;\n\t}\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "// Create OpenMP parallel region\n    #pragma omp parallel\n    {\n        // Create OpenMP for loop\n        #pragma omp for\n        for (size_t i = 0; i < s.size(); ++i) {\n            // Update bin based on first letter of string in s\n            bins[s[i][0] - 'a']++;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; i++){\n        bins[i] = 0;\n        for (size_t j = 0; j < s.size(); j++){\n            if (s[j][0] == 'a' + i){\n                bins[i]++;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n  for(auto i = 0; i < bins.size(); i++){\n    for(auto j = 0; j < s.size(); j++){\n      if(s[j][0] == 'a' + i){\n        bins[i]++;\n      }\n    }\n  }\n}",
            "/*\n    // this solution works but is not parallel\n    for (auto i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n    */\n\n    #pragma omp parallel for\n    for (auto i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n\n}",
            "#pragma omp parallel for\n    for (char c = 'a'; c <= 'z'; c++) {\n        for (auto& str: s) {\n            if (str[0] == c)\n                bins[c-'a']++;\n        }\n    }\n}",
            "std::array<size_t, 26> myBins{};\n    std::array<char, 26> arr = {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l','m', 'n', 'o', 'p', 'q', 'r','s', 't', 'u', 'v', 'w', 'x', 'y', 'z'};\n\n#pragma omp parallel for shared(s, myBins)\n    for (int i = 0; i < s.size(); i++) {\n        if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n            if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n                myBins[s[i][0] - 'a']++;\n            }\n        }\n    }\n\n    for (int i = 0; i < myBins.size(); i++) {\n        bins[i] += myBins[i];\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        #pragma omp atomic\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "for(auto i = 0; i < bins.size(); i++) {\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n      if(s[i].front() == (char)i + 'a') {\n        bins[i]++;\n      }\n    }\n  }\n}",
            "#pragma omp parallel\n  {\n    #pragma omp for schedule(static)\n    for(size_t i=0; i<s.size(); i++){\n      for(size_t j=0; j<s[i].length(); j++){\n        bins[s[i][j]-'a']++;\n      }\n    }\n  }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < 26; i++) {\n    for (auto word : s) {\n      if (word[0] == (char) i + 'a') {\n        bins[i]++;\n      }\n    }\n  }\n}",
            "// std::cout << \"here\" << std::endl;\n    // std::cout << \"s.size\" << s.size() << std::endl;\n    // std::cout << \"s[0]\" << s[0] << std::endl;\n    std::string temp;\n    int count = 0;\n    // std::cout << \"here\" << std::endl;\n    // std::cout << \"bins.size\" << bins.size() << std::endl;\n    // for (size_t i = 0; i < s.size(); i++) {\n    //     std::cout << s[i] << std::endl;\n    // }\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        temp = s[i];\n        count = (int)temp[0];\n        count = count - 97;\n        bins[count]++;\n        // std::cout << bins[count] << std::endl;\n    }\n}",
            "// fill bins with zeros\n    for (size_t i = 0; i < 26; ++i) bins[i] = 0;\n\n    #pragma omp parallel\n    {\n        std::array<size_t, 26> tmp{0};\n\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < s.size(); ++i) {\n            tmp[s[i][0] - 'a'] += 1;\n        }\n        // update bins with the tmp array\n        #pragma omp critical\n        {\n            for (size_t i = 0; i < 26; ++i) bins[i] += tmp[i];\n        }\n    }\n}",
            "#pragma omp parallel for\n  for(int i = 0; i < 26; ++i) {\n    for(auto j = 0; j < s.size(); ++j) {\n      if(i == s.at(j).front() - 'a') {\n        ++bins[i];\n      }\n    }\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < 26; i++)\n    bins[i] = 0;\n\n  for (size_t i = 0; i < s.size(); i++) {\n    #pragma omp atomic\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "#pragma omp parallel for\n  for(size_t i = 0; i < bins.size(); i++){\n    for(size_t j = 0; j < s.size(); j++){\n      if(s[j][0] == 'a' + i){\n        bins[i]++;\n      }\n    }\n  }\n}",
            "size_t const N = s.size();\n\tstd::array<size_t, 26> b{};\n\n\t// Parallelized using openMP\n\t#pragma omp parallel for\n\tfor(int i=0; i<N; i++){\n\t\tchar c = s[i].at(0);\n\t\tint index = c - 'a';\n\t\tb[index] += 1;\n\t}\n\tbins = b;\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for schedule(static)\n        for(int i = 0; i < 26; i++) {\n            bins[i] = 0;\n            for(std::string str: s) {\n                if(str[0] == 'a' + i) {\n                    bins[i]++;\n                }\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        for (auto j = 0; j < s.size(); j++) {\n            if (s[j][0] == (char) (i + 'a')) {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n\tfor (int i = 0; i < 26; i++) {\n\t\tsize_t counter = 0;\n\t\tfor (auto &j : s) {\n\t\t\tif (j[0] == i + 97) {\n\t\t\t\tcounter++;\n\t\t\t}\n\t\t}\n\t\tbins[i] = counter;\n\t}\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; i++)\n    {\n        for (auto str : s)\n        {\n            if (str[0] == 'a' + i)\n                bins[i]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; ++i) {\n        bins[i] = 0;\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); ++i) {\n        char first_letter = s[i][0];\n        int index = first_letter - 'a';\n        bins[index]++;\n    }\n}",
            "//TODO: implement\n#pragma omp parallel\n  {\n    #pragma omp for\n    for (int i = 0; i < s.size(); i++) {\n      if (s[i].size() > 0) {\n        bins[s[i][0] - 'a']++;\n      }\n    }\n  }\n}",
            "std::array<size_t, 26> bins_local{};\n\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); ++i) {\n        bins_local[s.at(i).at(0) - 'a'] += 1;\n    }\n\n    for (int i = 0; i < 26; ++i) {\n        bins[i] += bins_local[i];\n    }\n}",
            "std::array<size_t, 26> local_bins;\n    local_bins.fill(0);\n\n    for(std::string& str : s){\n        local_bins[str[0] - 'a']++;\n    }\n\n    #pragma omp parallel for num_threads(8)\n    for(int i = 0; i < 26; i++){\n        bins[i] += local_bins[i];\n    }\n}",
            "#pragma omp parallel for \n    for (int i = 0; i < 26; i++)\n    {\n        bins[i] = 0;\n        char c = 'a' + i;\n        for (auto word: s)\n        {\n            if (word[0] == c)\n            {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "for (int i = 0; i < 26; ++i) {\n    bins[i] = 0;\n  }\n  #pragma omp parallel for\n  for (int i = 0; i < s.size(); ++i) {\n    bins[s[i][0] - 'a'] += 1;\n  }\n}",
            "#pragma omp parallel for\n    for(int i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "// TODO: implement\n\t// #pragma omp parallel\n\t{\n\t\tchar letter = 'a';\n\t\tfor (int i = 0; i < s.size(); i++)\n\t\t{\n\t\t\tstd::string first_letter = s.at(i).substr(0, 1);\n\t\t\tif (first_letter.compare(letter) == 0)\n\t\t\t{\n\t\t\t\tbins.at(letter - 'a')++;\n\t\t\t}\n\t\t\tletter++;\n\t\t}\n\t}\n}",
            "// NOTE: The following is an example, you do not need to use the\n  // omp_lock_t lock in your solution.\n  static omp_lock_t lock;\n  omp_init_lock(&lock);\n#pragma omp parallel\n  {\n#pragma omp for\n    for (int i = 0; i < 26; i++) {\n      bins[i] = 0;\n    }\n#pragma omp for\n    for (size_t i = 0; i < s.size(); i++) {\n      omp_set_lock(&lock);\n      bins[s[i][0] - 'a'] += 1;\n      omp_unset_lock(&lock);\n    }\n  }\n  omp_destroy_lock(&lock);\n}",
            "size_t i = 0;\n    #pragma omp parallel for\n    for(;i<26;++i) {\n        for(auto it = s.begin();it!=s.end();++it)\n            if(it->at(0) == (char) (i+'a'))\n                ++bins[i];\n    }\n}",
            "for (int i = 0; i < 26; ++i) {\n        bins[i] = 0;\n    }\n\n#pragma omp parallel for\n    for (int i = 0; i < s.size(); ++i) {\n        bins[s[i][0] - 'a'] += 1;\n    }\n}",
            "for (auto& element : bins) {\n    element = 0;\n  }\n\n#pragma omp parallel for default(none) shared(s, bins)\n  for (size_t i = 0; i < s.size(); ++i) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; ++i) {\n        for (auto const& s_str: s) {\n            if (s_str[0] == (char) ('a' + i)) {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "// TODO: Your code here\n\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n#pragma omp parallel for schedule(static)\n    for(size_t i = 0; i < s.size(); ++i){\n        ++bins[s[i][0] - 'a'];\n    }\n}",
            "for (int i = 0; i < bins.size(); i++)\n        bins[i] = 0;\n\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        char first = s[i][0];\n        bins[first-'a']++;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (const auto& str: s) {\n        auto it = str.begin();\n        while (it!= str.end() && *it >= 'a' && *it <= 'z') {\n            ++bins[*it++ - 'a'];\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        char first = s[i][0];\n        bins[first-97]++;\n    }\n}",
            "for(auto & bin : bins)\n    {\n        bin = 0;\n    }\n\n    for(auto const& e: s)\n    {\n        ++bins[e[0] - 97];\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < 26; i++)\n    {\n        for (size_t j = 0; j < s.size(); j++)\n        {\n            if (i == (s[j][0] - 'a'))\n            {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "//TODO: Your code here\n\n\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < 26; i++){\n        char letter = 'a' + i;\n        bins[i] = 0;\n        for(size_t j = 0; j < s.size(); j++){\n            if (s[j][0] == letter){\n                bins[i]++;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "std::array<size_t, 26> local{};\n    for(const auto& i : s){\n        ++local[i.front() - 'a'];\n    }\n#pragma omp parallel for\n    for(size_t i = 0; i < 26; ++i){\n        bins[i] += local[i];\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < 26; i++){\n    for (int j = 0; j < s.size(); j++){\n      if (s[j][0] == 'a' + i)\n        bins[i]++;\n    }\n  }\n}",
            "// TODO\n}",
            "// TODO: implement\n  // #pragma omp parallel for schedule(guided)\n  // for (int i=0; i<s.size(); i++) {\n  //   bins[s[i][0] - 'a']++;\n  // }\n\n  std::vector<int> temp(26, 0);\n  for (auto const& str : s) {\n    temp[str[0] - 'a']++;\n  }\n  for (int i = 0; i < 26; i++) {\n    bins[i] = temp[i];\n  }\n}",
            "#pragma omp parallel for\n    for(size_t i = 0; i < s.size(); i++){\n        char letter = s[i].c_str()[0];\n        bins[(int)letter - 97]++;\n    }\n}",
            "// TODO: implement\n    int const N = s.size();\n    int const THREADS = omp_get_num_threads();\n    size_t const CHUNK = N/THREADS;\n\n    #pragma omp parallel\n    {\n        int const ID = omp_get_thread_num();\n        size_t const BASE = CHUNK * ID;\n\n        #pragma omp for\n        for (int i = BASE; i < BASE + CHUNK; ++i) {\n            std::string word = s[i];\n            if (word.size() > 0) {\n                word[0] -= 'a';\n                ++bins[word[0]];\n            }\n        }\n    }\n}",
            "std::array<size_t, 26> bins1;\n  for (int i = 0; i < 26; i++){\n    bins[i] = 0;\n  }\n  for (const auto &str:s){\n    int ch = str[0] - 'a';\n    bins[ch] += 1;\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < 26; i++) {\n    char letter = 'a' + i;\n    bins[i] = 0;\n    for (auto const& word : s) {\n      if (word[0] == letter) {\n        bins[i]++;\n      }\n    }\n  }\n}",
            "// start with 0 and use #pragma omp parallel for\n    for (int i = 0; i < bins.size(); i++) {\n        bins[i] = 0;\n    }\n\n    // your code here\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        char first = s[i][0];\n        if (first >= 'a' && first <= 'z') {\n            int ind = first - 'a';\n            bins[ind] += 1;\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < 26; i++) {\n    int index = int(s[0][0] - 'a');\n    for (size_t j = 0; j < s.size(); j++) {\n      if (s[j][0] == (char)(index + 'a')) {\n        bins[index]++;\n      }\n    }\n  }\n}",
            "bins.fill(0);\n\tfor (auto& str: s) {\n\t\t++bins[str[0] - 'a'];\n\t}\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < 26; i++) {\n    for (auto it = s.begin(); it!= s.end(); it++) {\n      if (tolower(*it)[0] == (char) (i + 'a')) {\n        bins[i]++;\n        break;\n      }\n    }\n  }\n}",
            "int tid = omp_get_thread_num();\n  int total_threads = omp_get_num_threads();\n\n  // The following code is incorrect. Fix it.\n  for (size_t i = 0; i < 26; i++) {\n    bins[i] = 0;\n  }\n\n  size_t per_thread_work = s.size() / total_threads;\n  size_t start = per_thread_work * tid;\n  size_t end = start + per_thread_work;\n\n  for (size_t i = start; i < end; i++) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "#pragma omp parallel for\n\tfor (int i = 0; i < s.size(); i++)\n\t{\n\t\tchar firstChar = s.at(i).at(0);\n\t\tif (firstChar >= 'a' && firstChar <= 'z') {\n\t\t\tbins[firstChar - 'a']++;\n\t\t}\n\t}\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++)\n    {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "for (int i=0; i<26; ++i) {\n    bins[i] = 0;\n  }\n\n  #pragma omp parallel for\n  for (int i=0; i<s.size(); ++i) {\n    char c = s[i][0];\n    bins[c-'a']++;\n  }\n}",
            "int max_threads = omp_get_max_threads();\n    int tid = omp_get_thread_num();\n    size_t n = s.size();\n\n    for (auto &i : bins)\n        i = 0;\n\n    #pragma omp parallel for num_threads(max_threads) schedule(dynamic, 1)\n    for (size_t i = 0; i < n; ++i) {\n        char first_letter = s[i][0];\n        if (first_letter >= 'a' && first_letter <= 'z')\n            bins[first_letter - 'a']++;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++)\n        bins[s[i][0] - 'a']++;\n}",
            "// FIXME: implement\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "#pragma omp parallel\n{\n#pragma omp for\n    for(size_t i = 0; i < s.size(); i++){\n        bins[s[i][0] - 'a']++;\n    }\n}\n}",
            "for (int i = 0; i < 26; i++) {\n        for (int j = 0; j < s.size(); j++) {\n            if (s[j][0] == (i + 'a'))\n                bins[i]++;\n        }\n    }\n}",
            "size_t thread_id = omp_get_thread_num();\n    size_t thread_num = omp_get_num_threads();\n    size_t thread_count = std::thread::hardware_concurrency();\n\n    // your code here\n    #pragma omp parallel num_threads(thread_count)\n    {\n        #pragma omp for\n        for (int i = 0; i < s.size(); i++)\n        {\n            bins[s[i][0] - 'a'] += 1;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n            bins[s[i][0] - 'a']++;\n        }\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (const auto& str : s) {\n        bins[tolower(str[0]) - 'a'] += 1;\n    }\n}",
            "for (const std::string &s_item : s)\n  {\n    char first_letter = s_item[0];\n    if(first_letter >= 'a' && first_letter <= 'z') {\n      bins[first_letter - 'a']++;\n    }\n  }\n}",
            "for (size_t i = 0; i < s.size(); i++) {\n        auto x = s.at(i).at(0);\n        if (x >= 'a' && x <= 'z') {\n            bins.at(x - 'a')++;\n        }\n    }\n}",
            "#pragma omp parallel\n    {\n        std::array<size_t, 26> local_bins;\n        #pragma omp for\n        for (int i = 0; i < s.size(); i++) {\n            local_bins[s[i][0] - 'a']++;\n        }\n        #pragma omp critical\n        for (int i = 0; i < 26; i++) {\n            bins[i] += local_bins[i];\n        }\n    }\n}",
            "// Your code here\n\t#pragma omp parallel\n\t{\n\t\t#pragma omp for\n\t\tfor(int i=0;i<26;i++)\n\t\t{\n\t\t\tbins[i]=0;\n\t\t}\n\t\t\n\t\t\n\t}\n\t#pragma omp parallel for\n\tfor(int i=0;i<s.size();i++)\n\t{\n\t\t\n\t\tbins[s[i][0]-'a']++;\n\t}\n\t\n\t\n\t\n\t\n\n}",
            "#pragma omp parallel\n    {\n#pragma omp for\n        for (int i = 0; i < 26; i++) {\n            bins[i] = 0;\n        }\n        for (auto const& str: s) {\n            int bin = (int) str[0] - 97;\n            bins[bin]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < 26; i++) {\n        for (size_t j = 0; j < s.size(); j++) {\n            if (s[j][0] == 'a' + i) {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "for (auto& bin : bins)\n    bin = 0;\n  // TODO: Implement\n  for (int i = 0; i < s.size(); i++){\n    bins[s[i][0] - 'a'] += 1;\n  }\n}",
            "std::array<size_t, 26> counts;\n    std::fill(counts.begin(), counts.end(), 0);\n\n#pragma omp parallel\n    {\n#pragma omp for\n        for (size_t i = 0; i < s.size(); i++) {\n            auto letter = s[i][0];\n            if (letter >= 'a' && letter <= 'z') {\n                counts[letter - 'a']++;\n            }\n        }\n    }\n\n    for (size_t i = 0; i < 26; i++) {\n        bins[i] = counts[i];\n    }\n}",
            "std::array<size_t, 26> bins1 = {};\n    for (int i = 0; i < s.size(); i++) {\n        bins1[s[i][0] - 'a']++;\n    }\n    #pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        bins[i] += bins1[i];\n    }\n}",
            "//TODO\n  #pragma omp parallel for\n  for (int i = 0; i < 26; i++){\n    int index = (int)('a'+i);\n    for (int j = 0; j < s.size(); j++){\n      if (s[j].front() == (char)index){\n        bins[i]++;\n      }\n    }\n  }\n}",
            "int n = s.size();\n    for(int i = 0; i<n; ++i)\n    {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "#pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        int num_threads = omp_get_num_threads();\n        int begin = (s.size() / num_threads) * tid;\n        int end = (s.size() / num_threads) * (tid + 1);\n        if (tid == num_threads - 1) {\n            end = s.size();\n        }\n        for (int i = begin; i < end; ++i) {\n            int index = s[i][0] - 97;\n            bins[index]++;\n        }\n    }\n}",
            "// Your code goes here\n    int num_threads = omp_get_max_threads();\n    std::vector<std::array<size_t, 26>> counts(num_threads);\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        char ch = s[i][0];\n        int tid = omp_get_thread_num();\n        counts[tid][ch-'a']++;\n    }\n    for (int i = 0; i < num_threads; i++) {\n        for (int j = 0; j < 26; j++) {\n            bins[j] += counts[i][j];\n        }\n    }\n}",
            "#pragma omp parallel for\n\tfor (int i = 0; i < s.size(); ++i)\n\t{\n\t\tsize_t idx = s.at(i).front() - 'a';\n\t\tbins[idx]++;\n\t}\n}",
            "// your code here\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for\n        for(size_t i = 0; i < 26; i++) {\n            bins[i] = 0;\n        }\n        #pragma omp for\n        for(size_t i = 0; i < s.size(); i++) {\n            bins[s[i][0] - 'a']++;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; ++i) {\n        bins[i] = 0;\n    }\n\n    for (auto &str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "std::array<size_t, 26> counts;\n  #pragma omp parallel for\n  for (size_t i = 0; i < s.size(); i++) {\n    char a = tolower(s[i][0]);\n    #pragma omp atomic\n    counts[a - 'a'] += 1;\n  }\n  #pragma omp parallel for\n  for (size_t i = 0; i < 26; i++) {\n    #pragma omp atomic\n    bins[i] += counts[i];\n  }\n}",
            "std::array<size_t, 26> counts{};\n    #pragma omp parallel for schedule(static) default(none) shared(s, bins)\n    for (int i = 0; i < s.size(); i++) {\n        const auto& str = s[i];\n        if (str.empty())\n            continue;\n        counts[tolower(str[0]) - 'a']++;\n    }\n\n    std::memcpy(bins.data(), counts.data(), sizeof(size_t) * counts.size());\n}",
            "int num_threads = omp_get_max_threads();\n    int thread_id = omp_get_thread_num();\n    size_t num_letters = s.size();\n    size_t block_size = num_letters / num_threads;\n    size_t start = thread_id * block_size;\n    size_t end = (thread_id + 1) * block_size;\n\n    for(size_t i = start; i < end; ++i){\n        ++bins[s[i][0] - 'a'];\n    }\n\n    return;\n}",
            "std::array<size_t, 26> count{};\n  for(auto& str: s){\n    ++count[str[0]-'a'];\n  }\n  #pragma omp parallel for\n  for(int i=0; i<count.size(); ++i){\n    bins[i]+=count[i];\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < bins.size(); ++i) {\n        for (auto &str : s) {\n            bins[i] += (str[0] - 'a') == i;\n        }\n    }\n}",
            "// your code here\n\tint i, j;\n\tfor (j = 0; j < 26; ++j)\n\t\tbins[j] = 0;\n\tfor (i = 0; i < s.size(); ++i)\n\t\tbins[s[i][0] - 'a']++;\n\treturn;\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < s.size(); i++) {\n    char first_letter = s[i][0];\n    // first_letter = 1 means 'a'\n    // first_letter = 10 means 'j'\n    // first_letter = 13 means'm'\n    // first_letter = 25 means 'z'\n    bins[first_letter - 'a']++;\n  }\n}",
            "// TODO: implement\n\n  std::array<size_t, 26> bins{};\n\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for (auto i = 0; i < s.size(); ++i) {\n      int x = s[i][0] - 'a';\n      ++bins[x];\n    }\n  }\n\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n\n    for (int i = 0; i < s.size(); i++) {\n        bins[s.at(i)[0] - 'a']++;\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; ++i) {\n        bins[i] = 0;\n    }\n    for (int i = 0; i < s.size(); ++i) {\n        #pragma omp parallel for\n        for (int j = 0; j < s[i].length(); ++j) {\n            if(s[i][j] >= 'a' && s[i][j] <= 'z') {\n                bins[s[i][j] - 'a']++;\n            }\n        }\n    }\n}",
            "omp_set_num_threads(4);\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < 26; ++i)\n\t{\n\t\tfor (size_t j = 0; j < s.size(); ++j)\n\t\t{\n\t\t\tif (s[j][0] == 'a' + i)\n\t\t\t\tbins[i]++;\n\t\t}\n\t}\n\n}",
            "#pragma omp parallel for \n  for (int i = 0; i < 26; i++){\n    bins[i] = 0;\n    for (std::string word: s){\n      if (word.length() > 0){\n        if (word[0] == 'a' + i){\n          bins[i]++;\n        }\n      }\n    }\n  }\n}",
            "// TODO\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i = 0; i < s.size(); i++) {\n            for (int j = 0; j < 26; j++) {\n                if (s[i][0] == j + 'a') {\n                    bins[j]++;\n                }\n            }\n        }\n    }\n}",
            "for (auto const& str : s)\n        if (str.size() > 0)\n            bins[str[0]-'a']++;\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < 26; i++)\n  {\n    bins[i] = 0;\n  }\n\n  for (size_t i = 0; i < s.size(); i++) {\n    size_t index = s[i][0] - 'a';\n    #pragma omp atomic\n    bins[index]++;\n  }\n}",
            "#pragma omp parallel for\n  for(size_t i=0; i<26; i++) {\n    bins[i]=0;\n  }\n\n  for(auto str : s) {\n    bins[str[0]-'a']++;\n  }\n}",
            "for (int i = 0; i < s.size(); ++i) {\n        int j = 0;\n        while (s[i][j]) {\n            bins[s[i][j] - 'a'] += 1;\n            j++;\n        }\n    }\n}",
            "for(const std::string &str : s) {\n        if(!str.empty()) {\n            ++bins[str.front() - 'a'];\n        }\n    }\n}",
            "int n = s.size();\n    //#pragma omp parallel for\n    for(int i = 0; i < n; i++) {\n        int num = s[i][0] - 'a';\n        #pragma omp atomic\n        bins[num]++;\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        for (size_t j = 0; j < s.size(); j++) {\n            if (s[j][0] - 'a' == i) {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < 26; i++) {\n    for (const auto & word : s) {\n      if (word[0] == (i + 'a'))\n        bins[i]++;\n    }\n  }\n}",
            "// TODO: write code here\n    int thread_count = 4;\n    #pragma omp parallel for num_threads(thread_count)\n    for(auto i=0;i<26;++i)\n    {\n        bins[i] = std::count_if(s.begin(),s.end(), [i](std::string str) {return (str[0]==i+'a');});\n    }\n}",
            "int nthreads = omp_get_max_threads();\n  std::vector<std::vector<size_t>> count_table(26, std::vector<size_t>(nthreads, 0));\n  #pragma omp parallel\n  {\n    int tid = omp_get_thread_num();\n    #pragma omp for\n    for (size_t i = 0; i < s.size(); ++i)\n      count_table[s[i][0] - 'a'][tid] += 1;\n  }\n  for (int i = 0; i < 26; ++i) {\n    bins[i] = 0;\n    for (int j = 0; j < nthreads; ++j)\n      bins[i] += count_table[i][j];\n  }\n}",
            "// TODO: implement\n  for (int i=0;i<s.size();i++){\n    if(s[i].size()!=0){\n      bins[s[i][0]-'a']++;\n    }\n  }\n}",
            "#pragma omp parallel for\n    for(int i = 0; i < s.size(); i++){\n        for(int j = 0; j < 26; j++){\n            bins[j] = 0;\n        }\n        if(!s[i].empty()){\n            for(int j = 0; j < 26; j++){\n                if(s[i][0] == 'a' + j){\n                    bins[j]++;\n                }\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i=0; i<26; i++){\n        for (int j=0; j<s.size(); j++){\n            if (s[j][0] == i + 97){\n                #pragma omp atomic\n                bins[i]++;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; i++)\n    {\n        char c = 'a' + i;\n        bins[i] = 0;\n        for (int j = 0; j < s.size(); j++)\n        {\n            if (s[j][0] == c)\n                bins[i] += 1;\n        }\n    }\n\n}",
            "// TODO: implement the function\n    int size = s.size();\n    #pragma omp parallel for\n    for(int i = 0; i < size; i++) {\n        std::string first = s[i].substr(0,1);\n        bins[first[0]-'a']++;\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; ++i) {\n        bins[i] = 0;\n        char letter = i + 'a';\n        for (auto &word : s) {\n            if (word[0] == letter) {\n                ++bins[i];\n            }\n        }\n    }\n}",
            "for(auto const& str : s){\n        bins[str.front() - 'a']++;\n    }\n}",
            "#pragma omp parallel for\n    for(int i = 0; i < 26; i++){\n        for(int j = 0; j < s.size(); j++){\n            if(s[j][0] == i+'a'){\n                bins[i]++;\n                break;\n            }\n        }\n    }\n}",
            "for (int i = 0; i < 26; i++) {\n    bins[i] = 0;\n  }\n#pragma omp parallel for\n  for (int i = 0; i < s.size(); i++) {\n    bins[s[i][0]-97]++;\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; ++i) {\n        for (size_t j = 0; j < s.size(); ++j) {\n            if (s[j][0] == 'a' + i) {\n                ++bins[i];\n            }\n        }\n    }\n}",
            "size_t num_threads = 4;\n    #pragma omp parallel num_threads(num_threads)\n    {\n        size_t tid = omp_get_thread_num();\n        // printf(\"Thread %ld is starting\\n\", tid);\n        size_t const length = s.size();\n\n        int bins_local[26] = {};\n        for (size_t i = tid; i < length; i += num_threads)\n        {\n            //printf(\"Thread %ld processing %ld\\n\", tid, i);\n            std::string const& word = s[i];\n            if (word.length() > 0)\n            {\n                ++bins_local[word[0] - 'a'];\n            }\n        }\n        #pragma omp critical\n        {\n            for (int i = 0; i < 26; ++i)\n            {\n                bins[i] += bins_local[i];\n            }\n        }\n        // printf(\"Thread %ld ending\\n\", tid);\n    }\n}",
            "// Fill this in\n#pragma omp parallel\n  {\n    // int tid = omp_get_thread_num();\n    for (int i = 0; i < s.size(); i++) {\n      int index = s[i][0] - 'a';\n      if (index >= 0) {\n        #pragma omp atomic\n        bins[index]++;\n      }\n    }\n  }\n}",
            "// Fill in starting code\n#pragma omp parallel for\n  for (int i = 0; i < 26; ++i) {\n    bins[i] = 0;\n    for (int j = 0; j < s.size(); ++j) {\n      if (s[j][0] == char(i + 97)) {\n        bins[i]++;\n      }\n    }\n  }\n}",
            "std::vector<char> chars;\n  for (auto const& ss: s)\n    chars.push_back(ss[0]);\n\n  for (char c: chars)\n    #pragma omp parallel for\n    for (int i=0; i<26; i++) {\n      if (c == 'a' + i)\n        bins[i]++;\n    }\n\n}",
            "for (int i = 0; i < 26; ++i) bins[i] = 0;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "//...\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "bins.fill(0);\n    for (auto const& str : s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "// TODO\n    int l = 0;\n    int count = 0;\n    for (const std::string& str : s)\n    {\n        for (const auto& ch : str)\n        {\n            if (ch == 'a' || ch == 'b' || ch == 'c')\n            {\n                count++;\n            }\n            if (ch == 'd' || ch == 'e' || ch == 'f')\n            {\n                count++;\n            }\n            if (ch == 'g' || ch == 'h' || ch == 'i')\n            {\n                count++;\n            }\n            if (ch == 'j' || ch == 'k' || ch == 'l')\n            {\n                count++;\n            }\n            if (ch =='m' || ch == 'n' || ch == 'o')\n            {\n                count++;\n            }\n            if (ch == 'p' || ch == 'q' || ch == 'r' || ch =='s')\n            {\n                count++;\n            }\n            if (ch == 't' || ch == 'u' || ch == 'v')\n            {\n                count++;\n            }\n            if (ch == 'w' || ch == 'x' || ch == 'y' || ch == 'z')\n            {\n                count++;\n            }\n        }\n    }\n    bins[l] = count;\n}",
            "for(auto i = 0; i<s.size(); ++i){\n    char first_letter = s.at(i).at(0);\n    int index = first_letter - 97;\n    bins[index] += 1;\n  }\n  return;\n}",
            "#pragma omp parallel for num_threads(omp_get_max_threads())\n    for (char c = 'a'; c <= 'z'; c++) {\n\t\tsize_t index = c - 'a';\n\t\tbins[index] = 0;\n        for (std::string const& str : s) {\n\t\t\tif (str[0] == c)\n\t\t\t\tbins[index]++;\n        }\n    }\n}",
            "int i;\n    for (i=0; i<26; i++)\n        bins[i] = 0;\n\n    for (std::string word : s) {\n        char first = word[0];\n        bins[first - 97] += 1;\n    }\n}",
            "size_t k = 0;\n    #pragma omp parallel for\n    for (size_t i = 0; i < 26; i++) {\n        for (size_t j = 0; j < s.size(); j++) {\n            if (s[j].at(0) == (char)(i + 'a')) {\n                k++;\n            }\n        }\n        bins[i] = k;\n        k = 0;\n    }\n}",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < 26; ++i) {\n\t\tbins[i] = 0;\n\t}\n\n\tfor (auto& str : s) {\n\t\tchar c = str[0];\n\t\tif (c >= 'a' && c <= 'z') {\n\t\t\tbins[c - 'a']++;\n\t\t}\n\t}\n}",
            "}",
            "// TODO: implement\n  for (auto i : bins) {\n    i = 0;\n  }\n  for (size_t i = 0; i < s.size(); i++) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for schedule(static)\n        for (int i = 0; i < s.size(); i++) {\n            bins[s[i][0] - 'a']++;\n        }\n    }\n}",
            "for(size_t i = 0; i < bins.size(); ++i)\n        bins[i] = 0;\n    #pragma omp parallel for\n    for(size_t i = 0; i < s.size(); ++i)\n        bins[s[i][0] - 'a']++;\n}",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < s.size(); i++) {\n\t\tbins[s[i][0] - 'a']++;\n\t}\n}",
            "// TODO: Your code here\n  for (int i = 0; i < 26; i++) {\n    #pragma omp parallel for\n    for (int j = 0; j < s.size(); j++) {\n      if (s[j][0] == 'a' + i) {\n        bins[i]++;\n      }\n    }\n  }\n}",
            "//std::fill(bins.begin(), bins.end(), 0);\n    for (std::string str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "//omp_set_num_threads(4);\n    #pragma omp parallel for \n    for(int i=0;i<26;i++){\n        char c = (char)('a'+i);\n        bins[i] = std::count_if(s.begin(),s.end(),[c](std::string s){return s.front() == c;});\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        for (int j = 0; j < s.size(); j++) {\n            if (s[j][0] == (char)('a' + i)) {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "// TODO: YOUR CODE HERE\n\n    #pragma omp parallel for\n    for (int i = 0; i < bins.size(); i++) {\n        bins[i] = 0;\n        for (int j = 0; j < s.size(); j++) {\n            if (s[j][0] == 'a' + i) {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "// Fill in starting code\n  //#pragma omp parallel for\n  for (int i = 0; i < s.size(); i++){\n    bins[s[i][0] - 'a'] += 1;\n  }\n}",
            "#pragma omp parallel for\n    for (int i=0; i<bins.size(); i++) {\n        std::string letter = static_cast<char>(i + 'a');\n        for (std::string const& word : s) {\n            if (word.size() > 0 && word[0] == letter[0]) {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        for (int j = 0; j < 26; j++) {\n            if (s[i][0] == 'a' + j) {\n                bins[j]++;\n                break;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < s.size(); ++i) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "for (int i = 0; i < 26; i++)\n\t{\n\t\tbins[i] = 0;\n\t}\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < s.size(); i++)\n\t{\n\t\tchar firstLetter = s[i][0];\n\t\tif (firstLetter >= 'a' && firstLetter <= 'z')\n\t\t{\n\t\t\tbins[firstLetter - 'a']++;\n\t\t}\n\t}\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "serial",
        "prompt": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "for (auto const& i : s) {\n        bins[i[0] - 'a']++;\n    }\n}",
            "for (auto const& word: s) {\n        if (word[0] >= 'a' && word[0] <= 'z') {\n            bins[word[0] - 'a'] += 1;\n        }\n    }\n}",
            "// Write your code here\n    std::array<size_t, 26> counts{};\n    for(auto const& str : s)\n        counts[str[0] - 'a']++;\n    bins = counts;\n}",
            "for(auto & i : s)\n    bins[i[0]-97] += 1;\n}",
            "// TODO\n}",
            "for (const auto& word : s) {\n        bins[word[0] - 'a']++;\n    }\n}",
            "for (int i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "for (auto const& i : s) {\n\t\t++bins[i[0] - 'a'];\n\t}\n}",
            "// TODO\n}",
            "for (auto const& word : s) {\n\t\tbins[word[0] - 'a']++;\n\t}\n}",
            "bins = std::array<size_t, 26>();\n    for (auto const& str : s) {\n        bins[str.front() - 'a'] += 1;\n    }\n}",
            "for (size_t i = 0; i < s.size(); ++i) {\n\t\tsize_t letter_index = (s[i][0] - 'a');\n\t\t++bins[letter_index];\n\t}\n}",
            "for(auto word: s){\n        bins[tolower(word[0]) - 'a'] += 1;\n    }\n}",
            "for (const auto &str : s) {\n        size_t count = 0;\n        for (const auto &c : str) {\n            count += (c >= 'a' && c <= 'z');\n        }\n        bins[count] += 1;\n    }\n}",
            "for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n    for (auto &str : s) {\n        if (str.size() > 0) {\n            bins[str[0]-'a']++;\n        }\n    }\n}",
            "std::array<size_t, 26> a{};\n\n    std::for_each(s.begin(), s.end(), [&](const auto& e) { a[e[0] - 'a']++; });\n    std::copy(a.begin(), a.end(), bins.begin());\n}",
            "for (int i = 0; i < 26; i++) {\n    bins[i] = 0;\n  }\n\n  for (std::string word : s) {\n    // if the first letter is a-z, then add 1 to the corresponding bin in the array\n    if (word[0] >= 'a' && word[0] <= 'z') {\n      bins[word[0] - 'a'] += 1;\n    }\n  }\n}",
            "for(int i = 0; i < s.size(); i++) {\n    bins[s[i].at(0) - 'a']++;\n  }\n}",
            "bins.fill(0);\n    for (size_t i = 0; i < s.size(); i++)\n        bins[s[i][0] - 'a']++;\n}",
            "// your code here\n    std::vector<std::string>::const_iterator it = s.begin();\n    for(int i = 0; i < 26; i++) {\n        std::string temp = \"\";\n        temp += 'a' + i;\n        while(it!= s.end()) {\n            if(it->find(temp) == 0) {\n                bins[i]++;\n                it++;\n            } else {\n                it++;\n            }\n        }\n    }\n}",
            "std::array<size_t, 26> counter;\n    std::fill(counter.begin(), counter.end(), 0);\n    std::fill(bins.begin(), bins.end(), 0);\n    for (auto const& str : s) {\n        ++counter[str[0] - 'a'];\n    }\n    for (size_t i = 0; i < counter.size(); ++i) {\n        bins[i] = counter[i];\n    }\n}",
            "}",
            "for (size_t i = 0; i < s.size(); ++i) {\n        if (s[i][0] >= 97 && s[i][0] <= 122)\n            ++bins[s[i][0] - 97];\n    }\n}",
            "for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "for (size_t i = 0; i < s.size(); i++) {\n    char letter = s[i].front();\n    bins[letter - 'a']++;\n  }\n}",
            "std::string alphabet = \"abcdefghijklmnopqrstuvwxyz\";\n    for (int i = 0; i < alphabet.length(); ++i) {\n        bins[i] = 0;\n    }\n    for (std::string str : s) {\n        char first = str[0];\n        for (int i = 0; i < alphabet.length(); ++i) {\n            if (first == alphabet[i]) {\n                ++bins[i];\n            }\n        }\n    }\n}",
            "std::array<size_t, 26> tmp = {0};\n    for (auto str : s) {\n        tmp[str[0] - 'a']++;\n    }\n    for (int i = 0; i < 26; ++i) {\n        bins[i] = tmp[i];\n    }\n}",
            "for (int i = 0; i < 26; i++) {\n    bins[i] = 0;\n  }\n  for (int i = 0; i < s.size(); i++) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "for (const auto& str : s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "for (const auto& str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "}",
            "}",
            "// your code goes here\n    for (size_t i = 0; i < bins.size(); ++i) {\n        bins[i] = 0;\n    }\n\n    for (const auto & str : s) {\n        if (!str.empty()) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "for (size_t i = 0; i < 26; ++i)\n        for (auto &str : s)\n            if (str.length() > 0 && str[0] == (char)('a' + i))\n                bins[i]++;\n}",
            "for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n    for (std::string str : s) {\n        bins[str[0] - 'a'] += 1;\n    }\n}",
            "for (size_t i = 0; i < s.size(); i++) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "std::vector<std::string> v1;\n    for (auto& s1: s) {\n        v1.push_back(s1[0]);\n    }\n\n    std::sort(v1.begin(), v1.end());\n    std::unique(v1.begin(), v1.end());\n\n    for (auto s1: v1) {\n        bins[s1-'a'] = 0;\n        for (auto s2: s) {\n            if (s2[0] == s1) {\n                bins[s2[0]-'a']++;\n            }\n        }\n    }\n}",
            "for (int i = 0; i < bins.size(); i++) {\n    bins[i] = 0;\n  }\n\n  for (auto & str : s) {\n    bins[str[0] - 'a']++;\n  }\n}",
            "bins = std::array<size_t, 26>();\n    for (auto &i : s)\n        bins[i[0] - 'a']++;\n}",
            "bins.fill(0);\n    for (size_t i = 0; i < s.size(); ++i) {\n        size_t index = s[i][0] - 'a';\n        bins[index]++;\n    }\n}",
            "for (const auto& letter : s) {\n    bins[letter[0] - 'a']++;\n  }\n}",
            "for(int i=0; i<26; i++){\n        bins[i] = 0;\n    }\n    for(std::string str: s){\n        for(char ch: str){\n            if(str[0] >= 'a' && str[0] <= 'z')\n                bins[str[0]-'a']++;\n        }\n    }\n}",
            "for (size_t i = 0; i < s.size(); i++) {\n        int index = s[i][0] - 'a';\n        bins[index]++;\n    }\n}",
            "for(auto const& e : s) {\n        bins[e[0]-'a']++;\n    }\n}",
            "for (std::vector<std::string>::const_iterator itr = s.begin(); itr!= s.end(); ++itr) {\n    std::string letter = *itr;\n    //std::cout << \"letter: \" << letter << std::endl;\n    bins[letter[0] - 'a']++;\n    //std::cout << \"bins: \" << bins[letter[0] - 'a'] << std::endl;\n  }\n\n}",
            "for (size_t i = 0; i < s.size(); i++) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "for (char c = 'a'; c <= 'z'; ++c) {\n    for (std::string const& str: s)\n      if (str[0] == c)\n        ++bins[c - 'a'];\n  }\n}",
            "for(std::string word : s) {\n    if (word.empty()) continue;\n    char first = tolower(word.front());\n    bins[first - 'a']++;\n  }\n}",
            "// TODO: Your code goes here\n    for (int i = 0; i < 26; ++i) {\n        for (auto const& str : s) {\n            if (str[0] == 'a' + i) {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "// Your code goes here\n}",
            "bins.fill(0);\n    for (auto const& s : s) {\n        bins[s.front() - 'a']++;\n    }\n}",
            "for (size_t i = 0; i < 26; ++i) {\n    bins[i] = 0;\n  }\n  for (auto str : s) {\n    ++bins[str[0] - 'a'];\n  }\n}",
            "for(auto const& str : s){\n\t\tbins[str[0]-'a']++;\n\t}\n}",
            "std::array<size_t, 26> bin;\n    bin.fill(0);\n    std::transform(s.begin(), s.end(), bin.begin(),\n                   [](const std::string& s){ return static_cast<size_t>(s[0]); });\n    for (size_t i = 0; i < 26; ++i) {\n        bins[i] = bin[i];\n    }\n}",
            "for (auto x : s) {\n    // count how many elements in s start with the letter x\n    bins[x[0] - 'a']++;\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  for (auto const& element : s) {\n    if (element.length() == 0) {\n      continue;\n    }\n    bins[element[0] - 'a']++;\n  }\n}",
            "for (auto& str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "for (int i = 0; i < 26; ++i) {\n\t\tbins[i] = 0;\n\t}\n\t\n\tfor (int i = 0; i < s.size(); ++i) {\n\t\tif (s[i][0] >= 'a' && s[i][0] <= 'z') {\n\t\t\tbins[s[i][0] - 'a']++;\n\t\t}\n\t}\n\n}",
            "for (auto &i : bins)\n    i = 0;\n  for (auto &s : s) {\n    int c = 0;\n    if (s.size()!= 0) {\n      c = s[0] - 'a';\n      bins[c] += 1;\n    }\n  }\n}",
            "for (const auto& str : s) {\n    if (str.size() > 0) {\n      ++bins[str[0] - 'a'];\n    }\n  }\n}",
            "for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n\n    for (auto s_it = s.begin(); s_it!= s.end(); s_it++) {\n        bins[(*s_it)[0] - 97]++;\n    }\n}",
            "bins = std::array<size_t, 26>{};\n\tfor (auto& elem : s) {\n\t\tchar first_letter = elem[0];\n\t\tbins[first_letter - 97] += 1;\n\t}\n}",
            "std::array<size_t, 26> count{};\n    for (auto &i : s) {\n        count[i.front() - 'a']++;\n    }\n    for (auto &i : count) {\n        bins[i]++;\n    }\n}",
            "for(auto word : s) {\n\t\tif(word.empty()) {\n\t\t\tcontinue;\n\t\t}\n\t\tsize_t index = (word[0] - 'a') % 26;\n\t\tbins[index] += 1;\n\t}\n}",
            "for(auto const& word : s) {\n\t\tchar c = word[0];\n\t\tif(c >= 'a' && c <= 'z') {\n\t\t\tbins[c - 'a']++;\n\t\t}\n\t}\n}",
            "std::string letter;\n    for (auto const& str: s) {\n        letter = str[0];\n        if (letter >= 'a' && letter <= 'z') {\n            bins[letter - 'a'] += 1;\n        }\n    }\n}",
            "for(auto& word : s)\n    {\n        bins[word[0]-97]++;\n    }\n}",
            "for (size_t i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n\n    for (std::string const& str: s) {\n        if (str.empty()) continue;\n        char first = str[0];\n        if (first >= 'a' && first <= 'z') {\n            bins[first - 'a']++;\n        }\n    }\n}",
            "for (std::string word : s) {\n    bins[tolower(word[0]) - 'a']++;\n  }\n}",
            "for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a'] += 1;\n    }\n}",
            "// TODO\n}",
            "for (auto const& e: s) {\n        if (e.empty())\n            continue;\n        bins.at(e.at(0) - 'a')++;\n    }\n}",
            "for (char c = 'a'; c <= 'z'; c++) {\n        size_t count = 0;\n        for (const auto &item : s) {\n            if (item[0] == c) {\n                count++;\n            }\n        }\n        bins[c-'a'] = count;\n    }\n}",
            "size_t i = 0;\n    for(std::string str : s) {\n        if(str.length() == 0) {\n            continue;\n        }\n        bins[str[0]-'a'] += 1;\n    }\n}",
            "size_t count;\n    std::array<int, 26> arr;\n    std::for_each(arr.begin(), arr.end(), [&](int &i) { i = 0; });\n    std::for_each(s.begin(), s.end(), [&](std::string const& str) {\n        count = 0;\n        if (str.length() == 0)\n            return;\n        else {\n            char letter = (char)tolower(str[0]);\n            ++arr[letter - 'a'];\n            return;\n        }\n    });\n    std::transform(arr.begin(), arr.end(), bins.begin(), [](int i) { return static_cast<size_t>(i); });\n}",
            "for (char c = 'a'; c <= 'z'; ++c) {\n        for (auto const& word : s) {\n            if (word[0] == c) {\n                bins[c-'a'] += 1;\n            }\n        }\n    }\n}",
            "bins = {};\n    for (auto &str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "for (auto i = 0; i < 26; ++i) {\n        bins[i] = 0;\n    }\n    for (auto &item : s) {\n        ++bins[item[0] - 97];\n    }\n}",
            "for (std::string word : s) {\n        if (word.length() > 0) {\n            ++bins[word[0] - 'a'];\n        }\n    }\n}",
            "for (size_t i = 0; i < 26; ++i) {\n\t\tbins[i] = 0;\n\t}\n\tfor (auto const& str : s) {\n\t\tif (str.size() < 1) {\n\t\t\tcontinue;\n\t\t}\n\t\tbins[str[0] - 'a']++;\n\t}\n}",
            "for (auto i = 0; i < 26; ++i) {\n        for (auto j = 0; j < s.size(); ++j) {\n            if (s[j][0] == static_cast<char> (i + 97)) {\n                bins[i] += 1;\n            }\n        }\n    }\n}",
            "bins.fill(0);\n    for (auto &str : s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "}",
            "for (auto const& word : s) {\n        int i = word[0] - 'a';\n        bins[i]++;\n    }\n}",
            "for (auto const& i : s) {\n    if (isalpha(i[0])) {\n      bins[tolower(i[0]) - 'a']++;\n    }\n  }\n}",
            "for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n    for (auto &str : s) {\n        if (str.empty()) {\n            continue;\n        }\n        if (str.length() == 1) {\n            continue;\n        }\n        bins[str[0] - 'a']++;\n    }\n}",
            "// TODO: YOUR CODE HERE\n}",
            "for (auto const& str : s) {\n        bins[tolower(str[0]) - 'a']++;\n    }\n}",
            "for (size_t i = 0; i < s.size(); i++) {\n\n        // get the ASCII value of the first letter\n        int ascii = s[i][0];\n        ascii -= 'a';\n\n        // if the first letter is not an alphabet, then ignore it\n        if (ascii < 0 || ascii > 25) {\n            continue;\n        }\n\n        // increment the bin number of the corresponding first letter\n        bins[ascii]++;\n    }\n}",
            "for (auto i = 0; i < s.size(); ++i) {\n        ++bins[s[i][0] - 97];\n    }\n}",
            "for (auto const& word : s) {\n        char first_letter = word[0];\n        if (first_letter >= 'a' && first_letter <= 'z') {\n            bins[first_letter - 'a'] += 1;\n        }\n    }\n}",
            "bins.fill(0);\n    for(auto const& it: s)\n        ++bins[it[0] - 'a'];\n}",
            "std::array<size_t, 26> tmp;\n\tfor (size_t i = 0; i < s.size(); i++) {\n\t\ttmp[s[i][0] - 'a']++;\n\t}\n\tbins = tmp;\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (const auto &word: s) {\n        bins[word[0] - 'a']++;\n    }\n}",
            "for (size_t i = 0; i < 26; ++i)\n        bins[i] = 0;\n\n    for (auto const& str : s)\n        if (str[0] >= 'a' && str[0] <= 'z')\n            ++bins[str[0] - 'a'];\n}",
            "for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "std::array<size_t, 26> result = {0};\n\n    for (const auto& word : s) {\n        result[word[0] - 'a']++;\n    }\n\n    bins = result;\n}",
            "bins = std::array<size_t, 26>({0});\n\tfor (auto const& word : s) {\n\t\tif (isalpha(word[0])) {\n\t\t\tbins[word[0] - 'a'] += 1;\n\t\t}\n\t}\n}",
            "for (int i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "for (size_t i = 0; i < s.size(); ++i)\n        if (!is_alphabet_char(s[i][0])) continue;\n        else bins[s[i][0] - 'a']++;\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    for (const std::string &str : s) {\n        bins[str[0] - 'a'] += 1;\n    }\n}",
            "std::array<size_t, 26> counts;\n    std::fill(counts.begin(), counts.end(), 0);\n\n    for (const auto& word : s) {\n        std::string lowerWord(word);\n        std::transform(lowerWord.begin(), lowerWord.end(), lowerWord.begin(), ::tolower);\n        ++counts[lowerWord[0] - 'a'];\n    }\n\n    std::copy(counts.begin(), counts.end(), bins.begin());\n}",
            "for (size_t i = 0; i < 26; ++i) {\n        bins[i] = 0;\n    }\n    std::string str;\n    for (auto& s_str : s) {\n        str = s_str.substr(0, 1);\n        bins[str[0] - 'a']++;\n    }\n}",
            "bins.fill(0);\n    for (auto const& word : s) {\n        bins[word[0] - 'a'] += 1;\n    }\n}",
            "// TODO\n}",
            "for (int i = 0; i < 26; i++){\n        bins[i] = 0;\n    }\n    for (auto const& str : s){\n        for (auto const& ch : str){\n            bins[ch - 'a']++;\n        }\n    }\n}",
            "// Your code goes here\n}",
            "std::array<size_t, 26> counts {};\n    for (auto const& word : s) {\n        counts[word[0] - 'a']++;\n    }\n    for (size_t i = 0; i < counts.size(); ++i) {\n        bins[i] = counts[i];\n    }\n}",
            "// COMPLETE THIS FUNCTION\n  // REMEMBER TO CHECK THE CASE WHERE THE INPUT STRING IS EMPTY.\n  for (auto i = 'a'; i <= 'z'; i++)\n  {\n    for (auto j = 0; j < s.size(); j++)\n    {\n      if (i == s[j][0])\n        bins[i - 'a']++;\n    }\n  }\n}",
            "// TODO\n\n}",
            "std::for_each(s.begin(), s.end(), [&bins](const std::string& s) {\n    bins[tolower(s[0]) - 'a']++;\n  });\n}",
            "for(auto it = s.begin(); it!= s.end(); it++) {\n    size_t bin = (*it)[0] - 'a';\n    bins[bin]++;\n  }\n}",
            "for (size_t i = 0; i < 26; ++i) {\n      bins[i] = 0;\n   }\n   for (auto const& str : s) {\n      if (!str.empty()) {\n         bins[str[0] - 'a']++;\n      }\n   }\n}",
            "for(auto x : s) {\n        bins[x.at(0) - 'a']++;\n    }\n}",
            "for (auto &i : bins) {\n        i = 0;\n    }\n\n    for (auto &v : s) {\n        char c = v.at(0);\n        if (c >= 'a' && c <= 'z') {\n            bins[c - 'a']++;\n        }\n    }\n}",
            "std::array<size_t, 26> counts = {0};\n  for(std::string const& word: s) {\n    ++counts[word[0] - 'a'];\n  }\n  std::copy(counts.begin(), counts.end(), bins.begin());\n}",
            "for (auto &s_elem : s) {\n        bins[s_elem[0] - 'a']++;\n    }\n}",
            "}",
            "for(auto const& str : s)\n   {\n      auto letter = str[0];\n      if ('a' <= letter && letter <= 'z')\n      {\n         bins[letter - 'a'] += 1;\n      }\n   }\n}",
            "for (size_t i = 0; i < s.size(); i++)\n        bins[s[i].front() - 'a']++;\n}",
            "for (auto const& string : s) {\n    bins[string[0] - 'a']++;\n  }\n}",
            "for (const auto &word : s) {\n    char first_letter = word[0];\n    if (first_letter < 'a' || first_letter > 'z') {\n      continue;\n    }\n    bins[first_letter - 'a']++;\n  }\n}",
            "for (size_t i = 0; i < bins.size(); i++) {\n      bins[i] = std::count_if(s.begin(), s.end(), [i](std::string const& str) {\n            return str[0] == 'a' + i;\n         });\n   }\n}",
            "for (size_t i = 0; i < s.size(); i++) {\n    char first_letter = s[i][0];\n    bins[first_letter - 'a'] += 1;\n  }\n}",
            "// TODO\n}",
            "// Fill in starting code here\n    for(int i = 0; i < 26; i++){\n        for(std::string &str: s){\n            if(str.at(0) == (char)(i + 'a'))\n                bins[i]++;\n        }\n    }\n}",
            "for(size_t i = 0; i < s.size(); i++){\n    if(s[i][0] >= 'a' && s[i][0] <= 'z'){\n      bins[s[i][0] - 'a']++;\n    }\n  }\n}",
            "for (size_t i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n    for (auto str : s) {\n        bins[str[0]-'a']++;\n    }\n}",
            "for (auto &str : s) {\n    bins[str.front() - 'a'] += 1;\n  }\n}",
            "for (auto &v: s) {\n        bins[v[0] - 'a']++;\n    }\n}",
            "// Your implementation here\n    for(int i=0; i < s.size(); i++) {\n        if(s[i][0] >= 97 && s[i][0] <= 122) {\n            int index = s[i][0] - 97;\n            bins[index]++;\n        }\n    }\n}",
            "for (std::string str : s) {\n        size_t index = str[0] - 'a';\n        bins[index]++;\n    }\n}",
            "int counter = 0;\n   for(auto const& s2 : s) {\n      if(s2[0] == 'a' || s2[0] == 'e' || s2[0] == 'i' || s2[0] == 'o' || s2[0] == 'u') {\n         bins[0]++;\n      }\n      if(s2[0] == 'b' || s2[0] == 'c' || s2[0] == 'd' || s2[0] == 'f' || s2[0] == 'g') {\n         bins[1]++;\n      }\n      if(s2[0] == 'h' || s2[0] == 'j' || s2[0] == 'k' || s2[0] == 'l' || s2[0] =='m') {\n         bins[2]++;\n      }\n      if(s2[0] == 'n' || s2[0] == 'p' || s2[0] == 'q' || s2[0] == 'r' || s2[0] =='s') {\n         bins[3]++;\n      }\n      if(s2[0] == 't' || s2[0] == 'v' || s2[0] == 'w' || s2[0] == 'x' || s2[0] == 'z') {\n         bins[4]++;\n      }\n      if(s2[0] == 'y') {\n         bins[5]++;\n      }\n      if(s2[0] == 'u' || s2[0] == 'i') {\n         bins[6]++;\n      }\n      if(s2[0] == 'o' || s2[0] == 'e') {\n         bins[7]++;\n      }\n      if(s2[0] == 'n' || s2[0] == 't') {\n         bins[8]++;\n      }\n      if(s2[0] =='s' || s2[0] == 'r') {\n         bins[9]++;\n      }\n      if(s2[0] == 'd' || s2[0] == 'l') {\n         bins[10]++;\n      }\n      if(s2[0] == 'c' || s2[0] == 'f') {\n         bins[11]++;\n      }\n      if(s2[0] == 'v' || s2[0] == 'g') {\n         bins[12]++;\n      }\n      if(s2[0] == 'j' || s2[0] == 'k') {\n         bins[13]++;\n      }\n      if(s2[0] == 'q' || s2[0] == 'p') {\n         bins[14]++;\n      }\n      if(s2[0] == 'h' || s2[0] == 'w') {\n         bins[15]++;\n      }\n      if(s2[0] =='m' || s2[0] == 'u') {\n         bins[16]++;\n      }\n      if(s2[0] == 'i' || s2[0] == 'e') {\n         bins[17]++;\n      }\n      if(s2[0] == 't' || s2[0] =='s') {\n         bins[18]++;\n      }\n      if(s2[0] == 'o' || s2[0] == 'r') {\n         bins[19]++;\n      }\n      if(s2[0] == 'b' || s2[0] == 'c') {\n         bins[20]++;\n      }\n      if(s2[0] == 'f' || s2[0] == 'd') {\n         bins[21]++;\n      }\n      if(s2[0] == 'x' || s2[0] == 'z') {\n         bins[22]++;\n      }\n      if(s2[0] == 'y') {\n         bins[23]++;\n      }\n      if(s2[0] == 'n') {\n         bins[",
            "for(int i = 0; i < 26; i++)\n  {\n    bins[i] = 0;\n  }\n  for(auto const& str : s)\n  {\n    char first = str[0];\n    int index = first - 'a';\n    bins[index] += 1;\n  }\n}",
            "for(auto& i:s){\n        ++bins[i[0]-'a'];\n    }\n}",
            "for(auto const& e : s) {\n      bins[e[0]-'a']++;\n   }\n}",
            "bins = {0};\n\n    for (auto& word : s) {\n        bins[word.front() - 'a']++;\n    }\n}",
            "bins.fill(0);\n    for (auto const& str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "for (char c = 'a'; c <= 'z'; c++) {\n        bins[c-'a'] = 0;\n    }\n\n    for (auto& str : s) {\n        char firstChar = str[0];\n        if (firstChar > 'z' || firstChar < 'a') {\n            continue;\n        }\n        bins[firstChar-'a']++;\n    }\n}",
            "bins.fill(0);\n   for (const std::string& str : s)\n   {\n      ++bins[tolower(str[0]) - 'a'];\n   }\n}",
            "std::array<size_t, 26> counts = {};\n   for (std::string const& str: s) {\n      auto it = counts.begin();\n      auto const end = counts.end();\n      for (char const& c: str) {\n         // auto const x = it + (c - 'a');\n         // if (x < end) {\n         //    ++*x;\n         // }\n         // else {\n         //    std::cout << \"str = \" << str << std::endl;\n         //    std::cout << \"c = \" << c << std::endl;\n         //    std::cout << \"it = \" << it << std::endl;\n         //    std::cout << \"end = \" << end << std::endl;\n         //    std::cout << \"x = \" << x << std::endl;\n         // }\n         ++it[c - 'a'];\n      }\n   }\n\n   for (size_t i = 0; i < counts.size(); ++i) {\n      bins[i] = counts[i];\n   }\n}",
            "// YOUR CODE HERE\n    for(auto& x : s) {\n        bins[x.at(0) - 'a']++;\n    }\n}",
            "for(int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n\n    for(std::string s_str : s) {\n        int i = s_str[0] - 97;\n        bins[i]++;\n    }\n}",
            "std::array<size_t, 26> b{};\n\n  for (auto i = 0; i < s.size(); i++) {\n    b[s.at(i).at(0)-97]++;\n  }\n\n  for (auto i = 0; i < b.size(); i++) {\n    bins[i] = b[i];\n  }\n}",
            "bins.fill(0);\n    for (std::string const& str : s) {\n        if (str.size() == 0) {\n            continue;\n        }\n        bins[str[0] - 'a'] += 1;\n    }\n}",
            "for (auto &i : bins) {\n    i = 0;\n  }\n\n  for (auto &word : s) {\n    if (word.size() > 0) {\n      ++bins[word.front() - 'a'];\n    }\n  }\n}",
            "// TODO\n    // size_t i, j;\n    // for (i = 0; i < 26; i++) {\n    //     bins[i] = 0;\n    // }\n    // for (i = 0; i < s.size(); i++) {\n    //     for (j = 0; j < 26; j++) {\n    //         if (s[i][0] == 'a' + j) {\n    //             bins[j]++;\n    //         }\n    //     }\n    // }\n    // return;\n\n    for (size_t i = 0; i < 26; i++) {\n        bins[i] = std::count_if(s.begin(), s.end(), [i](const std::string& str) {\n            return str[0] == 'a' + i;\n        });\n    }\n}",
            "for (int i = 0; i < 26; ++i) bins[i] = 0;\n  for (std::string &s : s) {\n    char first = s.front();\n    if ('a' <= first && first <= 'z') {\n      bins[first - 'a']++;\n    }\n  }\n}",
            "for (const auto &str : s) {\n    bins[str[0] - 97]++;\n  }\n}",
            "for(int i=0;i<26;i++){\n        bins[i] = 0;\n    }\n    for(auto word : s){\n        bins[word[0] - 'a'] += 1;\n    }\n}",
            "for (auto const& item : s) {\n    bins[item[0] - 'a']++;\n  }\n}",
            "std::array<size_t, 26> letters;\n    for (auto i = 0; i < 26; ++i)\n        letters[i] = 0;\n    for (auto &i : s)\n        letters[i[0] - 'a']++;\n    for (auto &i : bins)\n        i = letters[i];\n}",
            "std::array<size_t, 26> counts;\n    for (auto const& str : s)\n        ++counts[str[0] - 'a'];\n    bins = counts;\n}",
            "for (std::string const& word : s) {\n        if (word.length() == 0) {\n            continue;\n        }\n        size_t index = tolower(word[0]) - 'a';\n        bins[index]++;\n    }\n}",
            "std::vector<std::string>::const_iterator it;\n\n    for (char a = 'a'; a <= 'z'; a++)\n    {\n        for (it = s.begin(); it!= s.end(); it++)\n        {\n            if (*it[0] == a)\n                bins[a - 'a']++;\n        }\n    }\n}",
            "bins.fill(0);\n  for (auto const& i: s) {\n    bins[i[0] - 'a']++;\n  }\n}",
            "for (auto const& str : s) {\n        bins[str[0]-'a']++;\n    }\n}",
            "for (auto const& str: s)\n\t{\n\t\tbins.at(str.front() - 'a')++;\n\t}\n}",
            "//std::vector<std::string> v;\n    for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n    for (auto const& s1 : s) {\n        bins[s1[0] - 'a']++;\n    }\n}",
            "for (size_t i = 0; i < s.size(); ++i) {\n        bins[s[i][0] - 97] += 1;\n    }\n}",
            "for (char c = 'a'; c <= 'z'; ++c)\n\t{\n\t\tsize_t counter = 0;\n\t\tfor (auto word : s)\n\t\t{\n\t\t\tif (word[0] == c)\n\t\t\t{\n\t\t\t\t++counter;\n\t\t\t}\n\t\t}\n\t\tbins[c - 'a'] = counter;\n\t}\n}",
            "for (auto &word : s) {\n    char firstLetter = word[0];\n    // If it is a lower case letter, convert to an integer and add one to the appropriate bin\n    if (islower(firstLetter))\n      ++bins[toupper(firstLetter) - 'A'];\n  }\n}",
            "// Your code here\n    for(int i = 0; i < 26; ++i){\n        char ch = 'a' + i;\n        bins[i] = std::count_if(s.begin(), s.end(), [ch](std::string &str){ return str.front() == ch; });\n    }\n}",
            "// TODO\n    return;\n}",
            "// TODO\n}",
            "for (size_t i = 0; i < 26; i++) {\n        char firstLetter = 'a' + i;\n        for (auto &word : s) {\n            if (word[0] == firstLetter) {\n                bins[i] += 1;\n            }\n        }\n    }\n}",
            "for (auto const& str : s) {\n        if (str.size() > 0) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "for (int i = 0; i < 26; i++) {\n\t\tbins[i] = 0;\n\t}\n\n\tfor (std::string const& str : s) {\n\t\tif (str.empty()) {\n\t\t\tcontinue;\n\t\t}\n\t\tchar letter = tolower(str[0]);\n\t\tbins[letter - 'a']++;\n\t}\n}",
            "for (size_t i = 0; i < s.size(); ++i) {\n\t\tsize_t letter = s[i][0] - 'a';\n\t\t++bins[letter];\n\t}\n}",
            "for (size_t i = 0; i < s.size(); i++) {\n        if (s[i].empty()) {\n            continue;\n        }\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "for (size_t i = 0; i < bins.size(); ++i) {\n    for (auto &word : s) {\n      if (word[0] == 'a' + i) {\n        ++bins[i];\n      }\n    }\n  }\n}",
            "for (const auto& it : s) {\n    int index = it[0] - 'a';\n    bins[index]++;\n  }\n}",
            "std::unordered_map<char, int> m;\n\n\tfor (std::string str : s) {\n\t\tchar ch = str[0];\n\t\tif (m[ch] == 0) {\n\t\t\tbins[ch - 'a'] += 1;\n\t\t}\n\t}\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (auto const& i: s) {\n    char c = i[0];\n    if (c >= 'a' && c <= 'z')\n      ++bins[c - 'a'];\n  }\n}",
            "for (auto const& str : s) {\n        bins.at(str.at(0) - 'a')++;\n    }\n}",
            "//your code here\n    std::array<size_t, 26> result{};\n    std::for_each(s.begin(), s.end(), [&](const std::string& str) {\n        result[str[0] - 'a'] += 1;\n    });\n\n    for (size_t i = 0; i < result.size(); ++i) {\n        bins[i] = result[i];\n    }\n}",
            "for (auto& elem : s) {\n        if (elem.size() > 0) {\n            ++bins[elem.front() - 'a'];\n        }\n    }\n}",
            "// Your code here\n}",
            "}",
            "// Loop through each string and increment the appropriate count\n    for(auto const& str : s){\n        char firstLetter = str[0];\n        if(isalpha(firstLetter)){\n            bins[firstLetter - 'a']++;\n        }\n    }\n\n}",
            "for (auto const& s1 : s)\n        ++bins[s1[0] - 'a'];\n}",
            "bins.fill(0);\n    for (auto const& word : s) {\n        char c = word[0];\n        bins[c - 'a']++;\n    }\n}",
            "bins.fill(0);\n    for (auto const& str : s) {\n        if (str.size() == 0) continue;\n        ++bins[str[0] - 'a'];\n    }\n}",
            "for (auto const& i : s) {\n    size_t letter_num = 0;\n    for (auto const& j : i) {\n      if (j >= 'a' && j <= 'z') {\n        letter_num = j - 'a';\n        ++bins[letter_num];\n      }\n    }\n  }\n}",
            "// TODO: implement this for loop\n    for (int i = 0; i < 26; i++) {\n        for (int j = 0; j < s.size(); j++) {\n            if (s[j][0] == 'a' + i) {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "}",
            "// TO BE IMPLEMENTED\n}",
            "for (size_t i = 0; i < s.size(); i++) {\n    for (size_t j = 0; j < s[i].size(); j++) {\n      if (isalpha(s[i][j])) {\n        bins[tolower(s[i][j]) - 'a']++;\n      }\n    }\n  }\n}",
            "for (auto const& str : s) {\n    for (size_t i = 0; i < str.size(); i++) {\n      bins[str[i] - 'a']++;\n    }\n  }\n}",
            "// Fill this in.\n}",
            "// your code here\n\n    std::for_each(s.cbegin(), s.cend(),\n                  [&](const auto& word) {\n                      bins[static_cast<unsigned>(word[0] - 'a')]++;\n                  });\n}",
            "for (auto const& str: s) {\n    if (str.size() == 0) {\n      continue;\n    }\n    bins[str[0] - 'a']++;\n  }\n}",
            "}",
            "size_t i = 0;\n    std::array<size_t, 26> freq;\n    std::fill(freq.begin(), freq.end(), 0);\n    for (auto const& word : s) {\n        auto c = word[0] - 'a';\n        ++freq[c];\n    }\n    for (auto const& x : freq) {\n        bins[i++] = x;\n    }\n}",
            "bins.fill(0);\n    for (auto const& str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "for (auto const& word : s) {\n    if (word.empty()) {\n      continue;\n    }\n    char firstLetter = word[0];\n    bins[firstLetter - 'a'] += 1;\n  }\n}",
            "for(size_t i = 0; i < s.size(); ++i) {\n        ++bins[s[i][0] - 'a'];\n    }\n}",
            "for (size_t i = 0; i < s.size(); ++i) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "for(int i = 0; i < s.size(); ++i) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "std::array<size_t, 26> counters{};\n\n    for(auto const& word: s) {\n        if(word.empty()) {\n            continue;\n        }\n        char const first_letter = tolower(word.front());\n        counters[first_letter - 'a']++;\n    }\n\n    for(size_t i = 0; i < counters.size(); ++i) {\n        bins[i] = counters[i];\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "hip",
        "prompt": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\n__global__ void firstLetterCounts(const char **s, size_t N, size_t[26] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "__shared__ char c;\n    __shared__ int count;\n\n    if (threadIdx.x == 0) {\n        c = 'a';\n        count = 0;\n    }\n\n    __syncthreads();\n\n    for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n        if (s[i][0] == c) {\n            count++;\n        }\n    }\n    __syncthreads();\n\n    if (threadIdx.x == 0) {\n        bins[c - 'a'] = count;\n        c++;\n    }\n\n    __syncthreads();\n}",
            "// TODO: Replace this code with your first letter counts kernel\n  int threadId = threadIdx.x;\n  int stride = blockDim.x;\n  for(int i = threadId; i < N; i += stride) {\n    char c = s[i][0];\n    atomicAdd(&bins[c-'a'], 1);\n  }\n\n  // TODO: End first letter counts kernel\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N) {\n        bins[s[idx][0] - 'a'] += 1;\n    }\n}",
            "size_t tid = threadIdx.x;\n    for (size_t i = 0; i < N; i++) {\n        char c = tolower(s[i][tid]);\n        if (c >= 'a' && c <= 'z') {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n    }\n}",
            "//TODO\n}",
            "__shared__ size_t local_counts[26];\n\n    // Assign the threadIdx.x to a letter\n    int letter = threadIdx.x;\n    local_counts[letter] = 0;\n\n    // Loop through all strings\n    for (size_t i = 0; i < N; i++) {\n        // If the first letter of the string matches the letter of the thread, increment the thread's local count\n        if (letter == (s[i][0] - 'a')) {\n            local_counts[letter]++;\n        }\n    }\n\n    // Sum the local counts for each thread\n    size_t total_count = 0;\n    for (int i = 0; i < 26; i++) {\n        total_count += local_counts[i];\n    }\n\n    // Write the sum to the output\n    bins[letter] = total_count;\n}",
            "const size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n  const size_t stride = blockDim.x * gridDim.x;\n  for (size_t i = tid; i < N; i += stride) {\n    bins[s[i][0] - 'a'] += 1;\n  }\n}",
            "int thread_id = blockDim.x * blockIdx.x + threadIdx.x;\n    if (thread_id >= N)\n        return;\n    char first = s[thread_id][0];\n    atomicAdd(&bins[first - 'a'], 1);\n}",
            "const size_t gtid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (gtid < N) {\n        char c = tolower(s[gtid][0]);\n        atomicAdd(&bins[c - 'a'], 1);\n    }\n}",
            "const size_t threadId = blockDim.x*blockIdx.x+threadIdx.x;\n    if (threadId < N) {\n        char c = tolower(s[threadId][0]);\n        atomicAdd(&bins[c-'a'], 1);\n    }\n}",
            "size_t n = threadIdx.x;\n  if (n < N) {\n    int k = 0;\n    char c = s[n][k];\n    while (c!= '\\0' && c >= 'a' && c <= 'z') {\n      bins[c - 'a']++;\n      k++;\n      c = s[n][k];\n    }\n  }\n}",
            "int idx = threadIdx.x;\n  if (idx < 26) {\n    int i;\n    for (i = 0; i < N; i++) {\n      if (s[i][0] == 'a' + idx) bins[idx]++;\n    }\n  }\n}",
            "// Your code here\n}",
            "int i = threadIdx.x;\n    if (i < 26) {\n        bins[i] = 0;\n        for (int k = 0; k < N; k++) {\n            if (s[k][0] == 'a' + i)\n                bins[i]++;\n        }\n    }\n}",
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    if (tid < N) {\n        // TODO\n        char c = tolower(s[bid][tid][0]);\n        bins[c - 'a']++;\n    }\n}",
            "//TODO\n    char letter;\n    int num;\n    for (int i = 0; i < N; i++) {\n        letter = s[i][0];\n        num = 0;\n        for (int j = 0; j < N; j++) {\n            if (s[j][0] == letter) {\n                num += 1;\n            }\n        }\n        bins[letter - 'a'] = num;\n    }\n}",
            "// This function is not implemented.\n}",
            "// TODO\n\n}",
            "// The threadIndex variable determines which index we're working on.\n    // It's a thread number in [0, N-1].\n    size_t threadIndex = threadIdx.x + blockIdx.x * blockDim.x;\n\n    // For all threads in the grid, do the following:\n    if (threadIndex < N) {\n        // a variable to store the first letter of the string s[threadIndex]\n        char firstLetter = s[threadIndex][0];\n        // loop through the alphabet\n        for (char i = 0; i < 26; i++) {\n            // if the first letter of s[threadIndex] matches a letter in the alphabet\n            if (firstLetter == i + 97) {\n                // increment that letter's count in bins by 1\n                bins[threadIndex]++;\n            }\n        }\n    }\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n    if (tid >= N)\n        return;\n    char letter = tolower(s[tid][0]);\n    atomicAdd(&bins[letter], 1);\n}",
            "size_t tid = blockIdx.x*blockDim.x + threadIdx.x;\n  if (tid < N) {\n    for (char ch = 'a'; ch <= 'z'; ++ch) {\n      bins[ch - 'a'] += (s[tid][0] == ch);\n    }\n  }\n}",
            "const size_t offset = threadIdx.x * 26;\n  const size_t stride = blockDim.x * 26;\n\n  // Iterate over the input array\n  for (size_t i = offset + threadIdx.x; i < N; i += stride) {\n    // Count the number of strings that start with each letter\n    const char first = s[i][0];\n    atomicAdd(&bins[first - 'a'], 1);\n  }\n}",
            "// TODO: launch enough threads and implement the kernel\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    char c = s[i][0];\n    bins[c - 'a'] += 1;\n  }\n}",
            "int i = threadIdx.x;\n    int j = blockIdx.x;\n    if (i < 26) {\n        bins[i] = 0;\n        for (int idx = 0; idx < N; ++idx) {\n            if (s[idx][j] == (i + 'a')) {\n                bins[i] += 1;\n            }\n        }\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    for (int i = 0; i < 26; i++) {\n      if (s[tid][0] == 'a' + i) {\n        atomicAdd(&bins[i], 1);\n        break;\n      }\n    }\n  }\n}",
            "// TODO: Implement the function\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if (idx >= N)\n    return;\n  int letter = s[idx][0] - 'a';\n  atomicAdd(&bins[letter], 1);\n}",
            "// TODO\n}",
            "int idx = threadIdx.x;\n    int i = 0;\n    char c = idx;\n    for (i = 0; i < N; i++) {\n        if (s[i][0] == c) {\n            atomicAdd(&bins[idx], 1);\n        }\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        // TODO\n    }\n}",
            "}",
            "int i = threadIdx.x;\n   if (i < N) {\n      // TODO: implement me!\n   }\n}",
            "// TODO\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        char letter = s[i][0];\n        atomicAdd(&bins[letter - 'a'], 1);\n    }\n}",
            "int idx = threadIdx.x;\n    char letter = 'a' + idx;\n    if (idx < 26) {\n        int bin = 0;\n        for (size_t i = 0; i < N; ++i) {\n            if (s[i][0] == letter)\n                bin++;\n        }\n        bins[idx] = bin;\n    }\n}",
            "const size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n   const size_t stride = blockDim.x * gridDim.x;\n   const int alphabet = 'z' - 'a' + 1;\n   for (size_t i = tid; i < N; i += stride) {\n      bins[s[i][0] - 'a'] += 1;\n   }\n}",
            "__shared__ size_t local[26];\n\n  for (int i = 0; i < 26; i++) {\n    local[i] = 0;\n  }\n\n  for (int i = threadIdx.x; i < N; i += blockDim.x) {\n    char c = s[i][0];\n    atomicAdd(&local[c - 'a'], 1);\n  }\n\n  __syncthreads();\n\n  for (int i = threadIdx.x; i < 26; i += blockDim.x) {\n    atomicAdd(&bins[i], local[i]);\n  }\n}",
            "int i = threadIdx.x;\n    int j = 0;\n    int firstLetter = 0;\n    int letterCount = 0;\n    if (i < N) {\n        firstLetter = s[i][0];\n        letterCount = firstLetter - 'a';\n        bins[letterCount]++;\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  int nThreads = blockDim.x * gridDim.x;\n  int offset = blockIdx.x;\n  int idx;\n  char c;\n\n  for (idx = tid; idx < N; idx += nThreads) {\n    c = tolower(s[offset][idx]);\n    bins[c - 'a']++;\n  }\n}",
            "// TODO: implement\n}",
            "// Thread ID\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n  // Loop through the strings in vector s\n  if (tid < N) {\n\n    // Convert the first letter to lower case\n    char letter = tolower(s[tid][0]);\n\n    // Increment the bin that corresponds to the letter\n    atomicAdd(&bins[letter - 'a'], 1);\n\n  }\n}",
            "int threadIndex = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (threadIndex >= N)\n        return;\n\n    char c = tolower(s[threadIndex][0]);\n\n    atomicAdd(&bins[c - 'a'], 1);\n}",
            "size_t id = blockIdx.x * blockDim.x + threadIdx.x;\n    int offset = 'a';\n    if (id < N) {\n        for(int i = 0; i < 26; ++i) {\n            if(s[id][0] == offset) {\n                bins[i]++;\n                break;\n            }\n            offset++;\n        }\n    }\n}",
            "// TODO\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    if (tid < N) {\n        char c = s[bid][0];\n        atomicAdd(&bins[c - 'a'], 1);\n    }\n}",
            "// TODO\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n    while (tid < N) {\n        if (s[tid][0] >= 'a' && s[tid][0] <= 'z') {\n            atomicAdd(&bins[s[tid][0] - 'a'], 1);\n        }\n        tid += blockDim.x * gridDim.x;\n    }\n}",
            "int i = threadIdx.x + blockDim.x * blockIdx.x;\n    if (i < N) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        bins[tolower(s[tid][0]) - 'a']++;\n    }\n}",
            "int idx = threadIdx.x;\n  char ch = 'a' + idx;\n  for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    if (s[i][0] == ch) {\n      atomicAdd(&bins[idx], 1);\n    }\n  }\n}",
            "size_t tid = threadIdx.x;\n    size_t start = tid * N;\n\n    // TODO: Implement\n}",
            "size_t tid = blockDim.x*blockIdx.x + threadIdx.x;\n  char first_letter = 'a';\n  if (tid < N) {\n    first_letter = tolower(s[tid][0]);\n  }\n  __shared__ size_t s_bins[26];\n  s_bins[first_letter - 'a'] = 0;\n  __syncthreads();\n  if (tid < N) {\n    atomicAdd(&s_bins[first_letter - 'a'], 1);\n  }\n  __syncthreads();\n  if (tid == 0) {\n    for (char i = 0; i < 26; ++i) {\n      atomicAdd(&bins[i], s_bins[i]);\n    }\n  }\n}",
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    if (tid >= 0 && tid < 26)\n        for (int i = 0; i < N; ++i)\n            bins[tid] += (s[i][bid] == tid + 'a');\n}",
            "int t_id = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (t_id < N) {\n    bins[s[t_id][0] - 'a']++;\n  }\n}",
            "const size_t offset = blockIdx.x * blockDim.x + threadIdx.x;\n    if (offset >= N) return;\n\n    char c = s[offset][0];\n    c = (c - 'a') >= 0? (c - 'a') : 26;\n    atomicAdd(&bins[c], 1);\n}",
            "// TODO\n}",
            "const size_t i = threadIdx.x;\n    if (i < N) {\n        const size_t c = tolower(s[i][0]);\n        atomicAdd(&bins[c], 1);\n    }\n}",
            "// TODO\n}",
            "//TODO: implement me!\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if(tid < N) {\n        bins[tolower(s[tid][0]) - 'a']++;\n    }\n}",
            "__shared__ size_t buffer[26];\n\n  size_t idx = threadIdx.x;\n  size_t offset = blockIdx.x * blockDim.x;\n\n  if (idx < 26) {\n    buffer[idx] = 0;\n  }\n\n  for (size_t i = idx; i < N; i += blockDim.x) {\n    char c = s[i][0];\n    __syncthreads();\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&buffer[c - 'a'], 1);\n    }\n  }\n\n  __syncthreads();\n\n  for (size_t i = idx; i < 26; i += blockDim.x) {\n    atomicAdd(&bins[i], buffer[i]);\n  }\n}",
            "// Your code here\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n  if (tid >= N)\n    return;\n  size_t idx = s[tid][0] - 'a';\n  atomicAdd(&bins[idx], 1);\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    char c = s[tid][0];\n    bins[c - 'a']++;\n  }\n}",
            "int gid = threadIdx.x + blockIdx.x * blockDim.x;\n    int tid = threadIdx.x;\n\n    for (int i = gid; i < N; i += blockDim.x * gridDim.x) {\n        char ch = s[i][tid];\n        bins[ch - 'a']++;\n    }\n}",
            "// TODO: launch at least N threads\n  for (int i = threadIdx.x; i < N; i += blockDim.x) {\n    // TODO: Compute the counts of the first letters of each string in s\n    for (int j = 0; j < 26; j++) {\n      if (s[i][j] == 97) {\n        atomicAdd(&bins[0], 1);\n        break;\n      } else if (s[i][j] == 98) {\n        atomicAdd(&bins[1], 1);\n        break;\n      } else if (s[i][j] == 99) {\n        atomicAdd(&bins[2], 1);\n        break;\n      } else if (s[i][j] == 100) {\n        atomicAdd(&bins[3], 1);\n        break;\n      } else if (s[i][j] == 101) {\n        atomicAdd(&bins[4], 1);\n        break;\n      } else if (s[i][j] == 102) {\n        atomicAdd(&bins[5], 1);\n        break;\n      } else if (s[i][j] == 103) {\n        atomicAdd(&bins[6], 1);\n        break;\n      } else if (s[i][j] == 104) {\n        atomicAdd(&bins[7], 1);\n        break;\n      } else if (s[i][j] == 105) {\n        atomicAdd(&bins[8], 1);\n        break;\n      } else if (s[i][j] == 106) {\n        atomicAdd(&bins[9], 1);\n        break;\n      } else if (s[i][j] == 107) {\n        atomicAdd(&bins[10], 1);\n        break;\n      } else if (s[i][j] == 108) {\n        atomicAdd(&bins[11], 1);\n        break;\n      } else if (s[i][j] == 109) {\n        atomicAdd(&bins[12], 1);\n        break;\n      } else if (s[i][j] == 110) {\n        atomicAdd(&bins[13], 1);\n        break;\n      } else if (s[i][j] == 111) {\n        atomicAdd(&bins[14], 1);\n        break;\n      } else if (s[i][j] == 112) {\n        atomicAdd(&bins[15], 1);\n        break;\n      } else if (s[i][j] == 113) {\n        atomicAdd(&bins[16], 1);\n        break;\n      } else if (s[i][j] == 114) {\n        atomicAdd(&bins[17], 1);\n        break;\n      } else if (s[i][j] == 115) {\n        atomicAdd(&bins[18], 1);\n        break;\n      } else if (s[i][j] == 116) {\n        atomicAdd(&bins[19], 1);\n        break;\n      } else if (s[i][j] == 117) {\n        atomicAdd(&bins[20], 1);\n        break;\n      } else if (s[i][j] == 118) {\n        atomicAdd(&bins[21], 1);\n        break;\n      } else if (s[i][j] == 119) {\n        atomicAdd(&bins[22], 1);\n        break;\n      } else if (s[i][j] == 120) {\n        atomicAdd(&bins[23], 1);\n        break;\n      } else if (s[i][j] == 121) {\n        atomicAdd(&bins[24], 1);\n        break;\n      } else if (s[i][j] == 122) {\n        atomicAdd(&bins[25], 1);\n        break;\n      } else {",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "// Write your code here\n}",
            "// Initialize all the bins to zero.\n  size_t i;\n  for (i = 0; i < 26; i++) {\n    bins[i] = 0;\n  }\n\n  // Let's start at an arbitrary thread.\n  // Each thread will check whether the first letter of the string is in [a-z] and then increment the corresponding bin.\n  // It will check the entire alphabet because there is no guarantee that the strings are sorted in any order.\n  // To make the example easier, we can use a fixed seed for rand(), so that the results are predictable.\n  const size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  const int seed = 42;\n  srand(seed);\n  for (i = 0; i < N; i += blockDim.x * gridDim.x) {\n    // Let's choose a random thread to do the check and increment.\n    size_t idx = tid + i;\n    if (idx >= N)\n      break;\n    const int letter = s[idx][0] - 'a';\n    if (letter >= 0 && letter < 26) {\n      atomicAdd(&bins[letter], 1);\n    }\n  }\n}",
            "for (int i = threadIdx.x; i < 26; i += blockDim.x)\n    bins[i] = 0;\n\n  for (int i = threadIdx.x; i < N; i += blockDim.x) {\n    int bin = s[i][0] - 'a';\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "const size_t gtid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (gtid < N) {\n    bins[s[gtid][0] - 'a']++;\n  }\n}",
            "size_t offset = threadIdx.x;\n  size_t stride = blockDim.x;\n  for (int i = offset; i < N; i += stride) {\n    bins[(int)s[i][0] - 'a'] += 1;\n  }\n}",
            "size_t idx = threadIdx.x;\n  if (idx >= N)\n    return;\n  char firstLetter = (char)tolower(s[idx][0]);\n  atomicAdd(&bins[firstLetter - 'a'], 1);\n}",
            "// This function is just a wrapper for the firstLetterCountsKernel.\n  // It is the same as in the lab05.\n  firstLetterCountsKernel<<<1, N>>>(s, N, bins);\n}",
            "const char A = 'a';\n  const size_t A_i = 0;\n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    bins[s[i][0] - A] += 1;\n  }\n}",
            "__shared__ char buf[26]; // Use a shared memory array to store 26 letters\n  __shared__ size_t N_per_block;\n\n  // In the first thread of the block, compute the number of strings in the array, and store it in a global memory location.\n  if(threadIdx.x == 0) {\n    N_per_block = N / gridDim.x; // Compute the number of strings in the array per block\n    bins[threadIdx.x] = 0;\n  }\n\n  __syncthreads(); // Wait until all the threads have executed this line\n  for(int i = 0; i < 26; i++) {\n    if(threadIdx.x == i) {\n      buf[threadIdx.x] = i + 'a'; // Store the ith letter in the shared memory array\n    }\n  }\n\n  __syncthreads(); // Wait until all the threads have executed this line\n\n  // Each thread now computes the number of strings that start with the ith letter.\n  // The code below is very similar to what you had for `countLetters` exercise.\n  if(threadIdx.x < N_per_block) {\n    for(int i = 0; i < N_per_block; i++) {\n      char first_letter = s[blockIdx.x * N_per_block + i][0];\n      if(buf[threadIdx.x] == first_letter) {\n        atomicAdd(&bins[threadIdx.x], 1);\n      }\n    }\n  }\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n    if (tid >= N) return;\n    int letter = s[tid][0] - 'a';\n    atomicAdd(&bins[letter], 1);\n}",
            "// Your code here\n}",
            "/* 1 */\n  int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid >= N) return;\n\n  /* 2 */\n  int idx = s[tid][0] - 'a';\n\n  /* 3 */\n  atomicAdd(&bins[idx], 1);\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n  if (tid < N) {\n    for (int i = 0; i < 26; i++) {\n      if (s[tid][0] == 'a' + i) {\n        atomicAdd(&bins[i], 1);\n      }\n    }\n  }\n}",
            "// TODO: Your code here\n}",
            "// TODO: compute the number of strings starting with the first letter of each alphabet\n    //       store the results in bins\n\n    int idx = threadIdx.x;\n    if (idx < N) {\n        bins[s[idx][0] - 'a'] += 1;\n    }\n}",
            "/* Compute thread index. */\n  int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n  /* Each thread handles a specific string. */\n  if (tid < N) {\n\n    /* First letter of each string is stored in `first_letter`. */\n    char first_letter = tolower(s[tid][0]);\n\n    /* Increment the corresponding bin. */\n    atomicAdd(&bins[first_letter - 'a'], 1);\n  }\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n    if (tid < N) {\n        bins[s[tid][0] - 'a']++;\n    }\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (idx < N) {\n    unsigned int bin = 0;\n    char first = s[idx][0];\n    if (first >= 'a' && first <= 'z') {\n      bin = first - 'a';\n    }\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO\n\n}",
            "// TODO: Implement me!\n}",
            "// TODO\n}",
            "int i = threadIdx.x;\n    int threadCount = 32;\n    if (i < 26)\n        bins[i] = 0;\n    __syncthreads();\n    for (int j = 0; j < N; j += threadCount) {\n        if (j + i < N) {\n            bins[s[j][i] - 'a']++;\n        }\n        __syncthreads();\n    }\n}",
            "// thread idx in [0, N)\n    const size_t idx = threadIdx.x;\n    if (idx >= N) return;\n    for (size_t i = 0; i < 26; i++)\n        bins[i] = 0;\n    for (size_t i = 0; i < 26; i++) {\n        bins[s[idx][i] - 'a']++;\n    }\n}",
            "size_t global_id = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (global_id < N) {\n    int bin_index = 0;\n\n    for (char c : s[global_id]) {\n      if (c >= 'a' && c <= 'z') {\n        bin_index = c - 'a';\n        break;\n      }\n    }\n\n    atomicAdd(&bins[bin_index], 1);\n  }\n}",
            "// TODO: Implement using atomic operations.\n   int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n   if (threadId < N) {\n      int letter = s[threadId][0] - 97;\n      atomicAdd(&bins[letter], 1);\n   }\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  for (int i = tid; i < N; i += stride) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "const size_t tid = threadIdx.x;\n\n    if (tid < N) {\n        // TODO: implement\n        // bins[s[tid][0] - 'a']++;\n    }\n}",
            "const char* const myString = s[blockIdx.x * blockDim.x + threadIdx.x];\n    if (myString) {\n        bins[myString[0] - 'a'] += 1;\n    }\n}",
            "int threadIdx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (threadIdx < N) {\n    // TODO\n  }\n}",
            "unsigned int index = threadIdx.x + blockIdx.x * blockDim.x;\n  if (index < N) {\n    unsigned char c = (unsigned char)tolower(s[index][0]);\n    atomicAdd(&bins[c], 1);\n  }\n}",
            "// TODO: implement\n}",
            "// TODO\n}",
            "int i = threadIdx.x;\n    if (i < N) {\n        char c = tolower(s[i][0]);\n        atomicAdd(&bins[c - 'a'], 1);\n    }\n}",
            "// TODO: Implement me!\n}",
            "// Your code goes here\n}",
            "// TODO: Implement me!\n}",
            "__shared__ char currentString[MAX_STRING_LENGTH];\n  __shared__ char currentLetter;\n  __shared__ char currentStringLength;\n  __shared__ size_t startIdx;\n  __shared__ size_t endIdx;\n\n  // Each thread computes the count for one letter\n  if (threadIdx.x < 26) {\n    // Find the start and end indices of the string containing this letter.\n    size_t i = blockIdx.x;\n    char c = 'a' + threadIdx.x;\n    while (i < N) {\n      currentStringLength = strlen(s[i]);\n      currentLetter = s[i][0];\n      if (currentLetter == c) {\n        startIdx = i;\n        endIdx = i;\n        while ((currentLetter == c) && (++i < N)) {\n          currentStringLength = strlen(s[i]);\n          currentLetter = s[i][0];\n        }\n        break;\n      }\n    }\n    // Compute the count\n    bins[threadIdx.x] = 0;\n    if (i < N) {\n      for (size_t j = startIdx; j <= endIdx; ++j) {\n        bins[threadIdx.x] += (s[j][0] == c);\n      }\n    }\n  }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    unsigned char *c = (unsigned char *)s[tid];\n    atomicAdd(&bins[*c], 1);\n  }\n}",
            "int tid = threadIdx.x;\n\n  // Make sure each block processes its own block of input\n  __shared__ size_t blockStart[1024];\n  if (tid < 1024) {\n    blockStart[tid] = (tid * N) / 1024;\n  }\n\n  __syncthreads();\n\n  // Compute the bins for the block. Each thread will compute the first letter of the string starting at `blockStart[tid]`\n  size_t i = blockStart[tid];\n  for (; i < (blockStart[tid] + N / 1024); i++) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "size_t tid = threadIdx.x;\n    if (tid >= N) return;\n\n    size_t offset = 0;\n    size_t count = 0;\n    for (size_t i = 0; i < 26; i++) {\n        char c = (char)('a' + i);\n        if (s[tid][offset] == c) {\n            offset++;\n            count++;\n        }\n        if (s[tid][offset] == 0) {\n            bins[i] = count;\n            count = 0;\n            offset = 0;\n        }\n    }\n}",
            "const int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    const int stride = blockDim.x * gridDim.x;\n    for (int i = tid; i < N; i += stride) {\n        bins[(int)(s[i][0]) - 97]++;\n    }\n}",
            "size_t gtid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (gtid < N) {\n        unsigned int idx = s[gtid][0] - 'a';\n        atomicAdd(&bins[idx], 1);\n    }\n}",
            "int idx = threadIdx.x;\n  if (idx < N) {\n    bins[s[idx][0]-'a']++;\n  }\n}",
            "int i = threadIdx.x;\n  for (int j = 0; j < 26; j++) {\n    bins[j] = 0;\n  }\n  __syncthreads();\n\n  for (int j = 0; j < N; j++) {\n    bins[s[j][i] - 'a']++;\n  }\n}",
            "const int thread = threadIdx.x;\n  const int numThreads = blockDim.x;\n\n  for (size_t i = thread; i < 26; i += numThreads) {\n    bins[i] = 0;\n  }\n\n  __syncthreads();\n\n  for (size_t i = thread; i < N; i += numThreads) {\n    int offset = (s[i][0] - 'a');\n    atomicAdd(&bins[offset], 1);\n  }\n}",
            "// TODO\n}",
            "int tid = threadIdx.x;\n    char c = 'a' + tid;\n    int cnt = 0;\n    for (int i = 0; i < N; ++i) {\n        if (s[i][0] == c) {\n            cnt++;\n        }\n    }\n    bins[tid] = cnt;\n}",
            "size_t blockId = blockIdx.x;\n    size_t threadId = threadIdx.x;\n\n    // Get the thread's work item ID\n    const size_t tid = threadId + blockId * blockDim.x;\n\n    // Threads in the same warp will compute the same bin\n    const size_t warpId = tid >> 5;\n    const size_t warpTid = tid & 31;\n\n    // Threads in the same block will compute the same bin\n    const size_t blockTid = threadIdx.x;\n\n    // Compute the index into the output array\n    const size_t outIndex = blockId * 32 + warpTid;\n\n    // Store the total for each thread\n    size_t counts[26];\n    for (size_t i = 0; i < 26; i++) {\n        counts[i] = 0;\n    }\n\n    // Loop over all strings\n    for (size_t i = 0; i < N; i++) {\n        // Threads compute the bin for the ith string\n        const size_t bin = s[i][0] - 'a';\n\n        // Compute the total in the bin for the ith string\n        atomicAdd(&counts[bin], 1);\n    }\n\n    // Compute the final value for the warp\n    for (size_t i = 0; i < 26; i++) {\n        // Store the total for the warp\n        if (blockTid == i) {\n            bins[outIndex + i] = counts[i];\n        }\n\n        // Wait for the store to complete\n        __syncthreads();\n    }\n}",
            "const int gid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (gid >= N)\n        return;\n    char c = s[gid][0];\n    atomicAdd(&bins[c - 'a'], 1);\n}",
            "size_t i = threadIdx.x;\n    if (i < 26)\n        for (size_t j = 0; j < N; j++)\n            if (s[j][0] - 'a' == i)\n                atomicAdd(&bins[i], 1);\n}",
            "__shared__ size_t sm[26];\n  size_t i = threadIdx.x;\n  size_t j = i / 26;\n  size_t k = i % 26;\n  sm[k] = 0;\n  if (j < N) {\n    while (s[j][i]!= 0) {\n      sm[s[j][i] - 'a'] += 1;\n      i += blockDim.x;\n    }\n  }\n  __syncthreads();\n\n  for (int j = 0; j < 26; ++j) {\n    bins[j] = sm[j];\n  }\n}",
            "int i = threadIdx.x;\n    if (i < N) {\n        char c = tolower(s[i][0]);\n        atomicAdd(&bins[c - 'a'], 1);\n    }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (i < N) {\n    // s[i] is the input string, c is the first character of the string\n    const char *s_i = s[i];\n    int c = s_i[0];\n\n    // The ASCII code for 'a' is 97. Convert c into an index for bins.\n    c -= 97;\n\n    // Increment the corresponding bin in bins by 1.\n    atomicAdd(&bins[c], 1);\n  }\n}",
            "size_t thread_idx = threadIdx.x + blockDim.x * blockIdx.x;\n    size_t warp_idx = thread_idx / warpSize;\n    size_t lane_idx = thread_idx % warpSize;\n\n    // The below code will only work if the number of threads launched is a multiple of the warp size.\n    if (thread_idx < N) {\n        char first_letter = s[thread_idx][0];\n\n        // Using a warp-reduction method for each letter\n        size_t warp_count = __popc(0xAAAAAAAA * (__ballot(first_letter == 'a') >> 1) +\n                                   0xCCCCCCCC * (__ballot(first_letter == 'b') >> 2) +\n                                   0xF0F0F0F0 * (__ballot(first_letter == 'c') >> 4) +\n                                   0xFF00FF00 * (__ballot(first_letter == 'd') >> 8));\n\n        // Synchronize the warp\n        __syncwarp();\n\n        // Add the number of words starting with the current letter to bins\n        atomicAdd(&bins[first_letter - 'a'], warp_count);\n    }\n}",
            "// Allocate local memory for each thread in the block.\n    __shared__ char local_s[MAX_STRING_LENGTH];\n\n    // Get the current thread index\n    int index = threadIdx.x + blockIdx.x * blockDim.x;\n\n    // The number of threads per block is equal to the number of letters in the alphabet.\n    int alphabet_size = 26;\n\n    // Check if the thread index is out of bounds.\n    if (index < N) {\n        // Store the string in the local memory.\n        strcpy(local_s, s[index]);\n\n        // Find the first letter of the string.\n        char first_letter = local_s[0];\n\n        // Check the index of the letter in the alphabet.\n        if (first_letter >= 'a' && first_letter <= 'z') {\n            // The letter index is calculated by subtracting 'a' from the value.\n            // This is because the indices of the letters start with 0 and not 1.\n            int letter_index = first_letter - 'a';\n\n            // Increment the corresponding count of the letter.\n            atomicAdd(&bins[letter_index], 1);\n        }\n    }\n}",
            "// TODO\n}",
            "size_t start = threadIdx.x + blockDim.x * blockIdx.x;\n  for (size_t i = start; i < N; i += blockDim.x * gridDim.x) {\n    if (s[i][0] >= 'a' && s[i][0] <= 'z')\n      atomicAdd(&bins[s[i][0] - 'a'], 1);\n  }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n    // Only the first 26 letters are counted.\n    if (tid < 26) {\n        for (int i = 0; i < N; i++) {\n            // Is the current letter the first letter?\n            if (s[i][0] == 'a' + tid) {\n                bins[tid]++;\n            }\n        }\n    }\n}",
            "// TODO\n}",
            "// 26 bins\n    for (size_t idx = threadIdx.x; idx < 26; idx += blockDim.x) {\n        bins[idx] = 0;\n    }\n\n    __syncthreads();\n\n    for (size_t i = blockIdx.x; i < N; i += gridDim.x) {\n        char c = s[i][0];\n        int idx = (int) c - 97;\n        bins[idx]++;\n    }\n}",
            "// TODO\n}",
            "}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "size_t id = threadIdx.x + blockIdx.x * blockDim.x;\n    if (id >= N) {\n        return;\n    }\n    bins[s[id][0] - 'a']++;\n}",
            "const size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid >= N) return;\n  size_t index = 0;\n  for (index = 0; index < 26; ++index)\n    bins[index] = 0;\n  char str[100];\n  strcpy(str, s[tid]);\n  char first = str[0];\n  if (first >= 'a' && first <= 'z') {\n    index = first - 'a';\n    atomicAdd(&bins[index], 1);\n  }\n  return;\n}",
            "int idx = threadIdx.x + blockIdx.x*blockDim.x;\n    int stride = blockDim.x * gridDim.x;\n    for (int i = idx; i < N; i += stride) {\n        int c = s[i][0] - 'a';\n        atomicAdd(&bins[c], 1);\n    }\n}",
            "// TODO\n\n}",
            "// TODO: YOUR CODE HERE\n    // The first character of each string is at memory location s[i] + 0\n    int i = threadIdx.x;\n    int c = i + 'a';\n\n    if (i < N) {\n        bins[s[i][0]-'a']++;\n    }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx >= N) return;\n  for (int c = 0; c < 26; c++) {\n    if (s[idx][0] == ('a' + c)) bins[c]++;\n  }\n}",
            "// Calculate the thread ID\n  int tid = threadIdx.x + blockDim.x * blockIdx.x;\n  int stride = blockDim.x * gridDim.x;\n\n  // Calculate the starting point of the thread\n  int start = tid * 26;\n  int end = (tid + 1) * 26;\n\n  // For each element in the block, find the first letter in the word,\n  // and increment the corresponding index in the bins array.\n  // The end condition is necessary in case the number of blocks is not a multiple of 26.\n  // If this were not the case, the last block would be left out of the counting process.\n  for (int i = start; i < (end < N * 26? end : N * 26); i += stride) {\n\n    // Extract the first letter of the word\n    char word_first_letter = s[i / 26][i % 26];\n\n    // Increment the bins array at the index of the first letter\n    atomicAdd(&bins[word_first_letter - 'a'], 1);\n\n  }\n\n}",
            "// TODO: implement\n  for(auto i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    char c = s[i][0];\n    atomicAdd(&bins[c - 'a'], 1);\n  }\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (idx < N) {\n    unsigned char first = tolower(s[idx][0]);\n    atomicAdd(&bins[first - 'a'], 1);\n  }\n}",
            "// Get thread ID\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n  // Loop over all letters in the alphabet\n  for (char c = 'a'; c <= 'z'; ++c) {\n\n    // Check if thread ID is less than number of strings\n    if (tid < N) {\n\n      // If current string starts with current letter, increment corresponding bin\n      if (s[tid][0] == c)\n        atomicAdd(&bins[c - 'a'], 1);\n    }\n  }\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N)\n        return;\n\n    int c = s[idx][0] - 'a';\n    atomicAdd(&bins[c], 1);\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n  size_t stride = blockDim.x * gridDim.x;\n\n  for (size_t i = tid; i < N; i += stride) {\n    bins[s[i][0] - 'a'] += 1;\n  }\n}",
            "//TODO\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n    if (tid >= N)\n        return;\n\n    // char c = s[tid][0];\n    int c = (int)(s[tid][0]);\n    atomicAdd(&bins[c], 1);\n}",
            "// TODO: Implement\n}",
            "// TODO: implement\n}",
            "const int tid = threadIdx.x;\n    if (tid < 26) {\n        bins[tid] = 0;\n        for (int i = 0; i < N; i++) {\n            if (s[i][0] >= 'a' && s[i][0] < 'a' + 26) {\n                atomicAdd(&bins[s[i][0] - 'a'], 1);\n            }\n        }\n    }\n}",
            "unsigned int i = threadIdx.x;\n  if (i < 26)\n    for (size_t j = 0; j < N; j++)\n      if (s[j][0] - 'a' == i)\n        atomicAdd(&bins[i], 1);\n}",
            "// allocate shared memory\n  __shared__ char sharedS[26][32];\n\n  // copy first letter to shared memory\n  int idx = threadIdx.x;\n  for (int i = idx; i < N; i += blockDim.x) {\n    sharedS[threadIdx.x][threadIdx.y] = s[i][0];\n  }\n  __syncthreads();\n\n  // compute first letter\n  int threadRow = threadIdx.x;\n  int threadCol = threadIdx.y;\n  char letter = sharedS[threadRow][threadCol];\n\n  // compute thread grid\n  int gridRow = blockIdx.x;\n  int gridCol = blockIdx.y;\n\n  // compute bin\n  int bin = ((gridRow * blockDim.x) + threadRow + 'a') * blockDim.y + gridCol + 'a';\n\n  // count string starting with letter\n  int count = 0;\n  for (int i = idx; i < N; i += blockDim.x) {\n    if (s[i][0] == letter) {\n      count++;\n    }\n  }\n\n  // store count in global memory\n  bins[bin] = count;\n}",
            "const int threadID = blockDim.x * blockIdx.x + threadIdx.x;\n    if (threadID >= N) {\n        return;\n    }\n\n    unsigned char firstLetter = s[threadID][0];\n    atomicAdd(&bins[firstLetter - 'a'], 1);\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if(idx < N) {\n        // Calculate the index into bins for the first letter of the string.\n        char letter = tolower(s[idx][0]);\n        size_t binIdx = letter - 'a';\n        atomicAdd(&bins[binIdx], 1);\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n     int index = (int)s[i][0] - 97;\n     atomicAdd(&bins[index], 1);\n   }\n}",
            "int count = 0;\n    for (int i = 0; i < N; i++) {\n        char firstChar = s[i][0];\n        if (firstChar >= 'a' && firstChar <= 'z') {\n            atomicAdd(&bins[firstChar - 'a'], 1);\n            count++;\n        }\n    }\n}",
            "size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n  if (i < N) {\n    if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n      atomicAdd(&bins[s[i][0] - 'a'], 1);\n    }\n  }\n}",
            "for (int i = threadIdx.x; i < N; i += blockDim.x) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "// TODO\n}",
            "// TODO: Implement!\n}",
            "// TODO: implement\n}",
            "// TODO\n  // The `threadIdx.x` is the thread index in the block\n  // `blockIdx.x` is the block index in the grid\n  // `blockDim.x` is the number of threads in the block\n  // `blockDim.y` is the number of blocks in the grid\n  // `gridDim.x` is the number of blocks in the grid\n\n  // This function runs in parallel on each thread\n  // The first parameter is a pointer to the base of the array to scan\n  // The second parameter is the number of elements in the array\n  // The third parameter is the number of threads in the block\n  int threadId = threadIdx.x;\n  if (threadId < 26) {\n    bins[threadId] = 0;\n  }\n\n  __syncthreads();\n\n  size_t start = blockIdx.x * blockDim.x;\n  size_t end = min(start + blockDim.x, N);\n\n  for (size_t i = start; i < end; ++i) {\n    char firstLetter = tolower(s[i][0]);\n    atomicAdd(&bins[firstLetter], 1);\n  }\n}",
            "// TODO\n    // Assume we have an array of strings in the global memory\n    // Each thread will work on one string\n    // Each thread will increment the corresponding bins[0]...bins[25] counter\n}",
            "// TODO: YOUR CODE GOES HERE\n}",
            "char a = 'a';\n  const size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid >= N) return;\n  char c = s[tid][0];\n  bins[c - a]++;\n}",
            "}",
            "// compute the index of the current thread\n   size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if(tid < N) {\n      const char *word = s[tid];\n      // compute the index of the first letter in the word\n      unsigned int firstLetter = (unsigned int)(*(word)) - 97;\n      // increment the corresponding counter\n      atomicAdd(&bins[firstLetter], 1);\n   }\n}",
            "size_t tid = threadIdx.x;\n  for (char i = 'a'; i <= 'z'; i++) {\n    size_t j = 0;\n    for (size_t k = 0; k < N; k++) {\n      if (s[k][tid] == i) {\n        j++;\n      }\n    }\n    bins[i - 'a'] = j;\n  }\n}",
            "// Your code here.\n  for (size_t gid = threadIdx.x + blockIdx.x * blockDim.x; gid < N; gid += blockDim.x * gridDim.x) {\n    bins[s[gid][0] - 'a']++;\n  }\n}",
            "const char *string = s[blockIdx.x * blockDim.x + threadIdx.x];\n    if (string!= NULL) {\n        bins[string[0] - 'a']++;\n    }\n}",
            "const char* str = s[blockIdx.x * blockDim.x + threadIdx.x];\n\n    size_t bin = 0;\n    if(str[0] >= 'a' && str[0] <= 'z')\n        bin = str[0] - 'a';\n\n    atomicAdd(&bins[bin], 1);\n}",
            "// Use the thread number to count the number of occurrences of the first letter\n  // of the string s[i].\n}",
            "// Use AMD HIP to compute in parallel\n    unsigned int i = threadIdx.x;\n    if (i < N) {\n        // Convert the string to lower case\n        unsigned char* p = (unsigned char*)s[i];\n        while(*p) {\n            if (97 <= *p && *p <= 122)\n                break;\n            ++p;\n        }\n        unsigned int letter = *p - 97;\n        atomicAdd(&bins[letter], 1);\n    }\n}",
            "// Your code here.\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t gid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid >= N) return;\n  bins[s[tid][0] - 'a']++;\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        char letter = tolower(s[idx][0]);\n        if (letter >= 'a' && letter <= 'z') {\n            atomicAdd(&bins[letter - 'a'], 1);\n        }\n    }\n}",
            "size_t my_bins[26] = {};\n  for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n    const size_t c = s[i][0] - 'a';\n    my_bins[c] += 1;\n  }\n  for (size_t i = 0; i < 26; i++) {\n    bins[i] += my_bins[i];\n  }\n}",
            "size_t tid = threadIdx.x;\n    __shared__ size_t shBins[26];\n    __shared__ size_t num;\n    if (tid == 0) {\n        num = 0;\n        for (int i = 0; i < 26; i++)\n            shBins[i] = 0;\n    }\n\n    __syncthreads();\n    for (int i = tid; i < N; i += blockDim.x) {\n        int ch = s[i][0] - 'a';\n        atomicAdd(&shBins[ch], 1);\n    }\n    __syncthreads();\n    for (int i = 0; i < 26; i++)\n        atomicAdd(&bins[i], shBins[i]);\n}",
            "size_t tid = threadIdx.x;\n    size_t idx = blockIdx.x * blockDim.x + tid;\n\n    if (idx < N) {\n        bins[s[idx][0] - 'a']++;\n    }\n}",
            "size_t threadId = blockIdx.x * blockDim.x + threadIdx.x;\n    // Start by looping through the whole array of strings\n    for (size_t i = threadId; i < N; i += blockDim.x * gridDim.x) {\n        // Then for each string, get the first letter and increment its count\n        // Assume first letter is always lowercase.\n        bins[s[i][0] - 'a'] += 1;\n    }\n}",
            "// TODO: YOUR CODE HERE\n    // HINT: you should call the `s[i][0]` to get the first letter of the i-th string\n    // HINT: the character must be converted to a number between 0 and 25\n    // HINT: you must use atomicAdd to update the correct bin\n    // HINT: you should launch at least N threads\n}",
            "// 1) For each string in s, compute the index of the first letter in the alphabet.\n    //    The index is the letter's position in the alphabet.\n    // 2) Use the index to update the bin at that index.\n    //    If the bin does not exist yet, increment the counter by one.\n    //\n    //    (1) is done by calling `index()` on each string and then\n    //    (2) is done by adding to the corresponding bin.\n    //\n    // Hint:\n    //  - Use AMD's intrinsics (HIP) for atomic operations\n    //  - You may need to perform multiple atomic operations, one for each letter.\n    //  - Note that atomicAdd() only takes one argument, a memory address.\n    //  - The address of each element of the `bins` array is given by the address of `bins` plus the index of the array element times the size of each element.\n    //  - The address of an array element can be obtained by adding an offset to the address of the array.\n    //  - The offset of an element is given by the size of each element times the index of the element.\n    //  - Example:\n    //    int a[5];\n    //    int* p = &a[0]; // address of a[0]\n    //    int* q = &a[1]; // address of a[1]\n    //    int* r = &a[2]; // address of a[2]\n    //    r - p = sizeof(int) * 2\n    //    (q - p) / sizeof(int) = 1\n    //    (r - p) / sizeof(int) = 2\n    //    (q - p) / sizeof(int) == 2\n    //    (r - p) / sizeof(int) == 3\n    //    p + 2 == q\n    //    p + 3 == r\n    //  - You can obtain the index of an element in an array by subtracting the address of the first element from the address of the element, divided by the size of each element.\n    //\n    //  - The intrinsic function `index()` in the `firstLetterCounts()` function has the following signature:\n    //      int index(const char *s)\n    //      Return the index of the first letter in the string `s`.\n    //\n    //  - The function `index()` is already provided.\n    //  - The function `index()` returns the index of the first letter in the string `s`.\n    //  - The function `index()` returns a value between 0 and 25.\n    //  - If the string is empty, the function `index()` returns 26.\n    //\n    //  - The kernel function `firstLetterCounts()` has the following signature:\n    //      void firstLetterCounts(const char **s, size_t N, size_t[26] bins)\n    //      The kernel function `firstLetterCounts()` receives the following inputs:\n    //      - `s` is a pointer to an array of pointers to character arrays.\n    //      - `N` is the number of elements in the array.\n    //      - `bins` is an array of size 26.\n    //      The kernel function `firstLetterCounts()` computes the following output:\n    //      - `bins` contains the count of the number of strings in the array `s` that start with each letter of the alphabet.\n    //      Note that the index of a letter in the alphabet is given by the index of the letter in the alphabet, i.e., the index of the letter in the alphabet is the same as the letter itself.\n    //      For example, the index of the letter \"a\" in the alphabet is 0.\n    //      The index of the letter \"b\" in the alphabet is 1.\n    //      The index of the letter \"c\" in the alphabet is 2.\n    //     ...\n    //      The index of the letter \"z\" in the alphabet is 25.\n}",
            "int id = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (id < N) {\n        unsigned int x = (unsigned int)s[id][0];\n        atomicAdd(&bins[x - 'a'], 1);\n    }\n}",
            "// Use an array of 26 bins to count the letter counts.\n    // The 26 bins correspond to the English alphabet.\n    for(int i=0; i<26; i++){\n        bins[i] = 0;\n    }\n\n    for (size_t i=0; i<N; i++) {\n        // Add 1 to the letter count for the first letter of the string in vector s.\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "constexpr size_t B = 26;\n  constexpr size_t T = 32;\n  int c = threadIdx.x % 26;\n  int t = threadIdx.x / 26;\n  __shared__ size_t cache[T];\n  __shared__ int t_c_offset;\n  if (threadIdx.x == 0) {\n    t_c_offset = 0;\n  }\n\n  __syncthreads();\n\n  // TODO\n  for (int i = 0; i < N; i += B * T) {\n    if (i + t * B + t_c_offset < N) {\n      cache[t] = 0;\n      for (int j = 0; j < B; j++) {\n        if (i + t * B + j + t_c_offset < N && s[i + t * B + j + t_c_offset][t_c_offset] == c + 'a') {\n          cache[t]++;\n        }\n      }\n    }\n    __syncthreads();\n    if (i + t * B + t_c_offset < N) {\n      atomicAdd(&bins[c], cache[t]);\n    }\n    __syncthreads();\n    if (t_c_offset + 1 < 26) {\n      t_c_offset += 1;\n    }\n    else {\n      t_c_offset = 0;\n    }\n  }\n}",
            "// TODO\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n    for (int i = index; i < N; i += stride) {\n        int bin = s[i][0] - 'a';\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "const size_t i = threadIdx.x;\n  char c = 'a' + i;\n  for (size_t j = 0; j < N; ++j) {\n    if (s[j][0] == c) {\n      atomicAdd(&bins[i], 1);\n    }\n  }\n}",
            "// TODO: Add your code here.\n\n}",
            "// TODO\n}",
            "int threadIdx = threadIdx.x;\n  int stride = blockDim.x;\n  int i = threadIdx + blockIdx.x * blockDim.x;\n\n  if (i < N) {\n    int letter = s[i][0] - 'a';\n    atomicAdd(&bins[letter], 1);\n  }\n}",
            "// TODO: Your code here\n}",
            "const size_t tid = threadIdx.x;\n    const size_t stride = blockDim.x;\n    const size_t i = blockIdx.x * stride + tid;\n\n    // Use a character map to find the first letter for each word in the list\n    // Use the character to look up the index in the counts array\n    // and add one to that index\n    const char c = s[i][0];\n    const size_t binIndex = (c - 'a');\n    atomicAdd(&bins[binIndex], 1);\n}",
            "/* TODO: YOUR CODE GOES HERE */\n    //HINT: You may find the function 'getFirstLetter()' useful.\n    //HINT: Also, make sure you check whether you are within the bounds of the array\n    for (int i = 0; i < 26; i++)\n        bins[i] = 0;\n    for (size_t i = 0; i < N; i++) {\n        if (getFirstLetter(s[i]) < 26)\n            atomicAdd(&bins[getFirstLetter(s[i])], 1);\n    }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (i < N) {\n        if (s[i][0] >= 65 && s[i][0] <= 90) {\n            bins[s[i][0] - 65]++;\n        }\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid >= N) {\n    return;\n  }\n  bins[s[tid][0] - 'a']++;\n}",
            "// TODO: Your code here\n}",
            "const size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n    const size_t stride = blockDim.x * gridDim.x;\n\n    for (size_t i = tid; i < N; i += stride) {\n        const char first_letter = tolower(s[i][0]);\n        atomicAdd(&bins[first_letter - 'a'], 1);\n    }\n}",
            "int tid = threadIdx.x;\n  char c = s[tid][0];\n  atomicAdd(&bins[c - 'a'], 1);\n}",
            "// TODO\n  //\n  // Hint: You may find the function `firstLetter(const char*)` useful.\n}",
            "size_t i = threadIdx.x;\n    if (i < N) {\n        char c = s[i][0];\n        size_t bin = c - 'a';\n        bins[bin]++;\n    }\n}",
            "// Each thread processes one string\n  // TODO: Your code here\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of s. The result is stored in bins on rank 0.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "//Your code here\n    for (int i = 0; i < s.size(); i++) {\n        //std::cout << s[i][0] << std::endl;\n        bins[s[i][0] - 'a'] += 1;\n        //std::cout << bins[s[i][0] - 'a'] << std::endl;\n    }\n}",
            "std::array<size_t, 26> localBins = {0};\n\n    for (size_t i = 0; i < s.size(); i++)\n    {\n        size_t letter = s[i][0] - 'a';\n        localBins[letter] += 1;\n    }\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Reduce(localBins.data(), bins.data(), 26, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "//TODO: YOUR CODE HERE\n}",
            "int mpi_rank, mpi_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\n    // For each rank, take a part of the vector s, each part of size\n    // ceil(s.size() / mpi_size) and count the letter in the first position.\n    // Result of the counting is stored in bins.\n    size_t local_size = ceil(s.size() / mpi_size);\n    for (int i = 0; i < local_size; i++) {\n        std::string current = s[mpi_rank * local_size + i];\n        if (current[0] >= 'a' && current[0] <= 'z') {\n            bins[current[0] - 'a']++;\n        }\n    }\n\n    // Merge the results from each rank. The result is stored in bins.\n    std::array<size_t, 26> buffer;\n    MPI_Allreduce(MPI_IN_PLACE, buffer.data(), 26, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    for (int i = 0; i < 26; i++) {\n        bins[i] = buffer[i];\n    }\n}",
            "}",
            "// TODO\n}",
            "// TODO: implement here\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    std::vector<std::string> local_s;\n    local_s.assign(s.begin() + rank * size, s.begin() + (rank + 1) * size);\n    std::array<size_t, 26> local_bins;\n    for (size_t i = 0; i < 26; i++)\n        local_bins[i] = 0;\n    for (size_t i = 0; i < local_s.size(); i++)\n        local_bins[local_s[i][0] - 'a']++;\n    std::array<size_t, 26> global_bins;\n    MPI_Allreduce(local_bins.data(), global_bins.data(), 26, MPI_LONG_LONG_INT, MPI_SUM, MPI_COMM_WORLD);\n    bins = global_bins;\n}",
            "}",
            "size_t nprocs, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  size_t len = s.size();\n  std::vector<std::string> split(len);\n  std::vector<std::string> send(len);\n\n  for (size_t i = 0; i < len; i++) {\n    split[i] = s[i].substr(0,1);\n    send[i] = s[i];\n  }\n\n  std::vector<std::string> recv(len);\n\n  size_t n_per_proc = len / nprocs;\n  size_t n_per_proc_plus = (len % nprocs == 0)? n_per_proc : (n_per_proc + 1);\n\n  size_t recv_size = 0;\n  if (rank == 0) {\n    recv_size = nprocs * n_per_proc_plus;\n  }\n\n  std::vector<size_t> recv_sizes(nprocs);\n  MPI_Gather(&n_per_proc_plus, 1, MPI_UNSIGNED_LONG, recv_sizes.data(), 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n  std::vector<size_t> displs(nprocs);\n  displs[0] = 0;\n  for (int i = 1; i < nprocs; i++) {\n    displs[i] = displs[i - 1] + recv_sizes[i - 1];\n  }\n\n  MPI_Gatherv(send.data(), n_per_proc_plus, MPI_BYTE, recv.data(), recv_sizes.data(), displs.data(), MPI_BYTE, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (size_t i = 0; i < len; i++) {\n      char letter = recv[i].substr(0,1).at(0);\n      letter = (letter <= 'z' && letter >= 'a')? (letter - 'a') : 26;\n      bins[letter]++;\n    }\n  }\n}",
            "int n = s.size();\n  int rank = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Status status;\n  int len = 0;\n\n  MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  int number_per_node = n / 4;\n  int rank_start = number_per_node * rank;\n  int rank_end = rank_start + number_per_node;\n  if (rank == 3) {\n    rank_end = n;\n  }\n\n  for (int i = rank_start; i < rank_end; ++i) {\n    len = s[i].size();\n    if (len!= 0) {\n      bins[s[i][0] - 'a']++;\n    }\n  }\n  MPI_Reduce(MPI_IN_PLACE, bins.data(), 26, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "for (auto& word : s) {\n    bins[word[0] - 'a']++;\n  }\n}",
            "for (int i = 0; i < 26; i++) {\n    bins[i] = 0;\n  }\n\n  for (auto it = s.begin(); it!= s.end(); it++) {\n    std::string word = *it;\n    size_t length = word.size();\n    for (int i = 0; i < length; i++) {\n      char c = word[i];\n      if (c >= 'a' && c <= 'z') {\n        int index = c - 'a';\n        bins[index]++;\n      }\n    }\n  }\n}",
            "// TODO:\n  // Your code here\n  // For each letter count the number of strings in s that starts with that letter.\n  // Save the count in the bins array.\n}",
            "size_t ssize = s.size();\n    if (ssize == 0) return;\n    int rank, n_procs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &n_procs);\n\n    int n_procs_per_chunk = ssize / n_procs;\n    int n_procs_remainder = ssize % n_procs;\n    int n_procs_per_chunk_plus_one = n_procs_per_chunk + 1;\n    int n_procs_remainder_plus_one = n_procs_remainder + 1;\n    int proc_offset_start = n_procs_per_chunk_plus_one * rank;\n    int proc_offset_end = proc_offset_start + n_procs_per_chunk;\n    int proc_offset_end_plus_one = proc_offset_end + n_procs_remainder_plus_one;\n\n    for (size_t i = 0; i < 26; ++i)\n    {\n        bins[i] = 0;\n    }\n\n    if (n_procs == 1)\n    {\n        for (size_t i = 0; i < ssize; ++i)\n        {\n            bins[s[i][0] - 'a']++;\n        }\n    }\n    else\n    {\n        if (rank == 0)\n        {\n            std::array<size_t, 26> tmp_bins;\n            for (size_t i = 0; i < n_procs; ++i)\n            {\n                MPI_Status stat;\n                MPI_Recv(&tmp_bins, sizeof(tmp_bins), MPI_BYTE, i, 0, MPI_COMM_WORLD, &stat);\n                for (size_t j = 0; j < 26; ++j)\n                {\n                    bins[j] += tmp_bins[j];\n                }\n            }\n        }\n        else\n        {\n            std::array<size_t, 26> tmp_bins;\n            for (size_t i = 0; i < proc_offset_end_plus_one; ++i)\n            {\n                tmp_bins[s[i][0] - 'a']++;\n            }\n            MPI_Status stat;\n            MPI_Send(&tmp_bins, sizeof(tmp_bins), MPI_BYTE, 0, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    return;\n}",
            "// write your solution here\n}",
            "// TODO: Your code goes here\n    int mpi_rank, mpi_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\n    int local_length = s.size() / mpi_size;\n    int remaining_length = s.size() % mpi_size;\n\n    std::array<size_t, 26> local_bins = {0};\n\n    for (int i = mpi_rank * local_length; i < (mpi_rank * local_length) + local_length + remaining_length; i++) {\n        char first_letter = s[i][0];\n        local_bins[first_letter - 'a']++;\n    }\n\n    if (mpi_rank == 0) {\n        for (int i = 1; i < mpi_size; i++) {\n            std::array<size_t, 26> temp_bins;\n            MPI_Recv(&temp_bins, 26, MPI_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += temp_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(&local_bins, 26, MPI_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int rank;\n  int worldSize;\n  MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  std::array<size_t, 26> bins_local;\n  for (int i = 0; i < 26; i++) {\n    bins_local[i] = 0;\n  }\n\n  std::string first_letter;\n  for (auto &word : s) {\n    first_letter = word[0];\n    first_letter = first_letter - 'a';\n    bins_local[first_letter] += 1;\n  }\n\n  MPI_Allreduce(&bins_local, &bins, 26, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n}",
            "// TODO: complete this function\n\n    // initialize bins array\n    for (size_t i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n\n    // determine the size of each letter\n    size_t size_of_each_letter = s.size() / 26;\n\n    // count the number of strings in the vector s that start with that letter\n    for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a'] += size_of_each_letter;\n    }\n}",
            "MPI_Status status;\n    //get number of processes\n    int n_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n\n    int my_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n    //get chunk size\n    int chunk_size = s.size()/n_ranks;\n\n    //count string start with letter\n    for(size_t i = 0; i < 26; i++){\n        bins[i] = 0;\n    }\n    for(int i = 0; i < chunk_size; i++){\n        bins[s[i][0] - 'a'] += 1;\n    }\n\n    if(my_rank == 0) {\n        //collect data\n        std::vector<size_t> bins_collect(26, 0);\n        std::vector<size_t> bins_displacements(n_ranks, 0);\n\n        for(int i = 0; i < n_ranks; i++){\n            MPI_Send(&bins[0], 26, MPI_INT, i, 0, MPI_COMM_WORLD);\n        }\n\n        for(int i = 1; i < n_ranks; i++){\n            MPI_Recv(&bins_collect[0], 26, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n            for(int j = 0; j < 26; j++){\n                bins[j] += bins_collect[j];\n            }\n        }\n    }else{\n        MPI_Send(&bins[0], 26, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n    //MPI_Barrier(MPI_COMM_WORLD);\n    //MPI_Reduce(&bins[0], &bins_collect[0], 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    //if(my_rank == 0){\n    //    for(int i = 0; i < 26; i++){\n    //        bins[i] = bins_collect[i];\n    //    }\n    //}\n\n}",
            "bins = {};\n    int comm_sz = 1;\n    int rank = 0;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    const size_t chunk_size = s.size() / comm_sz;\n    size_t start_ind = rank * chunk_size;\n    size_t end_ind = (rank + 1) * chunk_size;\n    std::vector<std::string> my_s(s.begin() + start_ind, s.begin() + end_ind);\n\n    std::array<size_t, 26> my_bins{};\n\n    for (auto const& elem : my_s) {\n        my_bins[elem[0] - 'a'] += 1;\n    }\n\n    std::array<size_t, 26> recv_bins{};\n    MPI_Reduce(&my_bins, &recv_bins, 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = recv_bins;\n    }\n}",
            "// TODO: Your code goes here\n\n    int size;\n    int rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<std::string> local_s(s.begin() + rank * s.size() / size, s.begin() + (rank + 1) * s.size() / size);\n\n    for (size_t i = 0; i < local_s.size(); i++) {\n        char first = local_s[i][0];\n        bins[first - 'a']++;\n    }\n\n    for (size_t i = 0; i < 26; i++) {\n        MPI_Reduce(&bins[i], &bins[i], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "// your code here\n}",
            "const int n_ranks = MPI_Comm_size(MPI_COMM_WORLD);\n    const int rank_id = MPI_Comm_rank(MPI_COMM_WORLD);\n    const int strings_per_rank = s.size()/n_ranks;\n\n    if (rank_id == 0) {\n        std::vector<int> rank_counts(n_ranks);\n\n        for (int r = 0; r < n_ranks; r++) {\n            std::vector<std::string> s_r(s.begin() + r * strings_per_rank, s.begin() + (r + 1) * strings_per_rank);\n\n            std::array<size_t, 26> bins_r;\n            firstLetterCounts(s_r, bins_r);\n\n            MPI_Gather(&bins_r, 26, MPI_LONG_LONG, rank_counts.data(), 26, MPI_LONG_LONG, 0, MPI_COMM_WORLD);\n        }\n\n        for (int r = 0; r < n_ranks; r++) {\n            bins[r] += rank_counts[r];\n        }\n\n    } else {\n        std::array<size_t, 26> bins_r;\n        firstLetterCounts(s, bins_r);\n        MPI_Gather(&bins_r, 26, MPI_LONG_LONG, nullptr, 26, MPI_LONG_LONG, 0, MPI_COMM_WORLD);\n    }\n}",
            "assert(false); // TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // determine how many strings each process gets\n  int count = s.size() / size;\n  // determine how many strings each process gets from the last process\n  count += s.size() % size;\n\n  // determine which strings each process gets\n  std::vector<std::string> myStrings;\n  if (rank == 0) {\n    // first rank takes the last strings\n    myStrings = std::vector<std::string>(s.end() - count, s.end());\n  } else {\n    // all other ranks take strings beginning from the start\n    myStrings = std::vector<std::string>(s.begin(), s.begin() + count);\n  }\n\n  // loop through all the strings\n  for (std::string const& str : myStrings) {\n    // get the first letter of the string\n    char firstLetter = str[0];\n    // if the first letter is not alphabetical, skip\n    if (firstLetter < 'a' || firstLetter > 'z') {\n      continue;\n    }\n    // the first letter is in the alphabet, increment the bin\n    bins[firstLetter - 'a']++;\n  }\n\n}",
            "std::array<size_t, 26> bins_local = {};\n\n    // count each string\n    for(auto const& word : s) {\n        bins_local[word[0] - 'a']++;\n    }\n\n    // combine into final result\n    size_t offset = 0;\n    for (int i = 0; i < MPI_COMM_WORLD.size(); ++i) {\n        MPI_Send(&bins_local[0], offset, MPI_INT, i, 0, MPI_COMM_WORLD);\n        offset += bins_local.size();\n    }\n\n    std::array<size_t, 26> bins_final = {};\n    size_t total = 0;\n    for (int i = 0; i < MPI_COMM_WORLD.size(); ++i) {\n        MPI_Status status;\n        MPI_Recv(&bins_final[0], bins_final.size(), MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n        for(auto const& count : bins_final) {\n            total += count;\n        }\n    }\n\n    if (MPI_COMM_WORLD.rank() == 0) {\n        for (int i = 0; i < bins_local.size(); ++i) {\n            bins[i] = bins_local[i] + bins_final[i];\n        }\n        bins[bins.size() - 1] = total;\n    }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n  int rank, numProcs;\n  MPI_Comm_rank(comm, &rank);\n  MPI_Comm_size(comm, &numProcs);\n\n  int elementsPerProc = s.size() / numProcs;\n  int remainder = s.size() % numProcs;\n\n  std::vector<std::string> localS;\n  if (rank < remainder) {\n    localS.assign(s.begin() + (elementsPerProc + 1) * rank, s.begin() + (elementsPerProc + 1) * (rank + 1));\n  } else {\n    localS.assign(s.begin() + (elementsPerProc + 1) * rank + remainder, s.begin() + (elementsPerProc + 1) * (rank + 1) + remainder);\n  }\n\n  size_t* localCounts = new size_t[26];\n  for (auto const& str : localS) {\n    localCounts[str[0] - 'a'] += 1;\n  }\n  int globalCounts[26];\n  MPI_Allreduce(localCounts, globalCounts, 26, MPI_INT, MPI_SUM, comm);\n\n  for (int i = 0; i < 26; i++) {\n    bins[i] = globalCounts[i];\n  }\n\n  delete[] localCounts;\n  return;\n}",
            "for (auto const& str : s) {\n        bins[str.front() - 'a'] += 1;\n    }\n}",
            "size_t myCount = 0;\n    for (auto &i: s) {\n        if (islower(i[0]))\n            ++myCount;\n    }\n    int rank, numProcs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n\n    bins.fill(0);\n\n    std::vector<std::string> myStrings;\n    for (auto &i: s) {\n        if (islower(i[0]))\n            myStrings.push_back(i);\n    }\n\n    int numPerRank = myStrings.size() / numProcs;\n    int extra = myStrings.size() % numProcs;\n\n    std::vector<std::string> myStrings2;\n    if (rank < extra) {\n        myStrings2 = myStrings.begin() + rank * (numPerRank + 1);\n    } else {\n        myStrings2 = myStrings.begin() + (rank - extra) * numPerRank;\n    }\n\n    myStrings2 = std::vector<std::string>(myStrings2.begin(), myStrings2.begin() + numPerRank + 1);\n\n    std::vector<size_t> myCounts;\n    myCounts.resize(26);\n    for (auto &i: myStrings2) {\n        if (i.length() > 0)\n            ++myCounts[i[0] - 97];\n    }\n\n    int source = rank - 1;\n    int destination = rank + 1;\n    MPI_Status status;\n\n    if (destination < numProcs) {\n        MPI_Send(&myCounts[0], 26, MPI_LONG_LONG_INT, destination, 0, MPI_COMM_WORLD);\n    }\n\n    if (source >= 0) {\n        MPI_Recv(&myCounts[0], 26, MPI_LONG_LONG_INT, source, 0, MPI_COMM_WORLD, &status);\n    }\n\n    for (int i = 0; i < 26; ++i) {\n        bins[i] += myCounts[i];\n    }\n}",
            "if (s.empty())\n        return;\n\n    int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    std::vector<std::string> localS;\n\n    if (rank == 0) {\n        for (int i = 0; i < size; ++i) {\n            MPI_Status status;\n            MPI_Recv(localS.data(), s.size(), MPI_CHAR, i, 0, MPI_COMM_WORLD, &status);\n        }\n    } else {\n        MPI_Send(s.data(), s.size(), MPI_CHAR, 0, 0, MPI_COMM_WORLD);\n    }\n\n    for (const std::string& word : localS) {\n        if (word[0] < 'a' || word[0] > 'z')\n            continue;\n        ++bins[word[0] - 'a'];\n    }\n\n    if (rank == 0) {\n        for (size_t& bin : bins)\n            std::cout << bin <<'';\n        std::cout << std::endl;\n    }\n}",
            "}",
            "int rank;\n    int size;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::array<size_t, 26> local_bins = {0};\n\n    int block_size = (int)s.size() / size;\n    int remainder = (int)s.size() % size;\n\n    if (rank < remainder) {\n        std::vector<std::string> local_strings(block_size + 1);\n        for (int i = 0; i < block_size + 1; i++) {\n            local_strings[i] = s[rank * block_size + i];\n        }\n        for (auto &string : local_strings) {\n            if (!string.empty()) {\n                local_bins[string[0] - 'a']++;\n            }\n        }\n    }\n    else {\n        std::vector<std::string> local_strings(block_size);\n        for (int i = 0; i < block_size; i++) {\n            local_strings[i] = s[rank * block_size + i];\n        }\n        for (auto &string : local_strings) {\n            if (!string.empty()) {\n                local_bins[string[0] - 'a']++;\n            }\n        }\n    }\n\n    MPI_Allreduce(local_bins.data(), bins.data(), 26, MPI_LONG_LONG_INT, MPI_SUM, MPI_COMM_WORLD);\n}",
            "for(int i = 0; i < bins.size(); i++) {\n\t\tbins[i] = 0;\n\t}\n\tfor(std::string& word : s) {\n\t\tsize_t firstLetter = word.front() - 'a';\n\t\tbins[firstLetter]++;\n\t}\n}",
            "// TODO\n}",
            "for(size_t i = 0; i < s.size(); ++i) {\n        int c = s[i][0];\n        if (c < 'a' || c > 'z') {\n            throw \"error\";\n        }\n        bins[c-'a'] += 1;\n    }\n}",
            "// TODO: Your code here\n}",
            "}",
            "// TODO\n}",
            "std::string x;\n  for (int i = 0; i < s.size(); i++) {\n    x = s[i];\n    x = x.substr(0,1);\n    x = std::tolower(x[0]);\n    bins[x[0]-'a']++;\n  }\n}",
            "// TODO\n}",
            "for (size_t i=0; i<s.size(); i++) {\n        int b = s[i][0]-'a';\n        if (b>=0 && b<26) bins[b]++;\n    }\n}",
            "// TODO: Your code here\n}",
            "std::array<size_t, 26> bins0;\n\tfor (int i = 0; i < 26; i++) bins0[i] = 0;\n\n\tfor (int i = 0; i < s.size(); i++) {\n\t\tbins0[s[i][0] - 'a']++;\n\t}\n\n\tMPI_Allreduce(MPI_IN_PLACE, bins0.data(), 26, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\tfor (int i = 0; i < 26; i++) bins[i] = bins0[i];\n\n}",
            "bins = {};\n    for (int i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "size_t my_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    // TODO\n}",
            "MPI_Datatype stringType = MPI_BYTE;\n  MPI_Type_contiguous(s[0].length(), MPI_CHAR, &stringType);\n  MPI_Type_commit(&stringType);\n\n  int world_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  size_t stride = s.size() / world_size;\n  int n = world_size - 1;\n  if (world_rank == n) {\n    stride += s.size() % world_size;\n  }\n\n  // First find out how many strings start with each letter.\n  // Then use that to fill up the bins array.\n  for (size_t i = world_rank * stride; i < (world_rank + 1) * stride; i++) {\n    if (i < s.size()) {\n      char firstLetter = s[i][0];\n      MPI_Allreduce(&firstLetter, &bins[firstLetter - 97], 1, MPI_CHAR, MPI_SUM, MPI_COMM_WORLD);\n    }\n  }\n\n  MPI_Type_free(&stringType);\n}",
            "// Fill this in!\n  // Use MPI to count the first letter of each word\n  // Assume a complete copy of s is stored on every rank\n  // The output is stored in bins on rank 0\n  std::vector<size_t> counts(26);\n  std::vector<int> ranks(s.size());\n  MPI_Comm_rank(MPI_COMM_WORLD, &ranks[0]);\n  for (int i = 0; i < s.size(); ++i) {\n    counts[s[i][0] - 'a']++;\n  }\n  for (int i = 0; i < 26; ++i) {\n    bins[i] = 0;\n  }\n  MPI_Reduce(counts.data(), bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "//std::vector<std::string> s = {\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"};\n  //std::array<size_t, 26> bins;\n\n  for(int i = 0; i < 26; i++){\n    bins[i] = 0;\n  }\n\n  //size_t n = s.size();\n\n  //for(int i = 0; i < n; i++){\n  //  if(s[i].length() == 0){\n  //    continue;\n  //  }\n  //  char firstLetter = s[i].at(0);\n  //  if(firstLetter >= 'a' && firstLetter <= 'z'){\n  //    int index = (int)firstLetter - 97;\n  //    bins[index] += 1;\n  //  }\n  //}\n\n  for(std::string word : s){\n    if(word.length() == 0){\n      continue;\n    }\n    char firstLetter = word.at(0);\n    if(firstLetter >= 'a' && firstLetter <= 'z'){\n      int index = (int)firstLetter - 97;\n      bins[index] += 1;\n    }\n  }\n\n}",
            "/*\n     * Initialize the bin array to zero.\n     */\n    for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n\n    /*\n     * Count the number of elements in the vector for each letter.\n     */\n    for (int i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "// TODO: your code here\n    //\n    //\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    std::vector<std::string> s_rank(s.begin() + rank, s.begin() + rank + size);\n\n    // create a histogram\n    std::array<size_t, 26> histogram{};\n    for (const auto &x : s_rank) {\n        histogram[x[0] - 'a'] += 1;\n    }\n\n    // sum the histograms\n    std::vector<size_t> recvbuf(26);\n    MPI_Allreduce(histogram.data(), recvbuf.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    for (int i = 0; i < 26; ++i) {\n        bins[i] = recvbuf[i];\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t start = rank * s.size() / size;\n  size_t end = (rank + 1) * s.size() / size;\n  size_t n = 0;\n  for (int i = start; i < end; i++) {\n    if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n      bins[s[i][0] - 'a']++;\n      n++;\n    }\n  }\n\n  if (n!= 0) {\n    MPI_Reduce(bins.data(), bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n}",
            "int world_size;\n\tMPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\tint world_rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n\tstd::vector<int> message_counts;\n\tfor (int i = 0; i < 26; i++)\n\t\tmessage_counts.push_back(0);\n\tfor (int i = 0; i < s.size(); i++) {\n\t\tif (s[i].length() > 0)\n\t\t\tmessage_counts[s[i][0] - 'a']++;\n\t}\n\n\tstd::vector<int> received_counts(world_size, 0);\n\tMPI_Allgather(message_counts.data(), 26, MPI_INT, received_counts.data(), 26, MPI_INT, MPI_COMM_WORLD);\n\n\tfor (int i = 0; i < 26; i++)\n\t\tbins[i] = 0;\n\tfor (int i = 0; i < world_size; i++) {\n\t\tif (i == world_rank) {\n\t\t\tfor (int j = 0; j < 26; j++)\n\t\t\t\tbins[j] += message_counts[j];\n\t\t}\n\t\telse {\n\t\t\tfor (int j = 0; j < 26; j++)\n\t\t\t\tbins[j] += received_counts[i];\n\t\t}\n\t}\n}",
            "// TODO: fill out this function\n}",
            "int nproc, myrank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n  //TODO: Your code here\n\n}",
            "int size = s.size();\n    int rank, nprocs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    int block_size = size/nprocs;\n    int block_start = rank*block_size;\n    int block_end = block_start+block_size;\n    std::array<size_t, 26> bins_loc;\n    if (rank == 0) {\n        std::fill(bins.begin(), bins.end(), 0);\n    }\n\n    for (int i = block_start; i < block_end; i++) {\n        char first_letter = s[i][0];\n        bins_loc[first_letter-'a']++;\n    }\n\n    MPI_Gather(bins_loc.data(), 26, MPI_UNSIGNED_LONG_LONG, bins.data(), 26, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    std::vector<std::string> local_strings = s;\n\n    size_t local_strings_per_rank = local_strings.size() / world_size;\n\n    std::vector<std::string> local_strings_left = local_strings;\n    size_t local_strings_left_size = local_strings_left.size();\n\n    for (size_t i = 0; i < local_strings_per_rank; i++) {\n        local_strings_left.pop_back();\n    }\n    int local_rank = world_size - 1;\n    std::string local_string;\n    while (local_rank >= 0) {\n        if (local_rank < world_rank) {\n            MPI_Send(&local_string, 1, MPI_CHAR, local_rank, 0, MPI_COMM_WORLD);\n        } else {\n            MPI_Recv(&local_string, 1, MPI_CHAR, local_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            if (local_string.size() > 0) {\n                local_strings_left.push_back(local_string);\n                local_strings_left_size = local_strings_left.size();\n            }\n        }\n        local_rank--;\n    }\n\n    for (int i = world_rank; i < world_size; i++) {\n        MPI_Send(&local_string, 1, MPI_CHAR, i, 0, MPI_COMM_WORLD);\n    }\n\n    int current_rank = world_rank;\n    size_t current_string_size = 0;\n    std::string current_string;\n    while (current_rank < world_size) {\n        if (current_rank == world_rank) {\n            if (local_strings_left_size > 0) {\n                current_string = local_strings_left.back();\n                current_string_size = current_string.size();\n                local_strings_left_size--;\n            }\n        }\n        MPI_Recv(&current_string, 1, MPI_CHAR, current_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        if (current_string_size > 0) {\n            bins[current_string[0] - 'a']++;\n            current_string_size = 0;\n        }\n        current_rank++;\n    }\n}",
            "int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    std::vector<std::string> local_s = s;\n    std::vector<int> counts(26);\n\n    for (auto& word : local_s) {\n        ++counts[word[0] - 'a'];\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, counts.data(), 26, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    std::transform(counts.begin(), counts.end(), bins.begin(), [](int x) { return static_cast<size_t>(x); });\n}",
            "size_t total_size = s.size();\n  // if total_size is a multiple of 4, 6, 8, 10,...\n  // then every rank has 4, 6, 8, 10... elements\n  // otherwise, every rank has total_size/p elements\n  size_t size_per_rank = (total_size / MPI_COMM_WORLD.Get_size()) + 1;\n  size_t start = size_per_rank * MPI_COMM_WORLD.Get_rank();\n  size_t end = std::min(total_size, start + size_per_rank);\n\n  // the first string starts with 'a', which is the first letter in the alphabet\n  // count the number of strings that start with 'a'\n  for(int i=start; i<end; ++i)\n  {\n    bins[s[i][0]-'a']++;\n  }\n}",
            "}",
            "MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n  MPI_Comm_rank(MPI_COMM_WORLD, &worldRank);\n  bins.fill(0);\n\n  if (s.size() == 0) {\n    return;\n  }\n\n  std::vector<std::string> substrings;\n  size_t start, end;\n  start = worldRank * s.size() / worldSize;\n  end = start + (s.size() / worldSize);\n  for (size_t i = start; i < end; i++) {\n    substrings.push_back(s.at(i));\n  }\n\n  // send all the substrings to rank 0.\n  if (worldRank!= 0) {\n    MPI_Send(&substrings.at(0), substrings.size(), MPI_BYTE, 0, 0, MPI_COMM_WORLD);\n  }\n\n  // receive all the substrings from rank 0.\n  if (worldRank == 0) {\n    MPI_Status status;\n    for (int i = 1; i < worldSize; i++) {\n      int substringsSize = 0;\n      MPI_Recv(&substringsSize, 1, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n      std::vector<std::string> substringsRecv(substringsSize);\n      MPI_Recv(&substringsRecv.at(0), substringsSize, MPI_BYTE, i, 0, MPI_COMM_WORLD, &status);\n      substrings.insert(substrings.end(), substringsRecv.begin(), substringsRecv.end());\n    }\n  }\n\n  for (size_t i = 0; i < substrings.size(); i++) {\n    bins.at(substrings.at(i).at(0) - 'a')++;\n  }\n\n  // Send the counts from rank 0 to all the other ranks.\n  if (worldRank == 0) {\n    for (int i = 1; i < worldSize; i++) {\n      MPI_Send(&bins.at(0), bins.size(), MPI_INT, i, 0, MPI_COMM_WORLD);\n    }\n  }\n\n  // receive the counts from all the other ranks.\n  if (worldRank!= 0) {\n    MPI_Status status;\n    MPI_Recv(&bins.at(0), bins.size(), MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n  }\n}",
            "for(auto& str: s)\n    {\n        bins[str[0] - 97] += 1;\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int max = s.size() / size;\n    int left_overs = s.size() % size;\n    int local_size = 0;\n    int offset = 0;\n\n    std::vector<std::string> local_s;\n\n    if (rank == 0) {\n        for (int i = 0; i < left_overs; i++) {\n            local_s.push_back(s[i]);\n            local_size++;\n        }\n        offset = left_overs;\n    }\n\n    for (int i = offset; i < offset + max; i++) {\n        local_s.push_back(s[i]);\n        local_size++;\n    }\n\n    std::array<size_t, 26> local_bins;\n    for (auto &i: local_s) {\n        local_bins[i[0] - 'a']++;\n    }\n\n    MPI_Allreduce(local_bins.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "std::array<size_t, 26> counts;\n    for(const auto& w: s) {\n        counts[w[0]-'a'] += 1;\n    }\n\n    std::array<size_t, 26> sendCounts;\n    for(int i = 0; i < 26; i++){\n        sendCounts[i] = counts[i];\n    }\n\n    std::array<size_t, 26> recvCounts;\n\n    MPI_Alltoall(&sendCounts, 1, MPI_INT, &recvCounts, 1, MPI_INT, MPI_COMM_WORLD);\n\n    std::array<size_t, 26> displacements = {0};\n    displacements[0] = 0;\n    for(int i = 0; i < 25; i++){\n        displacements[i+1] = displacements[i] + recvCounts[i];\n    }\n    std::vector<size_t> recvBins(displacements[25]);\n    std::vector<size_t> sendBins(displacements[25]);\n\n    MPI_Alltoallv(&sendCounts, &recvCounts, &displacements, MPI_INT, &recvBins, &recvCounts, &displacements, MPI_INT, MPI_COMM_WORLD);\n\n    for(int i = 0; i < s.size(); i++){\n        sendBins[i] = counts[s[i][0]-'a'];\n    }\n    MPI_Alltoallv(&sendBins, &recvCounts, &displacements, MPI_INT, &recvBins, &recvCounts, &displacements, MPI_INT, MPI_COMM_WORLD);\n    for(int i = 0; i < 26; i++){\n        bins[i] = 0;\n        for(int j = 0; j < counts[i]; j++){\n            bins[i] += recvBins[j];\n        }\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int n_per_rank = s.size() / size;\n    int rem = s.size() % size;\n    int start = rank * n_per_rank + std::min(rank, rem);\n    int end = start + n_per_rank + (rank >= rem? 0 : 1);\n    std::array<size_t, 26> local_bins = {};\n    std::vector<std::string> local_s = s;\n    std::vector<std::string>::iterator it_s;\n    for (int i = start; i < end; i++) {\n        it_s = local_s.begin() + i;\n        local_bins[(*it_s)[0] - 'a']++;\n    }\n    std::vector<size_t> bins_all(26);\n    MPI_Allreduce(local_bins.data(), bins_all.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n    for (int i = 0; i < 26; i++) {\n        bins[i] = bins_all[i];\n    }\n}",
            "//YOUR CODE HERE\n\n}",
            "MPI_Status status;\n\tint world_size;\n\tMPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\tint world_rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\tint remainder = s.size() % world_size;\n\tint blocksize = s.size() / world_size;\n\tif (world_rank == 0) {\n\t\tfor (size_t i = 0; i < 26; i++) {\n\t\t\tbins[i] = 0;\n\t\t}\n\t}\n\tMPI_Bcast(&bins, 26, MPI_LONG_LONG, 0, MPI_COMM_WORLD);\n\tint start = world_rank * blocksize;\n\tint end = blocksize;\n\tif (world_rank == world_size - 1) {\n\t\tend = s.size();\n\t}\n\telse if (world_rank < remainder) {\n\t\tend += blocksize + 1;\n\t}\n\telse {\n\t\tend += blocksize;\n\t}\n\n\tfor (int i = start; i < end; i++) {\n\t\tstd::string str = s[i];\n\t\tchar str1 = str[0];\n\t\tint rank = int(str1 - 'a');\n\t\tMPI_Send(&rank, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n\t\tMPI_Recv(&bins, 26, MPI_LONG_LONG, 0, 0, MPI_COMM_WORLD, &status);\n\t}\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int n = s.size();\n  int nn = (int) ceil(1.0*n/size);\n  int extra = n%size;\n  int start = rank*nn + std::min(rank, extra);\n  int count = std::min(nn, n-start);\n  if (rank == 0) {\n    for (int i=0; i<26; i++)\n      bins[i] = 0;\n  }\n  MPI_Barrier(MPI_COMM_WORLD);\n  for (int i=0; i<count; i++) {\n    char c = s[start+i][0];\n    bins[c-'a'] += 1;\n  }\n  MPI_Barrier(MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i=1; i<size; i++) {\n      MPI_Recv(&bins[0], 26, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&bins[0], 26, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "const size_t rank = MPI::COMM_WORLD.Get_rank();\n  const size_t size = MPI::COMM_WORLD.Get_size();\n\n  std::vector<std::string> localS = s;\n  std::vector<std::string> v;\n  std::array<size_t, 26> localBins;\n\n  for(int i = 0; i < 26; i++)\n    localBins[i] = 0;\n\n  for(int i = 0; i < localS.size(); i++) {\n    localBins[localS[i][0] - 'a']++;\n  }\n\n  MPI::COMM_WORLD.Allreduce(&localBins, &bins, 26, MPI::UNSIGNED, MPI::SUM);\n  return;\n}",
            "std::array<size_t, 26> local_bins = {0};\n    std::array<size_t, 26> global_bins = {0};\n\n    // TODO\n\n    MPI_Allreduce(local_bins.data(), global_bins.data(), 26, MPI_UNSIGNED, MPI_SUM, MPI_COMM_WORLD);\n    for (int i = 0; i < 26; i++) {\n        bins[i] = global_bins[i];\n    }\n}",
            "// Your code here\n    MPI_Status status;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int num_strings = s.size();\n    int num_strings_per_process = (num_strings + size - 1) / size;\n\n    std::fill(bins.begin(), bins.end(), 0);\n    for (int i = 0; i < num_strings; i++) {\n        auto c = s.at(i).at(0);\n        bins[c - 'a']++;\n    }\n    // std::fill(bins.begin(), bins.end(), 0);\n\n    std::vector<std::array<size_t, 26>> bins_copy;\n    for (int i = 0; i < size; i++) {\n        std::array<size_t, 26> tmp;\n        if (i!= rank) {\n            MPI_Recv(&tmp, 26, MPI_UNSIGNED_LONG_LONG, i, 1, MPI_COMM_WORLD, &status);\n        }\n        bins_copy.push_back(tmp);\n    }\n\n    for (auto &a : bins_copy) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] += a[i];\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&bins, 26, MPI_UNSIGNED_LONG_LONG, i, 1, MPI_COMM_WORLD);\n        }\n    }\n}",
            "size_t n = s.size();\n    size_t m = n / 2;\n    size_t k = 26;\n\n    bins = {0};\n\n    // first find out how many processes there are\n    int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    // compute the number of bins\n    int local_bins = (m + 1) / world_size;\n\n    // compute the offset of my first bin\n    int offset = local_bins * world_rank;\n\n    // compute the end of my bin\n    int end = offset + local_bins;\n    if (end > m)\n        end = m;\n\n    // compute the number of strings to process\n    int strings_per_rank = end - offset;\n\n    // compute the offset of my first string\n    int string_offset = strings_per_rank * world_rank;\n\n    // loop over the strings\n    for (int i = string_offset; i < string_offset + strings_per_rank; i++) {\n        // compute the index in the string\n        size_t idx = 0;\n\n        // iterate over the chars in the string\n        for (size_t j = 0; j < s[i].size(); j++) {\n            // compute the index of the char in the alphabet\n            size_t idx_char = s[i][j] - 'a';\n\n            // if I am at the end of the string\n            if (j == s[i].size() - 1) {\n                // increment the corresponding bin\n                bins[idx_char]++;\n            }\n            // if I am at the beginning of the string\n            else if (j == 0) {\n                // add it to the beginning of the bins array\n                bins[idx_char]++;\n            }\n            // if I am in the middle of the string\n            else {\n                // check if the previous char matches the new char\n                if (s[i][j - 1] == s[i][j])\n                    // add it to the beginning of the bins array\n                    bins[idx_char]++;\n            }\n        }\n    }\n\n    // loop over the bins\n    for (int j = 1; j < k; j++) {\n        // compute the offset of the bin\n        int offset = j * world_size;\n\n        // compute the end of the bin\n        int end = offset + world_size;\n        if (end > k)\n            end = k;\n\n        // compute the number of strings to process\n        int strings_per_rank = end - offset;\n\n        // compute the offset of my first string\n        int string_offset = strings_per_rank * world_rank;\n\n        // loop over the strings\n        for (int i = string_offset; i < string_offset + strings_per_rank; i++) {\n            // if the offset is not in the bins\n            if (i >= strings_per_rank)\n                continue;\n            // increment the corresponding bin\n            bins[i] += bins[i - 1];\n        }\n    }\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int length = s.size() / size;\n    int remainder = s.size() % size;\n    // std::array<std::vector<std::string>, 26> arr;\n    int start = rank * length;\n    int end = start + length;\n    // if (rank == size - 1)\n    //     end = end + remainder;\n    for (int i = 0; i < 26; i++)\n        bins[i] = 0;\n    // for (int i = start; i < end; i++) {\n    //     char c = s[i][0];\n    //     if (c >= 'a' && c <= 'z')\n    //         bins[c - 'a']++;\n    // }\n    for (int i = start; i < end; i++) {\n        if (i < s.size()) {\n            char c = s[i][0];\n            if (c >= 'a' && c <= 'z')\n                bins[c - 'a']++;\n        }\n    }\n    MPI_Allreduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n    // for (int i = 0; i < 26; i++) {\n    //     arr[i] = std::vector<std::string>(bins[i]);\n    // }\n    // return arr;\n}",
            "int mpi_size;\n    int mpi_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    // split the work\n    std::array<size_t, 26> local_bins;\n    local_bins.fill(0);\n    std::vector<std::string> local_s;\n\n    int split = s.size() / mpi_size;\n    if (mpi_rank == 0) {\n        local_s = std::vector<std::string>(s.begin(), s.begin() + split);\n    }\n    else {\n        local_s = std::vector<std::string>(s.begin() + (split * mpi_rank),\n                s.begin() + (split * (mpi_rank + 1)));\n    }\n\n    // count the number of strings in each local string starts with 'a' - 'z'\n    for (auto const& i : local_s) {\n        local_bins[i[0] - 'a']++;\n    }\n\n    // MPI_Allreduce()\n    MPI_Allreduce(local_bins.data(), bins.data(), 26, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    if (mpi_rank!= 0) {\n        local_bins.fill(0);\n    }\n}",
            "return;\n}",
            "std::array<int, 26> counts{0};\n    for (auto const& str : s) {\n        ++counts[str[0] - 'a'];\n    }\n    for (size_t i = 0; i < counts.size(); ++i) {\n        bins[i] = counts[i];\n    }\n    MPI_Reduce(&bins[0], &bins[0], bins.size(), MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "size_t const N = s.size();\n\n    // if rank 0\n    if(world_rank == 0) {\n        for(int i = 0; i < 26; i++) {\n            for(size_t j = 0; j < N; j++) {\n                if(s[j][0] == 'a' + i) {\n                    bins[i]++;\n                }\n            }\n        }\n    }\n    // if not rank 0\n    else {\n        for(size_t j = 0; j < N; j++) {\n            if(s[j][0] == 'a' + world_rank) {\n                bins[world_rank]++;\n            }\n        }\n    }\n}",
            "// TODO\n\n}",
            "// TODO: implement this!\n    // Use MPI to send the 1st letter to the correct rank.\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    // Receive the first letter from each rank.\n    char letter;\n    MPI_Recv(&letter, 1, MPI_CHAR, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    // Update the number of words starting with the letter.\n    if (letter >= 'a' && letter <= 'z') {\n        bins[letter - 'a'] += 1;\n    }\n}",
            "std::vector<std::string> s1;\n    for(auto const& ss: s) {\n        if (ss.length() > 0) {\n            s1.push_back(ss);\n        }\n    }\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] = 0;\n        }\n    }\n\n    // split the workload evenly\n    int N = s1.size() / size;\n    int rem = s1.size() % size;\n    int start = rank * N;\n\n    if (rank == 0) {\n        start = 0;\n    }\n\n    if (rank < rem) {\n        N++;\n    }\n\n    // workload for each process\n    int end = start + N - 1;\n\n    for (int i = start; i < end; i++) {\n        std::string word = s1[i];\n        int c = word[0] - 'a';\n        MPI_Reduce(&c, &bins[c], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n\n    // last element\n    if (rank == rem) {\n        end = s1.size() - 1;\n        for (int i = start; i <= end; i++) {\n            std::string word = s1[i];\n            int c = word[0] - 'a';\n            MPI_Reduce(&c, &bins[c], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            if (bins[i] == 0) {\n                bins[i] = -1;\n            }\n        }\n    }\n}",
            "// TODO\n    int n_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &n_rank);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // first rank to process the data\n    if (rank == 0) {\n        for (int i = 0; i < s.size(); i++) {\n            std::string s_cur = s.at(i);\n            char cur_first = s_cur.at(0);\n            if (cur_first >= 'a' && cur_first <= 'z') {\n                bins[cur_first - 'a']++;\n            }\n        }\n    }\n\n    std::array<size_t, 26> bins_local = bins;\n    std::array<size_t, 26> bins_local_count;\n    std::array<size_t, 26> bins_all;\n\n    // every rank to get the local count of each letter and send it to rank 0\n    for (int i = 0; i < bins.size(); i++) {\n        if (rank == 0) {\n            bins_all[i] = bins[i];\n        } else {\n            if (rank == 1) {\n                MPI_Send(&bins_local[i], 1, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n            } else {\n                MPI_Recv(&bins_local_count[i], 1, MPI_UNSIGNED_LONG_LONG, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n                bins_all[i] = bins[i] + bins_local_count[i];\n                MPI_Send(&bins_all[i], 1, MPI_UNSIGNED_LONG_LONG, 1, 0, MPI_COMM_WORLD);\n            }\n        }\n    }\n\n    // rank 0 to get all the results from every rank\n    if (rank == 0) {\n        for (int i = 2; i < n_rank; i++) {\n            MPI_Recv(&bins_local_count, 26, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins_all[j] += bins_local_count[j];\n            }\n        }\n    }\n\n    // rank 0 to send all the results to the global\n    if (rank == 0) {\n        MPI_Send(&bins_all, 26, MPI_UNSIGNED_LONG_LONG, 1, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Recv(&bins_all, 26, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            std::cout << bins_all[i] << \" \";\n        }\n        std::cout << std::endl;\n    }\n}",
            "for (auto const& str : s) {\n    bins[str[0] - 'a']++;\n  }\n}",
            "std::array<size_t, 26> local_bins = {};\n    for (const auto& str : s) {\n        if (!str.empty()) {\n            local_bins[str.at(0) - 'a']++;\n        }\n    }\n    MPI_Reduce(local_bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: your code goes here\n}",
            "// TODO:\n}",
            "// TODO\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (rank == 0) {\n\t\tbins.fill(0);\n\t}\n\n\tMPI_Bcast(&bins, 26, MPI_LONG_LONG_INT, 0, MPI_COMM_WORLD);\n\n\tstd::vector<std::string> s_local = s;\n\n\tint count;\n\tint reminder = s_local.size() % size;\n\tcount = s_local.size() / size;\n\tcount += (rank < reminder? 1 : 0);\n\n\tfor (int i = 0; i < count; i++) {\n\t\tif (s_local[i].length() > 0) {\n\t\t\tbins[(int)s_local[i].front() - 97] += 1;\n\t\t}\n\t}\n\n\tstd::vector<size_t> bins_local;\n\tbins_local.resize(26);\n\tbins_local.assign(bins.begin(), bins.end());\n\n\tMPI_Reduce(&bins_local[0], &bins[0], 26, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\treturn;\n}",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<std::string> part_s = std::vector<std::string>(s.begin() + rank * s.size()/size, s.begin() + (rank+1) * s.size()/size);\n    //std::cout << part_s.size() << std::endl;\n\n    //for (auto str : part_s)\n        //std::cout << str << std::endl;\n\n    std::array<size_t, 26> local_bins = std::array<size_t, 26>();\n\n    for (auto str : part_s)\n        local_bins[str[0]-'a']++;\n\n    //std::cout << \"1\" << std::endl;\n    MPI_Reduce(local_bins.data(), bins.data(), 26, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n    //std::cout << \"2\" << std::endl;\n}",
            "assert(s.size() > 0);\n\n    size_t num_letters = s[0].length();\n\n    assert(s.size() > 0);\n\n    size_t rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // If the string's size is less than a char (1), then no letters can be counted\n    if (num_letters <= 1) {\n        return;\n    }\n\n    // Split the tasks\n    int remainder = s.size() % size;\n    size_t chunkSize = s.size() / size;\n    size_t myStart = rank * chunkSize;\n    size_t myEnd = myStart + chunkSize - 1;\n    if (rank < remainder) {\n        myEnd++;\n    }\n\n    // Go through all the strings in the vector and add the count to the proper bin\n    for (size_t i = myStart; i < myEnd; i++) {\n        if (s[i].length() > 0) {\n            bins[(int)s[i][0] - 97]++;\n        }\n    }\n}",
            "size_t num_procs;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    size_t my_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n    std::array<size_t, 26> local_bins{};\n    for (auto const& word : s) {\n        local_bins[word[0] - 'a']++;\n    }\n\n    std::array<size_t, 26> global_bins{};\n    if (my_rank == 0) {\n        MPI_Allreduce(&local_bins[0], &global_bins[0], 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n    } else {\n        MPI_Reduce(&local_bins[0], &global_bins[0], 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n\n    if (my_rank == 0) {\n        bins = global_bins;\n    }\n}",
            "int world_rank, world_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    for (auto& str : s){\n        auto first_letter = str[0] - 'a';\n        bins[first_letter]++;\n    }\n}",
            "auto numRanks = MPI_Comm_size(MPI_COMM_WORLD);\n  auto myRank = MPI_Comm_rank(MPI_COMM_WORLD);\n  std::array<std::vector<std::string>, numRanks> stringSlices;\n  auto elementsPerRank = s.size() / numRanks;\n  auto leftoverElements = s.size() % numRanks;\n  for (auto i = 0u; i < numRanks; i++) {\n    auto start = i * elementsPerRank;\n    auto end = start + elementsPerRank;\n    if (i < leftoverElements) {\n      end += 1;\n    }\n    stringSlices[i] = std::vector<std::string>(s.begin() + start, s.begin() + end);\n  }\n\n  // Calculate each rank's first letter counts\n  for (auto& slice : stringSlices) {\n    std::array<size_t, 26> counts;\n    for (auto& str : slice) {\n      auto firstChar = str[0];\n      counts[firstChar - 'a'] += 1;\n    }\n    // Scatter counts to other ranks\n    for (auto i = 0u; i < 26; i++) {\n      MPI_Send(&counts[i], 1, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD);\n    }\n  }\n\n  // Gather first letter counts to rank 0\n  if (myRank == 0) {\n    std::fill(bins.begin(), bins.end(), 0);\n  } else {\n    for (auto i = 0u; i < 26; i++) {\n      MPI_Recv(&bins[i], 1, MPI_UNSIGNED_LONG_LONG, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  }\n}",
            "int rank, nproc;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    std::array<size_t, 26> bins_local{};\n    for (const auto& word : s) {\n        bins_local[word[0] - 'a']++;\n    }\n    MPI_Allreduce(bins_local.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "//TODO: Your code here\n}",
            "std::vector<int> rank(26);\n    std::fill(rank.begin(), rank.end(), -1);\n\n    // For each rank, the starting position of the corresponding alphabet.\n    // The starting position of the alphabet A should be 0, and for B it should be 26, and so on.\n    int alphabet = 0;\n    for (int i = 0; i < 26; i++) {\n        if (rank[i] == -1) {\n            rank[i] = alphabet;\n            alphabet += (int) s.size() / 26 + ((s.size() % 26) > 0);\n        }\n    }\n\n    int nrank, rankid;\n    MPI_Comm_size(MPI_COMM_WORLD, &nrank);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rankid);\n\n    // Store the bins for a specific alphabet on a specific rank.\n    std::array<size_t, 26> bins_rank;\n    std::fill(bins_rank.begin(), bins_rank.end(), 0);\n\n    // Sort the strings in s to determine the starting position of each alphabet.\n    // This is done by using the prefix sum.\n    std::vector<std::string> s_temp = s;\n    std::sort(s_temp.begin(), s_temp.end());\n    std::vector<int> prefixSum(s_temp.size());\n    for (int i = 0; i < s_temp.size(); i++) {\n        int index = std::distance(s_temp.begin(), std::upper_bound(s_temp.begin(), s_temp.end(), s_temp[i]));\n        prefixSum[i] = index;\n    }\n\n    // Compute the bins for the alphabet of the current rank.\n    for (int i = rank[rankid]; i < rank[rankid + 1]; i++) {\n        bins_rank[s[i].front() - 'a'] += 1;\n    }\n\n    // Send the bins of the current rank to the corresponding rank, using MPI_Sendrecv.\n    std::array<size_t, 26> bins_temp;\n    std::fill(bins_temp.begin(), bins_temp.end(), 0);\n    for (int i = 0; i < 26; i++) {\n        if (rankid!= 0 && rank[i] < rank[i + 1]) {\n            MPI_Send(&bins_rank[i], 1, MPI_LONG_LONG_INT, 0, i, MPI_COMM_WORLD);\n            MPI_Recv(&bins_temp[i], 1, MPI_LONG_LONG_INT, 0, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        else if (rankid == 0) {\n            MPI_Recv(&bins_temp[i], 1, MPI_LONG_LONG_INT, 0, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Send(&bins_rank[i], 1, MPI_LONG_LONG_INT, 0, i, MPI_COMM_WORLD);\n        }\n    }\n    // Store the bins on rank 0.\n    if (rankid == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] = bins_temp[i];\n        }\n    }\n\n    // Compute the bins for the alphabet of the current rank.\n    for (int i = rank[rankid]; i < rank[rankid + 1]; i++) {\n        bins_rank[s[i].front() - 'a'] += bins_temp[s[i].front() - 'a'];\n    }\n\n    // Send the bins of the current rank to the corresponding rank, using MPI_Sendrecv.\n    std::fill(bins_temp.begin(), bins_temp.end(), 0);\n    for (int i = 0; i < 26; i++) {\n        if",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int binSize = 26 / size;\n  int extraBins = 26 % size;\n\n  std::array<size_t, 26> binsCopy;\n  std::fill(binsCopy.begin(), binsCopy.end(), 0);\n\n  int offset = 0;\n  for (int i = 0; i < s.size(); i++) {\n    binsCopy[s[i][0] - 'a']++;\n  }\n\n  for (int i = 0; i < extraBins; i++) {\n    bins[offset + i] = binsCopy[i];\n  }\n\n  for (int i = 0; i < binSize; i++) {\n    for (int j = 0; j < size; j++) {\n      if (j == rank) {\n        continue;\n      }\n      int offsetJ = 0;\n      if (i < extraBins) {\n        offsetJ = j;\n      } else {\n        offsetJ = j + extraBins;\n      }\n      bins[offset + offsetJ] += binsCopy[i + extraBins + offsetJ];\n    }\n  }\n\n  // for (int i = 0; i < 26; i++) {\n  //   printf(\"%lu, \", bins[i]);\n  // }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  if (rank!= 0) {\n    MPI_Send(bins.data(), 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  } else {\n    std::array<size_t, 26> binsPartial;\n    std::fill(binsPartial.begin(), binsPartial.end(), 0);\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(binsPartial.data(), 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 26; j++) {\n        bins[j] += binsPartial[j];\n      }\n    }\n  }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "const int mpi_size = MPI_Comm_size(MPI_COMM_WORLD);\n\tconst int mpi_rank = MPI_Comm_rank(MPI_COMM_WORLD);\n\n\tstd::array<size_t, 26> local_bins;\n\tlocal_bins.fill(0);\n\n\tfor (auto &str : s) {\n\t\tlocal_bins[str[0] - 'a']++;\n\t}\n\n\tstd::array<size_t, 26> tmp;\n\ttmp.fill(0);\n\n\tMPI_Reduce(local_bins.data(), tmp.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tif (mpi_rank == 0) {\n\t\tbins = tmp;\n\t}\n}",
            "for (size_t i = 0; i < 26; i++) {\n    bins[i] = 0;\n  }\n\n  for (std::string const& word : s) {\n    bins[word[0] - 'a']++;\n  }\n}",
            "// TODO\n}",
            "// Your code goes here\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, size;\n    MPI_Comm_size(comm, &size);\n    MPI_Comm_rank(comm, &rank);\n\n    // Get the chunk of the vector\n    size_t chunk_size = s.size() / size;\n    size_t local_start = rank * chunk_size;\n    size_t local_end = local_start + chunk_size;\n\n    // Initialize bins to 0\n    std::fill(bins.begin(), bins.end(), 0);\n    if (rank == 0) {\n        std::fill(bins.begin(), bins.end(), 0);\n    }\n\n    // Count local strings\n    for (size_t i = local_start; i < local_end; i++) {\n        bins[s[i][0] - 'a']++;\n    }\n\n    // Gather bins from each rank\n    std::vector<size_t> recv_bins(26);\n    MPI_Allgather(&bins[0], 26, MPI_INT, &recv_bins[0], 26, MPI_INT, comm);\n\n    // Update bins on rank 0\n    if (rank == 0) {\n        for (int i = 0; i < size; i++) {\n            for (int j = 0; j < 26; j++) {\n                bins[j] += recv_bins[26*i + j];\n            }\n        }\n    }\n}",
            "// Your code here\n}",
            "size_t size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<std::string> s_copy = s;\n    if(rank!= 0) {\n        MPI_Bcast(&(s_copy[0]), s.size(), MPI_CHAR, 0, MPI_COMM_WORLD);\n    }\n\n    std::array<size_t, 26> counts{};\n\n    for (auto const& str: s_copy) {\n        ++counts[str[0] - 'a'];\n    }\n\n    for (size_t i = 0; i < 26; ++i) {\n        bins[i] += counts[i];\n    }\n}",
            "// The number of ranks\n    int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // The size of the input vector s\n    int size_of_s = s.size();\n\n    // The length of each input string\n    int length_of_string = s[0].size();\n\n    // An array to hold the number of strings that start with each letter\n    std::array<size_t, 26> count_of_letter;\n\n    // Initialize the array to zero\n    std::fill(count_of_letter.begin(), count_of_letter.end(), 0);\n\n    // If this process has any work to do, do it\n    if (size_of_s > 0) {\n\n        // The number of strings to process on this process\n        int num_of_strings = size_of_s / size;\n        if (rank < size_of_s % size) {\n            num_of_strings++;\n        }\n\n        // The number of strings to process on each process\n        int local_num_of_strings = num_of_strings;\n\n        // The starting index of strings to process on this process\n        int start_index = rank * num_of_strings;\n\n        // The ending index of strings to process on this process\n        int end_index = (rank + 1) * num_of_strings;\n\n        // The loop to count the number of strings that start with each letter\n        for (int i = start_index; i < end_index; i++) {\n            if (i < size_of_s) {\n                if (s[i].size() > 0) {\n                    count_of_letter[s[i][0] - 'a']++;\n                }\n            }\n        }\n\n        // Send the number of strings that start with each letter to rank 0\n        if (rank == 0) {\n            for (int i = 1; i < size; i++) {\n                MPI_Recv(count_of_letter.data(), 26, MPI_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            }\n        }\n\n        // Receive the number of strings that start with each letter from rank 0\n        else {\n            MPI_Send(count_of_letter.data(), 26, MPI_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    // Copy the result to bins on rank 0\n    if (rank == 0) {\n        std::fill(bins.begin(), bins.end(), 0);\n        for (int i = 0; i < size; i++) {\n            if (i < size) {\n                MPI_Recv(bins.data(), 26, MPI_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            }\n        }\n    }\n\n    // Send the number of strings that start with each letter from rank 0\n    else {\n        MPI_Send(count_of_letter.data(), 26, MPI_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "for (int i = 0; i < bins.size(); ++i) {\n        bins[i] = 0;\n    }\n\n    for (auto& str : s) {\n        bins[str[0] - 'a'] += 1;\n    }\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // find the start and end point for each rank\n  int N = s.size();\n  int chunk = (int)N / (int)MPI_Comm_size(MPI_COMM_WORLD);\n  int start = rank * chunk;\n  int end = (rank + 1) * chunk;\n  if (end > N)\n    end = N;\n\n  // count the number of strings in the vector s that start with that letter.\n  for (auto i = start; i < end; ++i)\n    ++bins[s[i][0] - 'a'];\n\n  // sum up the counts from each rank\n  std::vector<size_t> counts(26);\n  MPI_Reduce(bins.data(), counts.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0)\n    bins = counts;\n}",
            "MPI_Status status;\n    int numProcs, rank;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // send strings to the next rank and receive strings from the previous rank\n    for (int i = 1; i < numProcs; i++) {\n        if (rank < i) {\n            // send strings to the previous rank\n            std::vector<std::string> new_s;\n            int new_size = 0;\n            int j = 0;\n            for (; j < (int)s.size() / numProcs; j++) {\n                new_s.push_back(s[j]);\n                new_size += 1;\n            }\n            MPI_Send(&new_size, 1, MPI_INT, rank - 1, 0, MPI_COMM_WORLD);\n            MPI_Send(&new_s[0], new_size, MPI_CHAR, rank - 1, 0, MPI_COMM_WORLD);\n        } else {\n            // receive strings from the previous rank\n            std::vector<std::string> new_s;\n            int new_size;\n            MPI_Recv(&new_size, 1, MPI_INT, rank - 1, 0, MPI_COMM_WORLD, &status);\n            MPI_Recv(&new_s[0], new_size, MPI_CHAR, rank - 1, 0, MPI_COMM_WORLD, &status);\n            new_s.resize(new_size);\n            int j = (int)s.size() / numProcs;\n            int k = 0;\n            for (; j < (int)s.size(); j++) {\n                new_s.push_back(s[j]);\n                k += 1;\n            }\n            s = new_s;\n            new_s.resize(0);\n        }\n    }\n\n    for (int i = 0; i < (int)s.size(); i++) {\n        char first_char = tolower(s[i][0]);\n        bins[first_char - 'a'] += 1;\n    }\n}",
            "assert(false);\n}",
            "std::array<size_t, 26> buffer{};\n  for (size_t i = 0; i < s.size(); ++i) {\n    ++buffer[s[i][0] - 'a'];\n  }\n\n  // MPI part\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (int i = 0; i < size; ++i) {\n    MPI_Send(&buffer[0], buffer.size(), MPI_UNSIGNED, i, 0, MPI_COMM_WORLD);\n  }\n\n  if (rank == 0) {\n    bins = {0};\n    for (int i = 1; i < size; ++i) {\n      MPI_Status status;\n      MPI_Recv(&bins[0], bins.size(), MPI_UNSIGNED, i, 0, MPI_COMM_WORLD, &status);\n    }\n  } else {\n    MPI_Status status;\n    MPI_Recv(&bins[0], bins.size(), MPI_UNSIGNED, 0, 0, MPI_COMM_WORLD, &status);\n  }\n\n  // If the rank is 0\n  if (rank == 0) {\n    for (int i = 0; i < size; ++i) {\n      for (size_t j = 0; j < buffer.size(); ++j) {\n        bins[j] += buffer[j];\n      }\n    }\n  }\n}",
            "auto letterCount = [](char c){ return c - 'a'; };\n\n  // 26 elements should be 26 bytes on a 64-bit system\n  bins.fill(0);\n  for (auto const& str : s) {\n    auto first = str.front();\n    if (first >= 'a' && first <= 'z') {\n      bins[letterCount(first)]++;\n    }\n  }\n}",
            "// TODO: Your code here\n  return;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t start = rank * s.size()/size;\n    size_t end = (rank+1) * s.size()/size;\n\n    for (size_t i = start; i < end; i++) {\n        auto it = std::find_if(s.begin(), s.end(), [&](const std::string &str) {\n            return str[0] == s[i][0];\n        });\n        if (it!= s.end()) {\n            std::size_t idx = s[i][0] - 'a';\n            bins[idx]++;\n        }\n    }\n\n    if (rank == 0) {\n        for (size_t i = 1; i < size; i++) {\n            MPI_Recv(&bins[0], 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&bins[0], 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return;\n}",
            "int rank, numRanks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n    size_t numItemsPerRank = s.size() / numRanks;\n    std::array<size_t, 26> counts;\n\n    // compute counts on each rank\n    for (const auto& str : s) {\n        auto count = str[0];\n        if (count >= 'a' && count <= 'z') {\n            counts[count - 'a']++;\n        }\n    }\n\n    // allgatherv\n    std::array<size_t, 26> recvCounts{};\n    std::vector<size_t> displs(numRanks, 0);\n\n    MPI_Allgather(&counts[0], 26, MPI_UNSIGNED_LONG_LONG, bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 1; i < numRanks; i++) {\n            for (int j = 0; j < 26; j++) {\n                bins[j] += recvCounts[i];\n            }\n        }\n    }\n}",
            "int rank;\n  int size;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // Create MPI Datatypes to send the strings and receive the counts\n  MPI_Datatype stringType;\n  MPI_Type_contiguous(s.front().size() + 1, MPI_CHAR, &stringType);\n  MPI_Type_commit(&stringType);\n\n  MPI_Datatype countType;\n  MPI_Type_contiguous(bins.size(), MPI_LONG_LONG, &countType);\n  MPI_Type_commit(&countType);\n\n  // The string to send in the message is the first string in s\n  std::string sendStr = s.front();\n\n  // Send the string to every other process\n  for (int i = 0; i < size; i++) {\n    MPI_Send(&sendStr, 1, stringType, i, 0, MPI_COMM_WORLD);\n  }\n\n  // Receive the counts from every other process\n  for (int i = 0; i < size; i++) {\n    if (i!= rank) {\n      MPI_Recv(&bins, 1, countType, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  }\n\n  // Create an array of strings that can contain all of s\n  std::vector<std::string> recvStrs;\n  recvStrs.reserve(s.size());\n\n  // Receive the strings from every other process\n  for (int i = 0; i < size; i++) {\n    if (i!= rank) {\n      MPI_Recv(&recvStrs, 1, stringType, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  }\n\n  // The string to receive in the message is the first string in recvStrs\n  std::string firstStr = recvStrs.front();\n\n  // Send the first string in s to every other process\n  for (int i = 0; i < size; i++) {\n    MPI_Send(&firstStr, 1, stringType, i, 0, MPI_COMM_WORLD);\n  }\n\n  // Receive the counts from every other process\n  for (int i = 0; i < size; i++) {\n    if (i!= rank) {\n      MPI_Recv(&bins, 1, countType, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  }\n\n  // Create an array of strings that can contain all of recvStrs\n  std::vector<std::string> nextStrs;\n  nextStrs.reserve(recvStrs.size());\n\n  // Receive the strings from every other process\n  for (int i = 0; i < size; i++) {\n    if (i!= rank) {\n      MPI_Recv(&nextStrs, 1, stringType, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  }\n\n  // Count the frequency of first letters in s\n  std::array<size_t, 26> counts{};\n  for (std::string const& str : s) {\n    counts[str[0] - 'a']++;\n  }\n\n  // Share the counts with other processes\n  // Process rank 0 sends the counts to process rank 1\n  // Process rank 1 sends the counts to process rank 2\n  // Process rank 2 sends the counts to process rank 0\n  // and so on\n  int target = (rank + 1) % size;\n  int source = (rank - 1 + size) % size;\n  MPI_Sendrecv(&counts, 1, countType, target, 0, &bins, 1, countType, source, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n  // Count the frequency of first letters in recvStrs\n  counts = {};\n  for",
            "// TODO\n}",
            "const int nproc = MPI::COMM_WORLD.Get_size();\n    const int rank = MPI::COMM_WORLD.Get_rank();\n\n    int n = s.size();\n    int block_size = n / nproc;\n    int n_left_over = n % nproc;\n\n    MPI::Status status;\n\n    for (int i = 0; i < block_size; ++i) {\n\n        if (rank == 0) {\n            std::string str = s[i + rank * block_size];\n            bins[str[0] - 'a']++;\n        } else if (rank < n_left_over) {\n            MPI::COMM_WORLD.Recv(&bins[rank], 1, MPI::INT, rank - 1, 0, MPI::COMM_WORLD);\n            std::string str = s[i + rank * block_size];\n            bins[str[0] - 'a']++;\n            MPI::COMM_WORLD.Send(&bins[rank], 1, MPI::INT, rank + 1, 0, MPI::COMM_WORLD);\n        } else {\n            MPI::COMM_WORLD.Recv(&bins[rank], 1, MPI::INT, rank - 1, 0, MPI::COMM_WORLD);\n            std::string str = s[i + rank * block_size];\n            bins[str[0] - 'a']++;\n            MPI::COMM_WORLD.Send(&bins[rank], 1, MPI::INT, rank - 1, 0, MPI::COMM_WORLD);\n        }\n\n    }\n\n    if (rank == 0) {\n        for (int i = n_left_over; i < n; i += block_size) {\n            std::string str = s[i];\n            bins[str[0] - 'a']++;\n        }\n    } else if (rank < n_left_over) {\n        MPI::COMM_WORLD.Recv(&bins[rank], 1, MPI::INT, rank - 1, 0, MPI::COMM_WORLD);\n        MPI::COMM_WORLD.Send(&bins[rank], 1, MPI::INT, rank + 1, 0, MPI::COMM_WORLD);\n    } else {\n        MPI::COMM_WORLD.Recv(&bins[rank], 1, MPI::INT, rank - 1, 0, MPI::COMM_WORLD);\n        MPI::COMM_WORLD.Send(&bins[rank], 1, MPI::INT, rank - 1, 0, MPI::COMM_WORLD);\n    }\n\n    MPI::COMM_WORLD.Barrier();\n\n    MPI_Reduce(MPI_IN_PLACE, bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "// TODO\n    int rank, numProcesses;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n    int blocks = s.size() / numProcesses;\n\n    // set up the local array\n    std::array<size_t, 26> localBins;\n    for(int i=0; i<26; i++){\n        localBins[i] = 0;\n    }\n\n    // count each letter\n    for(int i=0; i<s.size(); i++){\n        int index = (int)s[i].c_str()[0] - 97;\n        localBins[index]++;\n    }\n\n    // gather counts from all ranks\n    std::vector<std::array<size_t, 26>> binsArray(numProcesses, std::array<size_t, 26>());\n    MPI_Gather(&localBins, 26, MPI_INT, &binsArray[0], 26, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // process the local results\n    if (rank == 0) {\n        for(int i=0; i<numProcesses; i++){\n            for(int j=0; j<26; j++){\n                bins[j] += binsArray[i][j];\n            }\n        }\n    }\n}",
            "int world_size = 0, world_rank = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    if (world_rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] = 0;\n        }\n        for (std::string str : s) {\n            bins[str[0] - 'a']++;\n        }\n    } else {\n        MPI_Status status;\n        MPI_Recv(&bins[0], 26, MPI_INT, 0, 1, MPI_COMM_WORLD, &status);\n    }\n\n    for (int i = 1; i < world_size; i++) {\n        int recv_rank = i;\n        MPI_Status status;\n        MPI_Recv(&bins[0], 26, MPI_INT, i, 1, MPI_COMM_WORLD, &status);\n    }\n\n    for (int i = 0; i < 26; i++) {\n        MPI_Send(&bins[i], 1, MPI_INT, 0, 1, MPI_COMM_WORLD);\n    }\n\n}",
            "size_t rank, nranks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n  size_t stringsPerRank = s.size() / nranks;\n  std::vector<std::string> localStrings(s.begin() + stringsPerRank * rank, s.begin() + stringsPerRank * (rank + 1));\n\n  for (const std::string& str : localStrings) {\n    bins[str[0] - 'a']++;\n  }\n\n  std::array<size_t, 26> globalBins;\n\n  if (rank == 0) {\n    MPI_Reduce(bins.data(), globalBins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Reduce(bins.data(), NULL, 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n\n  if (rank == 0) {\n    for (size_t i = 0; i < 26; ++i) {\n      std::cout << globalBins[i] << \" \";\n    }\n    std::cout << std::endl;\n  }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n\tint rank;\n\tint nrank;\n\tMPI_Comm_rank(comm, &rank);\n\tMPI_Comm_size(comm, &nrank);\n\t\n\tint str_count = s.size() / nrank;\n\tstd::vector<std::string> local_strs;\n\tfor (int i = rank * str_count; i < (rank + 1) * str_count; i++)\n\t\tlocal_strs.push_back(s.at(i));\n\n\tstd::array<size_t, 26> local_bins = std::array<size_t, 26>{0};\n\tfor (const auto& str : local_strs) {\n\t\tsize_t c = str.at(0) - 97;\n\t\tlocal_bins[c]++;\n\t}\n\n\tMPI_Allreduce(MPI_IN_PLACE, local_bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, comm);\n\n\tfor (int i = 0; i < 26; i++)\n\t\tbins[i] += local_bins[i];\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t size_chunk = s.size() / size;\n    size_t remainder = s.size() % size;\n\n    std::vector<std::string> subvector;\n    if(rank == 0) {\n        subvector.insert(subvector.end(), s.begin(), s.begin() + size_chunk + remainder);\n    } else {\n        subvector.insert(subvector.end(), s.begin() + (rank - 1) * size_chunk, s.begin() + (rank - 1) * size_chunk + size_chunk);\n    }\n\n    std::array<size_t, 26> subbins;\n    std::array<size_t, 26> subbins_temp;\n\n    for(int i = 0; i < subvector.size(); i++) {\n        int first_letter_index = (int)subvector[i][0] - 97;\n        subbins[first_letter_index]++;\n    }\n\n    for(int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n\n    MPI_Reduce(&subbins[0], &bins[0], 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<std::string> s2;\n    s2.reserve(s.size() / size);\n\n    if (rank == 0) {\n        std::vector<std::string> s3;\n        s3.reserve(s.size());\n        s3 = s;\n        int num = s.size() / size;\n\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&num, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n            MPI_Send(&s3[0], num, MPI_CHAR, i, 0, MPI_COMM_WORLD);\n        }\n        for (int i = 0; i < num; i++) {\n            s2.push_back(s3[i]);\n        }\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&num, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            std::vector<std::string> temp(num);\n            MPI_Recv(&temp[0], num, MPI_CHAR, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < num; j++) {\n                s2.push_back(temp[j]);\n            }\n        }\n    } else {\n        int num;\n        MPI_Status status;\n        MPI_Recv(&num, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n        std::vector<std::string> temp(num);\n        MPI_Recv(&temp[0], num, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &status);\n        for (int i = 0; i < num; i++) {\n            s2.push_back(temp[i]);\n        }\n    }\n\n    std::array<size_t, 26> bins2;\n    for (int i = 0; i < 26; i++) {\n        bins2[i] = 0;\n    }\n\n    for (int i = 0; i < s2.size(); i++) {\n        char c = s2[i][0];\n        if (c >= 'a' && c <= 'z') {\n            bins2[c - 'a'] += 1;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < size; i++) {\n            MPI_Send(&bins2[0], 26, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD);\n        }\n        for (int i = 0; i < 26; i++) {\n            bins[i] = 0;\n        }\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&bins[0], 26, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&bins2[0], 26, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// your code here\n}",
            "int rank, nproc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    // find the smallest string\n    int min = INT_MAX;\n    if (s.size() > 0) {\n        min = s[0].length();\n    }\n    MPI_Allreduce(&min, &min, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n\n    // compute how many strings are left for each rank\n    int to_do = s.size() / nproc;\n    int extra = s.size() - to_do * nproc;\n\n    // compute how many strings we own\n    int my_to_do = to_do + (rank < extra);\n    int my_start = to_do * rank + std::min(rank, extra);\n\n    // compute the offset to use\n    int offset = 0;\n    if (rank == 0) {\n        offset = min;\n    }\n    MPI_Bcast(&offset, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    std::vector<int> letters(my_to_do);\n    for (int i = 0; i < my_to_do; i++) {\n        letters[i] = s[my_start + i].length() - offset;\n    }\n\n    // compute how many letters we own\n    int my_length = 0;\n    for (int i = 0; i < my_to_do; i++) {\n        if (letters[i] > 0) {\n            my_length++;\n        }\n    }\n\n    std::vector<int> counts(my_length);\n    for (int i = 0; i < my_length; i++) {\n        counts[i] = 0;\n    }\n\n    // do the actual work\n    for (int i = 0; i < my_to_do; i++) {\n        if (letters[i] > 0) {\n            char c = s[my_start + i][offset];\n            counts[letters[i] - offset - 1]++;\n        }\n    }\n\n    // add up the counts\n    MPI_Allreduce(counts.data(), bins.data(), 26, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n}",
            "std::array<size_t, 26> partialBins;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    for (int i = 0; i < 26; i++) {\n        partialBins[i] = 0;\n    }\n    int chunkSize = s.size()/size;\n    int remainder = s.size()%size;\n\n    for (int i = 0; i < chunkSize + remainder; i++) {\n        if (i < chunkSize + remainder && i >= chunkSize) {\n            i = i + (rank - 1)*chunkSize;\n        }\n        if (i < s.size()) {\n            char letter = s[i][0];\n            partialBins[letter-97] += 1;\n        }\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, partialBins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    for (int i = 0; i < 26; i++) {\n        bins[i] = partialBins[i];\n    }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, size;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &size);\n    std::array<size_t, 26> bins_temp;\n    int n = s.size();\n    int count;\n    int start = 0, end = 0;\n\n    for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < n; i++) {\n            bins_temp[s[i][0] - 'a'] += 1;\n        }\n    } else {\n        for (int i = 0; i < n; i++) {\n            if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n                start = (n / size) * rank;\n                end = start + (n / size);\n                if (i >= start && i < end) {\n                    bins_temp[s[i][0] - 'a'] += 1;\n                }\n            }\n        }\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, bins_temp.data(), 26, MPI_INT, MPI_SUM, comm);\n    for (int i = 0; i < 26; i++) {\n        bins[i] = bins_temp[i];\n    }\n}",
            "for(int i = 0; i < 26; i++)\n        bins[i] = 0;\n    for (auto const& word : s) {\n        bins[word[0] - 97]++;\n    }\n}",
            "for (size_t i = 0; i < 26; i++) {\n    bins[i] = 0;\n  }\n\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  size_t index = rank;\n  size_t offset = s.size() / MPI_COMM_WORLD.Size();\n\n  std::string string;\n\n  if (index < s.size() / MPI_COMM_WORLD.Size()) {\n\n    for (int i = 0; i < offset; i++) {\n      string = s[index + i];\n      char first_letter = string[0];\n      bins[first_letter - 'a']++;\n    }\n  }\n\n  int result[26];\n  MPI_Allreduce(bins.data(), result, 26, MPI_UNSIGNED, MPI_SUM, MPI_COMM_WORLD);\n  for (size_t i = 0; i < 26; i++) {\n    bins[i] = result[i];\n  }\n\n}",
            "// TODO\n}",
            "std::array<size_t, 26> localBins{};\n  std::vector<std::string> s_local = s;\n  for (const auto &str : s) {\n    if (str.empty())\n      continue;\n\n    char c = str[0];\n    if (c >= 'a' && c <= 'z')\n      localBins[c - 'a']++;\n  }\n  //std::cout << \"rank \" << rank << \"  localBins: \" << localBins << std::endl;\n  //std::cout << \"rank \" << rank << \"  s_local: \" << s_local << std::endl;\n\n  //MPI_Reduce(&localBins[0], &bins[0], 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  MPI_Reduce(&localBins[0], &bins[0], 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// Your code here\n}",
            "// TODO: Your code goes here\n\tfor (int i = 0; i < s.size(); i++) {\n\t\tint temp = 0;\n\t\tfor (int j = 0; j < s[i].size(); j++) {\n\t\t\tif (s[i][j] >= 97 && s[i][j] <= 122) {\n\t\t\t\ttemp = s[i][j] - 96;\n\t\t\t\tbins[temp]++;\n\t\t\t}\n\t\t}\n\t}\n}",
            "for (auto i : s){\n        bins[i[0] - 'a']++;\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (size < 2) {\n        // the vector s has a length of zero, so each rank has nothing to do\n        return;\n    }\n\n    size_t n = s.size();\n    // set the number of strings in the vector to each rank\n    int stringsPerRank = n / size;\n\n    // distribute the strings to each rank\n    std::vector<std::string> localStrings(stringsPerRank);\n    for (int i = 0; i < stringsPerRank; i++) {\n        localStrings[i] = s[rank * stringsPerRank + i];\n    }\n\n    std::vector<size_t> counts(26);\n    for (int i = 0; i < 26; i++) {\n        counts[i] = 0;\n    }\n\n    // count the number of strings in the localStrings vector\n    for (auto &str : localStrings) {\n        counts[str[0] - 'a'] += 1;\n    }\n\n    // sum the counts and get the result from rank 0\n    int sum;\n    for (int i = 0; i < 26; i++) {\n        sum = counts[i];\n        MPI_Allreduce(&sum, &counts[i], 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    }\n\n    // distribute the results from rank 0 to all other ranks\n    if (rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] = counts[i];\n        }\n    } else {\n        MPI_Status status;\n        for (int i = 0; i < 26; i++) {\n            MPI_Recv(&bins[i], 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n        }\n    }\n\n    return;\n}",
            "size_t size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // calculate the amount of work each process will do\n    std::vector<size_t> splitS;\n    splitS.reserve(size);\n    size_t split = s.size() / size;\n    size_t leftOver = s.size() % size;\n    splitS.resize(size);\n    for (size_t i = 0; i < size; i++) {\n        if (i == size - 1) {\n            splitS[i] = split + leftOver;\n        } else {\n            splitS[i] = split;\n        }\n    }\n\n    // send the amount of work each process needs to do\n    std::vector<size_t> recv;\n    recv.resize(size);\n    MPI_Gather(&splitS[rank], 1, MPI_UNSIGNED_LONG_LONG, recv.data(), 1, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n    // do the work\n    if (rank == 0) {\n        for (size_t i = 0; i < size; i++) {\n            for (size_t j = 0; j < recv[i]; j++) {\n                char letter = std::tolower(s[i * split + j][0]);\n                bins[letter - 'a']++;\n            }\n        }\n    }\n\n    // reduce results into bins\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&bins[0], 26, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&bins[0], 26, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int world_size;\n    int world_rank;\n    MPI_Comm_size(comm, &world_size);\n    MPI_Comm_rank(comm, &world_rank);\n\n    const int chunk_size = s.size() / world_size;\n    const int remainder = s.size() % world_size;\n\n    // Allocate memory for strings on this rank\n    std::vector<std::string> local_s;\n    local_s.reserve(chunk_size + remainder);\n    // Fill local_s\n    for (int i = 0; i < chunk_size; ++i) {\n        local_s.push_back(s[i + world_rank * chunk_size]);\n    }\n    if (remainder > 0) {\n        for (int i = 0; i < remainder; ++i) {\n            local_s.push_back(s[i + chunk_size * world_rank + s.size() - remainder]);\n        }\n    }\n\n    // Count letters on this rank\n    std::unordered_map<char, int> local_letter_counts;\n    for (auto const& str : local_s) {\n        if (!str.empty()) {\n            local_letter_counts[str[0]] += 1;\n        }\n    }\n\n    // Send letter counts to rank 0\n    MPI_Datatype mpi_letter_counts = MPI_UNSIGNED_LONG;\n    MPI_Type_contiguous(sizeof(int) / sizeof(char), MPI_CHAR, &mpi_letter_counts);\n    MPI_Type_commit(&mpi_letter_counts);\n\n    std::vector<int> letter_counts(26);\n    std::fill(letter_counts.begin(), letter_counts.end(), 0);\n    MPI_Gather(&local_letter_counts[0], 1, mpi_letter_counts, &letter_counts[0], 1, mpi_letter_counts, 0, comm);\n\n    if (world_rank == 0) {\n        MPI_Type_free(&mpi_letter_counts);\n        for (int i = 0; i < 26; ++i) {\n            bins[i] = letter_counts[i];\n        }\n    }\n}",
            "MPI_Request req[128];\n  MPI_Status status[128];\n\n  int nprocs, myrank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n  std::array<size_t, 26> local_bins{};\n\n  for (size_t i = 0; i < s.size(); i++) {\n    char c = s[i][0];\n    local_bins[c-'a']++;\n  }\n\n  if(myrank == 0) {\n    for(int i=1; i<nprocs; i++)\n      MPI_Irecv(&bins[0], 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &req[i]);\n    MPI_Waitall(nprocs-1, &req[1], &status[1]);\n  }\n  else {\n    MPI_Isend(&local_bins[0], 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD, &req[0]);\n    MPI_Waitall(1, &req[0], &status[0]);\n  }\n\n  if(myrank == 0) {\n    for(int i=1; i<nprocs; i++)\n      for(int j=0; j<26; j++)\n        bins[j] += bins[j];\n  }\n}",
            "}",
            "// TODO\n  for (size_t i = 0; i < s.size(); i++) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "size_t N = s.size();\n    size_t n_ranks = MPI::COMM_WORLD.Get_size();\n    size_t n_per_rank = N / n_ranks;\n\n    // Make sure each rank has a complete copy of s\n    size_t excess = N % n_ranks;\n    if(excess!= 0 && MPI::COMM_WORLD.Get_rank() < excess)\n        n_per_rank++;\n\n    std::vector<std::string> local_s(n_per_rank);\n    for(size_t i = 0; i < n_per_rank; i++)\n        local_s[i] = s[i + MPI::COMM_WORLD.Get_rank() * n_per_rank];\n\n    // Local first-letter counts\n    std::array<size_t, 26> local_bins;\n    for(size_t i = 0; i < local_s.size(); i++) {\n        local_bins[local_s[i][0] - 'a']++;\n    }\n\n    // MPI Reduce (sum) to compute the global first-letter counts\n    MPI::COMM_WORLD.Reduce(local_bins.data(), bins.data(), 26, MPI::UNSIGNED_LONG, MPI::SUM, 0);\n}",
            "MPI_Group world_group, local_group;\n    MPI_Comm local_comm;\n    int local_rank, local_size;\n    int world_rank, world_size;\n    int *l_ids, *w_ids;\n    int l_rank, w_rank;\n\n    MPI_Comm_group(MPI_COMM_WORLD, &world_group);\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    MPI_Group_incl(world_group, s.size(), &(w_ids[0]), &local_group);\n    MPI_Comm_create_group(MPI_COMM_WORLD, local_group, 0, &local_comm);\n    MPI_Group_rank(local_group, &local_rank);\n    MPI_Group_size(local_group, &local_size);\n\n    l_ids = new int[local_size];\n\n    for (int i = 0; i < local_size; i++) {\n        l_ids[i] = local_rank + i;\n    }\n\n    std::array<size_t, 26> local_bins;\n\n    for (size_t i = 0; i < s.size(); i++) {\n        int char_index = s[i][0] - 'a';\n        local_bins[char_index]++;\n    }\n\n    MPI_Gatherv(local_bins.data(), local_size, MPI_LONG,\n                bins.data(), l_ids, l_ids + 1, MPI_LONG,\n                0, MPI_COMM_WORLD);\n\n    MPI_Group_free(&world_group);\n    MPI_Group_free(&local_group);\n    MPI_Comm_free(&local_comm);\n\n    delete[] l_ids;\n\n}",
            "// TODO\n    //\n    // Initialize bins\n    //\n    // Use MPI_Gather to compute the bins for each rank.\n    //\n    // In the end, the bins array will be complete on rank 0.\n\n}",
            "std::array<size_t, 26> local_bins{};\n    size_t global_offset{0};\n\n    for (auto const& i : s) {\n        local_bins[i[0] - 'a']++;\n    }\n\n    std::vector<size_t> local_counts;\n    local_counts.reserve(26);\n    local_counts.assign(local_bins.begin(), local_bins.end());\n    // std::cout << \"local_counts before sending \" << local_counts << std::endl;\n\n    std::vector<size_t> global_counts(local_counts.size() * MPI_SIZE);\n    // std::cout << \"global_counts before sending \" << global_counts << std::endl;\n\n    MPI_Allgather(&local_counts[0], local_counts.size(), MPI_INT,\n            &global_counts[0], local_counts.size(), MPI_INT, MPI_COMM_WORLD);\n\n    std::vector<size_t> local_bins2;\n    local_bins2.reserve(26);\n    local_bins2.assign(local_bins.begin(), local_bins.end());\n\n    std::vector<size_t> global_bins(local_bins2.size() * MPI_SIZE);\n\n    int global_offset2 = 0;\n\n    for (int i = 0; i < MPI_SIZE; i++) {\n        global_offset2 += global_counts[i * 26];\n    }\n\n    global_offset += global_offset2;\n\n    for (int i = 0; i < MPI_SIZE; i++) {\n        for (int j = 0; j < 26; j++) {\n            global_bins[i * 26 + j] = global_counts[i * 26 + j] + global_offset;\n        }\n    }\n\n    for (int i = 0; i < 26; i++) {\n        bins[i] = global_bins[i];\n    }\n}",
            "//TODO\n}",
            "// TODO\n}",
            "for (int i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "// TODO\n  // MPI_Gather\n  // MPI_Scatter\n\n}",
            "int rank;\n    int size;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int i;\n\n    for (i = 0; i < s.size(); i++) {\n        char first = s[i][0];\n        if (first >= 'a' && first <= 'z') {\n            bins[first - 'a']++;\n        }\n    }\n\n    MPI_Reduce(bins.data(), bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint chunk_size = s.size() / size;\n\tint extra = s.size() % size;\n\n\tstd::array<size_t, 26> local_bins{};\n\n\tfor (int i = 0; i < chunk_size + extra; ++i) {\n\t\tif (i < extra)\n\t\t\tlocal_bins[s[i][0] - 'a']++;\n\t\telse\n\t\t\tlocal_bins[s[i - extra][0] - 'a']++;\n\t}\n\n\tstd::vector<std::array<size_t, 26>> bins_vec(size);\n\tMPI_Allgather(&local_bins, 26 * sizeof(size_t), MPI_CHAR, bins_vec.data(), 26 * sizeof(size_t), MPI_CHAR, MPI_COMM_WORLD);\n\n\tfor (int i = 0; i < size; ++i) {\n\t\tfor (int j = 0; j < 26; ++j) {\n\t\t\tbins[j] += bins_vec[i][j];\n\t\t}\n\t}\n}",
            "// TODO: your code here\n    int len=s.size();\n    int rank, n;\n    MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n    MPI_Comm_size(MPI_COMM_WORLD,&n);\n    int start_index = (len/n)*rank;\n    int end_index = (len/n)*(rank+1);\n    int start_bins = 97 + (rank)*26;\n    int end_bins = (rank+1)*26;\n    std::array<size_t, 26> local_bins;\n    std::fill(local_bins.begin(), local_bins.end(), 0);\n    for(int i=start_index; i<end_index; i++){\n        int char_ = s[i][0];\n        if(char_>96 && char_<97+26)\n            local_bins[char_-97]++;\n    }\n    MPI_Reduce(&local_bins[0], &bins[0], 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // split strings evenly by rank\n    std::vector<std::string> s_rank(s.size() / size);\n    std::copy(s.begin() + rank * s_rank.size(), s.begin() + (rank+1) * s_rank.size(), s_rank.begin());\n\n    // count letters\n    for (auto& str : s_rank) {\n        size_t first = str.find_first_not_of(' ');\n        if (first!= std::string::npos) {\n            bins[str[first] - 'a']++;\n        }\n    }\n\n    // sum bins and store in rank 0\n    int total = 0;\n    std::array<int, 26> bin_sum;\n    for (auto i = 0; i < 26; ++i) {\n        total += bins[i];\n        bin_sum[i] = total;\n    }\n\n    if (rank == 0) {\n        for (auto i = 0; i < size; ++i) {\n            std::vector<int> result(26, 0);\n            MPI_Recv(&result[0], 26, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (auto j = 0; j < 26; ++j) {\n                bin_sum[j] += result[j];\n            }\n        }\n        for (auto i = 0; i < 26; ++i) {\n            bins[i] = bin_sum[i];\n        }\n    } else {\n        MPI_Send(&bins[0], 26, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "size_t n = s.size();\n    size_t my_n = n / MPI_COMM_WORLD.Get_size();\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int other_rank;\n    if (rank == 0) {\n        other_rank = MPI_COMM_WORLD.Get_size() - 1;\n    } else {\n        other_rank = rank - 1;\n    }\n    MPI_Request request;\n    int flag;\n    MPI_Status status;\n\n    std::vector<std::string> s_left;\n    if (rank == 0) {\n        s_left = s;\n    } else {\n        s_left.resize(my_n);\n        MPI_Send(&(s[0]), my_n, MPI_CHAR, other_rank, 1, MPI_COMM_WORLD);\n    }\n    int j = 0;\n    for (int i = 0; i < my_n; ++i) {\n        if (rank == 0) {\n            if (s[i][0] < 'a' + j) {\n                ++j;\n            }\n            ++bins[s[i][0] - 'a'];\n        } else if (rank == MPI_COMM_WORLD.Get_size() - 1) {\n            MPI_Recv(&s[my_n], n - my_n, MPI_CHAR, other_rank, 1, MPI_COMM_WORLD, &status);\n            for (int i = my_n; i < n; ++i) {\n                if (s[i][0] < 'a' + j) {\n                    ++j;\n                }\n                ++bins[s[i][0] - 'a'];\n            }\n        } else {\n            if (s[i][0] < 'a' + j) {\n                ++j;\n            }\n            ++bins[s[i][0] - 'a'];\n        }\n    }\n    if (rank == 0) {\n        for (int i = 0; i < other_rank; ++i) {\n            MPI_Irecv(&(s[0]), my_n, MPI_CHAR, i, 1, MPI_COMM_WORLD, &request);\n            MPI_Wait(&request, &status);\n            for (int i = 0; i < my_n; ++i) {\n                if (s[i][0] < 'a' + j) {\n                    ++j;\n                }\n                ++bins[s[i][0] - 'a'];\n            }\n        }\n        MPI_Irecv(&s[0], my_n, MPI_CHAR, other_rank, 1, MPI_COMM_WORLD, &request);\n        MPI_Wait(&request, &status);\n        for (int i = 0; i < my_n; ++i) {\n            if (s[i][0] < 'a' + j) {\n                ++j;\n            }\n            ++bins[s[i][0] - 'a'];\n        }\n        MPI_Irecv(&s[0], n - my_n, MPI_CHAR, other_rank, 1, MPI_COMM_WORLD, &request);\n        MPI_Wait(&request, &status);\n        for (int i = my_n; i < n; ++i) {\n            if (s[i][0] < 'a' + j) {\n                ++j;\n            }\n            ++bins[s[i][0] - 'a'];\n        }\n    }\n    else if (rank == other_rank) {\n        for (int i = 0; i < my_n; ++i) {\n            if (s[i][0] < 'a' + j) {\n                ++j;\n            }\n            ++bins[s[i][0] - 'a'];\n        }\n        MPI_Send(&s[0], my_n, MPI_CHAR, 0, 1, MPI_COMM_WORLD);\n        MPI_Send(&s[my_n], n - my_n, MPI",
            "}",
            "}",
            "// TODO\n}",
            "int ntasks, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &ntasks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // 1st pass to compute sizes of message buffers\n  int localN = s.size() / ntasks;\n  int nLeft = s.size() % ntasks;\n  int msgSize = 0;\n\n  for (int i = 0; i < ntasks; i++) {\n    if (i < nLeft) {\n      msgSize += s[localN * i + i].length() + 1;\n    } else {\n      msgSize += s[localN * i + nLeft].length() + 1;\n    }\n  }\n\n  // 2nd pass to send data\n  std::vector<char> data(msgSize);\n  for (int i = 0; i < ntasks; i++) {\n    if (i < nLeft) {\n      std::strncpy(&data[msgSize * i], s[localN * i + i].c_str(), s[localN * i + i].length());\n      data[msgSize * i + s[localN * i + i].length()] = '\\0';\n    } else {\n      std::strncpy(&data[msgSize * i], s[localN * i + nLeft].c_str(), s[localN * i + nLeft].length());\n      data[msgSize * i + s[localN * i + nLeft].length()] = '\\0';\n    }\n  }\n\n  std::vector<char> data_recv(msgSize);\n  MPI_Gather(&data[0], msgSize, MPI_CHAR, &data_recv[0], msgSize, MPI_CHAR, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < ntasks; i++) {\n      for (int j = 0; j < 26; j++) {\n        if (data_recv[msgSize * i + j] == 'a' + j) {\n          bins[j]++;\n        }\n      }\n    }\n  }\n\n}",
            "int n_tasks = 1;\n  MPI_Comm_size(MPI_COMM_WORLD, &n_tasks);\n\n  // allocate space to store the results\n  std::array<size_t, 26> results;\n  std::fill(results.begin(), results.end(), 0);\n\n  // compute local counts\n  size_t rank = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  for (auto const& str : s) {\n    int letter = str[0] - 'a';\n    results[letter] += 1;\n  }\n\n  // compute global counts\n  std::vector<size_t> partial_sums(results.begin(), results.end());\n  partial_sums.resize(26);\n  std::partial_sum(partial_sums.begin(), partial_sums.end(), partial_sums.begin());\n\n  // gather results\n  std::vector<size_t> all_results;\n  all_results.resize(partial_sums.size());\n  MPI_Gather(&partial_sums[0], partial_sums.size(), MPI_UNSIGNED_LONG,\n             &all_results[0], partial_sums.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n  // return results\n  if (rank == 0) {\n    std::fill(bins.begin(), bins.end(), 0);\n    for (int i = 0; i < all_results.size(); i++) {\n      bins[i] = all_results[i];\n    }\n  }\n}",
            "for (size_t i = 0; i < 26; i++) {\n    bins[i] = 0;\n  }\n  for (const auto& s_entry : s) {\n    char first_letter = s_entry[0];\n    if (first_letter >= 97 && first_letter <= 122) {\n      bins[first_letter - 97]++;\n    }\n  }\n}",
            "// TODO: implement\n  // Call the function in the string utils file\n  return;\n}",
            "// TODO: complete this function\n}",
            "// TODO: Your code here\n\n}",
            "auto const world_size = MPI_Comm_size(MPI_COMM_WORLD);\n  auto const world_rank = MPI_Comm_rank(MPI_COMM_WORLD);\n  size_t const local_size = s.size() / world_size;\n\n  auto const begin = s.begin() + world_rank * local_size;\n  auto const end = s.begin() + (world_rank + 1) * local_size;\n\n  for (auto it = begin; it!= end; ++it) {\n    char first_letter = (*it)[0];\n    first_letter = std::tolower(first_letter);\n    if ((first_letter >= 'a') && (first_letter <= 'z')) {\n      bins[first_letter - 'a']++;\n    }\n  }\n\n  auto const local_bins = bins;\n  if (world_rank == 0) {\n    for (auto i = 1; i < world_size; ++i) {\n      MPI_Recv(bins.data(), 26, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < 26; ++j) {\n        bins[j] += local_bins[j];\n      }\n    }\n  } else {\n    MPI_Send(bins.data(), 26, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n\n  if (world_rank == 0) {\n    for (size_t i = 0; i < 26; ++i) {\n      std::cout << bins[i] << \" \";\n    }\n  }\n\n  // MPI_Barrier(MPI_COMM_WORLD);\n}",
            "// TODO:\n    // your code here\n}",
            "//TODO: Your code here\n\n\n}",
            "assert(s.size() % 26 == 0);\n    for (int i = 0; i < 26; i++)\n        bins[i] = 0;\n    size_t j = 0;\n    for (int i = 0; i < s.size(); i++) {\n        char c = s[i].c_str()[0];\n        c = (c >= 97)? c - 32 : c;\n        bins[c - 97] += 1;\n    }\n}",
            "size_t sz = s.size();\n  MPI_Datatype mpi_string;\n  MPI_Type_contiguous(s.front().size(), MPI_CHAR, &mpi_string);\n  MPI_Type_commit(&mpi_string);\n  for (int rank = 0; rank < size; ++rank) {\n    if (rank == 0) {\n      bins.fill(0);\n    }\n    std::vector<std::string> partial_s;\n    for (size_t i = rank; i < sz; i += size) {\n      partial_s.push_back(s[i]);\n    }\n    MPI_Gatherv(&partial_s[0], partial_s.size(), mpi_string,\n                &s[0], &bins[0], &bins[0], mpi_string, 0, MPI_COMM_WORLD);\n  }\n  MPI_Type_free(&mpi_string);\n}",
            "int rank;\n    int nranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    size_t size = s.size();\n    int bin = 0;\n    size_t n = 0;\n    size_t start = rank * (size / nranks);\n    size_t end = start + (size / nranks);\n    for (int i = start; i < end; i++) {\n        if (isalpha(s[i][0])) {\n            bin = s[i][0] - 'a';\n            n++;\n        }\n    }\n    MPI_Reduce(&n, &bins[bin], 1, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n  int n_procs, rank;\n  MPI_Comm_size(comm, &n_procs);\n  MPI_Comm_rank(comm, &rank);\n\n  std::vector<std::string> s_local;\n  int n_local = s.size() / n_procs;\n  if (rank == n_procs - 1) {\n    n_local = s.size() - n_local * (n_procs - 1);\n  }\n  for (int i = rank * n_local; i < (rank + 1) * n_local; i++) {\n    s_local.push_back(s[i]);\n  }\n\n  std::array<size_t, 26> bins_local;\n  for (int i = 0; i < 26; i++) {\n    bins_local[i] = 0;\n  }\n  for (int i = 0; i < s_local.size(); i++) {\n    bins_local[s_local[i][0] - 97]++;\n  }\n\n  MPI_Allreduce(MPI_IN_PLACE, bins_local.data(), 26, MPI_LONG, MPI_SUM, comm);\n\n  for (int i = 0; i < 26; i++) {\n    bins[i] = bins_local[i];\n  }\n}",
            "std::array<size_t, 26> localBins;\n\tstd::fill_n(localBins.data(), 26, 0);\n\n\tfor (const auto& i : s) {\n\t\tif (i.length() > 0)\n\t\t\tlocalBins[i[0] - 'a']++;\n\t}\n\n\t//TODO: compute the histogram on each rank, and send to rank 0\n\t\n\t//TODO: compute the histogram on rank 0 and store in the `bins` array\n\n\n\t//TODO: for debugging, print out the histogram on rank 0\n}",
            "// TODO\n\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  // int world_size;\n  // MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  if (rank == 0) {\n    for (int i = 0; i < s.size(); i++) {\n      if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n        bins[s[i][0] - 'a']++;\n      }\n    }\n  } else {\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    int chunkSize = s.size() / world_size;\n    int start = rank * chunkSize;\n    int end = start + chunkSize;\n    for (int i = start; i < end; i++) {\n      if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n        MPI_Send(&(s[i][0]), 1, MPI_CHAR, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(&bins[s[i][0] - 'a'], 1, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n      }\n    }\n  }\n\n\n\n  // int rank;\n  // MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  // int world_size;\n  // MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  // if (rank == 0) {\n  //   // for (int i = 0; i < s.size(); i++) {\n  //   //   if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n  //   //     bins[s[i][0] - 'a']++;\n  //   //   }\n  //   // }\n  // } else {\n  //   int chunkSize = s.size() / world_size;\n  //   int start = rank * chunkSize;\n  //   int end = start + chunkSize;\n  //   for (int i = start; i < end; i++) {\n  //     if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n  //       MPI_Send(&(s[i][0]), 1, MPI_CHAR, 0, 0, MPI_COMM_WORLD);\n  //       MPI_Send(&bins[s[i][0] - 'a'], 1, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n  //     }\n  //   }\n  // }\n  // // int world_size;\n  // // MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  // if (rank == 0) {\n  //   // for (int i = 0; i < s.size(); i++) {\n  //   //   if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n  //   //     bins[s[i][0] - 'a']++;\n  //   //   }\n  //   // }\n  //   // for (int i = 0; i < 26; i++) {\n  //   //   std::cout << bins[i] << \", \";\n  //   // }\n  //   // std::cout << std::endl;\n  //   // std::cout << \"Rank = \" << rank << \", \" << \"bins = \" << bins[0] << \", \" << bins[1] << \", \" << bins[2] << std::endl;\n  // } else {\n  //   // int chunkSize = s.size() / world_size;\n  //   // int start = rank * chunkSize;\n  //   // int end = start + chunkSize;\n  //   // for (int i = start; i < end; i++) {",
            "size_t n = s.size();\n    std::array<size_t, 26> hist;\n    std::fill(hist.begin(), hist.end(), 0);\n    for (size_t i=0; i<n; i++)\n        ++hist[s[i][0]-'a'];\n\n    MPI_Allreduce(&hist[0], &bins[0], 26, MPI_LONG_LONG_INT, MPI_SUM, MPI_COMM_WORLD);\n}",
            "for(int i = 0; i < 26; i++){\n        bins[i] = 0;\n    }\n\n    for(int i = 0; i < s.size(); i++){\n        char letter = s[i][0];\n        bins[letter - 'a'] += 1;\n    }\n}",
            "// TODO: implement here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int count = s.size();\n    int chunk = count/size;\n\n    std::vector<std::string> local_strings(chunk);\n\n    if (rank==0) {\n        local_strings = std::vector<std::string>(s.begin(),s.begin()+chunk);\n    }\n    if (rank!= 0) {\n        MPI_Send(s.data(), count, MPI_CHAR, 0, 0, MPI_COMM_WORLD);\n        MPI_Recv(local_strings.data(), count, MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    std::array<size_t, 26> local_counts{};\n\n    for (auto str : local_strings) {\n        int index = str.front()-'a';\n        local_counts[index]++;\n    }\n\n    std::array<size_t, 26> global_counts{};\n    MPI_Reduce(local_counts.data(), global_counts.data(), 26, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] = global_counts[i];\n        }\n    }\n}",
            "bins = {};\n    for (auto const& str : s) {\n        if (str.length() > 0) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "for (auto& str : s)\n        str[0] |= 32;\n    std::array<size_t, 26> counts;\n    std::fill(counts.begin(), counts.end(), 0);\n    for (auto str : s) {\n        ++counts[str[0] - 97];\n    }\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Allreduce(counts.data(), bins.data(), counts.size(), MPI_UNSIGNED, MPI_SUM, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "for(int i = 0; i < s.size(); i++)\n    bins[s[i][0]-97]++;\n}",
            "// TODO: your code here\n}",
            "MPI_Datatype MPI_STR;\n\n  MPI_Type_contiguous(s[0].size(), MPI_CHAR, &MPI_STR);\n  MPI_Type_commit(&MPI_STR);\n\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int nproc;\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  size_t len = s.size() / nproc;\n  size_t rem = s.size() % nproc;\n  size_t start = rank * len + std::min(rank, rem);\n  size_t end = start + len + (rank < rem? 1 : 0);\n  std::vector<std::string> local_strs(s.begin() + start, s.begin() + end);\n\n  std::array<size_t, 26> local_bins{};\n  for (auto str : local_strs) {\n    ++local_bins[str[0] - 'a'];\n  }\n\n  std::array<size_t, 26> global_bins{};\n  MPI_Reduce(local_bins.data(), global_bins.data(), 26, MPI_STR, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    bins = global_bins;\n  }\n\n  MPI_Type_free(&MPI_STR);\n}",
            "// YOUR CODE HERE\n    bins.fill(0);\n    for (int i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n    return;\n}",
            "int rank, nranks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n  std::vector<std::string> local_s = s;\n  std::array<size_t, 26> local_bins = {0};\n  for (auto i = 0; i < local_s.size(); ++i) {\n    ++local_bins[local_s[i][0] - 'a'];\n  }\n  MPI_Gather(&local_bins[0], local_bins.size(), MPI_UNSIGNED_LONG_LONG, bins.data(), local_bins.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (auto i = 1; i < nranks; ++i) {\n      for (auto j = 0; j < 26; ++j) {\n        bins[j] += bins[26*i + j];\n      }\n    }\n  }\n}",
            "MPI_Request request;\n    size_t nbins = 26;\n    MPI_Datatype MPI_STRUCT_TYPE = create_mpi_struct_type();\n\n    size_t bins_size = bins.size();\n    size_t s_size = s.size();\n\n    // rank 0:\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&bins, bins_size, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD);\n        }\n\n        // get the local bins counts\n        for (int j = 0; j < s_size; j++) {\n            bins[s[j][0] - 97]++;\n        }\n\n        // recieve the bins counts from other ranks\n        for (int j = 1; j < size; j++) {\n            MPI_Recv(&bins, bins_size, MPI_UNSIGNED_LONG_LONG, j, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n    else {\n        MPI_Irecv(&bins, bins_size, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD, &request);\n        // get the local bins counts\n        for (int j = 0; j < s_size; j++) {\n            bins[s[j][0] - 97]++;\n        }\n\n        // send the bins counts to rank 0\n        MPI_Send(&bins, bins_size, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n        MPI_Wait(&request, MPI_STATUS_IGNORE);\n    }\n\n    // destroy the MPI struct type\n    MPI_Type_free(&MPI_STRUCT_TYPE);\n}",
            "MPI_Datatype dt;\n    MPI_Type_contiguous(bins.size(), MPI_UNSIGNED, &dt);\n    MPI_Type_commit(&dt);\n\n    MPI_Status status;\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    size_t local_count = 0;\n    for (auto const& s_ : s) {\n        if (s_[0] >= 'a' && s_[0] <= 'z') {\n            local_count++;\n        }\n    }\n\n    size_t chunk_size = local_count / world_size;\n    size_t extra = local_count % world_size;\n\n    std::array<size_t, 26> local_bins;\n    local_bins.fill(0);\n\n    for (auto const& s_ : s) {\n        if (s_[0] >= 'a' && s_[0] <= 'z') {\n            local_bins[s_[0] - 'a']++;\n        }\n    }\n\n    if (chunk_size) {\n        std::vector<std::array<size_t, 26>> bins_vector(chunk_size);\n        bins_vector.fill(std::array<size_t, 26>());\n        bins_vector.front().fill(0);\n\n        MPI_Scatter(&local_bins[0], chunk_size, dt, &bins_vector[0], chunk_size, dt, 0, MPI_COMM_WORLD);\n\n        for (auto i = 0; i < chunk_size; i++) {\n            for (auto j = 0; j < 26; j++) {\n                bins[j] += bins_vector[i][j];\n            }\n        }\n    }\n\n    if (extra) {\n        std::array<size_t, 26> extra_bins;\n        extra_bins.fill(0);\n\n        if (rank == 0) {\n            MPI_Status status;\n            MPI_Recv(&extra_bins, 26, dt, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);\n        } else {\n            MPI_Status status;\n            MPI_Send(&local_bins[0], 26, dt, 0, rank, MPI_COMM_WORLD, &status);\n        }\n\n        for (auto i = 0; i < 26; i++) {\n            bins[i] += extra_bins[i];\n        }\n    }\n\n    if (chunk_size + extra!= local_count) {\n        std::array<size_t, 26> remainder_bins;\n        remainder_bins.fill(0);\n\n        if (rank == world_size - 1) {\n            MPI_Status status;\n            MPI_Recv(&remainder_bins, 26, dt, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);\n        } else {\n            MPI_Status status;\n            MPI_Send(&local_bins[0], 26, dt, world_size - 1, rank, MPI_COMM_WORLD, &status);\n        }\n\n        for (auto i = 0; i < 26; i++) {\n            bins[i] += remainder_bins[i];\n        }\n    }\n\n    MPI_Type_free(&dt);\n}",
            "// Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = s.size();\n    // int n = 100; // 100 for testing\n\n    int n_per_rank = (n + size - 1) / size;\n    int n_remainder = n % size;\n    int start = rank * n_per_rank;\n    int end = start + n_per_rank;\n\n    if (rank < n_remainder) {\n        start += rank;\n        end += rank + 1;\n    } else {\n        start += n_remainder;\n        end += n_remainder;\n    }\n\n    // printf(\"rank %d, start = %d, end = %d\\n\", rank, start, end);\n\n    std::array<size_t, 26> bins_local;\n\n    for (int i = start; i < end; i++) {\n        if (i < s.size()) {\n            char first_letter = s[i][0];\n            bins_local[first_letter - 'a']++;\n        }\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, bins_local.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    for (int i = 0; i < 26; i++) {\n        bins[i] = bins_local[i];\n    }\n}",
            "// fill in your code here\n\n}",
            "MPI_Datatype string_type;\n  MPI_Type_contiguous(s[0].size() + 1, MPI_CHAR, &string_type);\n  MPI_Type_commit(&string_type);\n\n  std::array<size_t, 26> bins_recv{};\n  int count{0};\n  for (auto& s_ : s) {\n    ++bins[s_.front() - 'a'];\n    ++count;\n  }\n\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  if (count!= 0) {\n    if (rank == 0) {\n      int chunksize = count / size;\n      int remainder = count % size;\n      int displs[size];\n      int chunk_count = 0;\n      for (int i = 0; i < size; ++i) {\n        if (i < remainder) {\n          displs[i] = chunksize + 1;\n          ++chunk_count;\n        } else {\n          displs[i] = chunksize;\n        }\n      }\n\n      MPI_Allgatherv(bins.data(), count, MPI_LONG_LONG, bins_recv.data(), displs, displs + 1, string_type, MPI_COMM_WORLD);\n\n      for (int i = 0; i < bins_recv.size(); ++i) {\n        bins[i] += bins_recv[i];\n      }\n    } else {\n      int displs;\n      if (rank < remainder) {\n        displs = rank * (chunksize + 1);\n      } else {\n        displs = rank * chunksize + remainder;\n      }\n      int recv_count = 0;\n      if (rank < remainder) {\n        recv_count = chunksize + 1;\n      } else {\n        recv_count = chunksize;\n      }\n\n      MPI_Gatherv(bins.data(), recv_count, string_type, bins_recv.data(), displs, displs + 1, string_type, 0, MPI_COMM_WORLD);\n    }\n  }\n  MPI_Type_free(&string_type);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < size; ++i) {\n            int letter_in_rank;\n            MPI_Recv(&letter_in_rank, 1, MPI_INT, MPI_ANY_SOURCE, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            if (letter_in_rank < 0) {\n                break;\n            }\n            bins[letter_in_rank]++;\n        }\n        MPI_Send(&rank, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (size_t i = 0; i < s.size(); ++i) {\n            if (s[i].empty()) {\n                continue;\n            }\n            if (s[i].front() < 'a' || s[i].front() > 'z') {\n                continue;\n            }\n            int letter_in_rank = s[i].front() - 'a';\n            MPI_Send(&letter_in_rank, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n            MPI_Recv(&rank, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "}",
            "// TODO\n    // Hint: use an MPI_Reduce\n\n    // MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    // MPI_Comm_size(MPI_COMM_WORLD, &size);\n    // MPI_Status status;\n    // MPI_Request request;\n\n    // if(rank == 0) {\n    //     // std::array<size_t, 26> tmp = {0};\n    //     // std::vector<std::string> s_local = s;\n    //     // for(auto const &i : s_local) {\n    //     //     bins[i[0] - 'a']++;\n    //     // }\n    // }\n    // else {\n    //     // std::vector<std::string> s_local = s;\n    //     // for(auto const &i : s_local) {\n    //     //     MPI_Isend(&bins, 1, MPI_UNSIGNED, rank, 0, MPI_COMM_WORLD, &request);\n    //     //     MPI_Wait(&request, &status);\n    //     //     bins[i[0] - 'a']++;\n    //     // }\n    // }\n\n    // MPI_Reduce(bins, bins, 26, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n    // MPI_Barrier(MPI_COMM_WORLD);\n\n}",
            "bins.fill(0);\n    for (auto const& str : s) {\n        char const firstLetter = str.front();\n        bins[firstLetter - 'a']++;\n    }\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  size_t n = s.size();\n  std::array<size_t, 26> localCounts = {0};\n  for (auto const& str : s) {\n    if (str.empty()) continue;\n    localCounts[str.front() - 'a'] += 1;\n  }\n  MPI_Allreduce(localCounts.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "std::array<size_t, 26> bins_local;\n\tint nranks = 0;\n\tMPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\tint rank = 0;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tfor (auto& s_i : s) {\n\t\tchar first = tolower(s_i[0]);\n\t\t++bins_local[first - 'a'];\n\t}\n\n\tif (nranks > 1) {\n\t\tMPI_Reduce(bins_local.data(), bins.data(), bins_local.size(), MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n\t}\n\telse {\n\t\tbins = bins_local;\n\t}\n}",
            "// Fill this in\n}",
            "}",
            "for (auto const& str : s) {\n    char firstLetter = tolower(str[0]);\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n      bins[firstLetter - 'a']++;\n    }\n  }\n}",
            "//Your code here\n\n    //create new vector to hold the first letter for each string\n    std::vector<std::string> s1;\n    for (auto& a: s) {\n        s1.push_back(a.substr(0, 1));\n    }\n    //get the total number of strings\n    int numStrings = s1.size();\n    //set up MPI\n    int rank;\n    int numProcs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n\n    //determine how many strings to process for each rank and send the first letter of each string to process 0\n    int numPerRank = numStrings / numProcs;\n    if (rank == 0) {\n        std::vector<std::string> firsts;\n        firsts.insert(firsts.end(), s1.begin(), s1.begin() + numPerRank);\n        //send the vector of first letters to process 0\n        MPI_Send(&firsts[0], numPerRank, MPI_CHAR, 0, 0, MPI_COMM_WORLD);\n        //get the number of strings to process for each rank\n        MPI_Bcast(&numPerRank, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n    else {\n        std::vector<std::string> firsts;\n        firsts.insert(firsts.end(), s1.begin() + (rank - 1) * numPerRank, s1.begin() + rank * numPerRank);\n        //send the vector of first letters to process 0\n        MPI_Send(&firsts[0], numPerRank, MPI_CHAR, 0, 0, MPI_COMM_WORLD);\n        MPI_Bcast(&numPerRank, 1, MPI_INT, 0, MPI_COMM_WORLD);\n        //set up bins for each rank\n        bins = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n        //determine where to start and stop\n        size_t startIndex = rank * numPerRank;\n        size_t stopIndex = startIndex + numPerRank;\n        size_t currentIndex = startIndex;\n        //loop through each letter and count the number of strings that start with that letter\n        while (currentIndex < stopIndex) {\n            std::string letter = s1[currentIndex];\n            if (letter == \"a\") {\n                bins[0] += 1;\n            }\n            else if (letter == \"b\") {\n                bins[1] += 1;\n            }\n            else if (letter == \"c\") {\n                bins[2] += 1;\n            }\n            else if (letter == \"d\") {\n                bins[3] += 1;\n            }\n            else if (letter == \"e\") {\n                bins[4] += 1;\n            }\n            else if (letter == \"f\") {\n                bins[5] += 1;\n            }\n            else if (letter == \"g\") {\n                bins[6] += 1;\n            }\n            else if (letter == \"h\") {\n                bins[7] += 1;\n            }\n            else if (letter == \"i\") {\n                bins[8] += 1;\n            }\n            else if (letter == \"j\") {\n                bins[9] += 1;\n            }\n            else if (letter == \"k\") {\n                bins[10] += 1;\n            }\n            else if (letter == \"l\") {\n                bins[11] += 1;\n            }\n            else if (letter == \"m\") {\n                bins[12] += 1;\n            }\n            else if (letter == \"n\") {\n                bins[13] += 1;\n            }",
            "size_t rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  size_t myCount = 0;\n  for (std::string const& str : s)\n    if (str[0] >= 'a' && str[0] <= 'z')\n      myCount++;\n\n  size_t total = (size_t)(myCount * size) / s.size();\n  bins[rank] = total;\n\n  if (rank == 0) {\n    std::array<size_t, 26> allBins;\n    MPI_Allreduce(bins.data(), allBins.data(), 26, MPI_LONG_LONG_INT, MPI_SUM, MPI_COMM_WORLD);\n    for (size_t i = 0; i < 26; i++)\n      bins[i] = allBins[i];\n  } else {\n    MPI_Reduce(bins.data(), NULL, 26, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int myrank, size;\n    MPI_Comm_size(comm, &size);\n    MPI_Comm_rank(comm, &myrank);\n\n    int len = s.size();\n    int* work = new int[len];\n    for (int i = 0; i < len; i++) {\n        work[i] = s[i][0] - 'a';\n    }\n    int* recv = new int[len];\n\n    MPI_Allreduce(work, recv, len, MPI_INT, MPI_SUM, comm);\n\n    for (int i = 0; i < len; i++) {\n        bins[recv[i]]++;\n    }\n\n    delete[] work;\n    delete[] recv;\n\n    if (myrank == 0) {\n        printArray(bins, 26);\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "cuda",
        "prompt": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\n__global__ void firstLetterCounts(const char **s, size_t N, size_t[26] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if(tid < N) {\n        char letter = s[tid][0];\n        atomicAdd(&bins[letter - 'a'], 1);\n    }\n}",
            "const int THREADS_PER_BLOCK = 1024;\n  const int numBlocks = (N + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n  // Initialize shared memory.\n  __shared__ int count[26];\n\n  for (int i = 0; i < 26; i++) {\n    count[i] = 0;\n  }\n\n  // Each thread counts the number of strings in the vector s that start with the same letter.\n  for (int i = 0; i < numBlocks; i++) {\n    int idx = threadIdx.x + i * THREADS_PER_BLOCK;\n\n    if (idx < N) {\n      count[s[idx][0] - 'a']++;\n    }\n  }\n\n  // Shared memory is now full with counts for the same letter. Each thread reduces the counts to 1 value.\n  for (int stride = THREADS_PER_BLOCK / 2; stride > 0; stride /= 2) {\n    __syncthreads();\n\n    if (threadIdx.x < stride) {\n      for (int i = 0; i < 26; i++) {\n        count[i] += count[i + stride];\n      }\n    }\n  }\n\n  // The last thread writes the result to the output array.\n  if (threadIdx.x == 0) {\n    for (int i = 0; i < 26; i++) {\n      bins[i] = count[i];\n    }\n  }\n}",
            "//TODO: Your code here\n    char c[26];\n    for(int i=0; i<26; i++){\n        c[i] = (char)i;\n    }\n    for(int i=0; i<26; i++){\n        bins[i] = 0;\n    }\n    for(int i=0; i<N; i++){\n        if(s[i][0]==c[threadIdx.x]){\n            bins[threadIdx.x]++;\n        }\n    }\n\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n    if (tid < N) {\n        char first = s[tid][0];\n        atomicAdd(&bins[first - 'a'], 1);\n    }\n}",
            "char firstLetter = 'a';\n\tunsigned int j = 0;\n\tfor (unsigned int i = 0; i < N; i++) {\n\t\tif (s[i][j]!= firstLetter) {\n\t\t\tif (firstLetter == 'z') {\n\t\t\t\tfirstLetter = 'a';\n\t\t\t\tj = 0;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tfirstLetter++;\n\t\t\t\tj = 0;\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\tif (firstLetter == 'z') {\n\t\t\t\tfirstLetter = 'a';\n\t\t\t\tj = 0;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tj++;\n\t\t\t}\n\t\t}\n\t\tbins[firstLetter - 'a']++;\n\t}\n}",
            "// TODO:\n}",
            "for (int i = threadIdx.x; i < 26; i += blockDim.x)\n        bins[i] = 0;\n\n    int n = N / blockDim.x;\n\n    int c = (threadIdx.x < N % blockDim.x)? s[threadIdx.x][0] : 0;\n    for (int i = 0; i < n; i++) {\n        c = (threadIdx.x + i * blockDim.x < N)? s[threadIdx.x + i * blockDim.x][0] : 0;\n        atomicAdd(&bins[c - 'a'], 1);\n    }\n\n    __syncthreads();\n\n    for (int i = threadIdx.x; i < 26; i += blockDim.x) {\n        bins[i] += bins[i - 1];\n    }\n\n    __syncthreads();\n\n    if (threadIdx.x < 26) {\n        for (int j = 0; j < n; j++) {\n            c = (threadIdx.x + j * blockDim.x < N)? s[threadIdx.x + j * blockDim.x][0] : 0;\n            bins[c - 'a'] -= 1;\n        }\n    }\n}",
            "int i;\n\tchar c;\n\tsize_t *count;\n\tcount = bins;\n\tfor(i = 0; i < N; i++)\n\t{\n\t\tc = tolower(s[i][0]);\n\t\tatomicAdd(&count[c], 1);\n\t}\n}",
            "size_t blockSize = blockDim.x * gridDim.x;\n  size_t threadOffset = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t i;\n  for (i = threadOffset; i < N; i += blockSize) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n  if (tid < N) {\n    int idx = (int) s[tid][0] - 'a';\n    atomicAdd(&bins[idx], 1);\n  }\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "// TODO: implement\n}",
            "int idx = threadIdx.x;\n    if (idx >= N) return;\n    const char *str = s[idx];\n    int bin = (int) str[0] - 'a';\n    bins[bin]++;\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid >= N)\n    return;\n  char letter = s[tid][0];\n  int idx = letter - 'a';\n  bins[idx]++;\n}",
            "// TODO\n}",
            "for (int i = threadIdx.x; i < N; i += blockDim.x) {\n\t\tbins[s[i][0] - 'a']++;\n\t}\n}",
            "// TODO: implement firstLetterCounts\n\n    int i = threadIdx.x;\n    int k = 0;\n    int alphabet = 26;\n    if(i < N){\n        while(k < alphabet){\n            if(s[i][0] == 'a' + k){\n                bins[k]++;\n                k = alphabet;\n            }\n            k++;\n        }\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    char c = 'a';\n\n    while (c <= 'z') {\n        if (tid < N && s[tid][0] == c) {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n        c++;\n    }\n}",
            "const char* thread_s = s[blockIdx.x * blockDim.x + threadIdx.x];\n    int index = thread_s[0] - 'a';\n    bins[index]++;\n}",
            "const char* input_strings[N];\n    for (int i = 0; i < N; i++) {\n        input_strings[i] = s[i];\n    }\n\n    // TODO: fill in the code here\n    for (int i = 0; i < N; i++) {\n        char first_letter = input_strings[i][0];\n        int num = 0;\n        for (int j = 0; j < N; j++) {\n            if (input_strings[j][0] == first_letter)\n                num++;\n        }\n        bins[first_letter - 'a'] = num;\n    }\n\n\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N) {\n        bins[s[idx][0] - 'a'] += 1;\n    }\n}",
            "__shared__ int temp[26];\n  int gtid = threadIdx.x;\n\n  if (gtid < 26) {\n    temp[gtid] = 0;\n  }\n\n  __syncthreads();\n\n  if (gtid < N) {\n    char firstLetter = s[gtid][0];\n    atomicAdd(&temp[firstLetter - 'a'], 1);\n  }\n\n  __syncthreads();\n\n  if (gtid < 26) {\n    bins[gtid] = temp[gtid];\n  }\n}",
            "// Your code here\n}",
            "// FIXME\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    int value = s[tid][0] - 'a';\n    atomicAdd(&bins[value], 1);\n  }\n}",
            "size_t tid = threadIdx.x;\n  int firstLetter = tolower(s[tid][0]);\n  int idx = firstLetter - 'a';\n  if (firstLetter >= 'a' && firstLetter <= 'z') {\n    atomicAdd(&bins[idx], 1);\n  }\n}",
            "// TODO\n}",
            "}",
            "int idx = threadIdx.x;\n  if (idx < 26) {\n    for (int i = 0; i < N; i++) {\n      bins[idx] += (s[i][0] == (char) (idx + 'a'));\n    }\n  }\n}",
            "}",
            "__shared__ size_t partial_sum[26];\n  int tid = threadIdx.x;\n  int i;\n  int sidx;\n\n  // Initialize partial sums to 0\n  partial_sum[tid] = 0;\n\n  // Reduce over all strings in parallel\n  for (sidx = blockIdx.x * blockDim.x + tid; sidx < N; sidx += blockDim.x * gridDim.x) {\n    partial_sum[tid] += (int)s[sidx][0];\n  }\n\n  // Aggregate partial sums\n  for (i = 1; i < 26; i <<= 1) {\n    __syncthreads();\n    if (tid < i) {\n      partial_sum[tid] += partial_sum[tid + i];\n    }\n  }\n\n  // Store result\n  if (tid == 0) {\n    for (i = 0; i < 26; i++)\n      bins[i] = partial_sum[i];\n  }\n}",
            "// Insert your code here.\n}",
            "//TODO\n}",
            "int idx = threadIdx.x + blockDim.x * blockIdx.x;\n  if (idx < N) {\n    int letter = s[idx][0] - 'a';\n    atomicAdd(&bins[letter], 1);\n  }\n}",
            "// TODO: replace 0 with appropriate value\n\tsize_t threadID = threadIdx.x + blockIdx.x * blockDim.x;\n\n\tif (threadID < N) {\n\t\t// TODO: increment corresponding bin for the first letter of s[threadID]\n\t}\n}",
            "char alphabet[26] = {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l','m', 'n', 'o', 'p', 'q', 'r','s', 't', 'u', 'v', 'w', 'x', 'y', 'z'};\n    int index = threadIdx.x;\n    int i = alphabet[index] - 97;\n    for(size_t j = 0; j < N; j++){\n        if(s[j][0] == alphabet[index]){\n            atomicAdd(&bins[i], 1);\n        }\n    }\n}",
            "//TODO\n}",
            "// TODO: Implement\n}",
            "int tid = threadIdx.x;\n    int ltid = tid;\n\n    while(ltid < N) {\n        int letter = (int)tolower(s[ltid][0]);\n        atomicAdd(&bins[letter], 1);\n        ltid += blockDim.x;\n    }\n}",
            "// This is just an example of a CUDA kernel that you need to write.\n    // The kernel launch is done in `main` function in `strings_test.cu`\n\n    int threadIndex = threadIdx.x;\n\n    // Each thread will process a letter.\n    // The index of the letter in the alphabet is threadIndex.\n\n    // For example, for threadIndex == 11 ('k'), the letter is 'k' and its index in the alphabet is 10.\n    // For threadIndex == 5 ('e'), the letter is 'e' and its index in the alphabet is 4.\n\n    if (threadIndex < 26) {\n        for (int i = 0; i < N; ++i) {\n            if (s[i][0] == threadIndex + 'a') {\n                atomicAdd(&bins[threadIndex], 1);\n            }\n        }\n    }\n}",
            "// TODO: Add code to implement the kernel\n}",
            "__shared__ int thread_num;\n\n    if (threadIdx.x == 0) {\n        thread_num = blockDim.x * blockIdx.x + threadIdx.x;\n    }\n    __syncthreads();\n\n    if (thread_num >= N) {\n        return;\n    }\n    const char first_letter = tolower(s[thread_num][0]);\n    __syncthreads();\n\n    if (first_letter >= 'a' && first_letter <= 'z') {\n        atomicAdd(&bins[first_letter - 'a'], 1);\n    }\n}",
            "const char *s_ = s[blockIdx.x * blockDim.x + threadIdx.x];\n  if (s_[0] >= 'a' && s_[0] <= 'z')\n    atomicAdd(&bins[s_[0] - 'a'], 1);\n}",
            "// TODO\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n\n\tif (idx >= N) {\n\t\treturn;\n\t}\n\n\tchar l = s[idx][0];\n\tatomicAdd(&bins[l - 'a'], 1);\n}",
            "int idx = threadIdx.x + blockDim.x * blockIdx.x;\n\tif (idx >= N) {\n\t\treturn;\n\t}\n\tconst char* str = s[idx];\n\tint idx_letter = 0;\n\twhile (*str) {\n\t\tidx_letter = *str - 'a';\n\t\tbins[idx_letter]++;\n\t\tstr++;\n\t}\n}",
            "// Your code here\n\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n\tint stride = blockDim.x * gridDim.x;\n\n\tfor (int i = tid; i < N; i += stride) {\n\t\tbins[s[i][0] - 'a'] += 1;\n\t}\n}",
            "unsigned int thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n    unsigned int stride = blockDim.x * gridDim.x;\n    unsigned int start = thread_id;\n    unsigned int end = N;\n\n    if (threadIdx.x == 0) {\n        printf(\"Starting firstLetterCounts kernel with blockIdx.x: %d, blockDim.x: %d, gridDim.x: %d\\n\", blockIdx.x, blockDim.x, gridDim.x);\n    }\n\n    for (unsigned int i = start; i < end; i += stride) {\n        //bins[s[i][0]-'a']++;\n        if (s[i][0] > 'Z') {\n            bins[s[i][0]-'a'-26]++;\n        } else {\n            bins[s[i][0]-'a']++;\n        }\n    }\n\n    if (threadIdx.x == 0) {\n        printf(\"Leaving firstLetterCounts kernel.\\n\");\n    }\n}",
            "/*\n  Fill in starting code\n  */\n}",
            "// TODO\n}",
            "}",
            "int idx = threadIdx.x;\n    int count = 0;\n    int alphabet[26];\n    for (int i=0; i<26; i++) alphabet[i] = 0;\n    for (int i=idx; i<N; i += blockDim.x) {\n        if (s[i][0] >= 'a' && s[i][0] <='z') alphabet[s[i][0] - 'a']++;\n    }\n    for (int i=0; i<26; i++) bins[i] += alphabet[i];\n}",
            "size_t letter = 0;\n    size_t count = 0;\n    for (int i = 0; i < N; ++i) {\n        letter = tolower(s[i][0]) - 'a';\n        if (letter >= 0 && letter <= 25) {\n            atomicAdd(&bins[letter], 1);\n        }\n    }\n}",
            "// TODO: implement kernel\n    for (int i = 0; i < N; i++) {\n        bins[s[i][0] - 97]++;\n    }\n}",
            "//TODO: Implement\n}",
            "const size_t thread_id = blockDim.x * blockIdx.x + threadIdx.x;\n\n\tif (thread_id < N)\n\t{\n\t\tfor (int i = 0; i < 26; i++)\n\t\t{\n\t\t\tif (s[thread_id][0] == 'a' + i)\n\t\t\t{\n\t\t\t\tbins[i]++;\n\t\t\t}\n\t\t}\n\t}\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  char letter = idx + 'a';\n  if (idx < 26) {\n    int count = 0;\n    for (size_t i = 0; i < N; i++) {\n      if (s[i][0] == letter) {\n        count++;\n      }\n    }\n    bins[idx] = count;\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int bid = blockIdx.x;\n    __shared__ size_t s_bins[26];\n    if (tid < 26) s_bins[tid] = 0;\n\n    __syncthreads();\n    for (int i = bid * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n            atomicAdd(&s_bins[s[i][0] - 'a'], 1);\n        }\n    }\n\n    __syncthreads();\n    if (tid < 26) bins[tid] = s_bins[tid];\n}",
            "int thread_id = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (thread_id < N) {\n        size_t bin = (size_t)s[thread_id][0] - 'a';\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    int letter = s[idx][0];\n    atomicAdd(&bins[letter - 'a'], 1);\n  }\n}",
            "// TODO\n    int i, j, k;\n    __shared__ int s_temp[26];\n    if (threadIdx.x == 0) {\n        for (i = 0; i < 26; i++) {\n            s_temp[i] = 0;\n        }\n    }\n    __syncthreads();\n    for (i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        j = s[i][0] - 'a';\n        s_temp[j]++;\n    }\n    __syncthreads();\n    for (i = threadIdx.x; i < 26; i += blockDim.x) {\n        atomicAdd(&bins[i], s_temp[i]);\n    }\n}",
            "// TODO\n}",
            "size_t thread_idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (thread_idx < N) {\n    char firstLetter = s[thread_idx][0];\n    bins[firstLetter - 'a'] += 1;\n  }\n}",
            "//TODO: Your code here\n}",
            "const char *ss[26] = {0};\n  int thread_id = threadIdx.x + blockDim.x * blockIdx.x;\n  int i;\n  while (thread_id < N) {\n    ss[s[thread_id][0] - 'a']++;\n    thread_id += blockDim.x * gridDim.x;\n  }\n  __syncthreads();\n  for (i = 0; i < 26; i++) {\n    atomicAdd(&bins[i], ss[i]);\n  }\n}",
            "int i, j;\n    int start_idx = blockIdx.x*blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n\n    for (i = start_idx; i < N; i += stride) {\n        bins[s[i][0] - 97] += 1;\n    }\n}",
            "}",
            "// TODO: Fill this in.\n}",
            "// TODO\n}",
            "// TODO\n}",
            "const int tid = threadIdx.x;\n    const int bid = blockIdx.x;\n    const int nthreads = blockDim.x;\n\n    if (bid * nthreads + tid >= N)\n        return;\n\n    char s_letter = s[bid][tid];\n    if (s_letter!= '\\0') {\n        bins[s_letter - 'a']++;\n    }\n}",
            "if(threadIdx.x >= N)\n        return;\n\n    char first = s[threadIdx.x][0];\n    atomicAdd(&bins[first - 'a'], 1);\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    bins[(int)s[tid][0] - (int)'a']++;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        char letter = s[i][0];\n        atomicAdd(&bins[letter - 'a'], 1);\n    }\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n    if (tid >= N)\n        return;\n    int bin = (int) s[tid][0] - (int) 'a';\n    bins[bin]++;\n}",
            "// TODO: implement\n}",
            "// TODO: implement firstLetterCounts\n}",
            "int i = threadIdx.x;\n    // check if thread ID is in range of number of strings\n    if (i < N) {\n        // compute the letter index for the given string\n        int idx = (int)s[i][0] - 97;\n        // assign the bins value\n        atomicAdd(&bins[idx], 1);\n    }\n}",
            "// start with your code here\n}",
            "// Compute thread ID\n    int tid = threadIdx.x + blockDim.x * blockIdx.x;\n\n    // Thread is within range of the input array\n    if (tid < N) {\n\n        // Calculate the letter index\n        int c = 0;\n        char a = tolower(s[tid][0]);\n        if (a >= 97 && a <= 122) {\n            c = (int)a - 96;\n        }\n\n        // Increment letter count in the output array\n        bins[c]++;\n    }\n}",
            "//TODO\n  \n}",
            "int threadIdx = threadIdx.x;\n  int index = blockIdx.x;\n  char letter;\n  if (index < N && threadIdx < 26) {\n    letter = s[index][threadIdx];\n    if (letter == 'a' || letter == 'b' || letter == 'c' || letter == 'd' || letter == 'e' ||\n        letter == 'f' || letter == 'g' || letter == 'h' || letter == 'i' || letter == 'j' ||\n        letter == 'k' || letter == 'l' || letter =='m' || letter == 'n' || letter == 'o' ||\n        letter == 'p' || letter == 'q' || letter == 'r' || letter =='s' || letter == 't' ||\n        letter == 'u' || letter == 'v' || letter == 'w' || letter == 'x' || letter == 'y' ||\n        letter == 'z') {\n      atomicAdd(&bins[letter - 'a'], 1);\n    }\n  }\n}",
            "__shared__ size_t smem[26];\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n\n  // clear shared memory\n  if (tid < 26) {\n    smem[tid] = 0;\n  }\n  __syncthreads();\n\n  // parallel loop over all strings\n  int i = bid * blockDim.x + tid;\n  while (i < N) {\n    smem[s[i][0] - 'a']++;\n    i += blockDim.x * gridDim.x;\n  }\n  __syncthreads();\n\n  // sum over shared memory\n  int n = blockDim.x / 2;\n  int i = tid;\n  while (n > 0) {\n    if (i < n) {\n      smem[i] += smem[i + n];\n    }\n    n >>= 1;\n    i += n;\n  }\n  __syncthreads();\n\n  // copy results to global memory\n  if (tid == 0) {\n    for (int i = 0; i < 26; ++i) {\n      bins[i] = smem[i];\n    }\n  }\n}",
            "int tid = threadIdx.x + blockDim.x*blockIdx.x;\n  if (tid >= N) {\n    return;\n  }\n\n  char letter = s[tid][0];\n  atomicAdd(&bins[letter - 'a'], 1);\n}",
            "for (unsigned int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n\t\tif (s[i][0] > 'z' || s[i][0] < 'a') {\n\t\t\tcontinue;\n\t\t}\n\n\t\tint idx = s[i][0] - 'a';\n\t\tatomicAdd(&bins[idx], 1);\n\t}\n}",
            "int idx = blockIdx.x*blockDim.x+threadIdx.x;\n\t\n\tif (idx >= N)\n\t\treturn;\n\n\tint offset = 0;\n\tfor (int i = 0; i < 26; i++) {\n\t\tif (s[idx][0] == offset + 'a') {\n\t\t\tatomicAdd(&bins[i], 1);\n\t\t\treturn;\n\t\t}\n\n\t\toffset += 26;\n\t}\n}",
            "// TODO: implement\n}",
            "const size_t idx = threadIdx.x;\n    const char letter = 'a' + idx;\n    for (size_t i = 0; i < N; ++i) {\n        if (s[i][0] == letter)\n            atomicAdd(&bins[idx], 1);\n    }\n}",
            "__shared__ char firsts[BLOCK_SIZE];\n  int thread_id = threadIdx.x + blockIdx.x * blockDim.x;\n  if (thread_id < N) {\n    char c = tolower(s[thread_id][0]);\n    if (c >= 'a' && c <= 'z') {\n      firsts[threadIdx.x] = c - 'a';\n      firsts[threadIdx.x] = atomicAdd(&bins[firsts[threadIdx.x]], 1);\n    }\n  }\n}",
            "for (size_t idx = threadIdx.x; idx < N; idx += blockDim.x) {\n        char firstLetter = s[idx][0];\n        // printf(\"%c\", firstLetter);\n        atomicAdd(&bins[firstLetter - 'a'], 1);\n    }\n}",
            "size_t index = threadIdx.x;\n\n    __shared__ char text[26];\n    __shared__ int textLength;\n\n    // First thread in block copies the string into shared memory\n    if (index == 0) {\n        for (int i = 0; i < 26; ++i) {\n            text[i] = s[blockIdx.x][i];\n        }\n\n        textLength = strlen(s[blockIdx.x]);\n    }\n\n    __syncthreads();\n\n    // All threads in the block count the number of occurrences\n    if (textLength > 0) {\n        int letterCount = 0;\n\n        for (int i = 0; i < textLength; ++i) {\n            char character = text[i];\n            if (character >= 'a' && character <= 'z') {\n                letterCount += (text[i] == s[blockIdx.x][i]);\n            }\n        }\n\n        atomicAdd(bins + letterCount - 1, 1);\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid >= N) {\n    return;\n  }\n  bins[(int)s[tid][0] - 'a']++;\n}",
            "const char *str = s[blockIdx.x * blockDim.x + threadIdx.x];\n  const size_t offset = threadIdx.x;\n  __shared__ char c[26];\n  __shared__ size_t len;\n  __shared__ int index;\n\n  if (threadIdx.x == 0) {\n    len = strlen(str);\n    if (len < 26)\n      printf(\"string too short\\n\");\n    else {\n      c[0] = str[0];\n      for (int i = 1; i < 26; i++)\n        c[i] = (i < len)? str[i] : 0;\n    }\n    index = 0;\n    bins[0] = 0;\n  }\n  __syncthreads();\n\n  if (threadIdx.x < 26 && offset < len) {\n    if (c[offset] >= 'a' && c[offset] <= 'z')\n      atomicAdd(&bins[c[offset] - 'a'], 1);\n  }\n  __syncthreads();\n}",
            "int i = threadIdx.x + blockDim.x * blockIdx.x;\n  if (i < N) {\n    char letter = tolower(s[i][0]);\n    atomicAdd(&bins[letter - 'a'], 1);\n  }\n}",
            "char first_letter = s[0][0];\n    int i = threadIdx.x;\n    if (i < N) {\n        bins[first_letter - 'a']++;\n    }\n}",
            "int gtid = threadIdx.x + blockIdx.x * blockDim.x;\n  // TODO\n  // __syncthreads();\n  if (gtid < N) {\n    if (s[gtid][0] <= 'z' && s[gtid][0] >= 'a') {\n      atomicAdd(&bins[(int)s[gtid][0] - 'a'], 1);\n    }\n  }\n}",
            "// Write your solution here\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int bin = s[tid][0] - 'a';\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n    char c = tolower(s[i][0]);\n    if (c >= 'a' && c <= 'z') {\n      bins[c - 'a'] += 1;\n    }\n  }\n}",
            "__shared__ size_t counts[26];\n\n    for (size_t i = 0; i < 26; i++) {\n        counts[i] = 0;\n    }\n\n    for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n        if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n            counts[s[i][0] - 'a']++;\n        }\n    }\n\n    __syncthreads();\n\n    for (size_t i = threadIdx.x; i < 26; i += blockDim.x) {\n        bins[i] = counts[i];\n    }\n}",
            "unsigned int i, j;\n   __shared__ char letters[26];\n   __shared__ size_t cnt[26];\n   // Initialize the cnt array to 0 and letters array to alphabet\n   for(i = 0; i < 26; i++) {\n      cnt[i] = 0;\n      letters[i] = 'a' + i;\n   }\n   // Increase the counter for each string that starts with that letter\n   for(i = threadIdx.x; i < N; i += blockDim.x) {\n      for(j = 0; j < 26; j++) {\n         if(s[i][0] == letters[j]) {\n            atomicAdd(&cnt[j], 1);\n            break;\n         }\n      }\n   }\n   __syncthreads();\n   // Store the result in the bins array\n   for(j = threadIdx.x; j < 26; j += blockDim.x) {\n      bins[j] = cnt[j];\n   }\n}",
            "char letter;\n    int tid = threadIdx.x + blockDim.x * blockIdx.x;\n    while (tid < N) {\n        letter = (s[tid])[0];\n        atomicAdd(&bins[letter - 'a'], 1);\n        tid += blockDim.x * gridDim.x;\n    }\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (idx >= N) return;\n\n\tint count = 0;\n\tfor (int i = 0; i < 26; i++) {\n\t\tif (s[idx][0] == i + 'a') {\n\t\t\tcount++;\n\t\t}\n\t}\n\tbins[idx] = count;\n}",
            "}",
            "// your code here\n}",
            "__shared__ size_t bins_[26];\n    if (threadIdx.x < 26) bins_[threadIdx.x] = 0;\n\n    size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n    for (; i < N; i += gridDim.x * blockDim.x) {\n        char letter = s[i][0];\n        int bin = letter - 'a';\n        atomicAdd(&bins_[bin], 1);\n    }\n\n    __syncthreads();\n    for (int i = threadIdx.x; i < 26; i += blockDim.x) {\n        atomicAdd(&bins[i], bins_[i]);\n    }\n}",
            "size_t threadNum = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (threadNum < N) {\n\t\tchar letter = s[threadNum][0];\n\t\tatomicAdd(&bins[letter - 'a'], 1);\n\t}\n}",
            "// YOUR CODE HERE\n\tint threadIdx = blockDim.x * blockIdx.x + threadIdx.x;\n\tif(threadIdx >= N) return;\n\tbins[s[threadIdx][0] - 'a']++;\n}",
            "//...\n}",
            "//TODO: implement the function\n}",
            "int thread_idx = threadIdx.x;\n  int block_idx = blockIdx.x;\n  int stride = blockDim.x;\n  int i;\n\n  for (i = thread_idx + block_idx * stride; i < N; i += stride * gridDim.x) {\n    int start = s[i][0];\n    atomicAdd(&(bins[start - 'a']), 1);\n  }\n}",
            "//TODO\n}",
            "size_t tid = threadIdx.x;\n\tconst size_t bid = blockIdx.x;\n\tif (bid >= N) return;\n\tconst size_t tid_offset = bid * blockDim.x;\n\n\t// Get the string\n\tconst char *str = s[tid_offset + tid];\n\n\t// If this thread has the final string\n\tif (tid == blockDim.x - 1) {\n\t\t// Store the last string in the global bins array\n\t\tbins[str[0] - 'a'] += 1;\n\t}\n}",
            "int idx = threadIdx.x;\n    char letter = 'a' + idx;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (s[i][0] == letter) {\n            atomicAdd(&bins[idx], 1);\n        }\n    }\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    bins[s[tid][0] - 'a'] += 1;\n  }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "__shared__ int partial[26];\n\n  if (threadIdx.x == 0) {\n    for (int i = 0; i < 26; i++)\n      partial[i] = 0;\n  }\n  __syncthreads();\n\n  char firstLetter = threadIdx.x + 'a';\n\n  if (threadIdx.x < N) {\n    char first = s[threadIdx.x][0];\n    if (first == firstLetter) {\n      atomicAdd(&partial[first - 'a'], 1);\n    }\n  }\n  __syncthreads();\n  if (threadIdx.x < 26) {\n    atomicAdd(&bins[threadIdx.x], partial[threadIdx.x]);\n  }\n}",
            "// TODO:\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    bins[s[tid][0]-'a']++;\n  }\n}",
            "}",
            "//TODO\n}",
            "// TODO\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        const char *c = s[tid];\n        if (*c >= 'a' && *c <= 'z')\n            atomicAdd(&bins[*c - 'a'], 1);\n    }\n}",
            "int threadId = blockDim.x * blockIdx.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  for (int i = threadId; i < N; i += stride) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "// TODO\n}",
            "unsigned int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "unsigned int tid = threadIdx.x;\n    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n    unsigned int stride = blockDim.x * gridDim.x;\n    unsigned int id;\n\n    for ( ; i < N; i += stride)\n    {\n        id = s[i][0] - 'a';\n        atomicAdd(&bins[id], 1);\n    }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (i < N) {\n\t\t//int start = i * 4;\n\t\tint c = s[i][0];\n\t\tint b = c - 'a';\n\t\tatomicAdd(&bins[b], 1);\n\t}\n}",
            "// TODO\n}",
            "int threadIndex = threadIdx.x + blockIdx.x * blockDim.x;\n    int totalThreads = blockDim.x * gridDim.x;\n\n    for (size_t i = threadIndex; i < N; i += totalThreads) {\n        bins[s[i][0] - 'a'] += 1;\n    }\n}",
            "//TODO: Implement the firstLetterCounts function here\n\n}",
            "}",
            "}",
            "/*\n       TODO: Implement this function\n    */\n}",
            "char c = 'a';\n\tfor (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n\t\tif (s[i][0] == c) {\n\t\t\tatomicAdd(&bins[s[i][0] - 'a'], 1);\n\t\t}\n\t}\n}",
            "}",
            "int threadId = blockIdx.x*blockDim.x + threadIdx.x;\n  if (threadId < N) {\n    int c = s[threadId][0];\n    bins[c - 'a']++;\n  }\n}",
            "}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  for (size_t i = 0; i < 26; i++)\n    bins[i] = 0;\n  if (idx < N) {\n    char ch = tolower(s[idx][0]);\n    atomicAdd(&bins[ch - 'a'], 1);\n  }\n}",
            "int thread_id = blockDim.x * blockIdx.x + threadIdx.x;\n\t// TODO: write code here\n}",
            "// Insert your code here\n    int thread_id = threadIdx.x + blockDim.x * blockIdx.x;\n    int stride = blockDim.x * gridDim.x;\n\n    for (int i = thread_id; i < N; i += stride) {\n        char letter = s[i][0];\n        atomicAdd(&bins[letter - 97], 1);\n    }\n}",
            "// TODO: Your code here\n    size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n    if (tid < N) {\n        bins[(s[tid][0] - 'a')]++;\n    }\n}",
            "int tid = threadIdx.x;\n\t__shared__ char s_array[N];\n\n\tif (tid < N) {\n\t\ts_array[tid] = s[tid][0];\n\t}\n\n\t__syncthreads();\n\n\tint stride = blockDim.x;\n\tint i = blockDim.x * blockIdx.x + threadIdx.x;\n\n\twhile (i < 26) {\n\t\tif (tid < N) {\n\t\t\tif (s_array[tid] == (char)(i + 'a')) {\n\t\t\t\tatomicAdd(&bins[i], 1);\n\t\t\t}\n\t\t}\n\n\t\t__syncthreads();\n\t\ti += stride;\n\t}\n}",
            "//TODO: Implement first letter counts\n\tchar ch;\n\tsize_t letterIndex;\n\tint threadIndex = threadIdx.x;\n\tsize_t tid = threadIndex + blockDim.x * blockIdx.x;\n\n\tif (tid < N) {\n\t\tch = tolower(s[tid][0]);\n\t\tletterIndex = ch - 'a';\n\t\tatomicAdd(&bins[letterIndex], 1);\n\t}\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N)\n        return;\n    char first = tolower(s[tid][0]);\n    atomicAdd(&bins[first-'a'], 1);\n}",
            "//TODO\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (i >= N) {\n    return;\n  }\n\n  // \n  // 1) Find the first letter in the string.\n  // 2) Add 1 to the appropriate bin.\n  // \n  // TODO\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n  size_t start = tid;\n  while (start < N) {\n    char ch = s[start][0];\n    ch = tolower(ch);\n    atomicAdd(&bins[ch - 'a'], 1);\n    start += blockDim.x * gridDim.x;\n  }\n}",
            "// TODO\n}",
            "size_t i = threadIdx.x;\n  if (i >= 26) {\n    return;\n  }\n\n  bins[i] = 0;\n\n  for (size_t j = 0; j < N; j++) {\n    if (s[j][0] == 'a' + i) {\n      bins[i]++;\n    }\n  }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        bins[s[tid][0] - 'a'] += 1;\n    }\n}",
            "int g_tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (g_tid < N) {\n    int c = tolower(s[g_tid][0]);\n    atomicAdd(&bins[c - 'a'], 1);\n  }\n}",
            "char alphabet[26] = {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l','m', 'n', 'o', 'p', 'q', 'r','s', 't', 'u', 'v', 'w', 'x', 'y', 'z'};\n  //__shared__ char alphabet[26];\n  __shared__ size_t binCount[26];\n\n  int id = threadIdx.x;\n  if(id < 26){\n    binCount[id] = 0;\n  }\n\n  __syncthreads();\n\n  for(int i = 0; i < N; i++){\n    int idx = alphabet[(int)s[i][0] - 97];\n    if(id == idx){\n      binCount[idx]++;\n    }\n  }\n\n  __syncthreads();\n\n  if(id < 26){\n    for(int i = 0; i < 26; i++){\n      bins[i] = bins[i] + binCount[i];\n    }\n  }\n}",
            "// TODO: YOUR CODE GOES HERE\n    // Use thread id to access the string at that index\n    int index = threadIdx.x;\n    // if thread id is less than N\n    if(index < N) {\n        // convert the string to lower case\n        char c = tolower(s[index][0]);\n        // if the character is between A-Z\n        if (c >= 'a' && c <= 'z') {\n            // increment the count at that index by one\n            atomicAdd(&bins[c-'a'], 1);\n        }\n    }\n}",
            "// TODO: Your code here\n\n}",
            "const char *s_start = s[blockIdx.x * blockDim.x + threadIdx.x];\n    if (s_start) {\n        bins[s_start[0] - 'a']++;\n    }\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n\t//char c;\n\t//if(tid < N){\n\t\t//c = s[tid][0];\n\t\t//if(c >= 97 && c <= 122){\n\t\t\t//atomicAdd(&bins[c - 97], 1);\n\t\t//}\n\t//}\n\t__syncthreads();\n}",
            "int firstLetter = threadIdx.x;\n   int i;\n\n   for(i = 0; i < N; i++) {\n      if(s[i][0] == firstLetter) {\n         bins[firstLetter]++;\n      }\n   }\n}",
            "char c = s[threadIdx.x][0];\n  int idx = c - 'a';\n  __shared__ size_t shared[26];\n  if (threadIdx.x == 0) {\n    for (int i = 0; i < 26; i++) {\n      shared[i] = 0;\n    }\n  }\n  __syncthreads();\n  shared[idx]++;\n  __syncthreads();\n  if (threadIdx.x == 0) {\n    for (int i = 0; i < 26; i++) {\n      bins[i] = shared[i];\n    }\n  }\n}",
            "int idx = threadIdx.x;\n    for (int i = idx; i < N; i += blockDim.x) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "char letter = 'a';\n    size_t idx = threadIdx.x + blockIdx.x*blockDim.x;\n    while(idx < N){\n        letter = (s[idx])[0];\n        bins[letter - 'a']++;\n        idx += blockDim.x*gridDim.x;\n    }\n}",
            "const int thread_id = threadIdx.x + blockIdx.x * blockDim.x;\n  if (thread_id < N) {\n    bins[s[thread_id][0] - 'a']++;\n  }\n}",
            "// TODO: implement this function\n    // HINT: each block handles a letter (e.g., \"a\", \"b\", \"c\", etc.)\n    // HINT: each thread (blockDim.x) handles a string\n    // HINT: use a shared array\n    // HINT: the array is indexed by the letter (0 = \"a\", 1 = \"b\", etc.)\n    // HINT: the array value is the count of strings starting with that letter\n    // HINT: the letter is extracted from the first character of the string\n    // HINT: the letter is converted to lower case\n    // HINT: the letter is between 'a' and 'z'\n\n    // This is the letter we are interested in.\n    const int letter_to_count = blockIdx.x;\n    const int thread_idx = threadIdx.x;\n\n    // This is the size of the blocks in each dimension.\n    const int block_width = blockDim.x;\n    const int block_height = blockDim.y;\n\n    // This is the position of the thread in the block.\n    const int thread_x = threadIdx.x;\n    const int thread_y = threadIdx.y;\n\n    // This is the position of the block in the grid.\n    const int block_x = blockIdx.x;\n    const int block_y = blockIdx.y;\n\n    // This is the size of the grid.\n    const int grid_width = gridDim.x;\n    const int grid_height = gridDim.y;\n\n    // This is the size of the grid.\n    const int grid_size = grid_width * grid_height;\n\n    // This is the index of the string in the global vector.\n    const int string_index = block_x * block_width + thread_x;\n\n    // We need to initialize the shared array.\n    if(thread_idx == 0){\n        bins[letter_to_count] = 0;\n    }\n\n    // Synchronize the threads.\n    __syncthreads();\n\n    // Do we have any strings to process?\n    if (string_index >= N){\n        return;\n    }\n\n    // How many strings do we have to process?\n    const int total_strings_to_process = N / block_width;\n\n    // We need to process all the strings in this block.\n    for(int i = thread_idx; i < total_strings_to_process; i+=block_width){\n\n        // The string to be processed.\n        const char *current_string = s[string_index + i];\n\n        // This is the first letter of the string.\n        const char first_letter = current_string[0];\n\n        // This is the index of the string in the shared array.\n        const int string_index_in_shared_array = first_letter - 'a';\n\n        // Increase the counter.\n        bins[letter_to_count]++;\n\n    }\n\n}",
            "// TODO: implement\n    return;\n}",
            "// TODO\n\n}",
            "// TODO\n}",
            "char c = 'a';\n  int i = 0;\n  while (i < N) {\n    c = tolower(s[i][0]);\n    if (c < 'a' || c > 'z') c = 'a';\n    atomicAdd(&bins[c - 'a'], 1);\n    i++;\n  }\n}",
            "int tID = threadIdx.x + blockIdx.x * blockDim.x;\n  int stride = blockDim.x * gridDim.x;\n  if(tID >= N) return;\n  int count = 0;\n  while(tID < N) {\n    char c = s[tID][0];\n    c = c - 'a';\n    if(c >= 0 && c < 26)\n      atomicAdd(&bins[c], 1);\n    tID += stride;\n  }\n}",
            "size_t tid = threadIdx.x;\n  size_t idx = blockIdx.x;\n\n  if (idx < N && s[idx][0]!= '\\0') {\n    bins[s[idx][0] - 97] += 1;\n  }\n}",
            "// YOUR CODE HERE\n  return;\n}",
            "// TODO: launch at least N threads.\n    // TODO: count the number of strings in the vector that start with each letter.\n    // TODO: use only global memory (no shared memory).\n    // TODO: place the result in the `bins` array.\n\n}",
            "// YOUR CODE HERE\n}",
            "int threadIndex = threadIdx.x + blockIdx.x * blockDim.x;\n\t// check if thread index is not exceeding the input size.\n\tif (threadIndex < N) {\n\t\t// convert input to lowercase.\n\t\tchar letter = tolower(s[threadIndex][0]);\n\t\t// check if it is a valid letter or not.\n\t\tif (letter < 'a' || letter > 'z') {\n\t\t\tbins[0]++;\n\t\t\treturn;\n\t\t}\n\t\t// get index of the letter.\n\t\tint index = letter - 'a';\n\t\t// increment the counter by one.\n\t\tatomicAdd(&bins[index], 1);\n\t}\n}",
            "// TODO: Your code here\n    return;\n}",
            "}",
            "for (int i = threadIdx.x; i < N; i += blockDim.x) {\n    int letter = s[i][0] - 'a';\n    atomicAdd(&bins[letter], 1);\n  }\n}",
            "// TODO: implement\n}",
            "// TODO: launch a kernel with at least N threads\n    // TODO: count the number of strings in the vector s that start with each letter in the alphabet\n    // TODO: store the output in `bins` array\n}",
            "const char *start = s[threadIdx.x];\n  unsigned int idx = start[0] - 'a';\n  atomicAdd(&bins[idx], 1);\n}",
            "int threadId = threadIdx.x + blockIdx.x * blockDim.x;\n  int stride = blockDim.x * gridDim.x;\n\n  for (int tid = threadId; tid < N; tid += stride) {\n    int first_letter_index = s[tid][0] - 'a';\n    atomicAdd(&bins[first_letter_index], 1);\n  }\n}",
            "const char *const firstLetter = s[blockIdx.x];\n  const size_t threadIdx_local = threadIdx.x;\n  if (threadIdx_local < 26) {\n    size_t count = 0;\n    for (int i = 0; i < N; i++) {\n      if (firstLetter[i] >= 'a' && firstLetter[i] <= 'z') {\n        if (firstLetter[i] - 'a' == threadIdx_local) {\n          count++;\n        }\n      }\n    }\n    bins[threadIdx_local] = count;\n  }\n}",
            "size_t tid = threadIdx.x;\n  char letter = 'a' + tid;\n  for (size_t i = tid; i < N; i += blockDim.x)\n    if (s[i][0] == letter)\n      atomicAdd(&bins[tid], 1);\n}",
            "int threadId = threadIdx.x;\n  const char **input = s;\n\n  if (threadId < N) {\n    int index = input[threadId][0] - 'a';\n    bins[index]++;\n  }\n}",
            "int threadID = threadIdx.x;\n  int blockID = blockIdx.x;\n  size_t index = threadID + blockID * 512;\n  if (index < N) {\n    bins[s[index][0] - 97]++;\n  }\n}",
            "for (int i = 0; i < N; i++) {\n    int offset = 1;\n    char temp = s[i][offset];\n    if (temp >= 'a' && temp <= 'z') {\n      atomicAdd(&bins[temp - 'a'], 1);\n    }\n  }\n}",
            "int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (threadId < N) {\n        bins[s[threadId][0] - 'a']++;\n    }\n}",
            "__shared__ char* myString[1024];\n\n\t// Copy strings to myString\n\tif (threadIdx.x == 0)\n\t{\n\t\tint index = blockIdx.x*1024 + threadIdx.x;\n\t\tif (index < N)\n\t\t{\n\t\t\tmyString[threadIdx.x] = s[index];\n\t\t}\n\t}\n\t__syncthreads();\n\n\t// Compute counts\n\tfor (int i = threadIdx.x; i < 26; i += blockDim.x)\n\t{\n\t\tint index = blockIdx.x*1024 + threadIdx.x;\n\t\tint myCount = 0;\n\t\twhile (index < N && myString[index]!= NULL)\n\t\t{\n\t\t\tif (myString[index][0] == i + 'a')\n\t\t\t\tmyCount++;\n\t\t\tindex += blockDim.x;\n\t\t}\n\t\t// Atomically add to bin\n\t\tatomicAdd(&bins[i], myCount);\n\t}\n}",
            "/*\n    Hint: Use an array of 26 threads to count the number of strings in the vector that start with each letter.\n    The letter that each thread is counting is determined by its thread id (e.g. thread 3 counts \"f\" and \"f\" only).\n  */\n\n  /*\n    Hint: Each thread should load a string from global memory and then check to see if the first letter is that thread's letter.\n    If so, the thread should increment a shared variable that counts the number of strings that start with that letter.\n    The threads should then loop through all strings in the vector.\n  */\n\n  // TODO: Implement\n}",
            "// TODO\n}",
            "int idx = threadIdx.x;\n    if (idx < 26) {\n        bins[idx] = 0;\n        for (size_t i = 0; i < N; ++i) {\n            if (s[i][0] == ('a' + idx)) {\n                bins[idx]++;\n            }\n        }\n    }\n}",
            "// TODO\n  char ch;\n  int bin_val = 0;\n  for(int i=0; i<N; i++) {\n    ch = s[i][0];\n    bin_val = (int)(ch - 'a');\n    atomicAdd(&bins[bin_val], 1);\n  }\n}",
            "/*\n\t * TODO\n\t */\n\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (i < N) {\n        int firstLetter = s[i][0] - 'a';\n        atomicAdd(&bins[firstLetter], 1);\n    }\n}",
            "// TODO: Your code here\n\n}",
            "//TODO\n}",
            "unsigned int tid = threadIdx.x + blockIdx.x*blockDim.x;\n    if (tid < N) {\n        char c = s[tid][0];\n        if (c >= 'a' && c <= 'z') {\n            atomicAdd(&bins[c-'a'], 1);\n        }\n    }\n}",
            "//TODO\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx >= N) return;\n  for (int i = 0; i < 26; ++i) bins[i] = 0;\n  for (int i = 0; i < 26; ++i) {\n    if (s[idx][0] == 'a' + i) {\n      atomicAdd(&bins[i], 1);\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of s. The result is stored in bins on rank 0.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// fill in code\n\n    // ensure we have the right number of entries in the array\n    bins = std::array<size_t, 26>();\n\n    // initialize bins to all zeroes\n    for(int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n\n    // count the bins\n    for(int i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n\n}",
            "size_t const n = s.size();\n    size_t nthreads = 1;\n    #pragma omp parallel\n    {\n        nthreads = omp_get_num_threads();\n    }\n    std::array<size_t, 26> tbins;\n    tbins.fill(0);\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; i++) {\n        size_t id = omp_get_thread_num();\n        tbins[s[i][0] - 'a']++;\n    }\n    for (size_t i = 0; i < nthreads; i++) {\n        size_t k = 0;\n        for (size_t j = 0; j < 26; j++) {\n            bins[k] += tbins[j];\n            k++;\n        }\n    }\n    if (nthreads > 1) {\n        size_t k = 0;\n        for (size_t j = 0; j < 26; j++) {\n            bins[k] = 0;\n            k++;\n        }\n        for (size_t i = 0; i < n; i++) {\n            size_t id = omp_get_thread_num();\n            tbins[s[i][0] - 'a']++;\n            if (id == nthreads - 1) {\n                k = 0;\n                for (size_t j = 0; j < 26; j++) {\n                    bins[k] += tbins[j];\n                    k++;\n                }\n            }\n        }\n    }\n}",
            "for (int i=0; i<s.size(); i++)\n        bins[s[i][0]-'a']++;\n}",
            "// TODO: Your code here\n    int my_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    int n_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n    int n_strings = s.size();\n    int n_strings_per_rank = n_strings/n_ranks;\n    std::array<size_t, 26> counts{};\n    std::array<size_t, 26> counts_local{};\n    // Counting for each letter\n    #pragma omp parallel for\n    for (int i = 0; i < n_strings; ++i) {\n        auto letter = s[i][0];\n        counts[letter-97]++;\n    }\n    // Reduction\n    MPI_Reduce(counts.data(), counts_local.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if(my_rank==0)\n        bins=counts_local;\n\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    const size_t local_size = s.size();\n    std::array<size_t, 26> local_counts;\n    std::fill(local_counts.begin(), local_counts.end(), 0);\n\n    // compute local counts\n    for (auto& str : s) {\n        local_counts[str[0] - 'a']++;\n    }\n\n    // exchange counts\n    std::array<size_t, 26> recv_counts;\n    MPI_Allreduce(local_counts.data(), recv_counts.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    // copy to output\n    bins = recv_counts;\n}",
            "const int MPI_COMM_WORLD = MPI_COMM_WORLD;\n  const int rank = omp_get_thread_num();\n  const int n_threads = omp_get_num_threads();\n  const int n_ranks = omp_get_num_procs();\n\n  const int n_letters = 26;\n\n  const size_t n = s.size();\n  size_t n_letters_per_rank = n / n_ranks;\n\n  size_t start_index = rank * n_letters_per_rank;\n\n  if (rank == n_ranks - 1) {\n    n_letters_per_rank = n - (n_ranks - 1) * n_letters_per_rank;\n  }\n\n  std::array<size_t, 26> local_bins{0};\n\n  for (size_t i = 0; i < n_letters_per_rank; ++i) {\n    local_bins[s[start_index + i][0] - 'a']++;\n  }\n\n  std::array<size_t, 26> global_bins{0};\n\n  MPI_Reduce(local_bins.data(), global_bins.data(), n_letters, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  bins = global_bins;\n}",
            "// TODO: Your code here\n    // Hint: use MPI_Gather, MPI_Status, MPI_Get_count\n\n}",
            "int n = s.size();\n    int rank = omp_get_thread_num();\n    int comm_sz = omp_get_num_threads();\n\n    int* r_start = new int[comm_sz+1];\n    int* r_end = new int[comm_sz];\n\n    if (rank == 0) {\n        r_start[0] = 0;\n        r_end[0] = 0;\n        for (int i = 1; i < comm_sz+1; i++) {\n            r_start[i] = r_end[i-1];\n            r_end[i] = std::min(r_start[i]+n/comm_sz, n);\n        }\n    }\n\n    std::string s_word = \"\";\n    std::string first_letter = \"\";\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n#pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        s_word = s[i];\n        first_letter = s_word.substr(0, 1);\n        if (first_letter.compare(\"a\") < 0 || first_letter.compare(\"z\") > 0) {\n            continue;\n        }\n        int r_index = first_letter.c_str()[0] - 'a';\n        int local_index = r_start[r_index] + i;\n        int global_index = r_start[rank] + i;\n        if (rank == 0) {\n            bins[local_index]++;\n        } else {\n            if (global_index >= r_start[rank] && global_index < r_end[rank]) {\n                bins[global_index - r_start[rank]]++;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < comm_sz; i++) {\n            MPI_Recv(&(bins[r_start[i]]), r_end[i]-r_start[i], MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&(bins[r_start[rank]]), r_end[rank]-r_start[rank], MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n    delete [] r_start;\n    delete [] r_end;\n}",
            "// TODO: Your code here\n\n    for (int i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n\n}",
            "size_t num_strings = s.size();\n    std::array<size_t, 26> local_bins = {};\n    for (size_t i = 0; i < num_strings; i++) {\n        char first_letter = s[i][0];\n        local_bins[first_letter - 'a']++;\n    }\n\n    int num_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    std::vector<size_t> tmp_bins(26);\n    int tmp_rank = 0;\n    for (size_t i = 0; i < num_strings; i++) {\n        int tmp = s[i][0] - 'a';\n        int rank = tmp / 26;\n        if (rank!= tmp_rank) {\n            MPI_Gather(local_bins.data(), 26, MPI_UNSIGNED_LONG, tmp_bins.data(), 26, MPI_UNSIGNED_LONG, tmp_rank,\n                       MPI_COMM_WORLD);\n            for (size_t i = 0; i < 26; i++) {\n                bins[i] += tmp_bins[i];\n            }\n            tmp_rank = rank;\n        }\n        local_bins[s[i][0] - 'a']++;\n    }\n    if (tmp_rank!= 0) {\n        MPI_Gather(local_bins.data(), 26, MPI_UNSIGNED_LONG, tmp_bins.data(), 26, MPI_UNSIGNED_LONG, 0,\n                   MPI_COMM_WORLD);\n        for (size_t i = 0; i < 26; i++) {\n            bins[i] += tmp_bins[i];\n        }\n    }\n}",
            "}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::array<size_t, 26> local_bins;\n    for (int i = 0; i < 26; ++i) {\n        local_bins[i] = 0;\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        char c = tolower(s[i][0]);\n        local_bins[c - 'a']++;\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, local_bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 26; ++i) {\n            bins[i] = local_bins[i];\n        }\n    }\n}",
            "int rank;\n    int size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t start = rank * s.size() / size;\n    size_t end = (rank + 1) * s.size() / size;\n\n    for (size_t i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i = start; i < end; i++) {\n        size_t bin = s[i].front() - 'a';\n        bins[bin] += 1;\n    }\n\n    if (rank == 0) {\n        MPI_Reduce(MPI_IN_PLACE, bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Reduce(bins.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "// Fill in starting code here\n\n}",
            "std::array<size_t, 26> count;\n    for(int i = 0; i < 26; ++i)\n        count[i] = 0;\n\n    std::vector<std::string> part(s.size());\n\n#pragma omp parallel for default(shared) schedule(dynamic, 10)\n    for(size_t i = 0; i < s.size(); ++i) {\n        part[i] = s[i];\n    }\n\n    int rank, nproc;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int remainder = s.size() % nproc;\n    int my_start = s.size()/nproc;\n    int my_stop = s.size();\n\n    if(rank == nproc - 1)\n        my_stop = my_start + remainder;\n    else\n        my_stop = my_start + my_start;\n\n    for(int i = my_start; i < my_stop; ++i)\n        count[part[i][0]-'a'] += 1;\n\n    MPI_Allreduce(&count, &bins, 26, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n}",
            "int mpi_rank, mpi_size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\n\t// Get the subvector of strings that this rank needs to process.\n\tsize_t i_start = mpi_rank * s.size() / mpi_size;\n\tsize_t i_end = (mpi_rank + 1) * s.size() / mpi_size;\n\n\t// Compute the number of strings in this subvector that start with each letter.\n\t#pragma omp parallel for\n\tfor (int i = 0; i < 26; i++) {\n\t\tfor (size_t j = i_start; j < i_end; j++) {\n\t\t\tif (s[j][0] - 'a' == i) bins[i]++;\n\t\t}\n\t}\n\n\t// Sum up the counts from all ranks.\n\tstd::array<size_t, 26> my_bins;\n\tMPI_Allreduce(bins.data(), my_bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\tbins = my_bins;\n}",
            "// TODO: implement\n\n    int rank, nproc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    if (rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] = 0;\n        }\n    }\n\n    int nstrings = s.size();\n    int len = s[0].length();\n    int nstrings_per_proc = nstrings/nproc;\n    int nstrings_rem = nstrings%nproc;\n    int start_idx = rank * nstrings_per_proc;\n    int end_idx = start_idx + nstrings_per_proc;\n    if (rank == nproc - 1) {\n        end_idx += nstrings_rem;\n    }\n\n    #pragma omp parallel for schedule(static) num_threads(nproc)\n    for (int i = start_idx; i < end_idx; i++) {\n        if (s[i].length() >= 1) {\n            if (s[i].at(0) >= 65 && s[i].at(0) <= 90) {\n                int num = s[i].at(0) - 65;\n                #pragma omp atomic\n                bins[num]++;\n            }\n        }\n    }\n\n    int* bins_par = new int[26];\n    MPI_Reduce(bins.data(), bins_par, 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] = bins_par[i];\n        }\n    }\n\n    delete [] bins_par;\n\n}",
            "#pragma omp parallel\n    {\n        std::array<size_t, 26> local_bins = {0};\n        #pragma omp for\n        for (size_t i = 0; i < s.size(); ++i) {\n            char first_letter = s[i][0];\n            local_bins[first_letter-'a']++;\n        }\n        #pragma omp critical\n        for (int i = 0; i < 26; ++i) {\n            bins[i] += local_bins[i];\n        }\n    }\n}",
            "int size;\n    int rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        #pragma omp parallel for\n        for (int i = 0; i < 26; ++i) {\n            bins[i] = 0;\n        }\n\n        for (int i = 0; i < s.size(); ++i) {\n            bins[s[i][0] - 'a']++;\n        }\n\n        for (int i = 1; i < size; ++i) {\n            MPI_Send(&(bins[0]), 26, MPI_UNSIGNED_LONG_LONG, i, 1, MPI_COMM_WORLD);\n        }\n    }\n    else {\n        MPI_Status status;\n        MPI_Recv(&(bins[0]), 26, MPI_UNSIGNED_LONG_LONG, 0, 1, MPI_COMM_WORLD, &status);\n    }\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int len = s.size();\n    int bin_size = len / size;\n    int s_start = rank * bin_size;\n    int s_end = s_start + bin_size;\n    if (rank == size - 1)\n        s_end = len;\n\n    for (int i = 0; i < 26; ++i)\n        bins[i] = 0;\n\n#pragma omp parallel for\n    for (int i = s_start; i < s_end; ++i) {\n        for (char c: s[i]) {\n            if (c >= 'a' && c <= 'z') {\n                ++bins[c - 'a'];\n                break;\n            }\n        }\n    }\n\n    for (int i = 1; i < size; ++i) {\n        MPI_Send(bins.data(), 26, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD);\n    }\n\n    MPI_Status status;\n    if (rank == 0) {\n        std::array<size_t, 26> partial_bins;\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(partial_bins.data(), 26, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, &status);\n            for (int j = 0; j < 26; ++j) {\n                bins[j] += partial_bins[j];\n            }\n        }\n    } else {\n        MPI_Recv(bins.data(), 26, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD, &status);\n    }\n}",
            "// You can use MPI_Allreduce to compute the global histogram\n    // In each thread, you have a piece of the histogram\n    // Use OpenMP reduction to update the piece of histogram\n    // Once the piece of histogram is ready, use MPI_Allreduce to get the global histogram\n    // For example, one thread may have the histogram for a-f, another one may have the histogram for g-p\n    // You must use MPI_Allreduce to combine the histogram from each thread\n    // Note that the string may not be a multiple of MPI_THREAD_SINGLE, so the string needs to be divided into\n    // multiple parts, and each thread handles a part\n    // One way to do it is to have a global thread number, say global_tid, which is assigned in a way that each thread\n    // has a unique global thread number. And then, each thread computes the histogram of the part of the string\n    // whose rank is less than global_tid.\n    // In this way, each thread will have a piece of the string, and the thread number will be unique.\n    // Finally, use MPI_Allreduce to get the global histogram\n\n    // MPI_Allreduce, MPI_Barrier, MPI_THREAD_SINGLE, MPI_THREAD_FUNNELED, MPI_THREAD_SERIALIZED, MPI_THREAD_MULTIPLE\n\n    int world_size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int global_tid = rank;\n    int nthreads = omp_get_max_threads();\n    int tid = omp_get_thread_num();\n    int count = s.size() / world_size;\n    int remain = s.size() % world_size;\n    int start = count * rank + min(remain, rank);\n    int end = count * (rank + 1) + min(remain, rank + 1);\n    if (rank == world_size - 1) {\n        end += remain;\n    }\n    std::array<size_t, 26> histo = {0};\n\n#pragma omp parallel\n    {\n        // each thread computes a part of the histogram\n        // you can use omp_get_thread_num() to get the thread number\n        // you can use omp_get_max_threads() to get the max number of threads\n\n        for (size_t i = start; i < end; ++i) {\n            char c = s[i][0];\n            if (c >= 'a' && c <= 'z') {\n                ++histo[c - 'a'];\n            }\n        }\n    }\n\n    // MPI_Allreduce, MPI_THREAD_SINGLE, MPI_THREAD_FUNNELED, MPI_THREAD_SERIALIZED, MPI_THREAD_MULTIPLE\n    MPI_Allreduce(MPI_IN_PLACE, &histo, 26, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < 26; ++i) {\n        bins[i] = histo[i];\n    }\n\n    if (rank == 0) {\n        std::cout << \"histo: \";\n        for (size_t i = 0; i < 26; ++i) {\n            std::cout << histo[i] << \" \";\n        }\n        std::cout << std::endl;\n    }\n}",
            "std::array<size_t, 26> counts = { 0 };\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i = 0; i < s.size(); ++i) {\n            auto c = s[i][0];\n            counts[c - 'a'] += 1;\n        }\n    }\n\n    // TODO\n    auto size = MPI_Comm_size(MPI_COMM_WORLD);\n    auto rank = MPI_Comm_rank(MPI_COMM_WORLD);\n\n    std::array<size_t, 26> localCounts = { 0 };\n    MPI_Allgather(&counts, 26, MPI_INT, &localCounts, 26, MPI_INT, MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (size_t i = 0; i < 26; i++) {\n            for (size_t j = 0; j < size; j++) {\n                bins[i] += localCounts[i*size+j];\n            }\n        }\n    }\n}",
            "int rank = 0;\n    int numRanks = 1;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n    size_t numStrings = s.size();\n    size_t stringsPerRank = numStrings / numRanks;\n    size_t remainder = numStrings % numRanks;\n    size_t start = stringsPerRank * rank + std::min(rank, remainder);\n    size_t end = start + stringsPerRank + (rank < remainder);\n\n    int numThreads = 1;\n    #pragma omp parallel\n    {\n        numThreads = omp_get_num_threads();\n    }\n\n    std::array<size_t, 26> binsRank;\n    binsRank.fill(0);\n\n    for (size_t i = start; i < end; ++i) {\n        size_t bin = (s[i].at(0) - 'a');\n        binsRank[bin]++;\n    }\n\n    #pragma omp parallel\n    {\n        int thread = omp_get_thread_num();\n        int numThreads = omp_get_num_threads();\n        int bin_begin = 26 / numThreads * thread;\n        int bin_end = 26 / numThreads * (thread + 1);\n        if (thread == numThreads - 1) {\n            bin_end = 26;\n        }\n        for (int bin = bin_begin; bin < bin_end; ++bin) {\n            #pragma omp atomic\n            bins[bin] += binsRank[bin];\n        }\n    }\n\n    if (rank == 0) {\n        bins[0] += bins[26];\n        bins[26] = 0;\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, bins.data(), 26, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n}",
            "#pragma omp parallel\n    {\n        std::array<size_t, 26> binsLocal{};\n        #pragma omp for\n        for (size_t i = 0; i < s.size(); ++i)\n        {\n            ++binsLocal[s[i][0] - 'a'];\n        }\n        #pragma omp critical\n        for (size_t i = 0; i < 26; ++i)\n        {\n            bins[i] += binsLocal[i];\n        }\n    }\n}",
            "std::array<size_t, 26> localBins;\n    for (auto i=0; i < s.size(); ++i) {\n        int c = s[i][0];\n        if (c >= 'a' && c <= 'z') {\n            localBins[c - 'a']++;\n        }\n    }\n\n    // get global bins\n    for (auto i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n    for (auto i = 0; i < 26; i++) {\n        int total = 0;\n        for (auto j = 0; j < omp_get_num_threads(); j++) {\n            total += localBins[i];\n        }\n        bins[i] += total;\n    }\n}",
            "size_t local_size = s.size();\n  std::array<size_t, 26> local_bins;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < local_size; i++)\n  {\n    local_bins[s[i][0] - 'a']++;\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    #pragma omp parallel\n    {\n        #pragma omp master\n        {\n            int r = 0;\n            std::array<size_t, 26> tempBins;\n            for(size_t i = rank; i < s.size(); i += size) {\n                ++tempBins[s[i][0] - 'a'];\n            }\n            MPI_Reduce(tempBins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        }\n    }\n}",
            "// TODO\n}",
            "std::array<size_t, 26> localBin;\n  for (size_t i = 0; i < s.size(); ++i) {\n    localBin[s[i][0] - 'a']++;\n  }\n#pragma omp parallel for schedule(static, 1)\n  for (size_t i = 0; i < 26; ++i) {\n    bins[i] += localBin[i];\n  }\n}",
            "assert(s.size() > 0);\n    assert(bins.size() == 26);\n\n    size_t size = s.size();\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, numprocs;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &numprocs);\n\n    std::array<size_t, 26> local_counts = {0};\n\n    #pragma omp parallel for schedule(guided)\n    for(int i=0; i<size; i++) {\n        std::string word = s[i];\n        local_counts[word[0] - 'a']++;\n    }\n\n    std::vector<size_t> all_counts;\n    MPI_Gather(&local_counts[0], 26, MPI_UNSIGNED_LONG_LONG,\n               &all_counts[0], 26, MPI_UNSIGNED_LONG_LONG,\n               0, comm);\n\n    if(rank == 0) {\n        for(int i=0; i<numprocs; i++) {\n            std::copy(all_counts.begin() + 26 * i, all_counts.begin() + 26 * (i + 1), bins.begin() + 26 * i);\n        }\n    }\n\n}",
            "int mpi_rank = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n  int mpi_size = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\n  // compute the local size of the vector\n  size_t local_size = s.size()/mpi_size;\n  size_t remainder = s.size() % mpi_size;\n  if (mpi_rank < remainder) {\n    local_size += 1;\n  }\n\n  // compute the local range to work on\n  size_t start = 0;\n  if (mpi_rank < remainder) {\n    start = mpi_rank * (local_size + 1);\n  }\n  else {\n    start = remainder * (local_size + 1) + mpi_rank * local_size;\n  }\n  size_t end = start + local_size;\n\n  // loop through the range and do the computations\n  for (size_t i = start; i < end; i++) {\n    if (i < s.size()) {\n      char c = s[i][0];\n      bins[c-'a']++;\n    }\n  }\n\n  // if there are more ranks, let them do their work\n  if (mpi_size > 1) {\n    int tag = 0;\n\n    // receive data\n    for (int i = 0; i < mpi_size; i++) {\n      if (i!= mpi_rank) {\n        std::array<size_t, 26> temp;\n        MPI_Recv(&temp[0], 26, MPI_UNSIGNED_LONG_LONG, i, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        for (int j = 0; j < 26; j++) {\n          bins[j] += temp[j];\n        }\n      }\n    }\n\n    // send data\n    for (int i = 0; i < mpi_size; i++) {\n      if (i!= mpi_rank) {\n        std::array<size_t, 26> temp;\n        for (int j = 0; j < 26; j++) {\n          temp[j] = bins[j];\n        }\n        MPI_Send(&temp[0], 26, MPI_UNSIGNED_LONG_LONG, i, tag, MPI_COMM_WORLD);\n      }\n    }\n  }\n\n  // output results\n  if (mpi_rank == 0) {\n    for (int i = 0; i < 26; i++) {\n      std::cout << bins[i] << \" \";\n    }\n  }\n}",
            "// You code goes here\n\n    // Your code goes here\n    int num_procs, my_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    std::vector<std::string> s_proc;\n    int num_of_s = s.size();\n    int chunk_size = num_of_s / num_procs;\n    int rem_size = num_of_s - chunk_size*num_procs;\n    int start = chunk_size * my_rank;\n    int end = chunk_size * (my_rank + 1);\n    if(my_rank == num_procs - 1)\n        end += rem_size;\n\n    #pragma omp parallel for\n    for(int i = start; i < end; i++){\n        if(my_rank == 0)\n            printf(\"[%d] s[%d] = %s\\n\", my_rank, i, s[i].c_str());\n        bins[s[i][0] - 'a']++;\n    }\n    if(my_rank == 0)\n        for(int i = 0; i < 26; i++)\n            printf(\"[0] bins[%d] = %zu\\n\", i, bins[i]);\n    #pragma omp barrier\n    if(my_rank!= 0)\n        for(int i = 0; i < 26; i++)\n            MPI_Reduce(&bins[i], &bins[i], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Bcast(&bins[0], 26, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "// TODO: your code here\n}",
            "assert(s.size() >= 1);\n\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Make sure we have enough bins for all strings\n    if (bins.size() < s.size()) {\n        std::cerr << \"Error: not enough bins allocated.\" << std::endl;\n        std::exit(EXIT_FAILURE);\n    }\n\n    // TODO: Count number of strings in s that start with each letter\n\n    // Distribute s in OpenMP parallel region\n    #pragma omp parallel\n    {\n        // Calculate work per thread\n        int numWork = s.size() / omp_get_num_threads();\n        int remain = s.size() % omp_get_num_threads();\n\n        int thread_num = omp_get_thread_num();\n        int begin = thread_num * numWork + std::min(thread_num, remain);\n        int end = begin + numWork + (thread_num < remain);\n\n        for (int i = begin; i < end; i++) {\n            // TODO: Loop over the elements in the s vector to count how many strings start with each letter\n            // For example, for string \"dog\" count the number of times the letter \"d\" appears at the beginning of the word\n            // Update bins[0] with the number of occurrences of letter 'd'\n            //...\n            // Update bins[25] with the number of occurrences of letter 'z'\n        }\n    }\n\n    // Reduce result to rank 0\n    // TODO: Use MPI_Reduce to sum the bins on each process\n    //...\n\n    if (rank == 0) {\n        // Print result to stdout\n        std::cout << \"Output: \";\n        for (size_t bin : bins) {\n            std::cout << bin << \", \";\n        }\n        std::cout << std::endl;\n    }\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO\n}",
            "int rank = 0;\n    int size = 1;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int binsPerRank = 26 / size;\n    int remainder = 26 % size;\n    int offset = 0;\n    int count = 0;\n\n    std::vector<int> myBins(binsPerRank, 0);\n\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        int pos = s[i][0] - 97;\n        myBins[pos]++;\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < binsPerRank; i++) {\n        MPI_Reduce(&myBins[i], &bins[i], 1, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < remainder; i++) {\n            bins[i + binsPerRank] += myBins[i + binsPerRank];\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int numProcs;\n  MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n  int stringSize = s.size();\n  int chunkSize = stringSize / numProcs;\n  std::vector<std::string> subStrings;\n  if (rank == 0) {\n    for (int i = 0; i < numProcs - 1; i++) {\n      int startIndex = i * chunkSize;\n      int endIndex = (i + 1) * chunkSize;\n      subStrings.insert(subStrings.end(), s.begin() + startIndex, s.begin() + endIndex);\n    }\n    int startIndex = (numProcs - 1) * chunkSize;\n    int endIndex = stringSize;\n    subStrings.insert(subStrings.end(), s.begin() + startIndex, s.begin() + endIndex);\n    //for (int i = 0; i < numProcs; i++) {\n    //  int startIndex = i * chunkSize;\n    //  int endIndex = (i + 1) * chunkSize;\n    //  subStrings.insert(subStrings.end(), s.begin() + startIndex, s.begin() + endIndex);\n    //}\n  }\n  std::array<int, 26> binsPerProcess;\n  std::fill(binsPerProcess.begin(), binsPerProcess.end(), 0);\n  for (int i = 0; i < subStrings.size(); i++) {\n    std::string str = subStrings.at(i);\n    char firstLetter = str.at(0);\n    int index = firstLetter - 'a';\n    binsPerProcess[index]++;\n  }\n  MPI_Reduce(MPI_IN_PLACE, &binsPerProcess[0], 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i = 0; i < 26; i++) {\n      bins[i] = binsPerProcess[i];\n    }\n  }\n}",
            "size_t n = s.size();\n  std::array<size_t, 26> localBins;\n\n  // TODO: fill in\n  #pragma omp parallel\n  {\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    #pragma omp for schedule(static, 1)\n    for (size_t i = 0; i < n; i++) {\n      localBins[s[i][0] - 'a'] += 1;\n    }\n\n    #pragma omp barrier\n\n    #pragma omp single\n    MPI_Allreduce(localBins.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n  }\n}",
            "#pragma omp parallel for\n\tfor (int i = 0; i < s.size(); i++) {\n\t\tbins[s[i][0] - 'a']++;\n\t}\n}",
            "int rank, nRanks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n\n    // TODO\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int num_of_strings = s.size();\n    int bin_size = num_of_strings / size;\n    int remainder = num_of_strings % size;\n    std::fill(bins.begin(), bins.end(), 0);\n    if (rank == 0) {\n        for (int i = 0; i < size; i++) {\n            MPI_Status status;\n            std::array<size_t, 26> first_letter_counts;\n            MPI_Recv(first_letter_counts.data(), 26, MPI_LONG_LONG_INT, i, 0, MPI_COMM_WORLD, &status);\n            for (int j = 0; j < 26; j++)\n                bins[j] += first_letter_counts[j];\n        }\n    } else {\n        std::array<size_t, 26> first_letter_counts;\n        #pragma omp parallel for\n        for (int i = 0; i < num_of_strings; i++) {\n            auto first_letter = s[i].front();\n            first_letter_counts[first_letter - 'a']++;\n        }\n        if (rank < remainder) {\n            bin_size++;\n        }\n        MPI_Send(first_letter_counts.data(), 26, MPI_LONG_LONG_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t n = s.size();\n    int chunk = n / size;\n    int extra = n % size;\n    int start = rank * (chunk + 1);\n    int end = start + chunk;\n    if (rank == size - 1) {\n        end += extra;\n    }\n\n    std::vector<std::string> sub_s = s;\n    if (rank == 0) {\n        #pragma omp parallel for\n        for (int i = 0; i < chunk + extra; i++) {\n            bins[s[i][0] - 'a']++;\n        }\n    }\n    else {\n        #pragma omp parallel for\n        for (int i = start; i < end; i++) {\n            bins[s[i][0] - 'a']++;\n        }\n    }\n\n    std::vector<size_t> bins_tmp(26);\n    MPI_Reduce(bins.data(), bins_tmp.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] = bins_tmp[i];\n        }\n    }\n\n    // printf(\"rank: %d, bins:\", rank);\n    // for (int i = 0; i < 26; i++) {\n    //     printf(\" %ld\", bins[i]);\n    // }\n    // printf(\"\\n\");\n}",
            "/* TODO */\n\n}",
            "for (size_t i=0; i<26; ++i) bins[i] = 0;\n\n  int rank, nRanks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n\n  int const nRows = s.size()/nRanks + (s.size()%nRanks > 0);\n  int const nCols = 26;\n\n  std::vector<size_t> local_bins(26,0);\n  for (int i=rank*nRows; i<(rank+1)*nRows; ++i) {\n    if (i < s.size()) {\n      std::string s_i = s[i];\n      if (s_i.size() >= 1) {\n        local_bins[s_i[0]-'a']++;\n      }\n    }\n  }\n  std::vector<size_t> global_bins(nCols);\n  MPI_Reduce(&local_bins[0], &global_bins[0], nCols, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i=0; i<26; ++i) bins[i] = global_bins[i];\n  }\n}",
            "int mpi_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    int mpi_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\n    if (mpi_rank == 0) {\n        int i = 0;\n        int j = 0;\n        int size = s.size();\n        int r = size / mpi_size;\n        int m = size % mpi_size;\n        int k = mpi_size - m;\n\n        size_t b[26];\n\n#pragma omp parallel for\n        for (i = 0; i < mpi_size; i++) {\n            b[i] = 0;\n            if (i < k) {\n                r = r + 1;\n            }\n            for (j = 0; j < r; j++) {\n                if (j < m) {\n                    b[i]++;\n                }\n                if (i < k) {\n                    j = j + 1;\n                }\n            }\n        }\n\n        for (int k = 0; k < 26; k++) {\n            bins[k] = b[k];\n        }\n    } else {\n\n        int i = 0;\n        int j = 0;\n        int size = s.size();\n        int r = size / mpi_size;\n        int m = size % mpi_size;\n        int k = mpi_size - m;\n\n        size_t b[26];\n\n#pragma omp parallel for\n        for (i = 0; i < mpi_size; i++) {\n            b[i] = 0;\n            if (i < k) {\n                r = r + 1;\n            }\n            for (j = 0; j < r; j++) {\n                if (j < m) {\n                    b[i]++;\n                }\n                if (i < k) {\n                    j = j + 1;\n                }\n            }\n        }\n\n        MPI_Gather(&b[mpi_rank], 26, MPI_UNSIGNED_LONG_LONG, &bins, 26, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n    }\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int N = s.size();\n   int workPerRank = N/size;\n\n   std::vector<size_t> bins_local(26, 0);\n   #pragma omp parallel for\n   for(int i = 0; i < workPerRank; ++i)\n   {\n     std::string& str = s[i+rank*workPerRank];\n     bins_local[str[0]-'a']++;\n   }\n\n   int displacement = 26*rank;\n   std::vector<size_t> bins_global(26*size, 0);\n   MPI_Allgather(bins_local.data(), 26, MPI_LONG_LONG, bins_global.data(), 26, MPI_LONG_LONG, MPI_COMM_WORLD);\n\n   int displacement_total = displacement + 26;\n   int size_total = size*26;\n   std::partial_sum(bins_global.begin(), bins_global.end(), bins.begin(), std::plus<size_t>());\n}",
            "#pragma omp parallel num_threads(omp_get_num_procs())\n    {\n        std::array<size_t, 26> threadBins{0};\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < s.size(); i++) {\n            threadBins[s[i].at(0)-97]++;\n        }\n        #pragma omp critical\n        {\n            for (int i = 0; i < 26; i++) {\n                bins[i] += threadBins[i];\n            }\n        }\n    }\n}",
            "const int nb_threads = omp_get_max_threads();\n    bins.fill(0);\n    #pragma omp parallel\n    {\n        std::array<size_t, 26> private_bins;\n        private_bins.fill(0);\n        size_t i_start, i_end;\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < s.size(); i++) {\n            private_bins[s[i][0] - 'a']++;\n        }\n        #pragma omp critical\n        {\n            for (int t = 0; t < nb_threads; t++) {\n                bins[t] += private_bins[t];\n            }\n        }\n    }\n}",
            "// TODO:\n}",
            "#pragma omp parallel\n    {\n        std::array<size_t, 26> localBins;\n        localBins.fill(0);\n\n        #pragma omp for\n        for(int i=0; i<s.size(); i++){\n            std::string str = s[i];\n            int count = 0;\n            for(int j=0; j<str.length(); j++){\n                if (str[j] >= 'a' && str[j] <= 'z'){\n                    count++;\n                    localBins[str[j]-'a']++;\n                }\n            }\n\n            #pragma omp critical\n            if(count == 0){\n                bins[26]++;\n            }\n        }\n\n        #pragma omp critical\n        for(int i=0; i<26; i++){\n            bins[i] += localBins[i];\n        }\n    }\n}",
            "assert(bins.size() == 26);\n    assert(s.size() > 0);\n\n    bins.fill(0);\n\n    #pragma omp parallel\n    {\n        // each thread has a local copy of bins\n        std::array<size_t, 26> bins_local = bins;\n\n        #pragma omp for\n        for (int i = 0; i < s.size(); ++i) {\n            // increment the letter count\n            ++bins_local[s[i][0] - 'a'];\n        }\n\n        #pragma omp critical\n        // update bins in a critical section\n        for (int i = 0; i < 26; ++i) {\n            bins[i] += bins_local[i];\n        }\n    }\n}",
            "int rank;\n\tint size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint chunk = s.size() / size;\n\tint rem = s.size() % size;\n\tint start, end;\n\tstart = rank * chunk + std::min(rank, rem);\n\tend = start + chunk;\n\tif (rank == size - 1) {\n\t\tend = s.size();\n\t}\n\t//omp_set_num_threads(8);\n\t#pragma omp parallel for\n\tfor (int i = start; i < end; i++) {\n\t\tint letter = s[i][0] - 97;\n\t\t#pragma omp atomic\n\t\tbins[letter]++;\n\t}\n}",
            "// Your code goes here\n\n\n    return;\n}",
            "size_t n = s.size();\n    std::vector<size_t> counts(26);\n    std::vector<size_t> bin_size(26);\n\n    // 1st step: count how many elements of each bucket\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; ++i) {\n        counts[s[i][0] - 'a']++;\n    }\n\n    // 2nd step: get the sizes of the buckets\n    bin_size[0] = counts[0];\n    for (size_t i = 1; i < 26; ++i) {\n        bin_size[i] = bin_size[i-1] + counts[i];\n    }\n\n    // 3rd step: distribute the data to bins\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; ++i) {\n        bins[bin_size[s[i][0] - 'a'] + i] = counts[s[i][0] - 'a'];\n    }\n}",
            "int size = s.size();\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int total_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &total_size);\n    int chunk_size = (int)size/total_size;\n    int remainder = (int)size%total_size;\n\n    std::vector<std::string> local_s;\n    local_s.resize(chunk_size + remainder);\n\n    if (rank == 0){\n        for (int i = 0; i < remainder; i++){\n            local_s[i] = s[i];\n        }\n    }\n    MPI_Bcast(&local_s[0], remainder, MPI_CHAR, 0, MPI_COMM_WORLD);\n    for (int i = 0; i < chunk_size; i++){\n        local_s[remainder+i] = s[i+rank*chunk_size];\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_s.size(); i++){\n        char letter = local_s[i][0];\n        int local_rank;\n        MPI_Comm_rank(MPI_COMM_WORLD, &local_rank);\n        bins[letter-'a'] += local_rank;\n    }\n}",
            "/*\n    Use OpenMP to parallelize this function.\n    Use the environment variable OMP_NUM_THREADS to control the number of threads.\n    */\n    #pragma omp parallel for\n    for (int i = 0; i < 26; i++){\n        bins[i] = 0;\n    }\n    //bins.fill(0);\n\n    for (int i = 0; i < s.size(); i++){\n\n        //std::cout << s[i][0] << std::endl;\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "// TODO\n}",
            "std::array<size_t, 26> bins_tmp;\n\n    for (size_t i = 0; i < bins.size(); i++) {\n        bins_tmp[i] = 0;\n    }\n\n#pragma omp parallel\n    {\n        int rank, size;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n        int num_strings = s.size();\n        int num_strings_per_rank = num_strings / size;\n        int start = rank * num_strings_per_rank;\n        int end = start + num_strings_per_rank;\n        for (int i = start; i < end; i++) {\n            if (s[i].size() > 0) {\n                char first_letter = s[i][0];\n                int index = first_letter - 'a';\n                bins_tmp[index]++;\n            }\n        }\n\n        for (int i = 0; i < bins_tmp.size(); i++) {\n            bins[i] += bins_tmp[i];\n        }\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  if (size < 1) return;\n\n  int strs_per_rank = s.size() / size;\n  if (s.size() % size) strs_per_rank++;\n\n  std::vector<std::string> strs_to_count(strs_per_rank);\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < strs_to_count.size(); i++) {\n      strs_to_count[i] = s[i * size + rank];\n    }\n  }\n\n  MPI_Bcast(strs_to_count.data(), strs_to_count.size(), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n  std::array<size_t, 26> local_counts{};\n\n  #pragma omp parallel for\n  for (int i = 0; i < strs_to_count.size(); i++) {\n    std::string str = strs_to_count[i];\n    int start = str[0] - 'a';\n    local_counts[start]++;\n  }\n\n  #pragma omp parallel for\n  for (int i = 0; i < 26; i++) {\n    bins[i] += local_counts[i];\n  }\n}",
            "// Initialize the bins array to 0s\n  for(size_t i = 0; i < 26; i++){\n    bins[i] = 0;\n  }\n\n  // Iterate through every word in s\n  #pragma omp parallel for\n  for(size_t i = 0; i < s.size(); i++){\n    // Fill in the appropriate bin\n    bins[s[i][0] - 'a'] += 1;\n  }\n\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    MPI_Group group_world, group_self;\n    MPI_Comm_group(comm, &group_world);\n\n    std::vector<size_t> ranks_per_thread(omp_get_max_threads(), 0);\n    omp_set_num_threads(omp_get_max_threads());\n    #pragma omp parallel for\n    for(int i = 0; i < omp_get_max_threads(); i++) {\n        int thread_rank = omp_get_thread_num();\n        ranks_per_thread[thread_rank] = thread_rank + i * omp_get_max_threads();\n    }\n    //std::vector<size_t> ranks_per_thread(omp_get_max_threads(), 0);\n    //#pragma omp parallel for\n    //for(int i = 0; i < omp_get_max_threads(); i++) {\n        //int thread_rank = omp_get_thread_num();\n        //ranks_per_thread[i] = thread_rank;\n    //}\n    //std::vector<size_t> ranks_per_thread(omp_get_max_threads(), 0);\n    //#pragma omp parallel for\n    //for(int i = 0; i < omp_get_max_threads(); i++) {\n    //    ranks_per_thread[i] = i;\n    //}\n\n    //std::vector<size_t> ranks_per_thread(omp_get_max_threads(), 0);\n    //for(int i = 0; i < omp_get_max_threads(); i++) {\n    //    ranks_per_thread[i] = i;\n    //}\n\n    //int thread_rank = omp_get_thread_num();\n    //int thread_num = omp_get_num_threads();\n\n    std::vector<size_t> ranks_per_thread(omp_get_max_threads(), 0);\n    for(int i = 0; i < omp_get_max_threads(); i++) {\n        int thread_rank = omp_get_thread_num();\n        ranks_per_thread[i] = thread_rank + i * omp_get_max_threads();\n    }\n    //std::vector<size_t> ranks_per_thread(omp_get_max_threads(), 0);\n    //for(int i = 0; i < omp_get_max_threads(); i++) {\n    //    ranks_per_thread[i] = i;\n    //}\n\n    //int thread_rank = omp_get_thread_num();\n    //int thread_num = omp_get_num_threads();\n\n    //int thread_rank = omp_get_thread_num();\n    //int thread_num = omp_get_num_threads();\n\n    //int thread_rank = omp_get_thread_num();\n    //int thread_num = omp_get_num_threads();\n\n    //int thread_rank = omp_get_thread_num();\n    //int thread_num = omp_get_num_threads();\n\n    //int thread_rank = omp_get_thread_num();\n    //int thread_num = omp_get_num_threads();\n\n    //int thread_rank = omp_get_thread_num();\n    //int thread_num = omp_get_num_threads();\n\n    //int thread_rank = omp_get_thread_num();\n    //int thread_num = omp_get_num_threads();\n\n    //int thread_rank = omp_get_thread_num();\n    //int thread_num = omp_get_num_threads();\n\n    //int thread_rank = omp_get_thread_num();\n    //int thread_num = omp_get_num_threads();\n\n    //int thread_rank = omp_get_thread_num();\n    //int thread_num = omp_get_num_threads();\n\n    //int thread_rank = omp_get_thread_num();\n    //int thread_num = omp_get_num_threads();\n\n    //int thread_rank",
            "}",
            "for (int i = 0; i < s.size(); i++) {\n        auto index = s[i][0] - 'a';\n        bins[index]++;\n    }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    #pragma omp parallel for\n    for (int i = 0; i < 26; i++)\n    {\n        char c = i + 'a';\n        bins[i] = 0;\n        for (auto const &str: s)\n        {\n            if (str[0] == c)\n            {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Every rank has a complete copy of s.\n\n    size_t s_size = s.size();\n    std::vector<std::string> s_copy = s;\n\n    std::vector<size_t> bins_copy(26);\n\n    // Each rank calculates the first letter counts of the strings in its copy of s.\n    // OpenMP is used here to divide the workload.\n    #pragma omp parallel for\n    for (int i = 0; i < s_size; i++) {\n        std::string str = s_copy[i];\n        char first_letter = str[0];\n        int bin_id = first_letter - 'a';\n        bins_copy[bin_id]++;\n    }\n\n    // Every rank collects the first letter counts from its copy of s to bins.\n    // MPI is used here to collect data from all ranks.\n    std::vector<size_t> all_bins;\n    all_bins.resize(26 * size);\n    MPI_Gather(bins_copy.data(), 26, MPI_UNSIGNED_LONG, all_bins.data(), 26, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        // Every rank on rank 0 collects the first letter counts from all ranks.\n        // Then, rank 0 puts the first letter counts into bins.\n        for (int i = 0; i < size; i++) {\n            size_t *bin_ptr = all_bins.data() + i * 26;\n            std::copy(bin_ptr, bin_ptr + 26, bins.begin());\n        }\n    }\n}",
            "size_t num_words = s.size();\n    size_t num_threads = omp_get_max_threads();\n    std::array<size_t, 26> local_bins{};\n    #pragma omp parallel for\n    for (size_t i = 0; i < num_words; i++) {\n        char c = s[i][0];\n        local_bins[c-'a']++;\n    }\n    MPI_Allreduce(local_bins.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n    return;\n}",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int chunk = (s.size() / size) + 1;\n\n    std::array<size_t, 26> local_bins{};\n\n    #pragma omp parallel for\n    for (int i = rank * chunk; i < std::min(s.size(), (rank + 1) * chunk); ++i) {\n        char first_letter = s[i][0];\n        if (first_letter >= 'a' && first_letter <= 'z') {\n            local_bins[first_letter - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < 26; ++i) {\n            bins[i] = 0;\n        }\n\n        for (int r = 1; r < size; ++r) {\n            MPI_Recv(&local_bins, 26, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int i = 0; i < 26; ++i) {\n                bins[i] += local_bins[i];\n            }\n        }\n    } else {\n        MPI_Send(&local_bins, 26, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "//Your code here\n}",
            "// TODO:\n  // Create a bins array of size 26 to store the counts\n  // bins[i] = # of strings in the vector s that start with the i-th letter of the alphabet\n  // (bins[0] will be the number of strings that start with \"a\", bins[1] will be the number of strings that start with \"b\", etc.)\n  //\n  // Implement the parallel version of the following code using MPI and OpenMP:\n\n  std::array<size_t, 26> bins_parallel;\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for (size_t i = 0; i < 26; i++) {\n      // TODO:\n      // Loop over all the strings in s and count the number of strings that start with the i-th letter of the alphabet\n      // (bins_parallel[i] should be the number of strings that start with the i-th letter)\n      //\n      // You can use:\n      //\n      //   int rank; // current rank\n      //   int nproc; // total number of processes\n      //   char first_letter; // the first letter of the current string\n      //   std::string &str; // the current string (referenced)\n      //\n      // to parallelize the code using MPI and OpenMP.\n      //\n      // Note:\n      //\n      //   If your strings are not stored in order, you will need to create a map for the first letter of each string.\n      //   You can use std::map<char, std::vector<std::string>> to store the first letter of the string and the list of strings that start with that letter.\n      //   For example, s[0] = \"cow\", s[1] = \"dog\", s[2] = \"cat\" will be stored as {{'c', {\"cow\"}}, {'d', {\"dog\"}}, {'c', {\"cat\"}}}.\n      //   Then, you can loop through all the entries in the map, and compute bins_parallel[i] using the values in the map.\n      //\n      //   If your strings are stored in order, you can parallelize the computation using a single OpenMP loop with a private variable to store the current string.\n    }\n  }\n  // TODO:\n  // Compute the sum of bins_parallel.\n  //\n  // You can use:\n  //\n  //   int rank; // current rank\n  //   int nproc; // total number of processes\n  //   std::array<size_t, 26> &bins; // bins array that stores the counts\n  //\n  // to parallelize the code using MPI and OpenMP.\n\n  // TODO:\n  // Copy the counts in bins_parallel to bins.\n  //\n  // You can use:\n  //\n  //   int rank; // current rank\n  //   int nproc; // total number of processes\n  //   std::array<size_t, 26> &bins; // bins array that stores the counts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  // \n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //",
            "}",
            "}",
            "int rank = 0;\n\tint num_ranks = 0;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n\tint size = (int) s.size();\n\tint chunk_size = size / num_ranks;\n\n\tint start = rank * chunk_size;\n\tint end = start + chunk_size;\n\n\tif (rank == num_ranks - 1) {\n\t\tend = size;\n\t}\n\n\t#pragma omp parallel for\n\tfor (int i = start; i < end; i++) {\n\t\tchar letter = s[i][0];\n\t\tbins[letter - 'a']++;\n\t}\n}",
            "#pragma omp parallel for\n  for(int i = 0; i < s.size(); i++) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "std::array<size_t, 26> local;\n  for (int i = 0; i < 26; i++) local[i] = 0;\n\n  for (auto& str : s) {\n    if (str[0] >= 'a' && str[0] <= 'z') {\n      auto c = (int) str[0] - (int) 'a';\n      local[c]++;\n    }\n  }\n\n  size_t total = 0;\n  MPI_Reduce(&local, &bins, 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// You code goes here\n\n\tMPI_Comm comm;\n\tMPI_Comm_dup(MPI_COMM_WORLD, &comm);\n\tint rank;\n\tMPI_Comm_rank(comm, &rank);\n\tint size;\n\tMPI_Comm_size(comm, &size);\n\tif (rank == 0) {\n\t\tint count = 0;\n\t\tsize_t offset = 0;\n\t\tint num_bins_per_process = bins.size() / size;\n\t\tint leftover = bins.size() % size;\n\t\tint bins_per_process = num_bins_per_process;\n\t\tint start = 0;\n\t\tint end = 0;\n\n\t\tfor (int i = 0; i < size; ++i) {\n\t\t\tstart = offset;\n\t\t\tend = offset + bins_per_process;\n\t\t\tif (leftover > 0) {\n\t\t\t\tbins_per_process++;\n\t\t\t\tleftover--;\n\t\t\t}\n\t\t\tomp_set_num_threads(omp_get_max_threads());\n\t\t\t#pragma omp parallel for\n\t\t\tfor (int j = start; j < end; ++j) {\n\t\t\t\tfor (int k = 0; k < s.size(); ++k) {\n\t\t\t\t\tint letter = s[k].front();\n\t\t\t\t\tif (letter == (j - 'a') + 'a') {\n\t\t\t\t\t\tbins[j]++;\n\t\t\t\t\t\tcount++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\toffset += bins_per_process;\n\t\t}\n\t\tprintf(\"rank 0 done %d\\n\", count);\n\t\tMPI_Barrier(comm);\n\t}\n\telse {\n\t\tsize_t offset = 0;\n\t\tint bins_per_process = bins.size() / size;\n\t\tint leftover = bins.size() % size;\n\t\tint bins_per_process_local = bins_per_process;\n\t\tint start = 0;\n\t\tint end = 0;\n\t\tif (leftover > 0) {\n\t\t\tbins_per_process_local++;\n\t\t\tleftover--;\n\t\t}\n\t\tomp_set_num_threads(omp_get_max_threads());\n\t\t#pragma omp parallel for\n\t\tfor (int i = 0; i < bins_per_process_local; ++i) {\n\t\t\tfor (int j = 0; j < s.size(); ++j) {\n\t\t\t\tint letter = s[j].front();\n\t\t\t\tif (letter == (i + rank - 'a') + 'a') {\n\t\t\t\t\tbins[i]++;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tMPI_Gather(bins.data(), bins.size(), MPI_LONG_LONG, bins.data(), bins.size(), MPI_LONG_LONG, 0, comm);\n\t}\n\n\n\t// MPI_Finalize();\n}",
            "MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  int num_threads;\n  #pragma omp parallel\n  {\n    num_threads = omp_get_num_threads();\n  }\n\n  std::cout << \"Started on \" << rank << \" of \" << num_procs << \" procs and \" << num_threads << \" threads\\n\";\n\n  if (num_procs!= num_threads) {\n    if (rank == 0) {\n      std::cout << \"Error: num_procs!= num_threads\\n\";\n    }\n    MPI_Abort(MPI_COMM_WORLD, 1);\n  }\n\n  // if (s.size() % num_procs!= 0) {\n  //   if (rank == 0) {\n  //     std::cout << \"Error: s.size() % num_procs!= 0\\n\";\n  //   }\n  //   MPI_Abort(MPI_COMM_WORLD, 1);\n  // }\n\n  int block_size = s.size() / num_procs;\n  int block_rem = s.size() % num_procs;\n  int start_idx = rank * block_size;\n  int end_idx = start_idx + block_size;\n\n  if (rank == num_procs - 1) {\n    end_idx = s.size();\n  }\n  else {\n    end_idx += block_rem;\n  }\n\n  int s_size = end_idx - start_idx;\n\n  std::vector<char> sub_string;\n  for (int i = 0; i < s_size; i++) {\n    sub_string.push_back(s[i + start_idx][0]);\n  }\n\n  int local_counts[26] = {0};\n\n  #pragma omp parallel for\n  for (int i = 0; i < s_size; i++) {\n    local_counts[sub_string[i] - 'a']++;\n  }\n\n  std::vector<int> thread_counts(26);\n  for (int i = 0; i < num_threads; i++) {\n    thread_counts[i] = 0;\n  }\n\n  std::vector<int> thread_block_sizes(num_threads);\n  for (int i = 0; i < num_threads; i++) {\n    thread_block_sizes[i] = block_size / num_threads;\n  }\n\n  // Assigning tasks to threads\n  #pragma omp parallel for\n  for (int i = 0; i < num_threads; i++) {\n    int thread_id = omp_get_thread_num();\n    int start = i * thread_block_sizes[i];\n    int end = start + thread_block_sizes[i];\n    if (i == num_threads - 1) {\n      end = block_size;\n    }\n    for (int j = start; j < end; j++) {\n      thread_counts[thread_id] += local_counts[j];\n    }\n  }\n\n  std::vector<int> all_thread_counts(num_threads * 26);\n  MPI_Gather(thread_counts.data(), 26, MPI_INT, all_thread_counts.data(), 26, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    // Storing the results in bins\n    for (int i = 0; i < num_procs; i++) {\n      for (int j = 0; j < 26; j++) {\n        bins[j] += all_thread_counts[26 * i + j];\n      }\n    }\n  }\n}",
            "// TODO\n\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        if (s[i].size() == 0) {\n            continue;\n        }\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "// Your code here\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tint chunkSize = s.size()/size;\n\tint remaining = s.size() % size;\n\t//printf(\"%d %d %d\\n\",rank, size, chunkSize);\n\tstd::array<size_t, 26> localBins;\n\tfor(int i=0; i<26; ++i)\n\t\tlocalBins[i] = 0;\n\n\tstd::vector<std::string> localStrings;\n\tint pos = 0;\n\tfor(int i=0; i<chunkSize; ++i)\n\t{\n\t\tchar letter = s[pos][0];\n\t\t//printf(\"%d %d %c\\n\",rank, pos, letter);\n\t\tif (letter > 'a' && letter < 'z')\n\t\t\t++localBins[letter-'a'];\n\t\t++pos;\n\t}\n\tfor(int i=0; i<remaining; ++i)\n\t{\n\t\tchar letter = s[pos][0];\n\t\t//printf(\"%d %d %c\\n\",rank, pos, letter);\n\t\tif (letter > 'a' && letter < 'z')\n\t\t\t++localBins[letter-'a'];\n\t\t++pos;\n\t}\n\n\t// std::vector<size_t> bins(26);\n\t// #pragma omp parallel\n\t// {\n\t// \t#pragma omp for\n\t// \tfor(size_t i=0; i<26; ++i)\n\t// \t\tbins[i] = 0;\n\t// \t#pragma omp for\n\t// \tfor(size_t i=0; i<s.size(); ++i)\n\t// \t{\n\t// \t\tchar letter = s[i][0];\n\t// \t\tif (letter > 'a' && letter < 'z')\n\t// \t\t\t++bins[letter-'a'];\n\t// \t}\n\t// }\n\t// if (rank == 0)\n\t// {\n\t// \tprintf(\"%d %d\\n\",rank, s.size());\n\t// \tfor(int i=0; i<26; ++i)\n\t// \t\tprintf(\"%d \", bins[i]);\n\t// \tprintf(\"\\n\");\n\t// }\n\n\t// MPI_Allreduce(&bins, &localBins, 26, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\tMPI_Reduce(&localBins, &bins, 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "std::vector<size_t> localBins(26);\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        localBins[s[i][0] - 'a']++;\n    }\n\n    std::array<size_t, 26> totalBins;\n    #pragma omp parallel\n    {\n        int rank;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        #pragma omp single\n        {\n            MPI_Allreduce(localBins.data(), totalBins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n        }\n    }\n\n    bins = totalBins;\n}",
            "// Your code here.\n    return;\n}",
            "auto countString = [](std::string const& word, size_t letter){\n        if (word.size() == 0)\n            return 0;\n        return (word[0] - 'a' == letter);\n    };\n    auto getLetter = [](std::string const& word, size_t letter){\n        if (word.size() == 0)\n            return 0;\n        return word[0] - 'a';\n    };\n    size_t size = s.size();\n\n    std::vector<size_t> localCounts(26);\n#pragma omp parallel for\n    for (size_t i = 0; i < size; ++i)\n        localCounts[getLetter(s[i], 0)]++;\n\n    MPI_Allreduce(localCounts.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n\t   Assume all strings are in lower case. Store the output in `bins` array.\n\t   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n\t   Every rank has a complete copy of s. The result is stored in bins on rank 0.\n\t   Example:\n\n\t   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n\t   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n\t*/\n\n\tint rank, n_ranks;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n\n\t// for each rank, split the vector into different chunks\n\tstd::vector<std::string> s_new;\n\tstd::vector<std::string> s_split = split(s, n_ranks);\n\tint start = rank * s_split.size() / n_ranks;\n\tint end = (rank + 1) * s_split.size() / n_ranks;\n\tfor (int i = start; i < end; ++i) {\n\t\ts_new.push_back(s_split[i]);\n\t}\n\n\t// allocate an array for storing the counts\n\tstd::array<size_t, 26> counts;\n\tstd::fill(counts.begin(), counts.end(), 0);\n\n\t// count the number of strings starting with each letter\n#pragma omp parallel for\n\tfor (int i = 0; i < s_new.size(); i++) {\n\t\tif (s_new[i].size() >= 1) {\n\t\t\tcounts[s_new[i][0] - 'a']++;\n\t\t}\n\t}\n\n\t// collect the counts from each rank\n\tstd::array<size_t, 26> counts_tmp;\n\tMPI_Allreduce(counts.data(), counts_tmp.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\tbins = counts_tmp;\n\n}",
            "const int size = s.size();\n    const int rank = omp_get_thread_num();\n    const int numThreads = omp_get_num_threads();\n    const int rankThread = rank * numThreads;\n    const int numRanks = omp_get_num_threads();\n\n    size_t binsLoc[26] = {0};\n    size_t counts[26] = {0};\n    const char* cStr;\n    for(int i=rankThread; i<size; i+=numRanks) {\n        cStr = s[i].c_str();\n        int c = (int)cStr[0];\n        if(c >= 'a' && c <= 'z') {\n            ++binsLoc[c - 'a'];\n        }\n    }\n\n    MPI_Allreduce(binsLoc, counts, 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    for(int i=0; i<26; ++i) {\n        bins[i] += counts[i];\n    }\n}",
            "int world_size;\n\tMPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\tint world_rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\tint alphabet_size = 26;\n\t\n\tstd::vector<std::string> local_s;\n\tif (world_rank == 0) {\n\t\tlocal_s = s;\n\t}\n\n\tstd::vector<std::string> local_s_tmp;\n\tMPI_Bcast(&local_s, local_s.size(), MPI_CHAR, 0, MPI_COMM_WORLD);\n\tMPI_Scatter(&local_s, local_s.size() / world_size, MPI_CHAR, &local_s_tmp, local_s_tmp.size() / world_size, MPI_CHAR, 0, MPI_COMM_WORLD);\n\n\t// count the number of strings in the vector s that start with that letter\n\t// use OpenMP and MPI to compute in parallel\n\tstd::array<size_t, 26> local_bins = {0};\n\t\n\t//#pragma omp parallel for\n\tfor (size_t i = 0; i < local_s_tmp.size(); i++) {\n\t\tlocal_bins[local_s_tmp[i][0] - 'a']++;\n\t}\n\n\tMPI_Reduce(local_bins.data(), bins.data(), alphabet_size, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: your code goes here\n}",
            "auto n = s.size();\n    for(int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n#pragma omp parallel for\n    for(int i = 0; i < n; i++) {\n        auto c = s[i][0];\n        if(c >= 'a' && c <= 'z')\n            bins[c - 'a'] += 1;\n    }\n}",
            "// your code here\n}",
            "// FIXME: add your code here\n}",
            "int rank, nranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n    size_t binSize = (s.size()/nranks);\n    size_t binOffset = rank*binSize;\n\n    size_t sum = 0;\n    for (size_t i=0; i < binSize; i++) {\n        size_t idx = binOffset+i;\n        sum += s[idx].size() > 0? 1 : 0;\n    }\n\n    bins[0] = sum;\n\n    for (size_t i=1; i < 26; i++) {\n        MPI_Reduce(&sum, &bins[i], 1, MPI_LONG_LONG_INT, MPI_SUM, i-1, MPI_COMM_WORLD);\n    }\n}",
            "auto const num_threads = omp_get_max_threads();\n  auto const rank = MPI_Comm_rank(MPI_COMM_WORLD);\n  auto const num_ranks = MPI_Comm_size(MPI_COMM_WORLD);\n\n  // Divide the workload among the ranks\n  auto const chunk_size = (s.size() + num_ranks - 1) / num_ranks;\n  auto const beg = chunk_size * rank;\n  auto const end = std::min(beg + chunk_size, s.size());\n  std::vector<std::string> chunk(s.begin() + beg, s.begin() + end);\n\n  // Create a private array to store the counts\n  std::array<size_t, 26> counts{};\n  #pragma omp parallel for num_threads(num_threads) schedule(static)\n  for (size_t i = 0; i < chunk.size(); ++i)\n  {\n    auto const letter = chunk[i][0] - 'a';\n    ++counts[letter];\n  }\n\n  // Sum the local counts\n  #pragma omp parallel for num_threads(num_threads) schedule(static) reduction(+:bins)\n  for (size_t i = 0; i < 26; ++i) {\n    auto const sum = static_cast<size_t>(omp_get_thread_num()) * counts[i] + counts[i];\n    bins[i] = sum;\n  }\n\n  // Gather the counts in the first rank\n  if (rank == 0) {\n    MPI_Allgather(MPI_IN_PLACE, 26, MPI_LONG_LONG_INT, bins.data(), 26, MPI_LONG_LONG_INT, MPI_COMM_WORLD);\n  } else {\n    MPI_Gather(bins.data(), 26, MPI_LONG_LONG_INT, bins.data(), 26, MPI_LONG_LONG_INT, 0, MPI_COMM_WORLD);\n  }\n}",
            "int rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\t// #pragma omp parallel for\n\tfor (int i = 0; i < 26; i++) {\n\t\tfor (int j = 0; j < s.size(); j++) {\n\t\t\tif (s[j][0] == (char)('a' + i)) {\n\t\t\t\tbins[i]++;\n\t\t\t}\n\t\t}\n\t}\n\tif (rank == 0) {\n\t\tstd::array<size_t, 26> global_bins;\n\t\tMPI_Reduce(bins.data(), global_bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\t\tfor (int i = 0; i < 26; i++) {\n\t\t\tstd::cout << global_bins[i] << \" \";\n\t\t}\n\t}\n}",
            "// TODO \n    // write your code here\n\n    // MPI COMMUNICATOR\n    MPI_Comm comm = MPI_COMM_WORLD;\n    // Get the size and rank\n    int nprocs;\n    int rank;\n    MPI_Comm_size(comm, &nprocs);\n    MPI_Comm_rank(comm, &rank);\n\n    // MPI GATHERV\n    // Initialize variable that will be used to gather information\n    int *recvcounts = (int *)malloc(nprocs * sizeof(int));\n    int *displs = (int *)malloc(nprocs * sizeof(int));\n\n    // Initialize the count and displacements\n    for (int i = 0; i < nprocs; i++)\n    {\n        recvcounts[i] = 0;\n        displs[i] = 0;\n    }\n\n    // Count the number of strings on each process\n    int counter = 0;\n    for (int i = 0; i < s.size(); i++)\n    {\n        // Get the first letter of the string and convert it to the index of the alphabet\n        char firstLetter = s[i][0];\n        int firstLetterIndex = (int)firstLetter - 97;\n\n        // If the current process is responsible for that letter, increment the counter\n        if (firstLetterIndex % nprocs == rank)\n            counter++;\n    }\n\n    // Get the count for that process and store it in the recvcounts array\n    recvcounts[rank] = counter;\n\n    // Get the displacements for each process\n    displs[0] = 0;\n    for (int i = 1; i < nprocs; i++)\n    {\n        displs[i] = displs[i - 1] + recvcounts[i - 1];\n    }\n\n    // Allocate the memory to store the strings\n    std::string *str = (std::string *)malloc(sizeof(std::string) * counter);\n\n    // Gather the strings\n    MPI_Gatherv(s.data(), counter, MPI_CHAR, str, recvcounts, displs, MPI_CHAR, 0, comm);\n\n    // On rank 0, initialize the bins array\n    if (rank == 0)\n    {\n        bins.fill(0);\n        for (int i = 0; i < nprocs; i++)\n        {\n            int count = recvcounts[i];\n            std::string *temp = (std::string *)malloc(sizeof(std::string) * count);\n            memcpy(temp, str + displs[i], count * sizeof(std::string));\n            // For each string, get the first letter and count the number of occurrences\n            for (int j = 0; j < count; j++)\n            {\n                char letter = temp[j][0];\n                int letterIndex = (int)letter - 97;\n                bins[letterIndex]++;\n            }\n            free(temp);\n        }\n        free(str);\n        free(displs);\n        free(recvcounts);\n    }\n\n    // On other ranks, just free the memory and return\n    else\n    {\n        free(str);\n        free(displs);\n        free(recvcounts);\n    }\n}",
            "const size_t size = s.size();\n    const size_t numThreads = 4;\n    #pragma omp parallel for num_threads(numThreads)\n    for (size_t i = 0; i < size; i++) {\n        size_t index = s[i][0] - 'a';\n        bins[index]++;\n    }\n}",
            "// TODO: Your code here\n  int nthreads, rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  nthreads = omp_get_max_threads();\n\n  if (rank == 0) {\n    for (size_t i = 0; i < 26; ++i) {\n      bins[i] = 0;\n    }\n  }\n\n  MPI_Bcast(bins.data(), 26, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    size_t max_string_length = std::max_element(s.begin(), s.end(), [](const std::string& lhs, const std::string& rhs) { return lhs.length() < rhs.length(); })->length();\n    size_t chunk_size = max_string_length / nthreads;\n    size_t left = max_string_length % nthreads;\n    size_t first = 0;\n    size_t last = 0;\n    for (size_t i = 0; i < nthreads; ++i) {\n      last += chunk_size + ((i < left)? 1 : 0);\n      if (rank == i) {\n        for (auto const& str : s) {\n          if (str[first] == 'a' + i) {\n            bins[i]++;\n          }\n        }\n      }\n      first += chunk_size + ((i < left)? 1 : 0);\n    }\n  } else {\n    size_t max_string_length = std::max_element(s.begin(), s.end(), [](const std::string& lhs, const std::string& rhs) { return lhs.length() < rhs.length(); })->length();\n    size_t chunk_size = max_string_length / nthreads;\n    size_t left = max_string_length % nthreads;\n    size_t first = rank * chunk_size + ((rank < left)? rank : left);\n    size_t last = rank * chunk_size + ((rank < left)? rank + 1 : left + 1);\n    for (auto const& str : s) {\n      if (str[first] == 'a' + rank) {\n        bins[rank]++;\n      }\n    }\n  }\n\n  MPI_Reduce(MPI_IN_PLACE, bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "auto const n = s.size();\n    auto const nprocs = omp_get_num_procs();\n    std::array<size_t, 26> bins_per_rank;\n\n    #pragma omp parallel default(none) shared(s, bins_per_rank)\n    {\n        int const rank = omp_get_thread_num();\n        int const nthreads = omp_get_num_threads();\n        std::array<size_t, 26> bins_per_thread;\n\n        #pragma omp for nowait\n        for (size_t i = rank; i < n; i += nthreads) {\n            std::string const& str = s[i];\n            bins_per_thread[str[0] - 'a']++;\n        }\n\n        #pragma omp critical\n        for (size_t i = 0; i < 26; i++)\n            bins_per_rank[i] += bins_per_thread[i];\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, bins_per_rank.data(), bins_per_rank.size(), MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < 26; i++)\n        bins[i] += bins_per_rank[i];\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_size = (size_t)s.size() / size;\n    size_t rem = (size_t)s.size() % size;\n\n    size_t begin_idx = rank * local_size + std::min(rem, rank);\n    size_t end_idx = begin_idx + local_size + (rank < rem? 1 : 0);\n    size_t n = end_idx - begin_idx;\n\n    std::array<size_t, 26> local_bins{};\n    for (size_t i = begin_idx; i < end_idx; ++i) {\n        ++local_bins[s[i][0] - 'a'];\n    }\n\n    std::array<size_t, 26> total_bins{};\n\n#pragma omp parallel for num_threads(26)\n    for (size_t i = 0; i < 26; ++i) {\n        MPI_Allreduce(&local_bins[i], &total_bins[i], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        for (size_t i = 0; i < n; ++i) {\n            bins[s[i][0] - 'a'] += total_bins[s[i][0] - 'a'];\n        }\n    }\n}",
            "#pragma omp parallel\n  {\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n      // This is the root process.\n      int nproc;\n      MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n      size_t n = s.size();\n      size_t chunk = (n + nproc - 1) / nproc;\n      size_t start = chunk * rank;\n\n      #pragma omp for\n      for (size_t i = start; i < std::min(n, start + chunk); ++i) {\n        auto c = s[i][0] - 'a';\n        ++bins[c];\n      }\n\n      // Gather bins from all processes\n      for (int i = 1; i < nproc; ++i) {\n        MPI_Status status;\n        MPI_Recv(bins.data(), 26, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, &status);\n      }\n    } else {\n      // This is a non-root process.\n\n      // Send bins to the root process\n      MPI_Status status;\n      MPI_Send(bins.data(), 26, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n  }\n}",
            "#pragma omp parallel\n   {\n      int rank = omp_get_thread_num();\n      int size = omp_get_num_threads();\n      std::vector<std::string> local_s = s;\n      std::array<size_t, 26> local_bins = {0,};\n      std::array<size_t, 26> bin_send_buffer = {0,};\n      std::array<size_t, 26> bin_receive_buffer = {0,};\n      size_t len = 0;\n\n      #pragma omp for\n      for (int i = 0; i < local_s.size(); i++)\n      {\n         len = local_s[i].size();\n         if (local_s[i][0] =='') continue;\n         local_bins[local_s[i][0] - 'a']++;\n      }\n\n      MPI_Gather(&local_bins, 26, MPI_LONG_LONG,\n            &bin_send_buffer, 26, MPI_LONG_LONG,\n            0, MPI_COMM_WORLD);\n\n      if (rank == 0) {\n         MPI_Reduce(MPI_IN_PLACE, &bin_send_buffer, 26, MPI_LONG_LONG,\n               MPI_SUM, 0, MPI_COMM_WORLD);\n      } else {\n         MPI_Reduce(&bin_send_buffer, &bin_receive_buffer, 26, MPI_LONG_LONG,\n               MPI_SUM, 0, MPI_COMM_WORLD);\n      }\n      if (rank == 0) {\n         bins = bin_receive_buffer;\n      }\n   }\n}",
            "//TODO: Parallelize using MPI and OpenMP\n  //Hint: Use std::getline\n  //Hint: Use a work-share construct to distribute work\n\n}",
            "int n_procs = omp_get_num_procs();\n    int my_rank = omp_get_thread_num();\n\n    size_t rank_size = s.size()/n_procs;\n    size_t my_size = rank_size + (s.size() % n_procs);\n\n    int n_letters = 26;\n    int * recv_counts = new int[n_procs];\n    MPI_Gather(&my_size, 1, MPI_INT, recv_counts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    char *recv_s = new char[my_size*sizeof(char)];\n    char *s_ptr = &s[0];\n\n    if(my_rank == 0){\n        char **recv_s_ptr = new char*[n_procs];\n        for(int i = 0; i < n_procs; i++){\n            recv_s_ptr[i] = new char[recv_counts[i]*sizeof(char)];\n        }\n\n        int offset = 0;\n        for(int i = 0; i < n_procs; i++){\n            for(size_t j = 0; j < recv_counts[i]; j++){\n                recv_s_ptr[i][offset] = s_ptr[offset];\n                offset++;\n            }\n        }\n\n        for(int i = 0; i < n_procs; i++){\n            MPI_Send(recv_s_ptr[i], recv_counts[i], MPI_CHAR, i, 0, MPI_COMM_WORLD);\n        }\n\n        for(int i = 0; i < n_procs; i++){\n            delete [] recv_s_ptr[i];\n            recv_s_ptr[i] = NULL;\n        }\n\n        delete [] recv_s_ptr;\n        recv_s_ptr = NULL;\n    }else{\n        MPI_Recv(recv_s, my_size, MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    char *start_ptr = &recv_s[0];\n    for(int i = 0; i < my_size; i++){\n        *start_ptr = tolower(*start_ptr);\n        start_ptr++;\n    }\n\n    for(int i = 0; i < my_size; i++){\n        size_t my_idx = i/rank_size;\n        if(my_rank == my_idx){\n            int bin_idx = recv_s[i] - 'a';\n            bins[bin_idx]++;\n        }\n    }\n\n    delete [] recv_s;\n    recv_s = NULL;\n\n    delete [] recv_counts;\n    recv_counts = NULL;\n}",
            "int numProc = omp_get_num_threads();\n  int rank = omp_get_thread_num();\n\n  // Split input to numProc parts and compute the bins for each part\n  std::vector<std::vector<std::string>> subS;\n  for (int i = 0; i < numProc; i++) {\n    int start = rank * s.size() / numProc;\n    int end = (rank + 1) * s.size() / numProc;\n    std::vector<std::string> subVector;\n    for (int j = start; j < end; j++) {\n      subVector.push_back(s[j]);\n    }\n    subS.push_back(subVector);\n  }\n\n  // Compute the bins for each part and add to the bins array\n  for (int i = 0; i < subS.size(); i++) {\n    std::vector<size_t> localBin(26);\n    for (int j = 0; j < subS[i].size(); j++) {\n      char letter = subS[i][j].at(0);\n      localBin[letter - 'a']++;\n    }\n\n    if (rank == 0) {\n      for (int j = 0; j < 26; j++) {\n        bins[j] += localBin[j];\n      }\n    }\n  }\n\n  // Wait for all ranks to complete their bins\n  MPI_Barrier(MPI_COMM_WORLD);\n  // Copy bins from local to global\n  if (rank == 0) {\n    std::array<size_t, 26> globalBin;\n    MPI_Gather(bins.data(), 26, MPI_UNSIGNED_LONG_LONG, globalBin.data(), 26, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n    for (int i = 0; i < 26; i++) {\n      bins[i] = globalBin[i];\n    }\n  }\n\n  // No more work to be done, so wait for all the threads to finish\n  omp_set_num_threads(1);\n  #pragma omp parallel\n  {\n    #pragma omp single\n    {\n      MPI_Barrier(MPI_COMM_WORLD);\n    }\n  }\n\n}",
            "#pragma omp parallel for\n    for(size_t i = 0; i < s.size(); i++) {\n        size_t offset = s[i][0] - 'a';\n        bins[offset] += 1;\n    }\n}",
            "// TODO\n\n  // Compute the alphabet size.\n  const size_t alphabetSize = 26;\n\n  // The size of s.\n  size_t const sSize = s.size();\n\n  // The number of ranks.\n  int const mpiSize = omp_get_max_threads();\n\n  // The rank of the current thread.\n  int const mpiRank = omp_get_thread_num();\n\n  // Let's make a copy of s, because I want to sort the local copy.\n  std::vector<std::string> sLocal;\n  sLocal = s;\n\n  // Let's sort the local copy of s.\n  std::sort(sLocal.begin(), sLocal.end());\n\n  // Let's compute the local first letter count.\n  size_t const localOffset = mpiRank * sSize / mpiSize;\n  size_t const localSize = sSize / mpiSize;\n  std::array<size_t, 26> binsLocal;\n  std::fill(binsLocal.begin(), binsLocal.end(), 0);\n\n  for (size_t i = localOffset; i < localOffset + localSize; i++) {\n    char letter = sLocal[i][0];\n    binsLocal[letter - 'a']++;\n  }\n\n  // Let's reduce the local first letter counts into the global first letter counts.\n  for (size_t i = 0; i < alphabetSize; i++) {\n    MPI_Allreduce(&binsLocal[i], &bins[i], 1, MPI_LONG, MPI_SUM, MPI_COMM_WORLD);\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  // Count the number of strings in the vector s that start with that letter.\n  for (const std::string& str : s)\n    ++bins[str[0] - 'a'];\n\n  // Use MPI and OpenMP to compute in parallel.\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t sSize = s.size() / size;\n\n  // Every rank has a complete copy of s.\n  std::vector<std::string> sRank(sSize);\n  std::copy(s.begin() + sSize * rank, s.begin() + sSize * rank + sSize, sRank.begin());\n\n#pragma omp parallel num_threads(size)\n  {\n    std::array<size_t, 26> binsRank;\n    std::fill(binsRank.begin(), binsRank.end(), 0);\n\n    // Count the number of strings in the vector s that start with that letter.\n    for (const std::string& str : sRank)\n      ++binsRank[str[0] - 'a'];\n\n    // Use MPI to accumulate the number of strings in the vector s that start with that letter.\n    for (int i = 0; i < 26; i++)\n      bins[i] = bins[i] + binsRank[i];\n  }\n\n  // Store the output in `bins` array on rank 0.\n  if (rank == 0)\n    for (int i = 0; i < 26; i++)\n      std::cout << bins[i] << \" \";\n}",
            "size_t size = s.size();\n\n    if (size == 0) return;\n\n    std::vector<int> first_letter(size, 0);\n    #pragma omp parallel for\n    for (int i = 0; i < size; ++i) {\n        first_letter[i] = s[i].front() - 'a';\n    }\n\n    int mpi_rank;\n    int mpi_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    int num_per_rank = size / mpi_size;\n    int num_remainder = size % mpi_size;\n    int begin = 0;\n    int end = 0;\n    if (mpi_rank == 0) {\n        std::vector<int> bins_local(26, 0);\n        begin = num_remainder;\n        end = num_remainder + num_per_rank;\n        for (int i = begin; i < end; ++i) {\n            int letter = first_letter[i];\n            bins_local[letter]++;\n        }\n        for (int i = 1; i < mpi_size; ++i) {\n            std::vector<int> bins_local_recv(26, 0);\n            MPI_Recv(&bins_local_recv[0], 26, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; ++j) {\n                bins_local[j] += bins_local_recv[j];\n            }\n        }\n        for (int i = 0; i < 26; ++i) {\n            bins[i] = bins_local[i];\n        }\n    }\n    else {\n        std::vector<int> bins_local(26, 0);\n        begin = num_remainder + num_per_rank * mpi_rank;\n        end = num_remainder + num_per_rank * mpi_rank + num_per_rank;\n        for (int i = begin; i < end; ++i) {\n            int letter = first_letter[i];\n            bins_local[letter]++;\n        }\n        MPI_Send(&bins_local[0], 26, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "std::vector<std::string> local_s;\n  std::vector<std::string> v;\n  for (int i = 0; i < s.size(); i++)\n  {\n    if (s[i].size() > 0)\n    {\n      local_s.push_back(s[i]);\n    }\n  }\n  std::sort(local_s.begin(), local_s.end());\n\n  int size = local_s.size();\n  int rank;\n  int n_process;\n  MPI_Comm_size(MPI_COMM_WORLD, &n_process);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int block = size/n_process;\n  int rem = size%n_process;\n  int block_first = rank*block;\n  int block_last = (rank+1)*block;\n\n  int lsize;\n  if (rem == 0)\n  {\n    lsize = block;\n    block_last = block_last + block;\n  }\n  else\n  {\n    if (rank == n_process - 1)\n    {\n      lsize = block + rem;\n    }\n    else\n    {\n      lsize = block;\n    }\n  }\n\n  std::vector<std::string> local_v;\n  if (rank == 0)\n  {\n    local_v = local_s;\n  }\n  else\n  {\n    local_v.resize(lsize);\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  if (rank == 0)\n  {\n    for (int i = 0; i < n_process; i++)\n    {\n      MPI_Status status;\n      std::vector<std::string> rcv_s;\n      if (i == 0)\n      {\n        rcv_s.resize(block_last);\n      }\n      else\n      {\n        rcv_s.resize(block);\n      }\n      MPI_Recv(&rcv_s[0], rcv_s.size(), MPI_CHAR, i, 0, MPI_COMM_WORLD, &status);\n\n      for (int j = 0; j < rcv_s.size(); j++)\n      {\n        local_s.push_back(rcv_s[j]);\n      }\n    }\n  }\n  else\n  {\n    MPI_Send(&local_s[0] + block_first, lsize, MPI_CHAR, 0, 0, MPI_COMM_WORLD);\n  }\n\n  #pragma omp parallel for\n  for (int i = 0; i < local_v.size(); i++)\n  {\n    char c = local_v[i][0];\n    int idx = c - 'a';\n    bins[idx]++;\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  if (rank == 0)\n  {\n    for (int i = 1; i < n_process; i++)\n    {\n      std::array<size_t, 26> tmp;\n      MPI_Status status;\n      MPI_Recv(&tmp, 26, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, &status);\n\n      for (int j = 0; j < 26; j++)\n      {\n        bins[j] = bins[j] + tmp[j];\n      }\n    }\n  }\n  else\n  {\n    MPI_Send(&bins, 26, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n  return;\n}",
            "size_t constexpr N_PER_RANK = 4;\n  std::array<size_t, 26> localBins;\n  std::fill(localBins.begin(), localBins.end(), 0);\n  #pragma omp parallel for\n  for(size_t i = 0; i < s.size(); ++i) {\n    localBins[s[i][0] - 'a']++;\n  }\n  MPI_Reduce(localBins.data(), bins.data(), 26, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "std::array<size_t, 26> hists {0};\n\n    // Compute the histogram of first letters\n#pragma omp parallel for default(none) shared(s) schedule(static, 1)\n    for (size_t i = 0; i < s.size(); ++i) {\n        int c = s[i].front();\n        if (c >= 'a' && c <= 'z') {\n            hists[c - 'a']++;\n        }\n    }\n\n    // Reduction\n    MPI_Reduce(hists.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "}",
            "MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n  MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n  bins.fill(0);\n  std::array<size_t, 26> tmpBins;\n  tmpBins.fill(0);\n  size_t n = s.size();\n\n  for (size_t i = 0; i < n; i++) {\n    tmpBins[s[i][0] - 'a']++;\n  }\n\n  MPI_Reduce(MPI_IN_PLACE, tmpBins.data(), tmpBins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (myRank == 0) {\n    for (int i = 0; i < 26; i++) {\n      bins[i] = tmpBins[i];\n    }\n  }\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    size_t s_length = s.size();\n    size_t const s_chunk_size = s_length / size;\n    size_t const s_chunk_rest = s_length % size;\n\n    std::vector<std::string> my_s;\n    if (rank == 0) {\n        std::vector<std::string> tmp_s;\n        tmp_s.resize(s_chunk_size + s_chunk_rest);\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&tmp_s[0], s_chunk_size, MPI_CHAR, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        tmp_s.resize(s_chunk_size + s_chunk_rest);\n        my_s.resize(s_chunk_size + s_chunk_rest);\n        my_s.assign(s.begin(), s.begin() + s_chunk_size + s_chunk_rest);\n    } else {\n        my_s.resize(s_chunk_size);\n        my_s.assign(s.begin() + (rank - 1) * s_chunk_size,\n                    s.begin() + (rank - 1) * s_chunk_size + s_chunk_size);\n        MPI_Send(&my_s[0], s_chunk_size, MPI_CHAR, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        // sort my_s\n        std::sort(my_s.begin(), my_s.end());\n        // sort bins\n        std::sort(bins.begin(), bins.end());\n\n        #pragma omp parallel for\n        for (int i = 0; i < 26; ++i) {\n            int const c = (i + 'a');\n            bins[i] = std::count_if(my_s.begin(), my_s.end(), [c](std::string const& x) {\n                                       return x[0] == c;\n                                   });\n        }\n    }\n}",
            "// TODO: Your code here\n\n}",
            "// Compute the local counts\n\tint num_strings = s.size();\n\tint local_count = 0;\n\tfor(int i = 0; i < num_strings; ++i) {\n\t\tif(s[i][0] >= 'a' && s[i][0] <= 'z') {\n\t\t\tlocal_count += 1;\n\t\t\tif(s[i][0] >= 'a' && s[i][0] <='m') {\n\t\t\t\tbins[s[i][0] - 'a'] += 1;\n\t\t\t}\n\t\t}\n\t}\n\n\t// Sum counts across the ranks\n\tint global_count = 0;\n\tMPI_Reduce(&local_count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tint local_sum = 0;\n\tint global_sum = 0;\n\t// Determine which rank has the most and the least counts\n\tMPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\t// Determine the delta between the total number of strings and the sum of the count of each rank\n\tint delta = num_strings - global_count;\n\n\t// If the current rank is the one that has the most number of strings\n\tif(global_sum == delta) {\n\t\t// Distribute the extra counts to the other ranks\n\t\tMPI_Bcast(&local_sum, 1, MPI_INT, MPI_COMM_WORLD);\n\t\tint delta2 = local_sum - num_strings;\n\t\t#pragma omp parallel for\n\t\tfor(int i = 0; i < 26; ++i) {\n\t\t\tbins[i] += delta2;\n\t\t}\n\t}\n\t// If the current rank is the one that has the least number of strings\n\telse {\n\t\t// Distribute the extra counts to the other ranks\n\t\tMPI_Bcast(&local_sum, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\t\tint delta2 = local_sum - num_strings;\n\t\t#pragma omp parallel for\n\t\tfor(int i = 0; i < 26; ++i) {\n\t\t\tbins[i] += delta2;\n\t\t}\n\t}\n\n}",
            "// write your code here\n\n}",
            "std::array<size_t, 26> local_bins;\n    std::array<size_t, 26> global_bins;\n    local_bins.fill(0);\n    global_bins.fill(0);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        auto first = s[i].front();\n        if (first < 'a' || first > 'z') {\n            continue;\n        }\n        local_bins[first - 'a']++;\n    }\n\n    MPI_Allreduce(local_bins.data(), global_bins.data(), 26, MPI_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    bins = global_bins;\n}",
            "// TODO: your code here\n  int rank, numproc;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &numproc);\n\n  int block = s.size() / numproc;\n  int extra = s.size() % numproc;\n\n  std::vector<std::string> local_strings;\n  for (int i = 0; i < block + (rank < extra? 1 : 0); i++) {\n    local_strings.push_back(s[i * numproc + rank]);\n  }\n  local_strings.resize(block + (rank < extra? 1 : 0));\n  int size = local_strings.size();\n  std::vector<size_t> counts(26, 0);\n\n  int rank_id = rank;\n  int left = rank - 1;\n  int right = rank + 1;\n  if (rank == 0)\n    left = numproc - 1;\n  if (rank == numproc - 1)\n    right = 0;\n\n  int left_size = 0, right_size = 0;\n  if (left >= 0) {\n    MPI_Send(local_strings.data(), local_strings.size(), MPI_CHAR, left, rank_id, MPI_COMM_WORLD);\n  }\n  if (right < numproc) {\n    MPI_Send(local_strings.data(), local_strings.size(), MPI_CHAR, right, rank_id, MPI_COMM_WORLD);\n  }\n  if (rank == 0) {\n    MPI_Recv(local_strings.data(), local_strings.size(), MPI_CHAR, left, rank_id, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    left_size = local_strings.size();\n    MPI_Recv(local_strings.data(), local_strings.size(), MPI_CHAR, right, rank_id, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    right_size = local_strings.size();\n  }\n  else {\n    MPI_Recv(local_strings.data(), local_strings.size(), MPI_CHAR, 0, rank_id, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    left_size = local_strings.size();\n    MPI_Recv(local_strings.data(), local_strings.size(), MPI_CHAR, 0, rank_id, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    right_size = local_strings.size();\n  }\n\n  for (int i = 0; i < left_size; i++) {\n    int pos = local_strings[i][0];\n    counts[pos]++;\n  }\n  for (int i = 0; i < right_size; i++) {\n    int pos = local_strings[i][0];\n    counts[pos]++;\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < size; i++) {\n      int pos = local_strings[i][0];\n      counts[pos]++;\n    }\n  }\n\n#pragma omp parallel for\n  for (int i = 0; i < 26; i++) {\n    bins[i] += counts[i];\n  }\n  return;\n}",
            "std::array<int, 26> bin_sum = {0};\n    for (int i = 0; i < 26; i++){\n        for (auto& str : s){\n            if (str.at(0) == 'a' + i){\n                bin_sum[i]++;\n            }\n        }\n    }\n\n    // MPI sum\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    for (int i = 0; i < 26; i++){\n        MPI_Allreduce(&bin_sum[i], &bins[i], 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0){\n        std::stringstream ss;\n        for (int i = 0; i < 26; i++){\n            ss << bins[i] << \" \";\n        }\n        std::cout << ss.str() << \"\\n\";\n    }\n}",
            "int rank, nproc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    std::vector<size_t> lcounts(26);\n\n    for (size_t i = rank; i < s.size(); i += nproc) {\n        int x = s[i][0] - 'a';\n        lcounts[x]++;\n    }\n\n    int size_of_chunk = (int) s.size() / nproc;\n    int offset = rank * size_of_chunk;\n    std::vector<size_t> chunk_lcounts(size_of_chunk);\n\n    if (rank == 0) {\n        int p;\n        for (p = 1; p < nproc; p++) {\n            MPI_Recv(&lcounts[0], 26, MPI_INT, p, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n\n        MPI_Send(&lcounts[0], 26, MPI_INT, 1, 0, MPI_COMM_WORLD);\n\n        for (int k = 0; k < size_of_chunk; k++) {\n            bins[k] = lcounts[k];\n        }\n    }\n\n    for (int i = 0; i < size_of_chunk; i++) {\n        chunk_lcounts[i] = lcounts[i + offset];\n    }\n\n    if (rank == 0) {\n        for (int p = 1; p < nproc; p++) {\n            MPI_Recv(&lcounts[0], 26, MPI_INT, p, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        int nthreads = omp_get_num_threads();\n\n        if (rank == 0) {\n            for (int i = 0; i < size_of_chunk; i++) {\n                for (int p = 0; p < nproc; p++) {\n                    bins[i] += lcounts[i + p * size_of_chunk];\n                }\n            }\n        } else {\n            for (int i = 0; i < size_of_chunk; i++) {\n                bins[i] += chunk_lcounts[i];\n            }\n        }\n    }\n}",
            "#pragma omp parallel\n  {\n    auto s_size = s.size();\n    std::array<size_t, 26> bins_private;\n    #pragma omp for\n    for (auto i = 0; i < s_size; i++) {\n      auto const& str = s[i];\n      if (str.empty()) continue;\n      auto const first_char = str[0];\n      auto const first_char_index = first_char - 'a';\n      bins_private[first_char_index]++;\n    }\n    #pragma omp critical\n    for (auto i = 0; i < 26; i++) {\n      bins[i] += bins_private[i];\n    }\n  }\n}",
            "// TODO: Your code goes here\n}",
            "assert(0 == s.size() % omp_get_max_threads());\n    assert(0 == s.size() % MPI_COMM_WORLD.Size());\n    std::array<size_t, 26> bins_private;\n\n#pragma omp parallel\n    {\n        int thread_id = omp_get_thread_num();\n        int rank = MPI_COMM_WORLD.Rank();\n        int n_threads = omp_get_max_threads();\n        int n_ranks = MPI_COMM_WORLD.Size();\n        int n_blocks = s.size() / n_threads;\n        int n_blocks_per_rank = s.size() / n_ranks;\n        int i_start = n_blocks_per_rank * thread_id;\n        int i_end = n_blocks_per_rank * (thread_id + 1);\n        int bin;\n\n        std::fill(bins_private.begin(), bins_private.end(), 0);\n\n        for (int i = i_start; i < i_end; ++i) {\n            bin = s[i][0] - 'a';\n            bins_private[bin]++;\n        }\n\n#pragma omp barrier\n\n        if (0 == thread_id) {\n            for (int i = 0; i < 26; i++) {\n                MPI_Reduce(&bins_private[i], &bins[i], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n            }\n\n            if (0 == rank) {\n                MPI_Reduce(&bins[0], &bins[0], 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n            }\n        }\n    }\n}",
            "//std::array<size_t, 26> bins{};\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    const size_t n = s.size();\n    std::vector<size_t> mys(n);\n    for (size_t i = 0; i < n; i++) {\n        mys[i] = s[i][0];\n    }\n    size_t n_threads = omp_get_max_threads();\n    size_t n_per_rank = n / size;\n    size_t n_per_rank_remainder = n % size;\n    size_t n_per_thread = (n_per_rank + n_per_rank_remainder) / n_threads;\n    size_t n_per_thread_remainder = n_per_rank_remainder % n_threads;\n    if (rank < n_per_rank_remainder) {\n        n_per_thread++;\n    }\n    if (rank < n_per_rank_remainder + n_per_thread_remainder) {\n        n_per_thread++;\n    }\n    size_t rank_offset = n_per_rank * rank + std::min(rank, n_per_rank_remainder);\n    size_t thread_offset = n_per_thread * rank + std::min(rank, n_per_thread_remainder);\n    #pragma omp parallel\n    {\n        int thread_id = omp_get_thread_num();\n        size_t offset = thread_offset + thread_id * n_per_thread;\n        for (size_t i = 0; i < n_per_thread; i++) {\n            char my_letter = 'a' + (mys[offset + i] - 'a');\n            if (my_letter >= 'a' && my_letter <= 'z') {\n                ++bins[my_letter - 'a'];\n            }\n        }\n    }\n    MPI_Allreduce(MPI_IN_PLACE, bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    size_t num_strings = s.size();\n    int chunk_size = num_strings / size;\n\n    std::array<size_t, 26> local_counts = {0};\n\n    // Compute local counts\n#pragma omp parallel for\n    for (size_t i = 0; i < num_strings; i++) {\n        char first_letter = s[i][0];\n        local_counts[first_letter - 'a']++;\n    }\n\n    // compute global counts\n    if (rank == 0) {\n        std::array<size_t, 26> global_counts = {0};\n        for (int i = 0; i < size; i++) {\n            MPI_Recv(&global_counts, 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++)\n                bins[j] += global_counts[j];\n        }\n    }\n    else {\n        MPI_Send(&local_counts, 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "bins.fill(0);\n\tfor (auto& str : s) {\n\t\tbins[str[0] - 'a'] += 1;\n\t}\n\tfor (size_t i = 0; i < 26; ++i) {\n\t\tsize_t sum = 0;\n\t\tMPI_Reduce(&bins[i], &sum, 1, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n\t\tif (rank == 0) {\n\t\t\tbins[i] = sum;\n\t\t}\n\t}\n\tif (rank == 0) {\n\t\tfor (auto& b : bins) {\n\t\t\tstd::cout << b << \" \";\n\t\t}\n\t\tstd::cout << std::endl;\n\t}\n}",
            "size_t nbins = s.size();\n    if (nbins < 1) {\n        for (size_t i = 0; i < 26; ++i) {\n            bins[i] = 0;\n        }\n        return;\n    }\n\n    size_t nproc = omp_get_num_threads();\n    size_t rank = omp_get_thread_num();\n\n    size_t nbins_per_thread = nbins / nproc;\n    size_t nbins_rem = nbins % nproc;\n    size_t my_start = rank * nbins_per_thread;\n    size_t my_end = my_start + nbins_per_thread;\n    if (rank < nbins_rem) {\n        my_start += rank;\n        my_end += rank + 1;\n    }\n    else {\n        my_start += nbins_rem;\n        my_end += nbins_rem;\n    }\n\n    for (size_t i = my_start; i < my_end; ++i) {\n        ++bins[s[i][0] - 'a'];\n    }\n}",
            "const size_t num_strings = s.size();\n    std::array<size_t, 26> bins_local = {0};\n    //#pragma omp parallel for schedule(static, 1)\n    for(size_t i = 0; i < num_strings; i++) {\n        size_t char_index = s[i][0] - 'a';\n        bins_local[char_index]++;\n    }\n    #pragma omp parallel for schedule(static, 1)\n    for(int i = 0; i < 26; i++) {\n        bins[i] += bins_local[i];\n    }\n}",
            "int rank;\n  int worldSize;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n\n  const size_t vectorLength = s.size();\n  const size_t maxThreads = omp_get_max_threads();\n  size_t maxThreadsPerRank = maxThreads / worldSize;\n\n  // Each rank will take an equal amount of work to do.\n  // This means we need to calculate the number of strings each rank will have to work on.\n  // The minimum amount of work we can have is 1, meaning we only have to work on the remainder of the vector.\n  // This will be 1 if vectorLength % maxThreadsPerRank == 0, else it will be (vectorLength % maxThreadsPerRank) + 1.\n  size_t stringsPerRank = 1;\n  if (maxThreadsPerRank > 0 && vectorLength % maxThreadsPerRank!= 0) {\n    stringsPerRank = (vectorLength % maxThreadsPerRank) + 1;\n  }\n\n  // If our rank is 0, we only need to work on the remainder of the vector.\n  // Otherwise we only need to work on the number of strings calculated above.\n  size_t stringsThisRank = (rank == 0)? vectorLength % maxThreadsPerRank : stringsPerRank;\n\n  // Calculate the number of strings each rank needs to work on in order to be completely done.\n  // This number is needed to determine when we can stop waiting for the other ranks to finish.\n  size_t stringsLeftToWorkOn = vectorLength;\n\n  // This array is used to store the strings on each rank.\n  std::vector<std::string> stringsOnRank(stringsThisRank);\n\n  if (stringsThisRank > 0) {\n    // Get the string that this rank will work on.\n    for (size_t i = 0; i < stringsThisRank; i++) {\n      stringsOnRank[i] = s[i + rank * stringsPerRank];\n    }\n\n    // Each thread will work on an equal amount of work to do.\n    // This means we need to calculate the number of letters each thread will work on.\n    // This will be 1 if stringsThisRank % maxThreads == 0, else it will be (stringsThisRank % maxThreads) + 1.\n    size_t lettersPerThread = 1;\n    if (maxThreads > 0 && stringsThisRank % maxThreads!= 0) {\n      lettersPerThread = (stringsThisRank % maxThreads) + 1;\n    }\n\n    // Calculate the index of the first letter this thread will work on.\n    size_t firstLetterIndex = (rank * maxThreadsPerRank) * lettersPerThread;\n\n    // Calculate the number of letters we need to process to be completely done.\n    size_t lettersLeftToProcess = stringsThisRank * lettersPerThread;\n\n    #pragma omp parallel\n    {\n      int threadID = omp_get_thread_num();\n      int numThreads = omp_get_num_threads();\n\n      // Calculate the index of the first letter this thread will work on.\n      size_t firstLetterIndex = (rank * maxThreadsPerRank + threadID) * lettersPerThread;\n\n      // Calculate the number of letters this thread needs to work on.\n      size_t lettersThisThread = (rank * maxThreadsPerRank + threadID + 1) * lettersPerThread - firstLetterIndex;\n\n      // Loop through each string on the current rank.\n      for (size_t i = 0; i < stringsOnRank.size(); i++) {\n        // Loop through each letter in the current string.\n        for (size_t j = firstLetterIndex; j < firstLetterIndex + lettersThisThread; j++) {\n          // Increment the count for the current letter.\n          bins[stringsOnRank[i][j] - 'a']++;\n        }\n      }\n\n      // Wait for everyone to be done.\n      #pragma omp barrier\n\n      // If this is not the last thread, send the count for the letter it",
            "// TODO\n\n    size_t s_size = s.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int chunk = s_size/size;\n\n    int * bins_dist = new int[26];\n    for (int i = 0; i < 26; i++)\n    {\n        bins_dist[i] = 0;\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < s_size; i++)\n    {\n        int j = s[i][0] - 'a';\n        bins_dist[j]++;\n    }\n\n    for (int i = 0; i < 26; i++)\n    {\n        bins[i] += bins_dist[i];\n    }\n\n    MPI_Reduce(bins_dist, bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if(rank == 0)\n    {\n        for (int i = 0; i < 26; i++)\n        {\n            std::cout << bins[i] << std::endl;\n        }\n    }\n\n    delete[] bins_dist;\n}",
            "size_t nstrings = s.size();\n   std::array<size_t, 26> partial_bins;\n   partial_bins.fill(0);\n\n   #pragma omp parallel\n   {\n      int rank = omp_get_thread_num();\n      int nranks = omp_get_num_threads();\n      int start_str = rank * (nstrings / nranks);\n      int end_str = (rank+1) * (nstrings / nranks);\n\n      for (size_t i = start_str; i < end_str; i++){\n         if (i >= nstrings) {\n            continue;\n         }\n         char c = s[i][0];\n         #pragma omp atomic\n         partial_bins[c - 'a'] += 1;\n      }\n   }\n   MPI_Reduce(partial_bins.data(), bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n   if (rank == 0) {\n      std::fill(bins.begin(), bins.end(), 0);\n   }\n}",
            "std::array<int, 26> bin{};\n\n    int size = s.size();\n    int rank, np;\n    MPI_Comm_size(MPI_COMM_WORLD, &np);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int count_str = size / np;\n\n#pragma omp parallel for\n    for (int i = rank * count_str; i < count_str * (rank + 1); i++) {\n        if (s[i][0] >= 'a' && s[i][0] <= 'z')\n            bin[s[i][0] - 'a'] += 1;\n    }\n    MPI_Reduce(bin.data(), bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "//your code here\n\treturn;\n}",
            "int nb_ranks = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &nb_ranks);\n\n    int rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // We split the vector s in \"nb_ranks\" pieces\n    // Then, each rank will work on its own slice of the vector\n    int nb_items_per_rank = (int)s.size() / nb_ranks;\n\n    // The last rank may receive more items\n    int nb_items_rank = nb_items_per_rank + s.size() % nb_ranks;\n\n    if (nb_items_rank > s.size()) {\n        std::cout << \"ERROR: The number of items for the rank \" << rank << \" is greater than the size of s!\" << std::endl;\n        exit(0);\n    }\n\n    // The first item of the slice\n    int first_item_rank = rank * nb_items_per_rank;\n    // The last item of the slice\n    int last_item_rank = first_item_rank + nb_items_rank;\n\n    // Create a local copy of the vector s on this rank\n    std::vector<std::string> s_rank;\n    for (int i = first_item_rank; i < last_item_rank; i++) {\n        s_rank.push_back(s[i]);\n    }\n\n    std::array<size_t, 26> bins_rank;\n\n    // Compute the result on this rank\n    for (int i = 0; i < nb_items_rank; i++) {\n        // std::cout << \"Rank \" << rank << \": \" << s_rank[i] << \" -> \" << s_rank[i][0] << std::endl;\n        bins_rank[s_rank[i][0] - 'a']++;\n    }\n\n    // Reduce the local result to the result on rank 0\n    if (rank == 0) {\n        for (int i = 1; i < nb_ranks; i++) {\n            MPI_Recv(bins_rank.data(), bins_rank.size(), MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n    else {\n        MPI_Send(bins_rank.data(), bins_rank.size(), MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        // Sum the results on rank 0\n        for (int i = 0; i < bins_rank.size(); i++) {\n            bins[i] += bins_rank[i];\n        }\n    }\n}",
            "int rank, size, ret;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int *bins_local = new int[26];\n    int *bins_global = new int[26];\n    for (int i = 0; i < 26; i++) {\n        bins_local[i] = 0;\n    }\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        int letter = s[i][0] - 'a';\n        bins_local[letter] += 1;\n    }\n    ret = MPI_Allreduce(bins_local, bins_global, 26, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    for (int i = 0; i < 26; i++) {\n        bins[i] = bins_global[i];\n    }\n    delete[] bins_local;\n    delete[] bins_global;\n}",
            "size_t local_bin = 0;\n\n    std::vector<std::string> copy_s = s;\n\n    #pragma omp parallel for num_threads(omp_get_max_threads()) default(none) shared(copy_s, bins) reduction(+:local_bin)\n    for (size_t i = 0; i < copy_s.size(); i++) {\n        std::string str = copy_s.at(i);\n        std::string first_letter = str.at(0);\n\n        if (first_letter == 'a') {\n            local_bin++;\n        } else if (first_letter == 'b') {\n            local_bin++;\n        } else if (first_letter == 'c') {\n            local_bin++;\n        } else if (first_letter == 'd') {\n            local_bin++;\n        } else if (first_letter == 'e') {\n            local_bin++;\n        } else if (first_letter == 'f') {\n            local_bin++;\n        } else if (first_letter == 'g') {\n            local_bin++;\n        } else if (first_letter == 'h') {\n            local_bin++;\n        } else if (first_letter == 'i') {\n            local_bin++;\n        } else if (first_letter == 'j') {\n            local_bin++;\n        } else if (first_letter == 'k') {\n            local_bin++;\n        } else if (first_letter == 'l') {\n            local_bin++;\n        } else if (first_letter =='m') {\n            local_bin++;\n        } else if (first_letter == 'n') {\n            local_bin++;\n        } else if (first_letter == 'o') {\n            local_bin++;\n        } else if (first_letter == 'p') {\n            local_bin++;\n        } else if (first_letter == 'q') {\n            local_bin++;\n        } else if (first_letter == 'r') {\n            local_bin++;\n        } else if (first_letter =='s') {\n            local_bin++;\n        } else if (first_letter == 't') {\n            local_bin++;\n        } else if (first_letter == 'u') {\n            local_bin++;\n        } else if (first_letter == 'v') {\n            local_bin++;\n        } else if (first_letter == 'w') {\n            local_bin++;\n        } else if (first_letter == 'x') {\n            local_bin++;\n        } else if (first_letter == 'y') {\n            local_bin++;\n        } else if (first_letter == 'z') {\n            local_bin++;\n        }\n\n        bins[local_bin] += 1;\n    }\n}",
            "// TODO\n}",
            "size_t n = s.size();\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, nranks;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &nranks);\n\n    // Every rank has a complete copy of s.\n    // The result is stored in bins on rank 0.\n    std::vector<size_t> localBins(26);\n    size_t localN = (n + nranks - 1) / nranks;\n    size_t localStart = rank * localN;\n    size_t localEnd = std::min(localStart + localN, n);\n\n    // Parallelize by letter.\n    #pragma omp parallel for\n    for (size_t i = localStart; i < localEnd; i++) {\n        int letter = s[i][0] - 'a';\n        localBins[letter]++;\n    }\n\n    // Global reduction.\n    std::vector<size_t> bins(26);\n    MPI_Reduce(localBins.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // Store the output in bins.\n    if (rank == 0) {\n        for (int letter = 0; letter < 26; letter++) {\n            bins[letter] = localBins[letter];\n        }\n    }\n}",
            "#pragma omp parallel for\n    for(size_t i = 0; i < 26; i++)\n        bins[i] = 0;\n\n    #pragma omp parallel for\n    for(size_t i = 0; i < s.size(); i++)\n    {\n        bins[s[i][0] - 'a'] += 1;\n    }\n}",
            "// TODO: compute bins for each letter in the alphabet\n\n\n  // TODO: combine the results and store in `bins` on rank 0\n\n}",
            "const int mpi_rank = 0;\n  const int mpi_size = 4;\n\n  size_t num_strings = s.size();\n\n  //TODO:\n  //1. split the input string into 4 equal parts\n  //2. create a 4-d array (bins_4d) that contains all the first letter counts for each part of s\n  //3. compute the first letter count for each part of s\n  //4. sum the first letter counts for each part of s and put the sum in bins[0]\n  //5. use MPI to compute the partial sums for each first letter count in bins_4d (use MPI_SUM)\n  //6. compute the partial sums for each first letter count in bins_4d and put the sums in bins[0]\n  //7. free the 4-d array bins_4d\n  //8. use OpenMP to compute the total sum of first letter counts (use #pragma omp parallel for)\n  //9. use MPI to compute the total sum of first letter counts (use MPI_SUM)\n  //10. compute the total sum of first letter counts and put the sum in bins[0]\n\n\n  //TODO: use MPI to create a 4-d array (bins_4d) that contains all the first letter counts for each part of s\n  size_t size_2d_array = 4 * num_strings;\n  size_t size_3d_array = 4 * 26;\n  std::array<size_t, 26>* bins_4d;\n  if (mpi_rank == 0) {\n    bins_4d = new std::array<size_t, 26>[size_3d_array];\n  }\n\n  //TODO: compute the first letter count for each part of s\n  for (int i = 0; i < num_strings; i++) {\n    for (int j = 0; j < 26; j++) {\n      char c = s[i][j];\n      if (islower(c)) {\n        bins_4d[i * 26 + j][c - 'a']++;\n      }\n    }\n  }\n\n  //TODO: use MPI to compute the partial sums for each first letter count in bins_4d (use MPI_SUM)\n  MPI_Request reqs[size_3d_array];\n  int counts[size_3d_array];\n  int displs[size_3d_array];\n  MPI_Status stats[size_3d_array];\n  for (int i = 0; i < size_3d_array; i++) {\n    displs[i] = i * 26;\n    counts[i] = 26;\n    MPI_Ireduce(bins_4d[i].data(), bins_4d[i].data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD, &reqs[i]);\n  }\n\n  //TODO: compute the partial sums for each first letter count in bins_4d and put the sums in bins[0]\n  MPI_Waitall(size_3d_array, reqs, stats);\n\n  //TODO: free the 4-d array bins_4d\n  if (mpi_rank == 0) {\n    delete[] bins_4d;\n  }\n\n  //TODO: use OpenMP to compute the total sum of first letter counts (use #pragma omp parallel for)\n  for (int i = 0; i < 26; i++) {\n    size_t sum = 0;\n    for (int j = 0; j < num_strings; j++) {\n      sum += bins[j][i];\n    }\n    bins[0][i] = sum;\n  }\n\n  //TODO: use MPI to compute the total sum of first letter counts (use MPI_SUM)\n  MPI_Request reqs2[26];\n  int counts2[26];\n  int displs2[26];\n  MPI_Status stats2[26];\n  for (int i = 0; i <",
            "// TODO: Your code here\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int chunk_size = s.size() / size;\n  std::vector<std::string> chunk;\n  std::vector<int> bins_chunk;\n  bins_chunk.resize(26);\n  if(rank == 0) {\n    for(int i=0; i<size; i++) {\n      if(i == 0) {\n        chunk = std::vector<std::string>(s.begin(), s.begin() + chunk_size);\n      } else {\n        int start_index = i*chunk_size;\n        int end_index = start_index + chunk_size;\n        if(end_index > s.size()) {\n          end_index = s.size();\n        }\n        chunk = std::vector<std::string>(s.begin() + start_index, s.begin() + end_index);\n      }\n      #pragma omp parallel for\n      for(int i=0; i<chunk.size(); i++) {\n        if(chunk.at(i).size() > 0) {\n          bins_chunk[chunk.at(i).at(0)-'a']++;\n        }\n      }\n      MPI_Barrier(MPI_COMM_WORLD);\n      #pragma omp parallel for\n      for(int i=0; i<26; i++) {\n        bins[i] += bins_chunk[i];\n      }\n    }\n  } else {\n    chunk = std::vector<std::string>(s.begin() + rank*chunk_size, s.begin() + (rank+1)*chunk_size);\n    #pragma omp parallel for\n    for(int i=0; i<chunk.size(); i++) {\n      if(chunk.at(i).size() > 0) {\n        bins_chunk[chunk.at(i).at(0)-'a']++;\n      }\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n    #pragma omp parallel for\n    for(int i=0; i<26; i++) {\n      bins[i] += bins_chunk[i];\n    }\n  }\n}",
            "auto start = MPI_Wtime();\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> bins_local = {0};\n\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        bins_local[s[i][0] - 'a']++;\n    }\n\n    std::array<size_t, 26> bins_reduce = {0};\n\n    if (rank == 0) {\n        for (int i = 0; i < size; i++) {\n            MPI_Status status;\n            MPI_Recv(&bins_reduce, 26, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, &status);\n        }\n    } else {\n        MPI_Send(&bins_local, 26, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] = bins_reduce[i];\n        }\n    }\n\n    auto stop = MPI_Wtime();\n\n    if (rank == 0) {\n        printf(\"Elapsed time: %f\\n\", stop - start);\n    }\n}",
            "for (int i = 0; i < s.size(); ++i) {\n        ++bins[s[i][0] - 'a'];\n    }\n}",
            "std::array<size_t, 26> local;\n    #pragma omp parallel\n    {\n        const size_t id = omp_get_thread_num();\n        for (const auto& str : s)\n            ++local[str[0] - 'a'];\n        for (size_t i = 0; i < 26; i++)\n            #pragma omp atomic\n                bins[i] += local[i];\n    }\n}",
            "int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    int size = s.size();\n    int count = s.size() / world_size + (world_rank < s.size() % world_size);\n    int start = count * world_rank;\n\n    int chunk_size = size / world_size + (world_rank < size % world_size);\n\n    std::vector<std::string> local_vector;\n\n    if (world_rank == 0)\n        local_vector.resize(s.size());\n\n    for (int i = 0; i < count; i++)\n        local_vector.push_back(s[start + i]);\n\n    // Split by words\n    #pragma omp parallel for\n    for (int i = 0; i < local_vector.size(); i++) {\n        std::string word = local_vector[i];\n        if (word.size() == 0)\n            continue;\n        if (word[0] >= 'A' && word[0] <= 'Z')\n            word[0] += 32;\n        local_vector[i] = word;\n    }\n\n    // Counting letters\n    #pragma omp parallel for\n    for (int i = 0; i < local_vector.size(); i++) {\n        if (local_vector[i][0] >= 'a' && local_vector[i][0] <= 'z') {\n            int letter_index = (int) (local_vector[i][0] - 'a');\n            bins[letter_index]++;\n        }\n    }\n\n    // Reduction\n    if (world_rank == 0) {\n        MPI_Reduce(MPI_IN_PLACE, bins.data(), 26, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Reduce(bins.data(), bins.data(), 26, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "std::array<size_t, 26> local_bins;\n    for (int i = 0; i < s.size(); i++) {\n        if (s[i][0] > 96 && s[i][0] < 123) {\n            local_bins[s[i][0] - 97]++;\n        }\n    }\n\n    MPI_Allreduce(local_bins.data(), bins.data(), 26, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  size_t n = s.size() / 2;\n  size_t nbins = 26;\n  int nranks = -1;\n  MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n  size_t offset = n / nranks * rank;\n  size_t count = n / nranks;\n  if (rank == nranks - 1) count = n - offset;\n\n  std::array<size_t, 26> local_bins;\n  for (size_t i = 0; i < nbins; ++i)\n    local_bins[i] = 0;\n\n  for (size_t i = offset; i < offset + count; ++i) {\n    char firstLetter = s[i][0];\n    local_bins[firstLetter - 'a']++;\n  }\n\n  std::array<size_t, 26> global_bins;\n  for (size_t i = 0; i < nbins; ++i)\n    global_bins[i] = 0;\n\n  for (size_t i = 0; i < nbins; ++i)\n    MPI_Reduce(&local_bins[i], &global_bins[i], 1, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  for (size_t i = 0; i < nbins; ++i)\n    bins[i] = global_bins[i];\n}",
            "// TODO: Your code here\n\n}",
            "// Your code here\n\tint mpi_size, mpi_rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n\tstd::vector<std::string> my_s;\n\tfor (auto i = mpi_rank * s.size() / mpi_size; i < (mpi_rank + 1) * s.size() / mpi_size; ++i) {\n\t\tmy_s.push_back(s[i]);\n\t}\n\n\t#pragma omp parallel for\n\tfor (auto j = 0; j < my_s.size(); ++j) {\n\t\tauto ch = my_s[j][0];\n\t\tif (ch >= 'a' && ch <= 'z') {\n\t\t\tbins[ch - 'a']++;\n\t\t}\n\t}\n}",
            "}",
            "int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    std::vector<int> result(26);\n    // 1. Parallelize the outer loop\n#pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        char c = 'a' + i;\n        int counter = 0;\n        for (auto const& w: s) {\n            if (w[0] == c) {\n                counter++;\n            }\n        }\n        result[i] = counter;\n    }\n\n    // 2. Add the counts\n    std::vector<int> sum_result(26);\n    MPI_Reduce(result.data(), sum_result.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // 3. Update bins if rank 0\n    if (world_rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] = sum_result[i];\n        }\n    }\n}",
            "size_t mpiRank, mpiSize;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpiSize);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpiRank);\n\n    int binSize = s.size() / mpiSize;\n    if (mpiRank == mpiSize - 1) {\n        binSize += s.size() % mpiSize;\n    }\n    std::array<size_t, 26> bin;\n    for (int i = 0; i < 26; i++) bin[i] = 0;\n\n#pragma omp parallel\n    {\n        int start, end;\n        int mpiRank;\n        MPI_Comm_rank(MPI_COMM_WORLD, &mpiRank);\n\n        start = mpiRank * binSize;\n        end = start + binSize;\n\n        for (int i = start; i < end; i++) {\n            size_t index = s[i][0] - 'a';\n            bin[index]++;\n        }\n    }\n    MPI_Gather(bin.data(), 26, MPI_UNSIGNED_LONG_LONG, bins.data(), 26, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "#pragma omp parallel\n  #pragma omp single\n  {\n    for (int rank = 0; rank < omp_get_num_threads(); rank++) {\n      if (rank == 0) {\n        for (int i = 0; i < 26; i++) {\n          bins[i] = 0;\n        }\n      }\n      #pragma omp task shared(bins)\n      {\n        int thread_num = omp_get_thread_num();\n        int rank_num = rank + thread_num * omp_get_num_threads();\n\n        if (rank_num >= s.size()) break;\n        // std::cout << rank_num << \"\\n\";\n\n        std::string word = s[rank_num];\n        char firstLetter = word[0];\n        int letter = (int)firstLetter;\n        if (letter >= 97 && letter <= 122) {\n          letter -= 97;\n          bins[letter]++;\n        }\n      }\n    }\n  }\n}",
            "// TODO:\n}",
            "}",
            "assert(s.size()!= 0);\n\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, nProc;\n    MPI_Comm_size(comm, &nProc);\n    MPI_Comm_rank(comm, &rank);\n\n    size_t local_size = s.size()/nProc;\n    size_t offset = rank * local_size;\n\n    // std::cout << \"Rank \" << rank << \": \" << local_size << \" entries in my slice\" << std::endl;\n\n    // count occurrences of the first letter in the strings in my slice\n    // #pragma omp parallel for\n    for(size_t i = 0; i < local_size; ++i) {\n        size_t letter = s[offset + i][0] - 'a';\n        ++bins[letter];\n    }\n\n    // #pragma omp parallel\n    {\n        int numThreads = omp_get_num_threads();\n        int threadID = omp_get_thread_num();\n\n        int start = numThreads * (offset/local_size);\n        int end = numThreads * ((offset+local_size)/local_size);\n        int range = (end - start) + 1;\n\n        if(rank == 0)\n            printf(\"Rank %d has %d threads (%d-%d)\\n\", rank, numThreads, start, end);\n\n        // printf(\"Rank %d thread %d has range %d\\n\", rank, threadID, range);\n\n        // scan bins on each thread\n        // #pragma omp barrier\n        std::array<size_t, 26> thread_bins = {0};\n        for(int i = 0; i < range; ++i)\n            thread_bins[i] = bins[start + i];\n\n        // #pragma omp barrier\n        // #pragma omp single\n        {\n            for(int i = 0; i < range; ++i)\n                thread_bins[i] = bins[start + i] + thread_bins[i - 1];\n        }\n\n        // #pragma omp barrier\n        for(int i = 0; i < range; ++i)\n            bins[start + i] = thread_bins[i];\n\n        // #pragma omp barrier\n    }\n\n    // gather results to rank 0\n    if(rank == 0) {\n        std::array<size_t, 26> all_bins = {0};\n        for(int i = 0; i < nProc; ++i)\n            MPI_Gather(&bins[0], 26, MPI_LONG, &all_bins[0], 26, MPI_LONG, 0, comm);\n\n        // print results\n        for(int i = 0; i < 26; ++i)\n            printf(\"%ld \", all_bins[i]);\n    } else {\n        // broadcast results from rank 0 to other ranks\n        MPI_Bcast(&bins[0], 26, MPI_LONG, 0, comm);\n    }\n\n}",
            "const size_t numStrings = s.size();\n    int rank = 0;\n    int worldSize = 1;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n\n    size_t stringsPerRank = numStrings / worldSize;\n    size_t remainder = numStrings % worldSize;\n\n    std::vector<size_t> myStrings(stringsPerRank);\n    size_t nStrings = 0;\n    int offset = 0;\n    if (rank < remainder) {\n        offset = stringsPerRank * rank + rank;\n        nStrings = stringsPerRank + 1;\n    } else {\n        offset = stringsPerRank * rank + remainder;\n        nStrings = stringsPerRank;\n    }\n\n    for (int i = 0; i < nStrings; i++) {\n        myStrings[i] = s[offset + i].size();\n    }\n\n    std::vector<size_t> strings(numStrings);\n\n    MPI_Allgather(&stringsPerRank, 1, MPI_UNSIGNED, &strings[0], 1, MPI_UNSIGNED, MPI_COMM_WORLD);\n\n    std::vector<size_t> offsets(worldSize);\n    for (int i = 0; i < worldSize; i++) {\n        offsets[i] = std::accumulate(strings.begin(), strings.begin() + i, 0);\n    }\n\n    std::vector<size_t> myOffsets(worldSize);\n    for (int i = 0; i < worldSize; i++) {\n        myOffsets[i] = std::accumulate(myStrings.begin(), myStrings.begin() + i, 0);\n    }\n\n#pragma omp parallel for\n    for (int i = 0; i < nStrings; i++) {\n        int letter = s[offset + i][0];\n        if (letter < 'a' || letter > 'z') {\n            bins[letter - 'A'] += 0;\n        } else {\n            bins[letter - 'a'] += 1;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < worldSize; i++) {\n            size_t offset = myOffsets[i];\n            size_t nStrings = strings[i];\n            for (int j = 0; j < nStrings; j++) {\n                int letter = s[offset + j][0];\n                if (letter < 'a' || letter > 'z') {\n                    bins[letter - 'A'] += 0;\n                } else {\n                    bins[letter - 'a'] += 1;\n                }\n            }\n        }\n    }\n\n    return;\n}",
            "int mpi_rank;\n  int mpi_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n  std::array<size_t, 26> bins_local;\n  // fill bins_local\n  #pragma omp parallel for\n  for (int i = 0; i < s.size(); ++i) {\n    bins_local[s[i][0]-'a']++;\n  }\n\n  std::array<size_t, 26> bins_global;\n\n  if (mpi_rank == 0) {\n    bins_global[0] = bins_local[0];\n    for (int i = 1; i < mpi_size; ++i) {\n      MPI_Recv(&bins_local[0], 26, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 26; ++j) {\n        bins_global[j] += bins_local[j];\n      }\n    }\n  } else {\n    MPI_Send(&bins_local[0], 26, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n  if (mpi_rank == 0) {\n    for (int i = 0; i < 26; ++i) {\n      bins[i] = bins_global[i];\n    }\n  }\n}",
            "// Write your code here\n}",
            "MPI_Comm comm;\n  int rank, nranks;\n  comm = MPI_COMM_WORLD;\n  MPI_Comm_rank(comm, &rank);\n  MPI_Comm_size(comm, &nranks);\n\n  int chunkSize = s.size() / nranks;\n  int lastChunkSize = s.size() % nranks;\n\n  std::vector<std::string> sRank;\n\n  int size;\n\n  if (rank == 0) {\n    size = 1;\n  } else {\n    size = 0;\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < lastChunkSize; i++) {\n      sRank.push_back(s[i]);\n    }\n  } else {\n    for (int i = 0; i < chunkSize; i++) {\n      sRank.push_back(s[size + i]);\n    }\n  }\n\n  std::array<size_t, 26> binsRank;\n  binsRank.fill(0);\n\n  for (int i = 0; i < sRank.size(); i++) {\n    binsRank[sRank[i][0] - 'a']++;\n  }\n\n  std::array<size_t, 26> binsGlob;\n  binsGlob.fill(0);\n  MPI_Reduce(&binsRank, &binsGlob, 26, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < 26; i++) {\n      bins[i] = binsGlob[i];\n    }\n  }\n}",
            "#pragma omp parallel\n    {\n        std::array<size_t, 26> bins_private;\n        std::fill(bins_private.begin(), bins_private.end(), 0);\n\n        #pragma omp for\n        for (int i = 0; i < s.size(); i++) {\n            size_t c = s[i][0] - 'a';\n            bins_private[c] += 1;\n        }\n\n        #pragma omp critical\n        for (int i = 0; i < 26; i++) {\n            bins[i] += bins_private[i];\n        }\n    }\n}",
            "int mpi_rank, mpi_size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n  int bin_size = s.size() / mpi_size + 1;\n\n  if (mpi_rank == 0) {\n    std::cout << \"rank \" << mpi_rank << \" has \" << bin_size << \" elements to process\" << std::endl;\n  }\n  //\n  // Your code here\n  //\n  //\n  // End of your code\n  //\n\n  if (mpi_rank == 0) {\n    for (int i = 0; i < bin_size; i++) {\n      std::cout << bins[i] << \" \";\n    }\n    std::cout << std::endl;\n  }\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  if (mpi_rank == 0) {\n    std::cout << \"---------------------------------\" << std::endl;\n    std::cout << \"rank \" << mpi_rank << \" finished.\" << std::endl;\n  }\n}",
            "bins = {0};\n\tomp_set_num_threads(8);\n#pragma omp parallel for shared(bins)\n\tfor (int i = 0; i < s.size(); i++) {\n\t\tbins[s[i][0] - 'a']++;\n\t}\n}",
            "// Your code here\n  #pragma omp parallel\n  {\n    int rank, num_procs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    for (size_t i = rank; i < s.size(); i += num_procs)\n    {\n      char c = s[i][0];\n      if (c >= 'a' && c <= 'z')\n        bins[c - 'a']++;\n    }\n  }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size_t ssize = s.size();\n    size_t stride = ssize / size;\n    int last = ssize - stride * size;\n    if (rank == 0) {\n        for (int i = 0; i < size; i++) {\n            for (size_t j = i * stride; j < (i + 1) * stride; j++) {\n                bins[s[j][0] - 'a']++;\n            }\n        }\n        if (last > 0) {\n            for (int i = 0; i < last; i++) {\n                bins[s[ssize - last + i][0] - 'a']++;\n            }\n        }\n    }\n\n    int* counts = new int[26];\n    MPI_Gather(bins.data(), 26, MPI_INT, counts, 26, MPI_INT, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] = 0;\n        }\n        for (int i = 0; i < size; i++) {\n            for (int j = 0; j < 26; j++) {\n                bins[j] += counts[i * 26 + j];\n            }\n        }\n    }\n    MPI_Bcast(bins.data(), 26, MPI_INT, 0, MPI_COMM_WORLD);\n    delete[] counts;\n}",
            "int const mpi_rank = MPI_PROC_NULL;\n    int const mpi_size = MPI_PROC_NULL;\n\n    omp_set_dynamic(0);\n    omp_set_num_threads(mpi_size);\n\n    if (mpi_rank == 0) {\n\n        std::array<size_t, 26> local_bins{};\n\n        int const chunks = mpi_size;\n        int const chunk_size = s.size() / chunks;\n        int const leftover = s.size() % chunks;\n        int const base = chunk_size * mpi_rank;\n        int const end = (base + chunk_size) - 1;\n        int const start = (base + leftover) + 1;\n\n        std::array<size_t, 26> my_bins{};\n\n        for (int i = start; i <= end; ++i) {\n            ++my_bins[s[i][0] - 'a'];\n        }\n\n        MPI_Request request;\n        MPI_Status status;\n\n        MPI_Ireduce(&my_bins[0], &bins[0], 26, MPI_LONG_LONG_INT, MPI_SUM, mpi_rank, MPI_COMM_WORLD, &request);\n        MPI_Wait(&request, &status);\n\n    }\n    else {\n\n        int const chunks = mpi_size;\n        int const chunk_size = s.size() / chunks;\n        int const leftover = s.size() % chunks;\n        int const base = chunk_size * mpi_rank;\n        int const end = (base + chunk_size) - 1;\n        int const start = (base + leftover) + 1;\n\n        std::array<size_t, 26> local_bins{};\n\n        for (int i = start; i <= end; ++i) {\n            ++local_bins[s[i][0] - 'a'];\n        }\n\n        MPI_Request request;\n        MPI_Status status;\n\n        MPI_Ireduce(&local_bins[0], &bins[0], 26, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD, &request);\n        MPI_Wait(&request, &status);\n\n    }\n\n}",
            "int rank, num_procs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n  if (rank == 0) {\n    int n = s.size();\n    std::array<size_t, 26> counts {0};\n    std::vector<size_t> sums(num_procs, 0);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n      counts[s[i][0] - 'a']++;\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n      sums[i%num_procs] += counts[i];\n    }\n\n    MPI_Reduce(sums.data(), bins.data(), num_procs, MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  } else {\n    int n = s.size();\n    std::array<size_t, 26> counts {0};\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n      counts[s[i][0] - 'a']++;\n    }\n\n    MPI_Reduce(counts.data(), bins.data(), 26, MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  }\n}",
            "#pragma omp parallel for\n    for(size_t i = 0; i < s.size(); i++)\n    {\n        char x = s[i].at(0);\n        int p = int(x);\n        p = p - 97;\n        bins[p]++;\n    }\n}",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    std::vector<std::string> s2 = s;\n    int remainder = s2.size() % size;\n    int quotient = s2.size() / size;\n    int offset = 0;\n\n    //distribute vector\n    if (rank!= 0) {\n        for (int i = 0; i < quotient; i++) {\n            MPI_Send(&s2[offset], 1, MPI_CHAR, 0, 0, MPI_COMM_WORLD);\n            offset += quotient;\n        }\n        for (int i = 0; i < remainder; i++) {\n            MPI_Send(&s2[offset], 1, MPI_CHAR, 0, 0, MPI_COMM_WORLD);\n            offset += remainder;\n        }\n    }\n\n    //retrieve vector\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&s2[offset], 1, MPI_CHAR, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            offset += quotient + 1;\n        }\n    }\n\n    //compute\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < s2.size(); i++) {\n        bins[s2[i][0] - 'a']++;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&bins[i * 26], 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n\n    if (rank!= 0) {\n        MPI_Send(&bins[rank * 26], 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&bins[i * 26], 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n\n}",
            "assert(s.size() >= 1);\n    size_t s_size = s.size();\n    int comm_sz = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);\n    int rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    std::array<size_t, 26> local_bins;\n\n    // if size of vector is not multiple of comm_sz\n    // add the remainder to the end of vector\n    if (s.size() % comm_sz!= 0) {\n        int diff = comm_sz - (s.size() % comm_sz);\n        for (int i = 0; i < diff; i++) {\n            s.push_back(\"\");\n        }\n    }\n\n    // distribute the string vector in a round robin fashion\n    // assign the first character of string to rank\n    // and then add it to that rank's bin\n    int bins_size = 26;\n    int comm_div = s.size() / comm_sz;\n    int start = 0, end = 0;\n    #pragma omp parallel num_threads(comm_sz)\n    {\n        int tid = omp_get_thread_num();\n        if (tid == 0) {\n            start = 0;\n            end = comm_div;\n        } else {\n            start = (tid-1)*comm_div;\n            end = tid*comm_div;\n        }\n        for (int i = start; i < end; i++) {\n            int first_char = s[i].at(0);\n            int idx = first_char - 'a';\n            #pragma omp atomic\n            local_bins[idx]++;\n        }\n    }\n\n    // MPI reduction to get the final array of bins\n    MPI_Allreduce(local_bins.data(), bins.data(), 26, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    // if input vector size is not a multiple of comm_sz\n    // remove the extra elements from vector\n    // and free memory\n    if (s.size() % comm_sz!= 0) {\n        int diff = comm_sz - (s.size() % comm_sz);\n        for (int i = 0; i < diff; i++) {\n            s.pop_back();\n        }\n        s.shrink_to_fit();\n    }\n}",
            "int mpi_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    size_t bin_index = 0;\n    std::array<size_t, 26> local_bins;\n    local_bins.fill(0);\n\n    for (auto const& str : s) {\n        local_bins[str[0] - 'a']++;\n        bin_index++;\n    }\n\n    std::array<size_t, 26> global_bins;\n    global_bins.fill(0);\n\n    MPI_Allreduce(&local_bins[0], &global_bins[0], 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    bins.fill(0);\n    int mpi_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    int remainder = bin_index % mpi_size;\n    int offset = bin_index / mpi_size;\n\n    if (mpi_rank == 0) {\n        for (int i = 0; i < mpi_size - 1; i++) {\n            MPI_Recv(&bins[i * offset], offset, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n    else {\n        MPI_Send(&global_bins[0], offset, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n    if (mpi_rank == 0) {\n        if (remainder!= 0) {\n            for (int i = 0; i < remainder; i++) {\n                bins[bin_index - remainder + i] = global_bins[i + (mpi_size - 1) * offset];\n            }\n        }\n    }\n}",
            "auto size = s.size();\n  auto local = std::vector<std::string>(size/omp_get_num_threads());\n  auto thread_count = omp_get_num_threads();\n#pragma omp parallel\n  {\n    int thread_id = omp_get_thread_num();\n    local[thread_id] = s[thread_id * (size/omp_get_num_threads()) + thread_id];\n  }\n  for (int i = 0; i < 26; i++) {\n    bins[i] = 0;\n  }\n\n  #pragma omp parallel for\n  for (int i = 0; i < thread_count; i++) {\n    for (int j = 0; j < local[i].size(); j++) {\n      bins[local[i][j]-'a']++;\n    }\n  }\n}",
            "int rank = 0;\n    int num_ranks = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    size_t s_size = s.size();\n\n    std::array<size_t, 26> locals;\n\n    for(int i = 0; i < 26; i++) {\n        locals[i] = 0;\n    }\n\n    #pragma omp parallel for\n    for(int i = 0; i < s_size; i++) {\n        char first_letter = s[i][0];\n        locals[first_letter - 'a'] += 1;\n    }\n\n    std::array<size_t, 26> globals;\n    for(int i = 0; i < 26; i++) {\n        globals[i] = 0;\n    }\n\n    MPI_Reduce(locals.data(), globals.data(), 26, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if(rank == 0) {\n        for(int i = 0; i < 26; i++) {\n            bins[i] = globals[i];\n        }\n    }\n}",
            "size_t N = s.size();\n\n#pragma omp parallel num_threads(26)\n    for(int i = 0; i < 26; ++i)\n    {\n        #pragma omp for\n        for(int j = 0; j < N; ++j)\n        {\n            int index = s[j].front() - 'a';\n            if(index == i)\n            {\n                ++bins[i];\n            }\n        }\n    }\n}",
            "// TODO\n}",
            "size_t my_counts[26];\n  memset(my_counts, 0, 26*sizeof(size_t));\n\n  int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int nstrings = s.size();\n\n  int chunk_size = nstrings / size;\n\n  //int remain = nstrings % size;\n\n  #pragma omp parallel for\n  for (int i = rank*chunk_size; i < (rank+1)*chunk_size; i++) {\n    if (i < nstrings) {\n      char first_letter = tolower(s[i][0]);\n      my_counts[first_letter-'a']++;\n    }\n  }\n\n  size_t total_counts[26];\n\n  MPI_Reduce(my_counts, total_counts, 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < 26; i++) {\n      bins[i] = total_counts[i];\n    }\n  }\n\n  return;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int num_strings_per_rank = s.size() / size;\n  int remainder = s.size() % size;\n  int start = rank * num_strings_per_rank;\n  int end = start + num_strings_per_rank + (rank < remainder? 1 : 0);\n  std::array<size_t, 26> bins_local;\n  for (auto& i : bins_local) i = 0;\n  #pragma omp parallel for\n  for (int i = start; i < end; i++) {\n    auto letter = s[i][0] - 'a';\n    bins_local[letter]++;\n  }\n  MPI_Reduce(&bins_local[0], &bins[0], 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "//TODO\n}",
            "//TODO\n\n}",
            "size_t size = s.size();\n    size_t block_size = size/26;\n    size_t left_over = size%26;\n    if (left_over>0)\n        block_size += 1;\n    bins.fill(0);\n    #pragma omp parallel for\n    for (size_t i = 0; i < size; ++i) {\n        std::string word = s[i];\n        int first_letter_num = word[0] - 'a';\n        bins[first_letter_num] += 1;\n    }\n}",
            "// TODO: your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int start = rank * (s.size() / size);\n    int end = start + (s.size() / size);\n    if (rank == size - 1) {\n        end = s.size();\n    }\n    for (size_t i = start; i < end; i++) {\n        bins[s[i][0] - 97]++;\n    }\n}",
            "int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // This is a \"dummy\" implementation, which can be replaced with a faster one\n  // using OpenMP and MPI.\n  std::array<size_t, 26> bins2 = {};\n  for (auto& str : s) {\n    auto first = str.front();\n    bins2[first - 'a']++;\n  }\n\n  std::array<size_t, 26> bins_loc;\n  bins_loc.fill(0);\n  bins2.swap(bins_loc);\n  // Sum on rank 0.\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&bins_loc, 26, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD,\n               MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < 26; j++) {\n        bins[j] += bins_loc[j];\n      }\n    }\n    MPI_Send(&bins, 26, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Send(&bins_loc, 26, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "auto str = std::string(\"abcdefghijklmnopqrstuvwxyz\");\n    for(int i = 0; i < s.size(); i++){\n        auto str_to_int = str.find(s[i][0]);\n        if(str_to_int!= std::string::npos){\n            bins[str_to_int]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n        for (size_t j = 0; j < s.size(); j++) {\n            if (s[j][0] == (char)('a' + i)) {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "bins.fill(0);\n\n  #pragma omp parallel for\n  for (size_t i=0; i<s.size(); i++) {\n    bins[s[i][0]-'a']++;\n  }\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t// Split the vector into equal parts, each rank takes care of a part.\n\tint n = s.size();\n\tint part_size = n / size;\n\tint n_part = n % size;\n\n\t// Allocate memory for the result.\n\tstd::array<size_t, 26>* counts;\n\tif (n_part == 0) {\n\t\tcounts = new std::array<size_t, 26>[size];\n\t}\n\telse {\n\t\tcounts = new std::array<size_t, 26>[size + 1];\n\t\tpart_size += 1;\n\t}\n\t#pragma omp parallel for\n\tfor (int i = 0; i < part_size; i++) {\n\n\t\tif (i < n_part) {\n\t\t\tint idx = rank * part_size + i;\n\t\t\tchar c = s[idx][0];\n\t\t\tcounts[rank][c - 'a']++;\n\t\t}\n\t}\n\n\t// Reduce.\n\t#pragma omp parallel for\n\tfor (int i = 0; i < size; i++) {\n\t\tfor (int j = 0; j < 26; j++) {\n\t\t\tif (rank == 0) {\n\t\t\t\tbins[j] += counts[i][j];\n\t\t\t}\n\t\t}\n\t}\n\n\t// Free memory.\n\tif (n_part == 0) {\n\t\tdelete[] counts;\n\t}\n\telse {\n\t\tdelete[] counts;\n\t}\n\n}",
            "// your code here\n\n    // For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n    // Assume all strings are in lower case. Store the output in `bins` array.\n    // Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n    // Every rank has a complete copy of s. The result is stored in bins on rank 0.\n    // Example:\n\n    // input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n    // output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n\n    std::vector<std::string> substrings;\n\n    substrings.resize(s.size());\n\n    for (int i = 0; i < s.size(); i++)\n    {\n        substrings[i] = s[i].substr(0, 1);\n    }\n\n    int n = 0;\n\n    for (int i = 0; i < substrings.size(); i++)\n    {\n        char letter = substrings[i][0];\n\n        n = (int)letter;\n        n = n - 97;\n\n        bins[n]++;\n    }\n\n\n\n    /*\n    // std::vector<std::string> substrings;\n\n    // substrings.resize(s.size());\n\n    // for (int i = 0; i < s.size(); i++)\n    // {\n    //     substrings[i] = s[i].substr(0, 1);\n    // }\n\n    int n = 0;\n\n    for (int i = 0; i < substrings.size(); i++)\n    {\n        char letter = substrings[i][0];\n\n        n = (int)letter;\n        n = n - 97;\n\n        bins[n]++;\n    }\n    */\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<size_t> local_bins(26);\n  #pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    auto first = s[i][0];\n    local_bins[first - 'a']++;\n  }\n\n  std::vector<size_t> all_bins(26);\n  std::vector<size_t> counts(size);\n  MPI_Allgather(&local_bins[0], 26, MPI_LONG, &all_bins[0], 26, MPI_LONG, MPI_COMM_WORLD);\n  #pragma omp parallel for\n  for (int i = 0; i < size; ++i) {\n    counts[i] = (i == 0)? all_bins[0] : all_bins[25] - all_bins[0] + all_bins[25] - all_bins[i - 1];\n  }\n\n  bins = {0};\n  #pragma omp parallel for\n  for (int i = 0; i < counts.size(); ++i) {\n    if (i!= 0) bins[i] = bins[i - 1] + counts[i - 1];\n  }\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < 26; ++i) {\n    for (size_t j = 0; j < counts[rank]; ++j) {\n      if (s[bins[rank] + j][0] == (char)('a' + i)) bins[rank]++;\n    }\n  }\n\n  MPI_Allreduce(&bins[0], &bins[0], 26, MPI_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int chunk_size = s.size() / size;\n  int last_size = s.size() % size;\n\n  int begin_index = chunk_size * rank;\n  int end_index = begin_index + chunk_size;\n\n  if (rank == size - 1)\n    end_index += last_size;\n\n  std::array<size_t, 26> local_bins = {0};\n  for (auto i = begin_index; i < end_index; i++) {\n    local_bins[s[i][0] - 'a']++;\n  }\n\n#pragma omp parallel\n  {\n#pragma omp master\n    {\n      for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n      }\n    }\n\n#pragma omp for\n    for (int i = 0; i < 26; i++) {\n      bins[i] += local_bins[i];\n    }\n  }\n\n  std::array<size_t, 26> local_bins_sum = {0};\n#pragma omp parallel for\n  for (int i = 0; i < 26; i++) {\n    local_bins_sum[i] = bins[i];\n  }\n\n  if (rank == 0) {\n    std::array<size_t, 26> bins_sum;\n#pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n      bins_sum[i] = local_bins_sum[i];\n    }\n    for (int i = 1; i < size; i++) {\n      std::array<size_t, 26> bins_from_rank;\n      MPI_Status status;\n      MPI_Recv(bins_from_rank.data(), 26, MPI_LONG_LONG, i, 10, MPI_COMM_WORLD, &status);\n      for (int j = 0; j < 26; j++) {\n        bins_sum[j] += bins_from_rank[j];\n      }\n    }\n    for (int i = 0; i < 26; i++) {\n      bins[i] = bins_sum[i];\n    }\n  } else {\n    MPI_Send(local_bins_sum.data(), 26, MPI_LONG_LONG, 0, 10, MPI_COMM_WORLD);\n  }\n}",
            "int commSize = 0;\n    int commRank = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &commSize);\n    MPI_Comm_rank(MPI_COMM_WORLD, &commRank);\n    // Each process should compute the count for the letters it owns\n    size_t size = s.size();\n\n    // Each process will get the number of strings that start with their letter\n    // Allocate the space for the output array\n    bins.fill(0);\n    // Create a 1D array with size of 26 for each process\n    size_t *bin_arr = new size_t[26];\n\n    // Assign the values for the alphabet from the array\n    for(int i = 0; i < 26; i++){\n        bin_arr[i] = 0;\n    }\n\n    // Distribute the string arrays for each process to compute the counts\n    std::vector<std::string> local_strings;\n    for(int i = commRank; i < size; i+= commSize) {\n        local_strings.push_back(s[i]);\n    }\n\n    // Check the first letter of the string\n    #pragma omp parallel for\n    for(int i = 0; i < local_strings.size(); i++){\n        bin_arr[local_strings[i][0] - 'a'] += 1;\n    }\n\n    // Send the count to the corresponding process\n    MPI_Gather(bin_arr, 26, MPI_UNSIGNED_LONG_LONG, bins.data(), 26, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n    delete [] bin_arr;\n}",
            "//TODO\n    return;\n}",
            "}",
            "for (int rank = 0; rank < nproc; rank++) {\n        MPI_Status status;\n        MPI_Recv(bins.data(), 26, MPI_LONG, rank, 0, MPI_COMM_WORLD, &status);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    std::vector<std::string> local_s = s;\n\n#pragma omp parallel for\n    for (int i = 0; i < nproc; i++) {\n        MPI_Status status;\n        int len;\n        char buf[256];\n        MPI_Recv(buf, 256, MPI_CHAR, i, 0, MPI_COMM_WORLD, &status);\n        len = std::atoi(buf);\n        local_s.resize(len);\n        MPI_Recv(local_s.data(), len, MPI_CHAR, i, 1, MPI_COMM_WORLD, &status);\n\n        for (int j = 0; j < len; j++) {\n            bins[local_s[j][0] - 'a'] += 1;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < nproc; i++) {\n            MPI_Send(bins.data(), 26, MPI_LONG, i, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    MPI_Finalize();\n}",
            "std::array<size_t, 26> local_counts{0};\n    //#pragma omp parallel for\n    for(size_t i=0;i<s.size();i++) {\n        local_counts[s[i][0]-'a']++;\n    }\n    MPI_Allreduce(local_counts.data(), bins.data(), 26, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if (rank == 0) {\n        std::array<size_t, 26> bins_tmp;\n#pragma omp parallel for\n        for (int i = 0; i < 26; i++) {\n            bins_tmp[i] = 0;\n        }\n        for (int i = 0; i < s.size(); i++) {\n            char firstLetter = s[i][0];\n            bins_tmp[firstLetter - 'a']++;\n        }\n        std::array<size_t, 26> bins_part;\n        MPI_Reduce(bins_tmp.data(), bins_part.data(), 26, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        std::copy(bins_part.begin(), bins_part.end(), bins.begin());\n    } else {\n        std::array<size_t, 26> bins_tmp;\n#pragma omp parallel for\n        for (int i = 0; i < 26; i++) {\n            bins_tmp[i] = 0;\n        }\n        for (int i = 0; i < s.size(); i++) {\n            char firstLetter = s[i][0];\n            bins_tmp[firstLetter - 'a']++;\n        }\n        std::array<size_t, 26> bins_part;\n        MPI_Reduce(bins_tmp.data(), bins_part.data(), 26, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO\n    // std::array<size_t, 26> bins;\n    // std::string s = \"dog\";\n    // int rank, size;\n    // MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    // MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // int remainder = (s.size() % size);\n    // int blockSize = s.size() / size + (rank < remainder? 1 : 0);\n    // int offset = rank * blockSize;\n    // int len = s.size() / size;\n\n    // if (rank == 0) {\n    //     std::array<size_t, 26> binTemp;\n    //     for (int i = 0; i < size; i++) {\n    //         std::string strTemp = s.substr(offset + i * len, len);\n    //         if (strTemp.length() > 0) {\n    //             binTemp[strTemp[0] - 'a']++;\n    //         }\n    //     }\n    //     MPI_Reduce(&binTemp, &bins, 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    // } else {\n    //     std::array<size_t, 26> binTemp;\n    //     for (int i = 0; i < blockSize; i++) {\n    //         std::string strTemp = s.substr(offset + i * len, len);\n    //         if (strTemp.length() > 0) {\n    //             binTemp[strTemp[0] - 'a']++;\n    //         }\n    //     }\n    //     MPI_Reduce(&binTemp, &bins, 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    // }\n\n    // int threadCount = omp_get_max_threads();\n    // for (int i = 0; i < threadCount; i++) {\n    //     std::array<size_t, 26> binTemp;\n    //     std::string str = s.substr(i * s.size() / threadCount, s.size() / threadCount);\n    //     for (int j = 0; j < str.length(); j++) {\n    //         binTemp[str[j] - 'a']++;\n    //     }\n    //     #pragma omp critical\n    //     {\n    //         for (int k = 0; k < 26; k++) {\n    //             bins[k] += binTemp[k];\n    //         }\n    //     }\n    // }\n\n    // std::array<size_t, 26> bins;\n    // #pragma omp parallel\n    // {\n    //     int rank = omp_get_thread_num();\n    //     if (rank == 0) {\n    //         for (int i = 0; i < s.size(); i++) {\n    //             bins[s[i] - 'a']++;\n    //         }\n    //     } else {\n    //         for (int i = 0; i < s.size() / threadCount; i++) {\n    //             bins[s[rank * s.size() / threadCount + i] - 'a']++;\n    //         }\n    //     }\n    // }\n    // #pragma omp parallel\n    // {\n    //     int rank = omp_get_thread_num();\n    //     if (rank!= 0) {\n    //         for (int i = 0; i < s.size() / threadCount; i++) {\n    //             bins[s[rank * s.size() / threadCount + i] - 'a']++;\n    //         }\n    //     }\n    // }\n}",
            "std::array<size_t, 26> local_bins{};\n    int nproc = omp_get_num_threads();\n    int rank = omp_get_thread_num();\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++)\n    {\n        size_t k = s[i][0] - 'a';\n        local_bins[k]++;\n    }\n    std::array<size_t, 26> global_bins{};\n    MPI_Reduce(local_bins.data(), global_bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    for (size_t i = 0; i < 26; i++)\n        bins[i] = global_bins[i];\n    return;\n}",
            "// TODO: Fill this in\n\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local = (s.size() - 1) / size + 1;\n    std::vector<std::string> local_strings;\n\n    // Distribute the vector to each rank\n    if (rank == 0) {\n        local_strings = std::vector<std::string>(s.begin(), s.begin() + local);\n    } else {\n        local_strings = std::vector<std::string>(s.begin() + rank * local, s.begin() + (rank + 1) * local);\n    }\n\n    // Run OpenMP on each rank\n    #pragma omp parallel for\n    for (int i = 0; i < local_strings.size(); i++) {\n        local_strings[i] = local_strings[i][0];\n    }\n\n    // Combine the vector from each rank into one vector\n    std::vector<std::string> global_strings;\n\n    MPI_Reduce(&local_strings[0], &global_strings[0], local_strings.size(), MPI_CHAR, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // Count the letters and fill the bins array\n    for (auto &item : global_strings) {\n        item = item - 'a' + 1;\n        bins[item - 1]++;\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(Kokkos::View<const double*> const& x, Kokkos::View<size_t[10]> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "using namespace Kokkos;\n\n    // TODO: Your code here\n    // TODO: Use the Kokkos view API to implement this algorithm\n    // You may need to create more than one range policy to parallelize\n    // this algorithm\n    // Don't forget to call `bins.sync()` after you're done!\n}",
            "// Fill in your solution here\n}",
            "// TODO: Fill in this function\n  // Hint: You can use Kokkos::subview to extract a view of x that contains just\n  //       the first 10 elements.\n  // Hint: You can use Kokkos::subview to extract a view of bins that contains\n  //       just the first 10 elements.\n  Kokkos::RangePolicy rp(0, x.size());\n  const auto subview_x = Kokkos::subview(x, rp);\n  const auto subview_bins = Kokkos::subview(bins, rp);\n\n  // Kokkos::parallel_reduce is a Kokkos primitive that implements reduction\n  // in parallel. The syntax is:\n  //\n  //   Kokkos::parallel_reduce(policy, functor, initial value)\n  //\n  // `policy` is a policy that specifies the parallelization scheme (i.e. how\n  // you want to parallelize the reduction, e.g. over the range 0 to x.size(),\n  // or over the first 10 elements of x).\n  //\n  // `functor` is a functor that contains the logic for the reduction. Here,\n  // it is a lambda function that computes the bin number for each value in x.\n  // It uses Kokkos::subview to extract the bin number for x, and increments\n  // the corresponding bin in the bins array.\n  //\n  // `initial value` is the value to add to bins to get the final result.\n  //\n  // To see how this works, try running this with `bins` initialized to all\n  // zeroes, then printing the final value of `bins`.\n  Kokkos::parallel_reduce(Kokkos::RangePolicy(0, subview_x.size()),\n                          KOKKOS_LAMBDA(const int i, size_t& l_bins) {\n                            const int bin = static_cast<int>(subview_x(i)/10.0);\n                            l_bins += subview_bins(bin);\n                          },\n                          Kokkos::View<size_t[10]>(\"\", 0));\n\n  // TODO: Kokkos::subview will be useful here\n}",
            "// Your code here\n}",
            "// TODO\n}",
            "// TODO\n    // Your code here\n\n    // No need to write any test code, as we will be using the unit tests that\n    // come with the starter code.\n}",
            "// TODO: Your code here\n}",
            "}",
            "// Fill in this function!\n}",
            "// TODO: YOUR CODE HERE\n    //...\n\n}",
            "//... your code here...\n}",
            "const size_t size = x.extent(0);\n    const size_t block_size = 100 / 10;\n    const size_t num_blocks = size / block_size;\n\n    Kokkos::parallel_for(num_blocks, KOKKOS_LAMBDA(const int idx) {\n        size_t start = idx * block_size;\n        size_t end = std::min(start + block_size, size);\n        for (size_t i = start; i < end; ++i) {\n            size_t b = (x(i) / 10);\n            Kokkos::atomic_fetch_add(&bins(b), 1);\n        }\n    });\n}",
            "//... Your code here...\n}",
            "// This is the number of elements in x\n  const size_t num_elements = x.size();\n\n  // This is the number of threads to use.\n  // This is one way to get the number of threads in Kokkos.\n  const size_t num_threads = Kokkos::Experimental::HPX::concurrency();\n\n  // Fill bins with zeroes.\n  Kokkos::deep_copy(bins, 0);\n\n  // TODO: Implement parallel_for using HPX\n  //       Kokkos::parallel_for should work, but it may be hard to\n  //       do it so that the range of `i` is correct.\n  //       You may need to use a different loop index variable.\n\n  // TODO: Ensure that the output is correct\n}",
            "/* This implementation will count every value from 0 to 100 inclusive,\n     * which is wrong.  (It would be easy to fix, though.  Think about it!)\n     */\n\n    /* Create a View with 100 elements, which will be used to accumulate the counts. */\n    Kokkos::View<size_t[100]> countArray(\"countArray\");\n\n    // Fill the counts array with zeroes.\n    // NOTE: You will need to include Kokkos_Arith.hpp for this to work.\n    Kokkos::deep_copy(countArray, 0);\n\n    /* Count the values in each of the 10 bins, and add them to countArray. */\n    // NOTE: You will need to include Kokkos_Arith.hpp for this to work.\n    Kokkos::parallel_for(\n        \"binsBy10Count\",\n        Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n        KOKKOS_LAMBDA(int i) {\n            int bin = x(i) / 10;\n            countArray[bin] += 1;\n        });\n\n    /* Copy the counts from countArray to bins. */\n    Kokkos::deep_copy(bins, countArray);\n}",
            "// TODO: Your code goes here\n}",
            "auto x_host = x.host_mirror();\n    auto bins_host = bins.host_mirror();\n\n    for (size_t i=0; i<x.size(); ++i) {\n        size_t k = floor((x_host(i) / 10.0));\n        bins_host(k) += 1;\n    }\n}",
            "// Write your code here.\n}",
            "// Your code here\n}",
            "size_t count = 0;\n  for(size_t i=0; i<x.size(); i++) {\n    count += (x[i] > 0 && x[i] <= 10)? 1 : 0;\n    count += (x[i] > 10 && x[i] <= 20)? 1 : 0;\n    count += (x[i] > 20 && x[i] <= 30)? 1 : 0;\n    count += (x[i] > 30 && x[i] <= 40)? 1 : 0;\n    count += (x[i] > 40 && x[i] <= 50)? 1 : 0;\n    count += (x[i] > 50 && x[i] <= 60)? 1 : 0;\n    count += (x[i] > 60 && x[i] <= 70)? 1 : 0;\n    count += (x[i] > 70 && x[i] <= 80)? 1 : 0;\n    count += (x[i] > 80 && x[i] <= 90)? 1 : 0;\n    count += (x[i] > 90 && x[i] <= 100)? 1 : 0;\n  }\n}",
            "Kokkos::RangePolicy<Kokkos::Cuda, Kokkos::IndexType<int>> policy(0, 10);\n    Kokkos::parallel_for(\"Kokkos_CUDA_Example\", policy, KOKKOS_LAMBDA(int i) {\n        Kokkos::View<double*, Kokkos::Cuda> v(x.data(), x.size());\n        double min = (i*10.0), max = (i+1)*10.0;\n        size_t num = 0;\n        for (size_t j = 0; j < v.size(); ++j) {\n            if (v(j) >= min && v(j) < max) {\n                num++;\n            }\n        }\n        bins(i) = num;\n    });\n}",
            "size_t N = x.extent(0);\n\tKokkos::parallel_for(\"bins\", N, KOKKOS_LAMBDA(const int& i) {\n\t\tint bin = std::floor(x[i]/10);\n\t\tbins[bin]++;\n\t});\n\tKokkos::fence();\n}",
            "constexpr size_t n = 10;\n    auto bins_host = Kokkos::create_mirror_view(bins);\n    Kokkos::deep_copy(bins_host, bins);\n    for (int i = 0; i < n; i++) {\n        printf(\"%lu \", bins_host(i));\n    }\n    printf(\"\\n\");\n\n    for (int i = 0; i < x.size(); i++) {\n        auto value = x(i);\n        if (value < 10) {\n            bins(0)++;\n        } else if (value < 20) {\n            bins(1)++;\n        } else if (value < 30) {\n            bins(2)++;\n        } else if (value < 40) {\n            bins(3)++;\n        } else if (value < 50) {\n            bins(4)++;\n        } else if (value < 60) {\n            bins(5)++;\n        } else if (value < 70) {\n            bins(6)++;\n        } else if (value < 80) {\n            bins(7)++;\n        } else if (value < 90) {\n            bins(8)++;\n        } else if (value < 100) {\n            bins(9)++;\n        }\n    }\n    Kokkos::deep_copy(bins_host, bins);\n    for (int i = 0; i < n; i++) {\n        printf(\"%lu \", bins_host(i));\n    }\n    printf(\"\\n\");\n}",
            "// your code here\n}",
            "// TODO: Replace this with Kokkos functionality\n}",
            "// This will generate the following code:\n    // binsBy10Count(x, bins);\n    // Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (int i) {\n    //   const int i1 = 10 * ((int) (i * 10.0));\n    //   ++bins(i1);\n    // });\n    // Kokkos::deep_copy(bins_host, bins);\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (int i) {\n      const int i1 = 10 * ((int) (i * 10.0));\n      ++bins(i1);\n    });\n\n    Kokkos::deep_copy(bins, bins);\n}",
            "//... fill in bins...\n}",
            "Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    bins[floor(x[i]/10)]++;\n  });\n\n}",
            "// TODO\n    //...\n}",
            "Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const size_t i) {\n        size_t bin = size_t(x(i) / 10.0);\n        if (bin < 10) bins(bin)++;\n        });\n}",
            "// TODO: Fill this in\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO\n}",
            "using Kokkos::RangePolicy;\n  Kokkos::parallel_for(\"binsBy10Count\", RangePolicy<>(0, x.size()), KOKKOS_LAMBDA(const int i) {\n    const double val = x[i];\n    // bins is defined as bins[0] = # of values 0 <= val <= 10, etc.\n    int bin = (int) (val / 10.0);\n    if (bin < 0)\n      bin = 0;\n    if (bin >= 10)\n      bin = 9;\n    bins[bin] += 1;\n  });\n}",
            "// TODO: Fill out this function to compute the number of elements in each of the\n  // bins. The input domain of binsBy10Count is not required to be sorted, so the\n  // values of x may not be in ascending order.\n\n  // ************************************************\n  // TODO: Implement binsBy10Count\n  // ************************************************\n  auto x_size = x.size();\n  Kokkos::parallel_for( \"bins_by_10_count\", \n    Kokkos::RangePolicy<>(0, x_size), \n    KOKKOS_LAMBDA(int i) {\n      int a = i / 10;\n      bins(a) = bins(a) + 1;\n    }\n  );\n}",
            "// Your code here\n}",
            "int N = x.size();\n\n  // TODO: Your code goes here\n\n}",
            "Kokkos::RangePolicy<Kokkos::Serial> range(0, x.size());\n    Kokkos::parallel_for(\"bincount\", range, [&] (int i) {\n        int bin = x(i) / 10;\n        ++bins(bin);\n    });\n}",
            "// Fill in your code here.\n}",
            "}",
            "// TODO: fill in\n}",
            "// TODO: Your code here.\n\n}",
            "// compute the bins of x and store the counts in bins\n  Kokkos::parallel_for(\"binsBy10Count\", Kokkos::RangePolicy<Kokkos::Serial>(0, x.size()),\n    KOKKOS_LAMBDA(const int& i) {\n      bins[int(x[i]/10)]++;\n    }\n  );\n}",
            "// TODO: Fill in this function.\n\n  // Hints:\n  //  1. You can initialize bins to all zero by doing\n  //        Kokkos::deep_copy(bins, 0);\n  //  2. To find out which bin the ith value in x falls into, use the formula\n  //         bin_num = (int) (x[i] / 10);\n  //     Note that you will have to cast the result of the division to an integer.\n  //  3. To find out how many values in x fall into a given bin, use the formula\n  //         num_vals = Kokkos::count(x, bin_num);\n  //  4. You can add counts in bins by doing:\n  //        Kokkos::deep_copy(bins, bins + num_vals);\n}",
            "// Your solution goes here\n}",
            "//TODO: Fill this in\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<>(0, x.size()), KOKKOS_LAMBDA (const int i) {\n    bins(static_cast<size_t>(x(i) / 10))++;\n  });\n}",
            "}",
            "// TODO: your code here\n}",
            "}",
            "// TODO: your code goes here\n}",
            "/*... */\n}",
            "// Create a View that maps the values of x into bins, which is 10 * the\n  // value of x.\n  Kokkos::View<int*> binned(Kokkos::view_alloc(Kokkos::WithoutInitializing, \"binned\"), x.extent(0));\n  Kokkos::parallel_for(\"binning\", x.size(), KOKKOS_LAMBDA(int i) {\n    binned(i) = (int) (x(i)/10.0);\n  });\n  Kokkos::fence();\n\n  // Count the number of values in each bin by counting the number of\n  // elements in each partition of binned.\n  Kokkos::Experimental::require_sort(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)), binned);\n  Kokkos::parallel_reduce(\"count\", x.extent(0), KOKKOS_LAMBDA(int i, size_t& lcount) {\n    if (i==0 || (binned(i)!= binned(i-1))) lcount++;\n  }, bins);\n  Kokkos::fence();\n}",
            "// Compute the number of elements in each bin\n  // Note: you should use a vector of threads, each with their own bin counter\n  // Hint: use the range policy for `parallel_for`\n  Kokkos::parallel_for(\n    Kokkos::RangePolicy<>(0, x.size()),\n    KOKKOS_LAMBDA(const int i) {\n      int bin = (int) (x(i) / 10.0);\n      bins[bin]++;\n    });\n\n  Kokkos::deep_copy(bins, bins);\n}",
            "// This Kokkos loop is parallel.\n   Kokkos::parallel_for(x.size(), [&](const int i) {\n      int bin = (int) std::floor(x[i]/10.0);\n      if (bin >= 0 && bin < 10) bins[bin]++;\n   });\n}",
            "// TODO: Your code goes here.\n    // Hint: You may use the following Kokkos functions:\n    //   Kokkos::parallel_for (Kokkos::RangePolicy)\n    //   Kokkos::atomic_fetch_add (&, const &)\n}",
            "constexpr double start = 0;\n    constexpr double end = 10;\n\n    // Write your code here.\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<>(0, x.size()), KOKKOS_LAMBDA(const size_t& i) {\n    auto value = x(i);\n    if (value >= 0.0 && value < 10.0) {\n      bins(0) += 1;\n    } else if (value >= 10.0 && value < 20.0) {\n      bins(1) += 1;\n    } else if (value >= 20.0 && value < 30.0) {\n      bins(2) += 1;\n    } else if (value >= 30.0 && value < 40.0) {\n      bins(3) += 1;\n    } else if (value >= 40.0 && value < 50.0) {\n      bins(4) += 1;\n    } else if (value >= 50.0 && value < 60.0) {\n      bins(5) += 1;\n    } else if (value >= 60.0 && value < 70.0) {\n      bins(6) += 1;\n    } else if (value >= 70.0 && value < 80.0) {\n      bins(7) += 1;\n    } else if (value >= 80.0 && value < 90.0) {\n      bins(8) += 1;\n    } else if (value >= 90.0 && value <= 100.0) {\n      bins(9) += 1;\n    }\n  });\n  \n}",
            "// TODO: Replace this with your code\n    Kokkos::parallel_for(\"binby10\", Kokkos::RangePolicy<>(0, x.extent(0)),\n            KOKKOS_LAMBDA (int i) {\n                if (x(i) <= 10)\n                    bins(0)++;\n                else if (x(i) > 10 && x(i) <= 20)\n                    bins(1)++;\n                else if (x(i) > 20 && x(i) <= 30)\n                    bins(2)++;\n                else if (x(i) > 30 && x(i) <= 40)\n                    bins(3)++;\n                else if (x(i) > 40 && x(i) <= 50)\n                    bins(4)++;\n                else if (x(i) > 50 && x(i) <= 60)\n                    bins(5)++;\n                else if (x(i) > 60 && x(i) <= 70)\n                    bins(6)++;\n                else if (x(i) > 70 && x(i) <= 80)\n                    bins(7)++;\n                else if (x(i) > 80 && x(i) <= 90)\n                    bins(8)++;\n                else if (x(i) > 90)\n                    bins(9)++;\n            });\n}",
            "using namespace Kokkos;\n    // your code here\n    const int num = x.extent(0);\n    Kokkos::parallel_for(\"binsBy10Count\", range_policy_t(0, num), KOKKOS_LAMBDA(int i) {\n        int j = int(x(i)/10);\n        if (j >= 10) j = 9;\n        Kokkos::atomic_increment(&bins(j));\n    });\n    // end code here\n}",
            "Kokkos::parallel_for(\"binsBy10Count\",\n    Kokkos::RangePolicy<Kokkos::Serial, size_t>(0, x.size()),\n    KOKKOS_LAMBDA (const size_t i) {\n      const size_t which = 10 * x[i] / 100;\n      bins(which) += 1;\n    }\n  );\n\n}",
            "// Fill in your code here\n}",
            "/* NOTE: the compiler cannot generate a vectorized solution to this loop\n     because there is no way to generate a condition that is true for any two\n     consecutive elements in the vector. We may be able to do this by writing\n     the loop as a while loop.\n\n     NOTE: I think we can do this by creating a new vector with the condition\n     (x >= i*10 && x < (i+1)*10), and then doing a vector sum of this vector.\n     We will need to make sure to handle the edge case at the beginning and end\n     of the vector.\n\n     NOTE: We could also make sure that the vector is padded, so that the\n     vectorized loop condition will always be true for two consecutive elements.\n  */\n\n  const size_t N = x.size();\n  const double range = 10.;\n\n  // Kokkos::parallel_for(\"binsBy10Count\", N, KOKKOS_LAMBDA(const int i) {\n  //   const double lower = i * range;\n  //   const double upper = (i + 1) * range;\n  //   auto bin = std::min((x[i] - lower) / range, 1.);\n  //   ++bins[bin];\n  // });\n\n  Kokkos::parallel_for(\"binsBy10Count\", N, KOKKOS_LAMBDA(const int i) {\n    const double lower = i * range;\n    const double upper = (i + 1) * range;\n    auto bin = std::min((x[i] - lower) / range, 1.);\n    // NOTE: we need to make sure we don't go over the size of the bins view\n    ++bins[std::min(bin, range - 1)];\n  });\n}",
            "Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n        int bin = 10*x(i);\n        if (bin < 10) {\n            bins(bin)++;\n        }\n    });\n}",
            "// TODO: your code here\n}",
            "// Implement this function\n}",
            "// Initialize counts to 0\n  Kokkos::deep_copy(bins, 0);\n\n  // TODO: Fill in the bins with the correct counts of elements of x\n  //       that fall into the appropriate range.\n\n\n}",
            "// This is a hint to the compiler that bins is not used until the end of the\n    // function, so it should be stored in registers.\n    constexpr int numBins = 10;\n\n    // TODO: create a loop that iterates over `x` and increments the corresponding\n    // bin in `bins`\n\n    // TODO: loop over x and store the number of values in each 10 unit bin in bins.\n    for(int i = 0; i < x.extent(0); i++){\n        //if the value is bigger than 10 or smaller than 0, then throw an error\n        if((x(i) < 0.0) || (x(i) >= 100.0)){\n            throw std::runtime_error(\"x contains a value outside the range [0.0, 100.0).\");\n        }\n        //if the value is between 10 and 20, increment bins(0) by one\n        if((x(i) >= 10.0) && (x(i) < 20.0)){\n            bins(0)++;\n        }\n        //if the value is between 20 and 30, increment bins(1) by one\n        if((x(i) >= 20.0) && (x(i) < 30.0)){\n            bins(1)++;\n        }\n        //if the value is between 30 and 40, increment bins(2) by one\n        if((x(i) >= 30.0) && (x(i) < 40.0)){\n            bins(2)++;\n        }\n        //if the value is between 40 and 50, increment bins(3) by one\n        if((x(i) >= 40.0) && (x(i) < 50.0)){\n            bins(3)++;\n        }\n        //if the value is between 50 and 60, increment bins(4) by one\n        if((x(i) >= 50.0) && (x(i) < 60.0)){\n            bins(4)++;\n        }\n        //if the value is between 60 and 70, increment bins(5) by one\n        if((x(i) >= 60.0) && (x(i) < 70.0)){\n            bins(5)++;\n        }\n        //if the value is between 70 and 80, increment bins(6) by one\n        if((x(i) >= 70.0) && (x(i) < 80.0)){\n            bins(6)++;\n        }\n        //if the value is between 80 and 90, increment bins(7) by one\n        if((x(i) >= 80.0) && (x(i) < 90.0)){\n            bins(7)++;\n        }\n        //if the value is between 90 and 100, increment bins(8) by one\n        if((x(i) >= 90.0) && (x(i) < 100.0)){\n            bins(8)++;\n        }\n        //if the value is between 100 and 110, increment bins(9) by one\n        if((x(i) >= 100.0) && (x(i) < 110.0)){\n            bins(9)++;\n        }\n    }\n}",
            "size_t n = x.extent(0);\n  double x_min = x(0);\n  double x_max = x(n-1);\n  double interval_size = (x_max - x_min) / 10;\n\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 10),\n  KOKKOS_LAMBDA(const int i) {\n    size_t count = 0;\n    size_t j = 0;\n    for(; j<n; j++) {\n      if (x(j) < x_min + i*interval_size || x(j) >= x_min + (i+1)*interval_size) continue;\n      count++;\n    }\n    bins[i] = count;\n  });\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: implement binsBy10Count here\n}",
            "// Your code here.\n\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    bins[4] = 0;\n    bins[5] = 0;\n    bins[6] = 0;\n    bins[7] = 0;\n    bins[8] = 0;\n    bins[9] = 0;\n\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int& i) {\n        int index = (int)(x(i)/10);\n        if (index < 10) {\n            bins[index]++;\n        }\n    });\n}",
            "const size_t size = x.size();\n  Kokkos::View<size_t*, Kokkos::HostSpace> host_bins(\"Host bins\", 10);\n\n  Kokkos::parallel_for(\"Bins by 10 count\", Kokkos::RangePolicy<Kokkos::Serial>(0, size), KOKKOS_LAMBDA(const int i) {\n    if (x(i) >= 0.0 && x(i) < 10.0)\n      host_bins(0)++;\n    else if (x(i) >= 10.0 && x(i) < 20.0)\n      host_bins(1)++;\n    else if (x(i) >= 20.0 && x(i) < 30.0)\n      host_bins(2)++;\n    else if (x(i) >= 30.0 && x(i) < 40.0)\n      host_bins(3)++;\n    else if (x(i) >= 40.0 && x(i) < 50.0)\n      host_bins(4)++;\n    else if (x(i) >= 50.0 && x(i) < 60.0)\n      host_bins(5)++;\n    else if (x(i) >= 60.0 && x(i) < 70.0)\n      host_bins(6)++;\n    else if (x(i) >= 70.0 && x(i) < 80.0)\n      host_bins(7)++;\n    else if (x(i) >= 80.0 && x(i) < 90.0)\n      host_bins(8)++;\n    else if (x(i) >= 90.0 && x(i) <= 100.0)\n      host_bins(9)++;\n  });\n\n  Kokkos::deep_copy(bins, host_bins);\n}",
            "// TODO: Fill in the function body.\n}",
            "// Implement the parallel loop to count values into bins\n    // This loop should use `Kokkos::RangePolicy` to parallelize over the array elements\n    // The loop should use a `Kokkos::TeamPolicy` to parallelize over the 10 bins.\n    // Each team should have a single thread\n    // Each thread should count the number of values in a particular range and add that to the corresponding bin\n    // This loop will modify the elements of `bins`\n\n    // Hint: each team will have a range of the bins.\n\n    // Note that Kokkos::View has `size()` and `data()` member functions\n    // `size()` returns the number of elements in the view\n    // `data()` returns a pointer to the data that the view references\n\n\n\n    // You'll need to use the `Kokkos::ThreadVectorRange` view range policy here.\n    // You'll need to use a `Kokkos::TeamThreadRange` range policy here.\n\n    // Note that the bins have the wrong type. You'll need to initialize them using a Kokkos::deep_copy\n\n    // Your code here\n\n\n    // Check the results by comparing with the result from `countBy10Sequential` in `countBy10.cpp`\n\n    // To check your solution, first build and run the `binsBy10Test` executable.\n    // If you get the expected output, then your code is correct!\n}",
            "// TO DO\n  // Compute bins by 10 counts using Kokkos.\n\n}",
            "}",
            "// Your code here\n}",
            "bins = 0;\n    // TODO: fill in bins here\n}",
            "// TODO\n}",
            "// TODO: Your code here\n  //...\n}",
            "// TODO\n  // Write a Kokkos function to count the values in `x` into `bins`.\n\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (size_t i) {\n\n    // TODO\n    // Replace this for loop with a Kokkos function\n    for(int j = 0; j < bins.extent(0); j++){\n      if (x(i) >= j * 10 && x(i) <= (j + 1) * 10){\n        bins(j)++;\n      }\n    }\n  });\n\n}",
            "// YOUR CODE HERE\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (int i) {\n        bins[int(x(i)/10.0)]++;\n    });\n}",
            "const int n = x.size();\n  Kokkos::parallel_for(n, [&](const int i) {\n    const int bin = i/10;\n    bins[bin]++;\n  });\n}",
            "const size_t n = x.extent(0);\n\n    Kokkos::parallel_for(\n        \"bins_by_10_count\",\n        Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, n),\n        [=](int i) {\n            const size_t index = (x(i) / 10);\n            bins(index)++;\n        });\n}",
            "Kokkos::parallel_for(x.size(), [&](const int i) {\n    size_t bin = size_t(x(i)/10);\n    if (bin >= 10) {\n      bin = 9;\n    }\n    Kokkos::atomic_increment(&bins(bin));\n  });\n}",
            "// your code here\n\n}",
            "// TODO: implement in terms of Kokkos\n}",
            "// Fill in your solution here\n}",
            "// TODO\n}",
            "// TODO: Fill this function in\n}",
            "Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const size_t i) {\n        bins[static_cast<size_t>((x[i]/10.0))]++;\n    });\n}",
            "// TODO: Your code here\n}",
            "// You can use any variables you want, but don't change anything that begins with \"x\"\n    // x is a View to a vector of doubles\n    // bins is a View to a 10 element vector of unsigned integers\n    // You should have no local variables\n\n\n    // You can't access elements of x directly, you have to use the [] operator\n    // You can't access elements of bins directly, you have to use the [] operator\n    // You can't use any for-loops\n    // bins = 0;\n\n    // Use Kokkos to do this in parallel\n\n}",
            "// TODO: Your code goes here.\n}",
            "// TODO: Your code here\n}",
            "// TODO: YOUR CODE HERE\n}",
            "// TODO\n  return;\n}",
            "// TODO\n}",
            "//...\n}",
            "bins = 0;\n  // TODO: Fill in the function body\n}",
            "// TODO\n}",
            "// TODO: implement!\n}",
            "using namespace Kokkos;\n\n    auto numElements = x.size();\n    const int blockSize = 10000;\n    const int numBlocks = numElements / blockSize + 1;\n\n    auto bin = Functor<DefaultHostExecutionSpace>(x, bins, blockSize, numBlocks);\n\n    Kokkos::parallel_for(numBlocks, bin);\n}",
            "// your code here\n}",
            "Kokkos::parallel_for(x.size(), [&](int i) {\n        bins[int(x[i]/10)]++;\n    });\n}",
            "// TODO: Implement me\n}",
            "// 1. Allocate and initialize a Kokkos::View of size 10 that can be used to store\n    // the number of elements per range.\n    //\n    // 2. Loop over `x` and add 1 to `bins` at the right index for each element.\n    // If you are familiar with C, this is equivalent to:\n    //\n    //     for (int i = 0; i < 10; i++) {\n    //         int lower = i * 10;\n    //         int upper = (i + 1) * 10;\n    //         for (int j = 0; j < x.size(); j++) {\n    //             if (x[j] >= lower && x[j] < upper) {\n    //                 bins[i]++;\n    //             }\n    //         }\n    //     }\n    //\n    // Hints:\n    // - Use Kokkos::deep_copy to copy a Kokkos::View to a std::vector.\n    // - The `x` View can be accessed with the subscript operator as follows:\n    //\n    //   for (int i = 0; i < 10; i++) {\n    //     std::cout << x[i];\n    //   }\n    //\n    // - The Kokkos::View constructor can be used to create a new Kokkos::View\n    // from an existing `std::vector` and a `Kokkos::Layout`.\n    //\n    // - The C++ `std::transform` function can be used to apply a function to each\n    // element of a `std::vector`.\n    //\n    // - The C++ `std::min` and `std::max` functions can be used to compute the\n    // minimum and maximum value of a `std::vector`.\n    //\n    // - The Kokkos::parallel_for and Kokkos::parallel_reduce functions can be used\n    // to apply a function to all elements of a Kokkos::View in parallel.\n    //\n    // - The Kokkos::deep_copy function can be used to copy a Kokkos::View to a\n    // `std::vector`.\n    //\n    // - Use `int lower = i * 10;` and `int upper = (i + 1) * 10;` to compute the\n    // lower and upper bounds of each range.\n\n    return;\n}",
            "// TODO: Implement this function\n}",
            "// TODO: implement this function\n  //\n  // Hint:\n  //\n  // 1. To make use of Kokkos, you should define a Kokkos kernel and then\n  //    launch it using Kokkos::parallel_for.\n  //\n  // 2. To iterate over `x`, use a Kokkos View of type `const double*` and\n  //    then use the `kokkos_lite::RangePolicy`. The code to create the\n  //    Kokkos View should look like this:\n  //\n  //       Kokkos::View<const double*, Kokkos::HostSpace>\n  //         x_host_view(\"x_host_view\", x.size());\n  //\n  //    and the code to copy data from `x` to `x_host_view` should look\n  //    like this:\n  //\n  //       Kokkos::deep_copy(x_host_view, x);\n  //\n  // 3. You can use `Kokkos::parallel_for` as follows:\n  //\n  //       Kokkos::parallel_for(Kokkos::RangePolicy<\n  //           Kokkos::Schedule<Kokkos::Static>>(\n  //           Kokkos::RangePolicy<>::Member_type(0, x_host_view.size()),\n  //           Kokkos::RangePolicy<>::Member_type(0, bins.size())),\n  //           functor)\n  //\n  //    The first argument to `Kokkos::parallel_for` is the range policy.\n  //    The second argument is the functor that gets called.\n  //\n  //    Note that for the range policy, the range is defined by the\n  //    constructor for `Kokkos::RangePolicy`. The first argument to\n  //    `Kokkos::RangePolicy` is the beginning and the second argument is\n  //    the end (as if it were a half-open range, like C++'s\n  //    `std::vector`).\n}",
            "// TODO\n}",
            "int N = x.size();\n  Kokkos::parallel_for(\"bins\", Kokkos::RangePolicy<Kokkos::Serial>(0, N), KOKKOS_LAMBDA (int i) {\n    int bin = x(i) / 10;\n    bins(bin) += 1;\n  });\n  Kokkos::fence();\n}",
            "}",
            "bins = Kokkos::View<size_t[10]>(Kokkos::ViewAllocateWithoutInitializing(\"binsBy10Count\"), 10);\n    Kokkos::parallel_for(\"binsBy10Count\", x.size(), KOKKOS_LAMBDA(int i) {\n        size_t i10 = i / 10;\n        size_t i1 = i10 * 10;\n        size_t bin = i10;\n        if (i1 < 10) {\n            bin = 0;\n        } else if (i1 < 20) {\n            bin = 1;\n        } else if (i1 < 30) {\n            bin = 2;\n        } else if (i1 < 40) {\n            bin = 3;\n        } else if (i1 < 50) {\n            bin = 4;\n        } else if (i1 < 60) {\n            bin = 5;\n        } else if (i1 < 70) {\n            bin = 6;\n        } else if (i1 < 80) {\n            bin = 7;\n        } else if (i1 < 90) {\n            bin = 8;\n        } else if (i1 < 100) {\n            bin = 9;\n        }\n        Kokkos::atomic_fetch_add(&bins[bin], 1);\n    });\n}",
            "int constexpr n = 10;\n\n  // Kokkos will split the input array into chunks of 10 elements and compute\n  // the sum of each chunk. This will result in an array of sums that has\n  // length n+1. The last element in the result array contains the sum of all\n  // the elements of the input array.\n  Kokkos::View<double[n+1], Kokkos::LayoutRight> sums(\"sums\");\n\n  Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(size_t idx, double& sum) {\n      sum += x(idx);\n    },\n    Kokkos::Sum<double>(sums));\n\n  // Now, for each index in the output array `bins`, compute the difference\n  // between the sum of all elements of the input and the sum of the first\n  // (idx+1)*10 elements. For example, the difference between the sum of all\n  // elements and the sum of elements 0 to 9 is 100. The difference between the\n  // sum of all elements and the sum of elements 10 to 19 is 90, and so on.\n  Kokkos::parallel_for(bins.extent(0), KOKKOS_LAMBDA(size_t idx) {\n      bins(idx) = sums(idx+1) - sums(idx);\n    });\n}",
            "//...\n}",
            "}",
            "//...\n}",
            "// TODO: insert your code here\n    // Note: the input vector is not necessarily sorted\n}",
            "// TODO: Your code here\n}",
            "auto device = x.device();\n\n    auto f = KOKKOS_LAMBDA(size_t i) {\n        size_t bin = (x(i) / 10) % 10;\n        bins(bin) += 1;\n    };\n\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, x.size()), f);\n\n    Kokkos::deep_copy(bins, bins);\n}",
            "auto x_host = Kokkos::create_mirror_view(x);\n  Kokkos::deep_copy(x_host, x);\n\n  bins[0] = 0; bins[1] = 0; bins[2] = 0; bins[3] = 0; bins[4] = 0; bins[5] = 0; bins[6] = 0; bins[7] = 0; bins[8] = 0; bins[9] = 0;\n\n  for (int i = 0; i < 10; i++) {\n    bins[i] = 0;\n  }\n\n  for (int i = 0; i < x.size(); i++) {\n    if (x_host(i) >= 0 && x_host(i) < 10) {\n      bins[0]++;\n    } else if (x_host(i) >= 10 && x_host(i) < 20) {\n      bins[1]++;\n    } else if (x_host(i) >= 20 && x_host(i) < 30) {\n      bins[2]++;\n    } else if (x_host(i) >= 30 && x_host(i) < 40) {\n      bins[3]++;\n    } else if (x_host(i) >= 40 && x_host(i) < 50) {\n      bins[4]++;\n    } else if (x_host(i) >= 50 && x_host(i) < 60) {\n      bins[5]++;\n    } else if (x_host(i) >= 60 && x_host(i) < 70) {\n      bins[6]++;\n    } else if (x_host(i) >= 70 && x_host(i) < 80) {\n      bins[7]++;\n    } else if (x_host(i) >= 80 && x_host(i) < 90) {\n      bins[8]++;\n    } else if (x_host(i) >= 90 && x_host(i) < 100) {\n      bins[9]++;\n    }\n  }\n}",
            "// TODO\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  bins[4] = 0;\n  bins[5] = 0;\n  bins[6] = 0;\n  bins[7] = 0;\n  bins[8] = 0;\n  bins[9] = 0;\n\n  Kokkos::parallel_for(int i = 0; i < x.size(); i++)\n  {\n    if (x(i) >= 0 && x(i) < 10)\n      bins[0]++;\n    else if (x(i) >= 10 && x(i) < 20)\n      bins[1]++;\n    else if (x(i) >= 20 && x(i) < 30)\n      bins[2]++;\n    else if (x(i) >= 30 && x(i) < 40)\n      bins[3]++;\n    else if (x(i) >= 40 && x(i) < 50)\n      bins[4]++;\n    else if (x(i) >= 50 && x(i) < 60)\n      bins[5]++;\n    else if (x(i) >= 60 && x(i) < 70)\n      bins[6]++;\n    else if (x(i) >= 70 && x(i) < 80)\n      bins[7]++;\n    else if (x(i) >= 80 && x(i) < 90)\n      bins[8]++;\n    else\n      bins[9]++;\n  }\n}",
            "// Your code here\n}",
            "// Your code here.\n\n  // Initialize Kokkos view\n  Kokkos::View<int*> num_counts(\"num_counts\", 10);\n\n  // Initialize counter\n  int counter = 0;\n\n  // Fill bins\n  for (int i = 0; i < x.extent(0); i++) {\n    if (x[i] >= 0 && x[i] <= 10) {\n      num_counts[0] += 1;\n    } else if (x[i] >= 10 && x[i] <= 20) {\n      num_counts[1] += 1;\n    } else if (x[i] >= 20 && x[i] <= 30) {\n      num_counts[2] += 1;\n    } else if (x[i] >= 30 && x[i] <= 40) {\n      num_counts[3] += 1;\n    } else if (x[i] >= 40 && x[i] <= 50) {\n      num_counts[4] += 1;\n    } else if (x[i] >= 50 && x[i] <= 60) {\n      num_counts[5] += 1;\n    } else if (x[i] >= 60 && x[i] <= 70) {\n      num_counts[6] += 1;\n    } else if (x[i] >= 70 && x[i] <= 80) {\n      num_counts[7] += 1;\n    } else if (x[i] >= 80 && x[i] <= 90) {\n      num_counts[8] += 1;\n    } else if (x[i] >= 90 && x[i] <= 100) {\n      num_counts[9] += 1;\n    } else {\n      // do nothing\n    }\n  }\n\n  // Copy count to bins\n  for (int i = 0; i < num_counts.extent(0); i++) {\n    bins[i] = num_counts[i];\n  }\n}",
            "// Your code here\n  Kokkos::parallel_for(\"binsBy10Count\", x.size(), KOKKOS_LAMBDA (size_t i) {\n    size_t xi = size_t(x(i));\n    bins(xi / 10) += 1;\n  });\n}",
            "// TODO: fill in the body of the function\n    // TODO: make sure you use a loop over x.size(), as well as a loop over the bins array\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = 0; j < bins.size(); ++j) {\n            if (x(i) >= j * 10.0 && x(i) < (j + 1) * 10.0) {\n                bins(j)++;\n            }\n        }\n    }\n\n}",
            "int num_elements = x.size();\n    // TODO: implement this parallel for loop using Kokkos\n    for(int i=0;i<num_elements;i++)\n    {\n        int temp = int(x(i)/10);\n        bins(temp) = bins(temp) + 1;\n    }\n}",
            "auto x_host = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(x_host, x);\n    for (int i = 0; i < 10; i++) {\n        bins(i) = 0;\n    }\n    for (int i = 0; i < x_host.size(); i++) {\n        bins(int(x_host(i) / 10))++;\n    }\n}",
            "const size_t N = x.extent(0);\n\n  // The last two parameters are optional. We'll use them later.\n  Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace, size_t> policy(0, N);\n\n  // Initialize `bins` to 0.\n  Kokkos::deep_copy(bins, 0);\n\n  // The second argument to range policy is optional.\n  Kokkos::parallel_for(policy, [=](size_t i) {\n    // Compute the value of `i` divided by 10 and round down.\n    const size_t bin = i / 10;\n    // Increment the value at `bin` in `bins`.\n    bins[bin] += 1;\n  });\n\n  // Copy `bins` to `bins_host` so that we can print it.\n  Kokkos::View<size_t[10], Kokkos::HostSpace> bins_host(\"bins_host\", 10);\n  Kokkos::deep_copy(bins_host, bins);\n\n  // Print `bins_host`.\n  std::cout << \"bins_host = \" << bins_host << \"\\n\";\n}",
            "using Kokkos::RangePolicy;\n\n    // TODO: define a view for counting each bin\n    // Define a range policy for the execution space\n    // Define a lambda for counting the elements\n    // Apply the lambda to the range policy\n\n    // TODO: sum the counts in bins\n}",
            "}",
            "// Initialize Kokkos view `bins` to zeros.\n\n    // Loop over `x` and count values in each of the 10 intervals.\n    // The loop index `i` is used to determine which bin `x[i]` belongs to.\n    // Use the \"subview\" function to extract the corresponding `i`th element of\n    // `bins`.\n\n    // After the loop, Kokkos will automatically free all the memory for the\n    // views.\n}",
            "// TODO\n}",
            "// Implement this function using Kokkos\n}",
            "// TODO: Implement this function.\n}",
            "auto n = x.size();\n  auto host_x = Kokkos::create_mirror_view_and_copy(Kokkos::HostSpace(), x);\n  auto host_bins = Kokkos::create_mirror_view(bins);\n  for (size_t i = 0; i < n; ++i) {\n    auto v = host_x[i];\n    auto idx = v / 10;\n    host_bins[idx]++;\n  }\n  Kokkos::deep_copy(bins, host_bins);\n}",
            "// TODO\n  // 1. use a parallel for loop to count each value into the corresponding bin\n  // 2. use a parallel reduction to add counts into `bins`\n\n\n  // TODO:\n  // 1. use a parallel for loop to count each value into the corresponding bin\n  // 2. use a parallel reduction to add counts into `bins`\n  Kokkos::parallel_for(x.size(), [=](int i) {\n      bins[Kokkos::atomic_fetch_add(bins + int(x(i)/10), 1)]++;\n  });\n\n  Kokkos::parallel_reduce(\"\", 10, 0, [=](int, int& update) {\n      for (int i = 0; i < 10; ++i)\n          update += bins[i];\n  }, bins);\n}",
            "using size_type = Kokkos::View<size_t*>::size_type;\n  const size_type n = x.size();\n\n  Kokkos::parallel_for(Kokkos::RangePolicy<>(0, n), [&](const size_type i) {\n    const double v = x(i);\n    const size_type bin = static_cast<size_type>((v / 10.0));\n    bins(bin)++;\n  });\n}",
            "// Write your code here, including a for loop to compute the counts.\n    // You can use Kokkos::RangePolicy, but we encourage you to try Kokkos::MDRangePolicy\n\n}",
            "/* Compute bins and update counts in bins by parallelizing over x */\n}",
            "using Kokkos::RangePolicy;\n    using Kokkos::HostSpace;\n\n    RangePolicy<HostSpace> policy(0, x.size());\n    auto functor = KOKKOS_LAMBDA(const int& i) {\n        int bin = x(i)/10;\n        if (bin < 10)\n            bins(bin)++;\n    };\n    Kokkos::parallel_for(policy, functor);\n}",
            "}",
            "// Write your code here.\n}",
            "//...\n}",
            "//TODO:\n}",
            "const size_t N = x.size();\n  // Fill bins with 0\n  Kokkos::deep_copy(bins, Kokkos::View<size_t*>(\"0s\", 10));\n\n  // Compute the counts in parallel\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Serial>(0, N), KOKKOS_LAMBDA(const size_t i) {\n    const int bin = x(i) / 10;\n    bins(bin) += 1;\n  });\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// NOTE: You cannot call any host-side functions within this function!\n    Kokkos::parallel_for(Kokkos::RangePolicy<>(0, x.size()), KOKKOS_LAMBDA(int i) {\n        int bin = int(x(i) / 10.0);\n        bins(bin)++;\n    });\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here.\n}",
            "using Device = typename Kokkos::DefaultExecutionSpace;\n  using Kokkos::RangePolicy;\n\n  // Compute the total number of elements in x\n  const size_t n = x.size();\n\n  // Make a new View that will contain the indices\n  Kokkos::View<int*, Device> indices(Kokkos::ViewAllocateWithoutInitializing(\"indices\"), n);\n\n  // Fill the indices with the corresponding indices in [0, 100]\n  Kokkos::deep_copy(indices, x);\n\n  // Use parallel_for to execute this loop\n  Kokkos::parallel_for(\"my-bin-loop\", RangePolicy<Device>(0, n), [=] (const int i) {\n    // Store the index for a given i in bins[i/10]\n    // Note: Kokkos uses zero-based indexing, so `i/10` is the\n    // index in the bin (0, 1, 2,...) for the index `i`\n    bins[i/10]++;\n  });\n}",
            "// TODO: YOUR CODE HERE\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(const int i, size_t &count) {\n      if (x(i) >= 0 && x(i) <= 10) {\n          count++;\n      }\n    },\n    Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static>>(0, x.size()));\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(const int i, size_t &count) {\n      if (x(i) >= 10 && x(i) <= 20) {\n          count++;\n      }\n    },\n    Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static>>(0, x.size()));\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(const int i, size_t &count) {\n      if (x(i) >= 20 && x(i) <= 30) {\n          count++;\n      }\n    },\n    Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static>>(0, x.size()));\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(const int i, size_t &count) {\n      if (x(i) >= 30 && x(i) <= 40) {\n          count++;\n      }\n    },\n    Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static>>(0, x.size()));\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(const int i, size_t &count) {\n      if (x(i) >= 40 && x(i) <= 50) {\n          count++;\n      }\n    },\n    Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static>>(0, x.size()));\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(const int i, size_t &count) {\n      if (x(i) >= 50 && x(i) <= 60) {\n          count++;\n      }\n    },\n    Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static>>(0, x.size()));\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(const int i, size_t &count) {\n      if (x(i) >= 60 && x(i) <= 70) {\n          count++;\n      }\n    },\n    Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static>>(0, x.size()));\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(const int i, size_t &count) {\n      if (x(i) >= 70 && x(i) <= 80) {\n          count++;\n      }\n    },\n    Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static>>(0, x.size()));\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(const int i, size_t &count) {\n      if (x(i) >= 80 && x(i) <= 90) {\n          count++;\n      }\n    },\n    Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static>>(0, x.size()));\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(const int i, size_t &count) {\n      if (x(i) >= 90 && x(i) <= 100) {\n          count++;\n      }\n    },\n    Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static>>(0, x.size()));\n  Kokkos::deep_copy(bins",
            "// Loop over each index in x, and update the appropriate bin with each value.\n  // Hint: use a for loop, Kokkos::subview, and the range constructor.\n\n\n  // Hint: you can do multiple things in one loop, like\n  //   for (int i = 0; i < 10; ++i) {\n  //     for (int j = 0; j < 10; ++j) {\n  //       for (int k = 0; k < 10; ++k) {\n  //         bins(i)(j)(k) = j;\n  //       }\n  //     }\n  //   }\n\n\n  // At the end, the output should look like\n  // [0] => [0]\n  // [1] => [0]\n  // [2] => [0]\n  // [3] => [1]\n  // [4] => [1]\n  // [5] => [2]\n  // [6] => [3]\n  // [7] => [4]\n  // [8] => [3]\n  // [9] => [2]\n\n}",
            "// TODO: Your code here\n  // HINT: See \"Algorithm 1\" in https://www.cs.umd.edu/class/fall2019/cmsc858l/Kokkos-Tutorial.pdf\n\n}",
            "//TODO\n}",
            "auto exec = Kokkos::Cuda();\n\tusing policy_type = Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Dynamic>, exec>;\n\n\tKokkos::parallel_for(\"BinsBy10Count\", policy_type(0, x.size()), KOKKOS_LAMBDA(const int& idx) {\n\t\tint count = 0;\n\t\twhile (x[idx] >= count * 10) {\n\t\t\tcount++;\n\t\t}\n\t\tif (count > 0)\n\t\t\tbins[count - 1]++;\n\t});\n\tKokkos::fence();\n}",
            "// Your code here\n   // Hint: you can use a Kokkos range policy for the parallel loop\n}",
            "// TODO: implement\n}",
            "using policy_type = Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Dynamic> >;\n  policy_type policy(0, 100);\n  Kokkos::parallel_for(policy, [&](int i) {\n    if (x(i) >= 0 && x(i) < 10) {\n      bins(0) += 1;\n    } else if (x(i) >= 10 && x(i) < 20) {\n      bins(1) += 1;\n    } else if (x(i) >= 20 && x(i) < 30) {\n      bins(2) += 1;\n    } else if (x(i) >= 30 && x(i) < 40) {\n      bins(3) += 1;\n    } else if (x(i) >= 40 && x(i) < 50) {\n      bins(4) += 1;\n    } else if (x(i) >= 50 && x(i) < 60) {\n      bins(5) += 1;\n    } else if (x(i) >= 60 && x(i) < 70) {\n      bins(6) += 1;\n    } else if (x(i) >= 70 && x(i) < 80) {\n      bins(7) += 1;\n    } else if (x(i) >= 80 && x(i) < 90) {\n      bins(8) += 1;\n    } else if (x(i) >= 90 && x(i) <= 100) {\n      bins(9) += 1;\n    }\n  });\n}",
            "// TODO: Your code here.\n}",
            "// TODO: fill in this function\n}",
            "// Hint: use Kokkos::parallel_reduce to compute in parallel\n}",
            "auto bin_counts = Kokkos::create_mirror_view(bins);\n  for (int i = 0; i < 10; i++) bin_counts[i] = 0;\n  const size_t num_vals = x.extent(0);\n  auto x_it = x.data();\n  auto bin_counts_it = bin_counts.data();\n\n  for (size_t i = 0; i < num_vals; ++i) {\n    if (x_it[i] < 10) bin_counts_it[0]++;\n    else if (x_it[i] < 20) bin_counts_it[1]++;\n    else if (x_it[i] < 30) bin_counts_it[2]++;\n    else if (x_it[i] < 40) bin_counts_it[3]++;\n    else if (x_it[i] < 50) bin_counts_it[4]++;\n    else if (x_it[i] < 60) bin_counts_it[5]++;\n    else if (x_it[i] < 70) bin_counts_it[6]++;\n    else if (x_it[i] < 80) bin_counts_it[7]++;\n    else if (x_it[i] < 90) bin_counts_it[8]++;\n    else bin_counts_it[9]++;\n  }\n  Kokkos::deep_copy(bins, bin_counts);\n}",
            "}",
            "bins.fill(0);\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.size()), [=] (int i) {\n    int bin = (x(i) / 10);\n    bins[bin] += 1;\n  });\n}",
            "const int length = x.extent(0);\n  const int nbins = bins.extent(0);\n  double * x_ptr = x.data();\n  size_t * bins_ptr = bins.data();\n  int i, j, k, offset;\n  for(i = 0; i < length; i++) {\n    offset = (int) (x_ptr[i] / 10);\n    bins_ptr[offset]++;\n  }\n}",
            "// TODO\n}",
            "int N = x.size();\n\n    Kokkos::parallel_for(N, [&](int i) {\n        int index = x(i) / 10;\n        bins(index)++;\n    });\n}",
            "Kokkos::parallel_for(\"binsBy10Count\", 10, KOKKOS_LAMBDA(const int i) {\n        bins(i) = 0;\n    });\n    Kokkos::parallel_for(\"binsBy10Count\", x.extent(0), KOKKOS_LAMBDA(const int i) {\n        bins(x(i) / 10)++;\n    });\n}",
            "const size_t n = x.size();\n  Kokkos::parallel_for(\"bin_by_10\", n, KOKKOS_LAMBDA (const size_t i) {\n    const int bin = int(x(i) / 10.0);\n    bins[bin] += 1;\n  });\n}",
            "auto x_host = Kokkos::create_mirror_view(x);\n  Kokkos::deep_copy(x_host, x);\n  std::fill(bins.data(), bins.data()+bins.size(), 0);\n  const double max = 100.0;\n  const double min = 0.0;\n  const double bucketWidth = (max - min) / 10.0;\n  for (size_t i = 0; i < x_host.extent(0); ++i) {\n    auto index = static_cast<size_t>(std::floor(x_host(i) / bucketWidth));\n    ++bins(index);\n  }\n}",
            "}",
            "// TODO: write your code here\n\n}",
            "const size_t size = x.size();\n  \n  // TODO: Your solution here!\n  \n  Kokkos::parallel_for(\"binsBy10Count\",size, KOKKOS_LAMBDA (const int &i) {\n      const double val = x(i);\n      if (val >= 0 && val < 10){\n        ++bins[0];\n      }\n      else if (val >= 10 && val < 20){\n        ++bins[1];\n      }\n      else if (val >= 20 && val < 30){\n        ++bins[2];\n      }\n      else if (val >= 30 && val < 40){\n        ++bins[3];\n      }\n      else if (val >= 40 && val < 50){\n        ++bins[4];\n      }\n      else if (val >= 50 && val < 60){\n        ++bins[5];\n      }\n      else if (val >= 60 && val < 70){\n        ++bins[6];\n      }\n      else if (val >= 70 && val < 80){\n        ++bins[7];\n      }\n      else if (val >= 80 && val < 90){\n        ++bins[8];\n      }\n      else if (val >= 90 && val <= 100){\n        ++bins[9];\n      }\n    });\n}",
            "Kokkos::parallel_for(\"binsBy10Count\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.size()), [=] (const int i) {\n    const size_t idx = (size_t) ((double) x[i]/10.);\n    ++bins[idx];\n  });\n}",
            "// TODO\n}",
            "bins = 0;\n  Kokkos::parallel_for(\"binsBy10Count\", Kokkos::RangePolicy<>(0, x.size()), KOKKOS_LAMBDA (const int i) {\n    const size_t bin = (size_t) (10. * x(i));\n    bins(bin)++;\n  });\n}",
            "// TODO: Implement\n}",
            "// TODO: Your solution here\n}",
            "// Initialize the result to zero.\n    Kokkos::deep_copy(bins, 0);\n\n    // TODO: implement this function\n}",
            "const int n = x.size();\n  const int nbins = bins.size();\n  Kokkos::parallel_for(\"binsBy10Count\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0,n),\n    KOKKOS_LAMBDA(const int i) {\n      const int idx = static_cast<int>(x(i)/10.0);\n      if (idx < nbins) {\n        bins(idx)++;\n      }\n    }\n  );\n}",
            "using Kokkos::TeamPolicy;\n    using Kokkos::Experimental::Iterate;\n\n    TeamPolicy tp(100, 10);\n    auto t = tp.team_scratch(Kokkos::ScratchSpaceOptions::with_dynamic_sizing());\n    auto s = t.team_shmem();\n\n    const int team_size = tp.team_size();\n    const int vector_length = tp.vector_length();\n    const int member_count = team_size / vector_length;\n    const int team_count = tp.league_size();\n\n    // Calculate the global start and end index of each team.\n    auto idx = t.team_rank();\n    const int team_idx = idx / team_size;\n    const int team_offset = idx % team_size;\n\n    const size_t team_begin = team_idx * team_size;\n    const size_t team_end = team_begin + team_size;\n\n    const int offset = team_offset * vector_length;\n\n    // Initialize bins to 0.\n    Kokkos::parallel_for(Kokkos::TeamThreadRange(t, 10), [&] (const int i) {\n        bins(i) = 0;\n    });\n\n    // Count values in each bin.\n    Kokkos::parallel_for(Kokkos::TeamThreadRange(t, team_end), [&] (const int i) {\n        size_t j = i - team_begin;\n        const int bin = (x(j) / 10.0);\n        bins(bin)++;\n    });\n\n    // Reduce the results from each team.\n    Kokkos::parallel_for(Kokkos::TeamThreadRange(t, 10), [&] (const int i) {\n        size_t j = i * team_count;\n        size_t sum = 0;\n        Kokkos::parallel_reduce(Kokkos::TeamThreadRange(t, team_count), [&] (const int k, size_t &update) {\n            update += bins(j + k);\n        }, sum);\n        bins(i) = sum;\n    });\n}",
            "// Your code here\n}",
            "const size_t N = x.size();\n    const size_t block_size = N/10;\n    Kokkos::RangePolicy<Kokkos::Serial> rpolicy(0, N);\n    // TODO: create a policy that uses all available threads and that divides the\n    // range into blocks of size block_size\n    Kokkos::parallel_for(\"binsBy10Count\", rpolicy, [=] (const int& i) {\n        if(x(i) >= 0 && x(i) <= 10) {\n            bins(0)++;\n        }\n        else if(x(i) >= 10 && x(i) <= 20) {\n            bins(1)++;\n        }\n        else if(x(i) >= 20 && x(i) <= 30) {\n            bins(2)++;\n        }\n        else if(x(i) >= 30 && x(i) <= 40) {\n            bins(3)++;\n        }\n        else if(x(i) >= 40 && x(i) <= 50) {\n            bins(4)++;\n        }\n        else if(x(i) >= 50 && x(i) <= 60) {\n            bins(5)++;\n        }\n        else if(x(i) >= 60 && x(i) <= 70) {\n            bins(6)++;\n        }\n        else if(x(i) >= 70 && x(i) <= 80) {\n            bins(7)++;\n        }\n        else if(x(i) >= 80 && x(i) <= 90) {\n            bins(8)++;\n        }\n        else if(x(i) >= 90 && x(i) <= 100) {\n            bins(9)++;\n        }\n    });\n}",
            "// TODO: write code\n}",
            "}",
            "auto kokkos_range = Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.size());\n  Kokkos::parallel_for(kokkos_range, [=] KOKKOS_LAMBDA (const int i) {\n    // TODO\n  });\n  Kokkos::parallel_for(kokkos_range, [=] KOKKOS_LAMBDA (const int i) {\n    // TODO\n  });\n  Kokkos::parallel_for(kokkos_range, [=] KOKKOS_LAMBDA (const int i) {\n    // TODO\n  });\n  Kokkos::parallel_for(kokkos_range, [=] KOKKOS_LAMBDA (const int i) {\n    // TODO\n  });\n  Kokkos::parallel_for(kokkos_range, [=] KOKKOS_LAMBDA (const int i) {\n    // TODO\n  });\n  Kokkos::parallel_for(kokkos_range, [=] KOKKOS_LAMBDA (const int i) {\n    // TODO\n  });\n  Kokkos::parallel_for(kokkos_range, [=] KOKKOS_LAMBDA (const int i) {\n    // TODO\n  });\n  Kokkos::parallel_for(kokkos_range, [=] KOKKOS_LAMBDA (const int i) {\n    // TODO\n  });\n  Kokkos::parallel_for(kokkos_range, [=] KOKKOS_LAMBDA (const int i) {\n    // TODO\n  });\n}",
            "Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        if (x(i) < 10.) bins(0) += 1;\n        else if (x(i) >= 10. && x(i) < 20.) bins(1) += 1;\n        else if (x(i) >= 20. && x(i) < 30.) bins(2) += 1;\n        else if (x(i) >= 30. && x(i) < 40.) bins(3) += 1;\n        else if (x(i) >= 40. && x(i) < 50.) bins(4) += 1;\n        else if (x(i) >= 50. && x(i) < 60.) bins(5) += 1;\n        else if (x(i) >= 60. && x(i) < 70.) bins(6) += 1;\n        else if (x(i) >= 70. && x(i) < 80.) bins(7) += 1;\n        else if (x(i) >= 80. && x(i) < 90.) bins(8) += 1;\n        else if (x(i) >= 90. && x(i) <= 100.) bins(9) += 1;\n    });\n}",
            "// Fill in code here\n\n}",
            "// Compute the number of elements in each bin.\n\n  // TODO: YOUR CODE HERE\n\n}",
            "auto count_fct = KOKKOS_LAMBDA (const int i) {\n    double value = x(i);\n    size_t bin = value / 10;\n    bins(bin)++;\n  };\n\n  Kokkos::parallel_for(\"bins_by_10_count\", count_fct);\n}",
            "// Fill this in\n}",
            "// TODO\n}",
            "// fill in your code here\n}",
            "// YOUR CODE HERE\n}",
            "using namespace Kokkos;\n\n    // TODO: Implement this function!\n    //\n    // Hint: You can make your kernel run in parallel with the following\n    //       syntax:\n    //\n    //       ParallelFor<void, Kokkos::Schedule<Kokkos::Static>>([&] (int i) {\n    //          ...\n    //       });\n}",
            "// TODO: Your solution here\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        int bin = x[i] / 10;\n        if (bin < 0) {\n            bin = 0;\n        }\n        else if (bin > 9) {\n            bin = 9;\n        }\n        ++bins[bin];\n    }\n}",
            "size_t N = x.size();\n  #pragma omp parallel for\n  for (int i = 0; i < N; ++i) {\n    int index = (int)std::floor(x[i]/10);\n    if (index < 10 && index >= 0)\n      ++bins[index];\n  }\n}",
            "size_t sum = 0;\n#pragma omp parallel for reduction(+:sum)\n  for(int i=0; i<x.size(); i++) {\n    double x_i = x.at(i);\n    size_t index = 0;\n    if(x_i >= 10) index = 0;\n    else if(x_i >= 20) index = 1;\n    else if(x_i >= 30) index = 2;\n    else if(x_i >= 40) index = 3;\n    else if(x_i >= 50) index = 4;\n    else if(x_i >= 60) index = 5;\n    else if(x_i >= 70) index = 6;\n    else if(x_i >= 80) index = 7;\n    else if(x_i >= 90) index = 8;\n    else if(x_i >= 100) index = 9;\n    bins[index] += 1;\n    sum++;\n  }\n  std::cout << \"sum: \" << sum << std::endl;\n}",
            "// Fill the `bins` array with zeros:\n  std::fill(bins.begin(), bins.end(), 0);\n\n  // TODO: fill in the rest of this function\n\n}",
            "int const n = x.size();\n    size_t *bins_d = new size_t[10]();\n\n    #pragma omp parallel for\n    for(int i=0; i<n; i++){\n        size_t index = x[i]/10;\n        bins_d[index]++;\n    }\n\n    for(int i=0; i<10; i++){\n        bins[i] = bins_d[i];\n    }\n\n    delete[] bins_d;\n}",
            "size_t nthreads = omp_get_max_threads();\n\n  #pragma omp parallel for\n  for (int i = 0; i < 10; i++) {\n    #pragma omp parallel for\n    for (size_t j = 0; j < x.size(); j++) {\n      if (x[j] >= 10*i && x[j] < 10*(i+1)) {\n        #pragma omp atomic\n        bins[i] += 1;\n      }\n    }\n  }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    int index = (x[i] / 10) - 1;\n    if (index >= 0)\n      bins[index]++;\n  }\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i = 0; i < x.size(); ++i)\n        {\n            if (x[i] > 100 || x[i] < 0)\n                continue;\n            bins[(size_t)std::floor((x[i]+1)/10)]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    if (x[i] < 10) {\n      ++bins[0];\n    }\n    else if (x[i] < 20) {\n      ++bins[1];\n    }\n    else if (x[i] < 30) {\n      ++bins[2];\n    }\n    else if (x[i] < 40) {\n      ++bins[3];\n    }\n    else if (x[i] < 50) {\n      ++bins[4];\n    }\n    else if (x[i] < 60) {\n      ++bins[5];\n    }\n    else if (x[i] < 70) {\n      ++bins[6];\n    }\n    else if (x[i] < 80) {\n      ++bins[7];\n    }\n    else if (x[i] < 90) {\n      ++bins[8];\n    }\n    else if (x[i] < 100) {\n      ++bins[9];\n    }\n  }\n\n}",
            "bins = std::array<size_t, 10>{0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        int idx = x.at(i)/10;\n        bins.at(idx)++;\n    }\n}",
            "std::vector<int> result(10,0);\n  #pragma omp parallel for\n  for (int i = 0; i < 10; i++) {\n    for (size_t j = 0; j < x.size(); j++) {\n      if (x[j] >= i*10 && x[j] < (i+1)*10) {\n        result[i]++;\n      }\n    }\n  }\n  bins = result;\n}",
            "for(size_t i=0; i<10; ++i)\n        bins[i] = 0;\n\n    int thread_num = omp_get_num_threads();\n    int thread_id = omp_get_thread_num();\n\n    int start = thread_id * (x.size() / thread_num);\n    int end = (thread_id + 1) * (x.size() / thread_num);\n\n    if(thread_id == thread_num-1)\n        end = x.size();\n\n    int bin = 0;\n    for(size_t i=start; i<end; ++i) {\n        bin = x[i] / 10;\n        ++bins[bin];\n    }\n}",
            "int n = x.size();\n    std::vector<int> result(n);\n\n    int i = 0;\n#pragma omp parallel shared(n,x,result) private(i)\n    {\n#pragma omp for\n        for (i = 0; i < n; i++)\n        {\n            result[i] = (int)(x[i]/10);\n        }\n\n        for(int j = 0; j < 10; j++)\n        {\n            bins[j] = 0;\n        }\n#pragma omp for\n        for (i = 0; i < n; i++)\n        {\n            bins[result[i]]++;\n        }\n    }\n\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n        size_t val = (x[i] - 1) / 10;\n        omp_set_lock(&bins[val]);\n        bins[val]++;\n        omp_unset_lock(&bins[val]);\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] >= 0 && x[i] < 10) {\n      bins[0]++;\n    }\n    else if (x[i] >= 10 && x[i] < 20) {\n      bins[1]++;\n    }\n    else if (x[i] >= 20 && x[i] < 30) {\n      bins[2]++;\n    }\n    else if (x[i] >= 30 && x[i] < 40) {\n      bins[3]++;\n    }\n    else if (x[i] >= 40 && x[i] < 50) {\n      bins[4]++;\n    }\n    else if (x[i] >= 50 && x[i] < 60) {\n      bins[5]++;\n    }\n    else if (x[i] >= 60 && x[i] < 70) {\n      bins[6]++;\n    }\n    else if (x[i] >= 70 && x[i] < 80) {\n      bins[7]++;\n    }\n    else if (x[i] >= 80 && x[i] < 90) {\n      bins[8]++;\n    }\n    else {\n      bins[9]++;\n    }\n  }\n}",
            "#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tsize_t bin = (x[i] / 10);\n\t\tbins[bin]++;\n\t}\n}",
            "// parallel for\n    #pragma omp parallel for\n    for (int i = 0; i < 10; ++i) {\n        bins[i] = 0;\n    }\n    for (double d: x) {\n        int bin = (int) d / 10;\n        #pragma omp atomic\n        bins[bin]++;\n    }\n}",
            "// TODO: Your code here\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        size_t bucket = std::floor(x[i] / 10.0);\n        //std::cout << \"Bucket: \" << bucket << std::endl;\n        bins[bucket]++;\n        //std::cout << \"Bin: \" << bins[bucket] << std::endl;\n    }\n}",
            "std::array<int, 10> counts = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n\n    for (int i = 0; i < x.size(); i++){\n        if(x[i] >= 0 && x[i] < 10){\n            counts[0] += 1;\n        }\n        if(x[i] >= 10 && x[i] < 20){\n            counts[1] += 1;\n        }\n        if(x[i] >= 20 && x[i] < 30){\n            counts[2] += 1;\n        }\n        if(x[i] >= 30 && x[i] < 40){\n            counts[3] += 1;\n        }\n        if(x[i] >= 40 && x[i] < 50){\n            counts[4] += 1;\n        }\n        if(x[i] >= 50 && x[i] < 60){\n            counts[5] += 1;\n        }\n        if(x[i] >= 60 && x[i] < 70){\n            counts[6] += 1;\n        }\n        if(x[i] >= 70 && x[i] < 80){\n            counts[7] += 1;\n        }\n        if(x[i] >= 80 && x[i] < 90){\n            counts[8] += 1;\n        }\n        if(x[i] >= 90 && x[i] <= 100){\n            counts[9] += 1;\n        }\n    }\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < 10; i++) {\n        bins[i] = counts[i];\n    }\n}",
            "for (int i = 0; i < 10; i++) {\n    bins[i] = 0;\n  }\n\n  #pragma omp parallel for reduction(+:bins)\n  for (int i = 0; i < x.size(); i++) {\n    int bin = (int)x[i]/10;\n    bins[bin] += 1;\n  }\n}",
            "// Initialize `bins` to 0\n    bins.fill(0);\n\n    // #pragma omp parallel for\n    // for (int i = 0; i < 10; i++) {\n    //     int count = 0;\n    //     for (auto& num : x) {\n    //         if (num >= i * 10 && num < (i+1) * 10) {\n    //             count++;\n    //         }\n    //     }\n    //     bins[i] = count;\n    // }\n\n    #pragma omp parallel for reduction(+:bins[:])\n    for (int i = 0; i < 10; i++) {\n        int count = 0;\n        for (auto& num : x) {\n            if (num >= i * 10 && num < (i+1) * 10) {\n                count++;\n            }\n        }\n        bins[i] = count;\n    }\n}",
            "// TODO: Your code here\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    int pos = x[i] / 10;\n    #pragma omp atomic\n    ++bins[pos];\n  }\n}",
            "// TODO: Your code goes here\n\t// You may want to use the std::copy algorithm\n\t// and the std::partition algorithm to partition\n\t// x into 10 subranges.\n\t//\n\t// If you have trouble with this part of the assignment,\n\t// try reading Chapter 18 of the textbook.\n\t//\n\t// If you're using g++ on a Mac, you may need to add\n\t// -lgomp to your Makefile.\n}",
            "#pragma omp parallel\n  {\n    int i_thread = omp_get_thread_num();\n    int n_threads = omp_get_num_threads();\n\n    int start = (i_thread * x.size())/n_threads;\n    int end = ( (i_thread+1) * x.size())/n_threads;\n\n    //std::cout << \"thread: \" << i_thread << \" start: \" << start << \" end: \" << end << std::endl;\n\n    for (size_t i = start; i < end; i++) {\n      if (x[i] >= 0 && x[i] < 10) {\n\tbins[0]++;\n      } else if (x[i] >= 10 && x[i] < 20) {\n\tbins[1]++;\n      } else if (x[i] >= 20 && x[i] < 30) {\n\tbins[2]++;\n      } else if (x[i] >= 30 && x[i] < 40) {\n\tbins[3]++;\n      } else if (x[i] >= 40 && x[i] < 50) {\n\tbins[4]++;\n      } else if (x[i] >= 50 && x[i] < 60) {\n\tbins[5]++;\n      } else if (x[i] >= 60 && x[i] < 70) {\n\tbins[6]++;\n      } else if (x[i] >= 70 && x[i] < 80) {\n\tbins[7]++;\n      } else if (x[i] >= 80 && x[i] < 90) {\n\tbins[8]++;\n      } else if (x[i] >= 90 && x[i] < 100) {\n\tbins[9]++;\n      }\n    }\n  }\n}",
            "for (int i = 0; i < bins.size(); ++i) {\n\t\tbins[i] = 0;\n\t}\n\t#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); ++i) {\n\t\tbins[static_cast<size_t>(x[i]/10) - 1] += 1;\n\t}\n}",
            "// TODO: parallelize the loop\n#pragma omp parallel\n    {\n        int bin = 0;\n        for (auto i = x.begin(); i!= x.end(); ++i)\n            if (*i < 10.0) {\n                ++bins[bin++];\n            } else if (*i < 20.0) {\n                ++bins[bin++];\n            } else if (*i < 30.0) {\n                ++bins[bin++];\n            } else if (*i < 40.0) {\n                ++bins[bin++];\n            } else if (*i < 50.0) {\n                ++bins[bin++];\n            } else if (*i < 60.0) {\n                ++bins[bin++];\n            } else if (*i < 70.0) {\n                ++bins[bin++];\n            } else if (*i < 80.0) {\n                ++bins[bin++];\n            } else if (*i < 90.0) {\n                ++bins[bin++];\n            } else if (*i < 100.0) {\n                ++bins[bin++];\n            }\n    }\n}",
            "for(int i = 0; i < 10; i++)\n  {\n    int j = 0;\n    int count = 0;\n    #pragma omp parallel for reduction(+:j, count)\n    for(int k = 0; k < x.size(); k++)\n    {\n      if(x[k] >= i * 10 && x[k] < (i+1) * 10)\n      {\n        count++;\n        j++;\n      }\n    }\n    bins[i] = count;\n  }\n}",
            "#pragma omp parallel for\n    for(size_t i=0; i<x.size(); i++) {\n        int pos = x[i]/10;\n        #pragma omp atomic\n        bins[pos]++;\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        // TODO\n    }\n}",
            "bins.fill(0);\n\n#pragma omp parallel for\n    for(auto idx=0; idx<x.size(); idx++) {\n        const auto value = std::floor(x[idx]);\n        const auto idx_bin = value / 10;\n        if(idx_bin >= 0 && idx_bin < 10) {\n            ++bins[idx_bin];\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 10) {\n            bins[0]++;\n        } else if (x[i] < 20) {\n            bins[1]++;\n        } else if (x[i] < 30) {\n            bins[2]++;\n        } else if (x[i] < 40) {\n            bins[3]++;\n        } else if (x[i] < 50) {\n            bins[4]++;\n        } else if (x[i] < 60) {\n            bins[5]++;\n        } else if (x[i] < 70) {\n            bins[6]++;\n        } else if (x[i] < 80) {\n            bins[7]++;\n        } else if (x[i] < 90) {\n            bins[8]++;\n        } else if (x[i] < 100) {\n            bins[9]++;\n        }\n    }\n}",
            "// TODO\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        int bin = (int)(x[i]/10);\n        ++bins[bin];\n    }\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for\n        for(size_t i=0; i<x.size(); ++i) {\n            int bin = std::floor(x[i]/10.0);\n            ++bins[bin];\n        }\n    }\n}",
            "int n = x.size();\n  for (size_t i=0; i<n; i++) {\n    bins[int(x[i]/10)] += 1;\n  }\n}",
            "for(auto i = 0; i < 10; i++) {\n    bins[i] = 0;\n  }\n  #pragma omp parallel for\n  for(auto i = 0; i < x.size(); i++) {\n    size_t bin = floor(x[i]/10.0);\n    bins[bin]++;\n  }\n}",
            "// Fill in your solution here\n}",
            "/*\n  #pragma omp parallel for\n  for (int i=0; i<(x.size()); i++) {\n    std::cout<<\"i is \"<<i<<\" and x is \"<<x[i]<<std::endl;\n    if (x[i]<10) {\n      bins[0]++;\n    }\n    if (x[i]>=10 && x[i]<20) {\n      bins[1]++;\n    }\n    if (x[i]>=20 && x[i]<30) {\n      bins[2]++;\n    }\n    if (x[i]>=30 && x[i]<40) {\n      bins[3]++;\n    }\n    if (x[i]>=40 && x[i]<50) {\n      bins[4]++;\n    }\n    if (x[i]>=50 && x[i]<60) {\n      bins[5]++;\n    }\n    if (x[i]>=60 && x[i]<70) {\n      bins[6]++;\n    }\n    if (x[i]>=70 && x[i]<80) {\n      bins[7]++;\n    }\n    if (x[i]>=80 && x[i]<90) {\n      bins[8]++;\n    }\n    if (x[i]>=90 && x[i]<100) {\n      bins[9]++;\n    }\n  }\n  */\n  int k = 0;\n  for(int i=0; i<10; i++) {\n    for(int j=i*10; j<=(i+1)*10-1; j++) {\n      int t = 0;\n      for(int n=0; n<x.size(); n++) {\n\tif(x[n]>=j && x[n]<j+10) {\n\t  t++;\n\t}\n      }\n      bins[i] = t;\n    }\n  }\n}",
            "int n_bins = 10;\n  size_t n = x.size();\n\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for(int i = 0; i < n; i++) {\n      int index = (int)(x[i]/10);\n      if(index < n_bins && index >= 0)\n        #pragma omp atomic\n        bins[index]++;\n    }\n  }\n}",
            "#pragma omp parallel for schedule(dynamic,1)\n    for (size_t i = 0; i < x.size(); i++) {\n        bins[(size_t) floor(x[i] / 10)].fetch_add(1, std::memory_order_relaxed);\n    }\n}",
            "const double max_value = x[x.size() - 1];\n    const double step = 10.0 / max_value;\n\n    const int num_threads = omp_get_max_threads();\n\n    std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel for num_threads(num_threads)\n    for (int i = 0; i < x.size(); i++) {\n        const size_t bin = x[i] / step;\n        bins[bin]++;\n    }\n}",
            "bins.fill(0);\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for (size_t i = 0; i < x.size(); ++i) {\n      bins[int(10.0 * x[i] / 100.0)]++;\n    }\n  }\n}",
            "std::array<size_t, 10> counts{};\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    auto rounded = (x[i]/10);\n    counts[rounded]++;\n  }\n  bins = counts;\n}",
            "#pragma omp parallel for\n  for (auto i = 0; i < x.size(); i++) {\n    int idx = floor(x[i]/10);\n    #pragma omp atomic\n    bins[idx]++;\n  }\n}",
            "}",
            "bins.fill(0);\n#pragma omp parallel for\n    for (size_t i=0; i<x.size(); i++) {\n        double xi = x[i];\n        int xi_int = int(xi);\n        if (xi_int < 10) bins[0] += 1;\n        else if (xi_int < 20) bins[1] += 1;\n        else if (xi_int < 30) bins[2] += 1;\n        else if (xi_int < 40) bins[3] += 1;\n        else if (xi_int < 50) bins[4] += 1;\n        else if (xi_int < 60) bins[5] += 1;\n        else if (xi_int < 70) bins[6] += 1;\n        else if (xi_int < 80) bins[7] += 1;\n        else if (xi_int < 90) bins[8] += 1;\n        else bins[9] += 1;\n    }\n}",
            "// compute size of each bin\n  int binsize = x.size()/10;\n\n  #pragma omp parallel for\n  for(int i=0;i<10;i++){\n    for(int j=i*binsize;j<i*binsize+binsize;j++){\n      if(j>=x.size()){\n        break;\n      }\n      bins[i] += 1;\n    }\n  }\n}",
            "// TODO\n    #pragma omp parallel for\n    for(size_t i = 0; i < x.size(); i++)\n    {\n        int n = (int)x[i]/10;\n        if (n >= 10)\n        {\n            n = 9;\n        }\n        bins[n]++;\n    }\n}",
            "for (int i = 0; i < 10; i++)\n        bins[i] = 0;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] >= 0 && x[i] < 10)\n            bins[0]++;\n        else if (x[i] >= 10 && x[i] < 20)\n            bins[1]++;\n        else if (x[i] >= 20 && x[i] < 30)\n            bins[2]++;\n        else if (x[i] >= 30 && x[i] < 40)\n            bins[3]++;\n        else if (x[i] >= 40 && x[i] < 50)\n            bins[4]++;\n        else if (x[i] >= 50 && x[i] < 60)\n            bins[5]++;\n        else if (x[i] >= 60 && x[i] < 70)\n            bins[6]++;\n        else if (x[i] >= 70 && x[i] < 80)\n            bins[7]++;\n        else if (x[i] >= 80 && x[i] < 90)\n            bins[8]++;\n        else if (x[i] >= 90 && x[i] <= 100)\n            bins[9]++;\n    }\n}",
            "// TODO\n}",
            "// Your code here.\n}",
            "// Fill in this function!\n\n  //TODO: add your code here\n\n}",
            "int nthreads = omp_get_max_threads();\n    int ithread = omp_get_thread_num();\n    int thread_id = 0;\n\n    // Get number of elements in array per thread\n    int elems_per_thread = x.size() / nthreads;\n\n    // Set up thread specific bins\n    std::array<size_t, 10> thread_bins;\n\n    // Divide the vector x into nthreads subarrays and assign each thread to\n    // its own subarray\n    std::vector<std::vector<double>> subx(nthreads);\n    for (int i = 0; i < nthreads; ++i) {\n        thread_id = i * elems_per_thread;\n        if (thread_id < x.size()) {\n            subx[i].assign(x.begin() + thread_id, x.begin() + thread_id + elems_per_thread);\n        }\n    }\n\n    // Counts per thread\n    std::array<size_t, 10> thread_counts;\n\n    // Count the number of elements in each subarray\n    #pragma omp parallel for\n    for (int i = 0; i < nthreads; ++i) {\n        thread_id = i * elems_per_thread;\n        if (thread_id < x.size()) {\n            thread_counts = count_per_10(subx[i]);\n            for (int j = 0; j < 10; ++j) {\n                thread_bins[j] += thread_counts[j];\n            }\n        }\n    }\n\n    // Compute the sum of counts from each thread\n    size_t sum = 0;\n    for (int j = 0; j < 10; ++j) {\n        sum += thread_bins[j];\n    }\n\n    // If we are not on the last thread, we have an extra thread_id to\n    // account for, since we don't compute the last thread.\n    if (ithread!= nthreads - 1) {\n        sum += thread_bins[ithread];\n    }\n\n    // Update the counts\n    #pragma omp critical\n    {\n        for (int j = 0; j < 10; ++j) {\n            bins[j] = sum;\n            sum -= bins[j];\n        }\n    }\n}",
            "// TODO: Your code here\n#pragma omp parallel for\n    for (int i = 0; i < 10; i++) {\n        double lower = i * 10;\n        double upper = lower + 10;\n        size_t counter = 0;\n        for (auto j = x.begin(); j!= x.end(); j++) {\n            if (*j >= lower && *j < upper) {\n                counter++;\n            }\n        }\n        bins[i] = counter;\n    }\n}",
            "}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    // Your code here\n  }\n}",
            "std::vector<int> tmp_bins;\n    tmp_bins.resize(10);\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int index = x[i]/10;\n        if (index < 10) {\n            tmp_bins[index] += 1;\n        }\n    }\n\n    for (size_t i = 0; i < 10; ++i) {\n        bins[i] = tmp_bins[i];\n    }\n}",
            "int nthreads = omp_get_max_threads();\n    for(int i = 0; i < 10; ++i) bins[i] = 0;\n\n#pragma omp parallel\n    {\n        int i, j;\n        int thread_id = omp_get_thread_num();\n        int n_threads = omp_get_num_threads();\n\n        int n = x.size();\n        if (n%n_threads!= 0) {\n            for (i = 0; i < n; i+=n_threads) {\n                j = (thread_id+1)*(n/n_threads);\n                if (j > n) j = n;\n                for (; i < j; ++i) {\n                    if (x[i] >= i*10 && x[i] < (i+1)*10) {\n                        bins[i]++;\n                    }\n                }\n            }\n        }\n        else {\n            for (i = 0; i < n; i+=n_threads) {\n                j = (thread_id+1)*(n/n_threads);\n                for (; i < j; ++i) {\n                    if (x[i] >= i*10 && x[i] < (i+1)*10) {\n                        bins[i]++;\n                    }\n                }\n            }\n        }\n    }\n}",
            "#pragma omp parallel for schedule(dynamic, 10)\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tint index = (int)(x.at(i) / 10.0);\n\t\tbins[index]++;\n\t}\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < 10; ++i) {\n    bins[i] = 0;\n  }\n  for (auto const& val : x) {\n    int bin = int(val / 10);\n    #pragma omp atomic\n    bins[bin]++;\n  }\n}",
            "// Your code goes here\n\n}",
            "bins.fill(0);\n    int n = x.size();\n#pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        int val = (int) std::floor(x[i]);\n        if (val < 10) {\n            bins[0] += 1;\n        } else if (val < 20) {\n            bins[1] += 1;\n        } else if (val < 30) {\n            bins[2] += 1;\n        } else if (val < 40) {\n            bins[3] += 1;\n        } else if (val < 50) {\n            bins[4] += 1;\n        } else if (val < 60) {\n            bins[5] += 1;\n        } else if (val < 70) {\n            bins[6] += 1;\n        } else if (val < 80) {\n            bins[7] += 1;\n        } else if (val < 90) {\n            bins[8] += 1;\n        } else if (val < 100) {\n            bins[9] += 1;\n        } else {\n            std::cout << \"error\" << std::endl;\n        }\n    }\n}",
            "// Write your code here\n    //omp_set_num_threads(2);\n    #pragma omp parallel for\n    for(int i = 0; i < x.size(); i++) {\n        int idx = (int)std::floor(x[i] / 10.0);\n        #pragma omp atomic\n        bins[idx]++;\n    }\n}",
            "int nthreads = omp_get_num_threads();\n  int tid = omp_get_thread_num();\n  size_t size = x.size();\n  size_t chunk = size / nthreads;\n\n  #pragma omp parallel shared(x, bins) private(tid, chunk)\n  {\n    #pragma omp for schedule(static, chunk)\n    for (size_t i = tid * chunk; i < chunk * nthreads; ++i) {\n      double val = x[i];\n      if (val < 10)\n        ++bins[0];\n      else if (val >= 10 && val < 20)\n        ++bins[1];\n      else if (val >= 20 && val < 30)\n        ++bins[2];\n      else if (val >= 30 && val < 40)\n        ++bins[3];\n      else if (val >= 40 && val < 50)\n        ++bins[4];\n      else if (val >= 50 && val < 60)\n        ++bins[5];\n      else if (val >= 60 && val < 70)\n        ++bins[6];\n      else if (val >= 70 && val < 80)\n        ++bins[7];\n      else if (val >= 80 && val < 90)\n        ++bins[8];\n      else if (val >= 90 && val <= 100)\n        ++bins[9];\n    }\n  }\n}",
            "bins.fill(0);\n\n  #pragma omp parallel for\n  for(size_t i = 0; i < x.size(); i++) {\n    size_t bin = (size_t) (10 * (x[i] / 10.0));\n    bins[bin]++;\n  }\n}",
            "// TODO:\n  //\n  // 1. Fill in the code below. The code should be parallelized with OpenMP.\n  //\n  // 2. Run the code to verify that the output is correct.\n  //\n  // 3. Run cmake with -DCHECK=ON to enable runtime checks and verify that the\n  //    output is correct.\n  //\n\n  #pragma omp parallel for\n  for (int i = 0; i < 10; i++)\n  {\n    int j = 0;\n    while (x[j] <= 10*i)\n    {\n      j++;\n    }\n    bins[i] = j;\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        auto val = static_cast<int>(x[i] / 10);\n        if (val >= 10)\n            val = 9;\n        bins[val] += 1;\n    }\n}",
            "}",
            "int size = x.size();\n    int binsize = 10;\n    int binsize_int = binsize;\n    int nbins = size / binsize;\n    if (size % binsize > 0) nbins++;\n    #pragma omp parallel for\n    for (int i = 0; i < nbins; i++) {\n        for (int j = 0; j < binsize_int; j++) {\n            if (j * nbins + i < size && x[j * nbins + i] >= i * binsize && x[j * nbins + i] < (i + 1) * binsize) {\n                bins[i]++;\n                break;\n            }\n        }\n    }\n}",
            "int nbins = 10;\n    size_t k;\n\n    #pragma omp parallel for shared(nbins, x, bins) private(k)\n    for (k=0; k < nbins; k++) {\n        size_t count = 0;\n\n        for (int i=0; i<x.size(); i++) {\n            if (x[i] >= k*10.0 && x[i] < (k+1)*10.0)\n                count++;\n        }\n        bins[k] = count;\n    }\n}",
            "bins.fill(0);\n\n  // TODO\n\n}",
            "int num_threads = omp_get_max_threads();\n\n#pragma omp parallel for num_threads(num_threads)\n  for (int i = 0; i < x.size(); i++) {\n    bins[x[i]/10]++;\n  }\n}",
            "// TODO: Fill in code to implement function\n    size_t i = 0;\n    size_t j = 0;\n    size_t k = 0;\n    size_t l = 0;\n    size_t n = 0;\n    size_t m = 0;\n    size_t q = 0;\n    size_t r = 0;\n    size_t s = 0;\n    size_t t = 0;\n    size_t u = 0;\n    size_t v = 0;\n    size_t w = 0;\n    size_t x1 = 0;\n    size_t y1 = 0;\n    size_t z1 = 0;\n    size_t a1 = 0;\n    size_t b1 = 0;\n    size_t c1 = 0;\n    size_t d1 = 0;\n    size_t e1 = 0;\n    size_t f1 = 0;\n    size_t g1 = 0;\n    size_t h1 = 0;\n    size_t i1 = 0;\n    size_t j1 = 0;\n    size_t k1 = 0;\n    size_t l1 = 0;\n    size_t m1 = 0;\n    size_t n1 = 0;\n    size_t o1 = 0;\n    size_t p1 = 0;\n    size_t q1 = 0;\n    size_t r1 = 0;\n    size_t s1 = 0;\n    size_t t1 = 0;\n    size_t u1 = 0;\n    size_t v1 = 0;\n    size_t w1 = 0;\n    size_t x2 = 0;\n    size_t y2 = 0;\n    size_t z2 = 0;\n    size_t a2 = 0;\n    size_t b2 = 0;\n    size_t c2 = 0;\n    size_t d2 = 0;\n    size_t e2 = 0;\n    size_t f2 = 0;\n    size_t g2 = 0;\n    size_t h2 = 0;\n    size_t i2 = 0;\n    size_t j2 = 0;\n    size_t k2 = 0;\n    size_t l2 = 0;\n    size_t m2 = 0;\n    size_t n2 = 0;\n    size_t o2 = 0;\n    size_t p2 = 0;\n    size_t q2 = 0;\n    size_t r2 = 0;\n    size_t s2 = 0;\n    size_t t2 = 0;\n    size_t u2 = 0;\n    size_t v2 = 0;\n    size_t w2 = 0;\n\n    for(i = 0; i < x.size(); i++){\n        for(j = 0; j < 10; j++){\n            if(x[i] <= (10 * j)){\n                bins[j] += 1;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        int pos = x[i] / 10;\n        if (pos >= 0 && pos < 10)\n            bins[pos]++;\n    }\n}",
            "int b = 0;\n  #pragma omp parallel for shared(bins)\n  for (int i=0; i<x.size(); i++) {\n    bins[b]++;\n    if (x[i] % 10 == 0) {\n      b++;\n    }\n  }\n}",
            "std::array<size_t, 10> counts{};\n    for(auto const& i : x) {\n        counts[std::floor(i / 10.0)]++;\n    }\n    bins = counts;\n}",
            "// Fill in this function\n\n}",
            "auto count = [](std::vector<double> &x, std::array<size_t, 10> &bins,\n                    double a, double b) {\n        double total = 0.0;\n        for (auto itr = x.begin(); itr!= x.end(); itr++) {\n            if (*itr >= a && *itr < b) {\n                total++;\n            }\n        }\n        bins[a/10.0 - 1] = total;\n    };\n\n#pragma omp parallel\n    {\n        std::array<size_t, 10> bins;\n        for (auto i = 0; i < 10; i++)\n            bins[i] = 0;\n        std::vector<double> x(100);\n        for (auto i = 0; i < 100; i++)\n            x[i] = (double)(i);\n\n        auto nth = omp_get_num_threads();\n        auto id = omp_get_thread_num();\n        auto inc = (100 + nth - 1) / nth;\n        auto start = inc * id;\n        auto end = start + inc;\n        if (end > 100)\n            end = 100;\n        count(x, bins, start, end);\n        for (auto i = 0; i < 10; i++)\n            bins[i] += bins[i+1];\n\n        // print out bins\n        for (auto i = 0; i < 10; i++)\n            std::cout << bins[i] << \" \";\n        std::cout << std::endl;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    int num_threads = omp_get_max_threads();\n    size_t num_items = x.size();\n    int num_bins = 10;\n    size_t chunk_size = num_items/num_threads;\n    #pragma omp parallel\n    {\n        int thread_id = omp_get_thread_num();\n        size_t start = thread_id * chunk_size;\n        size_t end = (thread_id+1) * chunk_size;\n        if(thread_id == num_threads - 1) {\n            end = num_items;\n        }\n        for (size_t i = start; i < end; i++) {\n            int bin = x[i]/10;\n            if (bin < num_bins) {\n                bins[bin]++;\n            }\n        }\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    int bucket = (int) x[i] / 10;\n    bins[bucket]++;\n  }\n}",
            "//TODO: your code here\n\n\n    return;\n}",
            "//...\n}",
            "assert(x.size() > 0);\n\n    const size_t N = x.size();\n    const size_t nthreads = omp_get_max_threads();\n\n    const size_t chunk = N / nthreads;\n\n    #pragma omp parallel for schedule(static, chunk)\n    for (size_t i = 0; i < N; ++i) {\n        bins[int(x[i] / 10)] += 1;\n    }\n\n    //#pragma omp parallel for schedule(static, chunk)\n    //for (size_t i = 0; i < N; ++i) {\n    //    std::cout << x[i] / 10 << std::endl;\n    //}\n\n    //std::array<size_t, 10> bins;\n    //for (size_t i = 0; i < N; ++i) {\n    //    const size_t idx = int(x[i] / 10);\n    //    bins[idx] += 1;\n    //}\n\n    //for (size_t i = 0; i < 10; ++i) {\n    //    std::cout << bins[i] << std::endl;\n    //}\n}",
            "// your code here\n\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for\n        for (int i = 0; i < 1000; i++)\n            bins[i/10]++;\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        size_t bin = x[i] / 10;\n        if (bin < 10) {\n            bins[bin]++;\n        }\n    }\n}",
            "//TODO: compute `bins` in parallel\n    #pragma omp parallel for\n    for (int i=0; i<x.size(); i++)\n    {\n        int index = (int)((x[i] / 10));\n        if (index >= 0 && index <= 9)\n        {\n            bins[index]++;\n        }\n    }\n}",
            "// TODO: Implement this function.\n\n    std::vector<double> v(x);\n    std::sort(v.begin(),v.end());\n    std::vector<double> b(10,0.0);\n    for (size_t i=0; i<v.size(); i+=10) {\n        if (i+10<v.size()) {\n            b[i/10] = (v[i+10]-v[i])/10.0;\n        }\n        else {\n            b[i/10] = (10.0-v[i-10])/10.0;\n        }\n    }\n\n    //copy the results to bins\n    for (size_t i=0; i<b.size(); i++) {\n        bins[i]=b[i];\n    }\n\n    //std::cout << \"input: \";\n    //for (size_t i=0; i<v.size(); i++) {\n    //    std::cout << v[i] << \" \";\n    //}\n    //std::cout << std::endl;\n\n    //std::cout << \"output: \";\n    //for (size_t i=0; i<bins.size(); i++) {\n    //    std::cout << bins[i] << \" \";\n    //}\n    //std::cout << std::endl;\n}",
            "int my_rank = omp_get_thread_num();\n    int nprocs = omp_get_num_threads();\n    int size = x.size();\n\n    // Divide workload between processes.\n    int blockSize = size / nprocs;\n    int blockStart = blockSize * my_rank;\n    int blockEnd = blockSize * (my_rank + 1);\n\n    // Make sure the last process gets the last elements of the array.\n    if (my_rank == nprocs - 1)\n    {\n        blockEnd = size;\n    }\n\n    // Iterate over all elements in the block and increment the corresponding bin.\n    // The bin is calculated using a simple formula.\n    for (int i = blockStart; i < blockEnd; ++i)\n    {\n        int binIndex = int(x[i] / 10);\n        bins[binIndex]++;\n    }\n}",
            "for (int i = 0; i < bins.size(); i++) {\n    bins[i] = 0;\n  }\n  int count = 0;\n  for (auto value : x) {\n    count++;\n    bins[static_cast<int>(value / 10)]++;\n  }\n}",
            "for (auto i = 0; i < x.size(); i++) {\n        double value = x[i];\n        if (value > 0 && value < 10) {\n            bins[0]++;\n        } else if (value > 10 && value < 20) {\n            bins[1]++;\n        } else if (value > 20 && value < 30) {\n            bins[2]++;\n        } else if (value > 30 && value < 40) {\n            bins[3]++;\n        } else if (value > 40 && value < 50) {\n            bins[4]++;\n        } else if (value > 50 && value < 60) {\n            bins[5]++;\n        } else if (value > 60 && value < 70) {\n            bins[6]++;\n        } else if (value > 70 && value < 80) {\n            bins[7]++;\n        } else if (value > 80 && value < 90) {\n            bins[8]++;\n        } else if (value > 90 && value < 100) {\n            bins[9]++;\n        }\n    }\n}",
            "size_t i, j, k, start, stop;\n    size_t num_bins = x.size();\n    size_t counter;\n    bins.fill(0);\n\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for(i=0; i<num_bins; i++) {\n            start = (size_t) (floor(x[i] / 10.0));\n            counter = (size_t) (floor(x[i] / 10.0)) * 10;\n            if(x[i] > 9 && x[i] <= 19) {\n                bins[counter]++;\n            } else if (x[i] > 19 && x[i] <= 29) {\n                bins[counter]++;\n                bins[counter+1]++;\n            } else if (x[i] > 29 && x[i] <= 39) {\n                bins[counter]++;\n                bins[counter+1]++;\n                bins[counter+2]++;\n            } else if (x[i] > 39 && x[i] <= 49) {\n                bins[counter]++;\n                bins[counter+1]++;\n                bins[counter+2]++;\n                bins[counter+3]++;\n            } else if (x[i] > 49 && x[i] <= 59) {\n                bins[counter]++;\n                bins[counter+1]++;\n                bins[counter+2]++;\n                bins[counter+3]++;\n                bins[counter+4]++;\n            } else if (x[i] > 59 && x[i] <= 69) {\n                bins[counter]++;\n                bins[counter+1]++;\n                bins[counter+2]++;\n                bins[counter+3]++;\n                bins[counter+4]++;\n                bins[counter+5]++;\n            } else if (x[i] > 69 && x[i] <= 79) {\n                bins[counter]++;\n                bins[counter+1]++;\n                bins[counter+2]++;\n                bins[counter+3]++;\n                bins[counter+4]++;\n                bins[counter+5]++;\n                bins[counter+6]++;\n            } else if (x[i] > 79 && x[i] <= 89) {\n                bins[counter]++;\n                bins[counter+1]++;\n                bins[counter+2]++;\n                bins[counter+3]++;\n                bins[counter+4]++;\n                bins[counter+5]++;\n                bins[counter+6]++;\n                bins[counter+7]++;\n            } else if (x[i] > 89 && x[i] <= 99) {\n                bins[counter]++;\n                bins[counter+1]++;\n                bins[counter+2]++;\n                bins[counter+3]++;\n                bins[counter+4]++;\n                bins[counter+5]++;\n                bins[counter+6]++;\n                bins[counter+7]++;\n                bins[counter+8]++;\n            } else if (x[i] > 99 && x[i] <= 100) {\n                bins[counter]++;\n                bins[counter+1]++;\n                bins[counter+2]++;\n                bins[counter+3]++;\n                bins[counter+4]++;\n                bins[counter+5]++;\n                bins[counter+6]++;\n                bins[counter+7]++;\n                bins[counter+8]++;\n                bins[counter+9]++;\n            }\n        }\n    }\n}",
            "// Fill in your code here\n    #pragma omp parallel\n    {\n        #pragma omp for nowait\n        for (size_t i = 0; i < x.size(); i++) {\n            size_t idx = (size_t)(x[i] / 10.0);\n            if (idx >= 10) idx = 9;\n            bins[idx]++;\n        }\n    }\n}",
            "// TODO\n}",
            "constexpr double ten = 10;\n#pragma omp parallel for\n    for(size_t i = 0; i < x.size(); ++i) {\n        double val = x[i];\n        // TODO: Fill this in\n    }\n}",
            "std::cout << \"Starting binsBy10Count\\n\";\n#pragma omp parallel for\n    for (int i = 0; i < (int)x.size(); i++) {\n        if (x[i] >= 0 && x[i] < 10) {\n            bins[0] += 1;\n        } else if (x[i] >= 10 && x[i] < 20) {\n            bins[1] += 1;\n        } else if (x[i] >= 20 && x[i] < 30) {\n            bins[2] += 1;\n        } else if (x[i] >= 30 && x[i] < 40) {\n            bins[3] += 1;\n        } else if (x[i] >= 40 && x[i] < 50) {\n            bins[4] += 1;\n        } else if (x[i] >= 50 && x[i] < 60) {\n            bins[5] += 1;\n        } else if (x[i] >= 60 && x[i] < 70) {\n            bins[6] += 1;\n        } else if (x[i] >= 70 && x[i] < 80) {\n            bins[7] += 1;\n        } else if (x[i] >= 80 && x[i] < 90) {\n            bins[8] += 1;\n        } else if (x[i] >= 90 && x[i] < 100) {\n            bins[9] += 1;\n        }\n    }\n    std::cout << \"Finished binsBy10Count\\n\";\n}",
            "size_t n = x.size();\n    bins.fill(0);\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; i++) {\n        bins[(int)x[i]/10]++;\n    }\n}",
            "// TODO\n}",
            "size_t length = x.size();\n  bins.fill(0);\n\n  // OpenMP parallel code\n  #pragma omp parallel for\n  for (int i = 0; i < length; ++i) {\n    double value = x[i];\n    if (value >= 0 && value < 10) {\n      bins[0]++;\n    } else if (value >= 10 && value < 20) {\n      bins[1]++;\n    } else if (value >= 20 && value < 30) {\n      bins[2]++;\n    } else if (value >= 30 && value < 40) {\n      bins[3]++;\n    } else if (value >= 40 && value < 50) {\n      bins[4]++;\n    } else if (value >= 50 && value < 60) {\n      bins[5]++;\n    } else if (value >= 60 && value < 70) {\n      bins[6]++;\n    } else if (value >= 70 && value < 80) {\n      bins[7]++;\n    } else if (value >= 80 && value < 90) {\n      bins[8]++;\n    } else if (value >= 90 && value < 100) {\n      bins[9]++;\n    }\n  }\n\n  // Check your answer\n  ASSERT_EQUAL(bins, std::array<size_t, 10>{1, 2, 0, 3, 0, 0, 1, 2, 0, 1});\n}",
            "#pragma omp parallel for shared(x,bins)\n  for (size_t i = 0; i < x.size(); ++i) {\n    int index = (x[i]+1)/10 - 1;\n    bins[index]++;\n  }\n}",
            "#pragma omp parallel\n{\n#pragma omp for\n    for (int i = 0; i < x.size(); i++) {\n        int bin = (int)x[i]/10;\n        bins[bin]++;\n    }\n}\n\n}",
            "for (int i = 0; i < x.size(); i++) {\n        auto bucket = static_cast<size_t>((x[i])/10.0);\n        if (bucket < 10) {\n            bins[bucket] += 1;\n        }\n    }\n\n    return;\n}",
            "#pragma omp parallel for\n  for (int i=0; i < 10; i++) {\n    bins[i] = 0;\n    for (int j=0; j < x.size(); j++) {\n      if (x[j] < i*10+10 && x[j] >= i*10) {\n        bins[i] += 1;\n      }\n    }\n  }\n}",
            "int nthreads = omp_get_max_threads();\n  int threadid = omp_get_thread_num();\n  size_t bins_per_thread = x.size()/nthreads;\n  size_t offset = bins_per_thread * threadid;\n  size_t num_vals = threadid == nthreads - 1? x.size() - offset : bins_per_thread;\n\n  for(size_t i = 0; i < num_vals; ++i){\n    size_t bin_index = static_cast<size_t>(x[offset + i]/10);\n    ++bins[bin_index];\n  }\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for\n        for (int i = 0; i < (int) x.size(); ++i) {\n            int idx = floor((x[i] - 1) * 10);\n            if (idx < 0) {\n                idx = 0;\n            }\n            #pragma omp atomic\n            bins[idx]++;\n        }\n    }\n}",
            "// fill this in!\n    size_t const num_bins = bins.size();\n    for(int i = 0; i < num_bins; ++i) {\n        bins[i] = 0;\n    }\n    int const size = x.size();\n    #pragma omp parallel for\n    for(int i = 0; i < size; ++i) {\n        int bin = (int)floor(x[i]/10);\n        bins[bin] += 1;\n    }\n    // if using a range-based for-loop, remove the line above and uncomment\n    // the following two lines:\n    // int const size = x.size();\n    // for(double bin : x) {\n        // int const bin = (int)floor(bin/10);\n        // ++bins[bin];\n    // }\n}",
            "bins.fill(0);\n    std::vector<double> x2 = x;\n    //omp_set_num_threads(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        #pragma omp critical\n        {\n            bins.at((x2.at(i) / 10.0)) += 1;\n        }\n    }\n    //std::cout << bins.at(0) << std::endl;\n    //std::cout << bins.at(1) << std::endl;\n    //std::cout << bins.at(2) << std::endl;\n    //std::cout << bins.at(3) << std::endl;\n    //std::cout << bins.at(4) << std::endl;\n    //std::cout << bins.at(5) << std::endl;\n    //std::cout << bins.at(6) << std::endl;\n    //std::cout << bins.at(7) << std::endl;\n    //std::cout << bins.at(8) << std::endl;\n    //std::cout << bins.at(9) << std::endl;\n}",
            "#pragma omp parallel for\n    for(int i = 0; i < x.size(); i++){\n        int bin = int(x[i] / 10);\n        if (bin == 10) bin--;\n        bins[bin]++;\n    }\n}",
            "#pragma omp parallel\n  #pragma omp for\n  for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] >= 0 && x[i] < 10)\n      bins[0]++;\n    if (x[i] >= 10 && x[i] < 20)\n      bins[1]++;\n    if (x[i] >= 20 && x[i] < 30)\n      bins[2]++;\n    if (x[i] >= 30 && x[i] < 40)\n      bins[3]++;\n    if (x[i] >= 40 && x[i] < 50)\n      bins[4]++;\n    if (x[i] >= 50 && x[i] < 60)\n      bins[5]++;\n    if (x[i] >= 60 && x[i] < 70)\n      bins[6]++;\n    if (x[i] >= 70 && x[i] < 80)\n      bins[7]++;\n    if (x[i] >= 80 && x[i] < 90)\n      bins[8]++;\n    if (x[i] >= 90 && x[i] <= 100)\n      bins[9]++;\n  }\n}",
            "for(auto& b : bins)\n        b = 0;\n\n    #pragma omp parallel for\n    for (size_t i=0; i < x.size(); i++) {\n        size_t bin = x[i] / 10;\n        bins[bin]++;\n    }\n}",
            "size_t bins_size = sizeof(bins)/sizeof(size_t);\n    assert(x.size() == bins_size);\n\n    size_t size = x.size();\n\n    //#pragma omp parallel for\n    for (size_t i = 0; i < size; i++) {\n        size_t index = (size_t) (x.at(i) / 10.0);\n        bins[index]++;\n    }\n}",
            "#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tint bin = x.at(i)/10;\n\t\tbins.at(bin)++;\n\t}\n}",
            "std::vector<double> xbins(10);\n    #pragma omp parallel for\n    for(int i = 0; i < 10; i++)\n        xbins[i] = 0;\n    #pragma omp parallel for\n    for(int i = 0; i < x.size(); i++) {\n        int t = (int) (x[i] / 10.0);\n        if(t > 9)\n            t = 9;\n        xbins[t]++;\n    }\n    for(int i = 0; i < 10; i++) {\n        bins[i] = xbins[i];\n    }\n}",
            "omp_set_num_threads(8);\n\n\t#pragma omp parallel\n\t{\n\t\tint thread_id = omp_get_thread_num();\n\t\tint total_num_threads = omp_get_num_threads();\n\n\t\tint i = thread_id * x.size() / total_num_threads;\n\t\tint end = (thread_id + 1) * x.size() / total_num_threads;\n\n\t\tfor (; i < end; ++i) {\n\t\t\tint index = (int)(x[i] / 10.0);\n\t\t\tbins[index] += 1;\n\t\t}\n\t}\n}",
            "// TODO: fill this in\n\n}",
            "std::vector<size_t> counts(10, 0);\n\n    #pragma omp parallel for reduction(+:counts[0:10])\n    for(size_t i = 0; i < x.size(); i++) {\n        size_t bin = static_cast<size_t>(x[i]);\n        bin = bin / 10;\n        bin = bin > 9? 9 : bin;\n        counts[bin] += 1;\n    }\n\n    for(size_t i = 0; i < 10; i++) {\n        bins[i] = counts[i];\n    }\n}",
            "std::vector<double> x_copy(x);\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    bins[static_cast<size_t>(x_copy[i] / 10)]++;\n  }\n}",
            "// This is a trivial implementation. You should replace this with a more\n  // efficient implementation.\n\n  int nThreads = omp_get_max_threads();\n  #pragma omp parallel\n  {\n    int threadID = omp_get_thread_num();\n\n    #pragma omp for\n    for (int i = 0; i < x.size(); i++) {\n      int bucketID = threadID*(x.size() / nThreads) + i;\n      bins[bucketID]++;\n    }\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    int bin = x[i]/10;\n    #pragma omp atomic\n    bins[bin]++;\n  }\n\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 10; i++){\n        for (auto j = i * 10; j < (i + 1) * 10; j++){\n            for (auto k = 0; k < x.size(); k++){\n                if (x[k] >= j && x[k] < j + 10)\n                    bins[i]++;\n            }\n        }\n    }\n}",
            "for (int i = 0; i < 10; ++i) {\n        bins[i] = 0;\n    }\n    int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        int index = static_cast<int>(std::floor(x[i] / 10));\n        bins[index]++;\n    }\n}",
            "// Fill in this function!\n}",
            "// TODO: your code here\n#pragma omp parallel num_threads(x.size())\n    {\n        // TODO: your code here\n        int i = omp_get_thread_num();\n        int tid = omp_get_num_threads();\n        bins[x[i]/10]++;\n\n    }\n\n}",
            "// FIXME: Your code here\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    int a = (int) x[i];\n    bins[a/10] += 1;\n  }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin = static_cast<size_t>(x[i] / 10);\n        bins[bin] += 1;\n    }\n}",
            "std::array<size_t, 10> temp;\n    //std::fill(bins.begin(), bins.end(), 0);\n    #pragma omp parallel for\n    for (int i = 0; i < 10; i++)\n    {\n        size_t j = 0;\n        for (j = i*10; j < i*10+10; j++)\n        {\n            if (j < x.size())\n            {\n                temp[i] += 1;\n            }\n        }\n    }\n    for (int i = 0; i < 10; i++)\n    {\n        bins[i] = temp[i];\n    }\n}",
            "// YOUR CODE HERE\n  size_t bin_min = 0;\n  size_t bin_max = 10;\n\n  for (int i = 0; i < x.size(); i++) {\n    if(x[i] >= bin_min && x[i] < bin_max) {\n      bins[0] += 1;\n    }\n    else if(x[i] >= bin_max && x[i] < bin_min + 10) {\n      bins[1] += 1;\n    }\n    else if(x[i] >= bin_max + 10 && x[i] < bin_min + 20) {\n      bins[2] += 1;\n    }\n    else if(x[i] >= bin_max + 20 && x[i] < bin_min + 30) {\n      bins[3] += 1;\n    }\n    else if(x[i] >= bin_max + 30 && x[i] < bin_min + 40) {\n      bins[4] += 1;\n    }\n    else if(x[i] >= bin_max + 40 && x[i] < bin_min + 50) {\n      bins[5] += 1;\n    }\n    else if(x[i] >= bin_max + 50 && x[i] < bin_min + 60) {\n      bins[6] += 1;\n    }\n    else if(x[i] >= bin_max + 60 && x[i] < bin_min + 70) {\n      bins[7] += 1;\n    }\n    else if(x[i] >= bin_max + 70 && x[i] < bin_min + 80) {\n      bins[8] += 1;\n    }\n    else if(x[i] >= bin_max + 80 && x[i] < bin_min + 90) {\n      bins[9] += 1;\n    }\n\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        int j = x[i]/10;\n        if (j >= 0 && j < 10)\n            bins[j]++;\n    }\n}",
            "#pragma omp parallel for\n\n    for (int i = 0; i < x.size(); i++) {\n\n        // Checks if the value is between 0 and 10 and adds 1 to the correct\n        // position in the array.\n        if (x[i] >= 0 && x[i] < 10) {\n            #pragma omp atomic\n            bins[0]++;\n        }\n\n        // Checks if the value is between 10 and 20 and adds 1 to the correct\n        // position in the array.\n        else if (x[i] >= 10 && x[i] < 20) {\n            #pragma omp atomic\n            bins[1]++;\n        }\n\n        // Checks if the value is between 20 and 30 and adds 1 to the correct\n        // position in the array.\n        else if (x[i] >= 20 && x[i] < 30) {\n            #pragma omp atomic\n            bins[2]++;\n        }\n\n        // Checks if the value is between 30 and 40 and adds 1 to the correct\n        // position in the array.\n        else if (x[i] >= 30 && x[i] < 40) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n\n        // Checks if the value is between 40 and 50 and adds 1 to the correct\n        // position in the array.\n        else if (x[i] >= 40 && x[i] < 50) {\n            #pragma omp atomic\n            bins[4]++;\n        }\n\n        // Checks if the value is between 50 and 60 and adds 1 to the correct\n        // position in the array.\n        else if (x[i] >= 50 && x[i] < 60) {\n            #pragma omp atomic\n            bins[5]++;\n        }\n\n        // Checks if the value is between 60 and 70 and adds 1 to the correct\n        // position in the array.\n        else if (x[i] >= 60 && x[i] < 70) {\n            #pragma omp atomic\n            bins[6]++;\n        }\n\n        // Checks if the value is between 70 and 80 and adds 1 to the correct\n        // position in the array.\n        else if (x[i] >= 70 && x[i] < 80) {\n            #pragma omp atomic\n            bins[7]++;\n        }\n\n        // Checks if the value is between 80 and 90 and adds 1 to the correct\n        // position in the array.\n        else if (x[i] >= 80 && x[i] < 90) {\n            #pragma omp atomic\n            bins[8]++;\n        }\n\n        // Checks if the value is between 90 and 100 and adds 1 to the correct\n        // position in the array.\n        else if (x[i] >= 90 && x[i] < 100) {\n            #pragma omp atomic\n            bins[9]++;\n        }\n\n        // Checks if the value is higher than 100.\n        else {\n            std::cout << \"Out of bounds\" << std::endl;\n        }\n\n    }\n}",
            "for (auto &elem : bins) {\n        elem = 0;\n    }\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        auto val = x[i];\n        int idx = static_cast<int> (floor(val / 10.0));\n        bins[idx]++;\n    }\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "size_t x_size = x.size();\n  #pragma omp parallel for\n  for(int i=0; i<x_size; i++) {\n    int bin = std::floor((double)x[i] / 10);\n    if (bin < 0) {\n      bins[0]++;\n    } else if (bin > 9) {\n      bins[9]++;\n    } else {\n      bins[bin]++;\n    }\n  }\n}",
            "for (size_t i = 0; i < 10; ++i) {\n    bins[i] = 0;\n  }\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    size_t bin = (size_t)(x[i] / 10);\n    bins[bin]++;\n  }\n}",
            "#pragma omp parallel for schedule(static, 1)\n  for (int i = 0; i < x.size(); ++i) {\n    double val = x[i];\n    int bin_id = (val / 10) * 10;\n    ++bins[bin_id];\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    int bin = static_cast<int>(x[i] / 10);\n    bins[bin]++;\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    int num_threads = omp_get_max_threads();\n    int chunk_size = x.size() / num_threads;\n    int remainder = x.size() % num_threads;\n    #pragma omp parallel\n    {\n        int id = omp_get_thread_num();\n        int start = id * chunk_size;\n        int end = start + chunk_size;\n        if(id == num_threads - 1) end += remainder;\n        for(int i = start; i < end; i++) {\n            int xi = (int)(x[i]);\n            int bin = xi / 10;\n            #pragma omp atomic\n            bins[bin]++;\n        }\n    }\n}",
            "int n = x.size();\n    omp_set_num_threads(4);\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        bins[int(x[i]/10)] += 1;\n    }\n}",
            "size_t bin = 0;\n\n#pragma omp parallel\n  {\n    std::array<size_t, 10> private_bins;\n#pragma omp for\n    for(size_t i = 0; i < x.size(); ++i) {\n      bin = static_cast<size_t>(x[i] / 10.0);\n      ++private_bins[bin];\n    }\n\n#pragma omp critical\n    for(size_t i = 0; i < 10; ++i) {\n      bins[i] += private_bins[i];\n    }\n  }\n\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n    int index = (x.at(i) / 10);\n    if (index >= 0 && index < 10) {\n      bins.at(index)++;\n    }\n  }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    auto bin = (int)(x[i]/10);\n    #pragma omp atomic\n    bins[bin]++;\n  }\n}",
            "size_t n = x.size();\n\n#pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    int j = floor((x[i] - 0) / 10) + 1;\n    bins[j]++;\n  }\n}",
            "size_t count = 0;\n    for (size_t i = 0; i < x.size(); i++) {\n        count += x[i];\n        if (count > 10) {\n            if (x[i] == 10) {\n                bins[i] = 9;\n            } else {\n                bins[i] = x[i];\n            }\n            count -= 10;\n        }\n    }\n}",
            "int nt = omp_get_max_threads();\n    int tid = omp_get_thread_num();\n    size_t sum = 0;\n\n    for (int i = 0; i < 10; ++i) {\n        if(tid == 0) {\n            sum += bins[i];\n        }\n    }\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        int bin = (int)(x[i] / 10);\n\n        if(tid == 0) {\n            #pragma omp atomic\n            bins[bin] += 1;\n        }\n\n    }\n\n    if(tid == 0) {\n        size_t res = 0;\n        for (int i = 0; i < 10; ++i) {\n            #pragma omp atomic\n            res += bins[i];\n        }\n        assert(res == sum);\n    }\n\n}",
            "// TODO: Implement this function!\n  // Fill in the values of bins based on the input x.\n  // This function should be thread-safe.\n  // This function must be parallelizable.\n\n  #pragma omp parallel for\n  for (auto i = 0; i < 10; i++) {\n    bins[i] = 0;\n  }\n\n  #pragma omp parallel for\n  for (auto i = 0; i < x.size(); i++) {\n    for (auto j = 0; j < 10; j++) {\n      if (x[i] > 10 * j && x[i] <= 10 * j + 10) {\n        bins[j]++;\n      }\n    }\n  }\n}",
            "// TODO: implement me!\n\n  #pragma omp parallel for\n  for(int i=0; i<x.size(); i++){\n      int n=static_cast<int> (x[i]/10);\n      #pragma omp atomic\n      bins[n]++;\n  }\n}",
            "size_t nThreads = omp_get_num_threads();\n    std::vector<size_t> threadBins[nThreads];\n    for (int i = 0; i < nThreads; i++)\n        threadBins[i].resize(10);\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        size_t bin = x[i] / 10;\n        threadBins[omp_get_thread_num()][bin]++;\n    }\n    for (int i = 0; i < nThreads; i++)\n        for (int j = 0; j < 10; j++)\n            bins[j] += threadBins[i][j];\n}",
            "#pragma omp parallel for num_threads(10)\n  for (size_t i = 0; i < x.size(); ++i)\n  {\n    int id = int(x[i]/10.0);\n    // std::cout<<\"id is \"<<id<<std::endl;\n    if (id >=0 && id < 10) bins[id]++;\n  }\n}",
            "#pragma omp parallel for\n    for(size_t i = 0; i < x.size(); i++) {\n        int n = (x[i]/10) + 1;\n        bins[n]++;\n    }\n}",
            "#pragma omp parallel for\n  for (auto i = 0; i < 10; i++)\n    bins[i] = 0;\n\n  #pragma omp parallel for reduction(+:bins)\n  for (auto i = 0; i < x.size(); i++) {\n    int bin = x[i]/10;\n    if (bin >= 0 && bin < 10)\n      bins[bin]++;\n  }\n}",
            "bins = std::array<size_t, 10>{};\n    int thread_count = 1;\n    if (x.size() > 100) {\n        thread_count = x.size() / 100;\n    }\n\n    #pragma omp parallel for num_threads(thread_count)\n    for (int i = 0; i < x.size(); ++i) {\n        if (i % thread_count == 0) {\n            bins[x[i]/10]++;\n        }\n    }\n}",
            "for(auto &bin : bins) {\n    bin = 0;\n  }\n\n  for(size_t i = 0; i < x.size(); ++i) {\n    int bin = int(x[i] / 10.0);\n    if(bin >= 0 && bin < 10) {\n      bins[bin]++;\n    }\n  }\n}",
            "std::vector<size_t> x_bin(x.size());\n    size_t bin_size = 10;\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++){\n        x_bin[i] = x[i]/bin_size;\n    }\n    bins.fill(0);\n    #pragma omp parallel for\n    for (size_t i = 0; i < x_bin.size(); i++){\n        bins[x_bin[i]] += 1;\n    }\n}",
            "for (auto &bin : bins) {\n    bin = 0;\n  }\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double value = x[i];\n    if (value >= 0.0 && value < 10.0) {\n      ++bins[0];\n    } else if (value >= 10.0 && value < 20.0) {\n      ++bins[1];\n    } else if (value >= 20.0 && value < 30.0) {\n      ++bins[2];\n    } else if (value >= 30.0 && value < 40.0) {\n      ++bins[3];\n    } else if (value >= 40.0 && value < 50.0) {\n      ++bins[4];\n    } else if (value >= 50.0 && value < 60.0) {\n      ++bins[5];\n    } else if (value >= 60.0 && value < 70.0) {\n      ++bins[6];\n    } else if (value >= 70.0 && value < 80.0) {\n      ++bins[7];\n    } else if (value >= 80.0 && value < 90.0) {\n      ++bins[8];\n    } else if (value >= 90.0 && value <= 100.0) {\n      ++bins[9];\n    }\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  //#pragma omp parallel\n  //#pragma omp for\n  for (size_t i = 0; i < x.size(); i++) {\n    int idx = std::floor(x[i]/10);\n    bins[idx]++;\n  }\n}",
            "size_t const num = x.size();\n\n  #pragma omp parallel for schedule(static, num/10)\n  for (size_t i = 0; i < num; ++i) {\n\n    size_t const index = x[i]/10;\n    ++bins[index];\n  }\n\n  return;\n}",
            "// TODO: Your code here\n\n  // create omp parallel section\n  // create omp for section\n  // create omp single section\n\n  size_t bins_size = sizeof(bins)/sizeof(size_t);\n\n  // for loop to go through each element of x\n  // if statement to check if element of x is in a certain range\n  // add 1 to the element in bins\n\n\n  // parallel section for loop through each element of x\n  // for loop through each element of x\n  // if statement to check if element of x is in a certain range\n  // add 1 to the element in bins\n}",
            "// TODO\n    #pragma omp parallel num_threads(8)\n    {\n        int id = omp_get_thread_num();\n        if (id == 0)\n            for (size_t i = 0; i < 10; i++)\n                bins[i] = 0;\n        int start = id * 10;\n        int end = start + 9;\n        for (size_t i = 0; i < x.size(); i++)\n            if (x[i] >= start && x[i] <= end)\n                bins[x[i]-start]++;\n    }\n}",
            "// TODO: Your code here\n  for (size_t i = 0; i < x.size(); i++) {\n    bins[static_cast<size_t>(x[i] / 10)]++;\n  }\n}",
            "size_t n = x.size();\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; ++i) {\n        //std::cout << \"Thread id: \" << omp_get_thread_num() << std::endl;\n        size_t value = static_cast<size_t>(x[i]);\n        if(value < 10) {\n            bins[0]++;\n        }\n        else if(value < 20) {\n            bins[1]++;\n        }\n        else if(value < 30) {\n            bins[2]++;\n        }\n        else if(value < 40) {\n            bins[3]++;\n        }\n        else if(value < 50) {\n            bins[4]++;\n        }\n        else if(value < 60) {\n            bins[5]++;\n        }\n        else if(value < 70) {\n            bins[6]++;\n        }\n        else if(value < 80) {\n            bins[7]++;\n        }\n        else if(value < 90) {\n            bins[8]++;\n        }\n        else {\n            bins[9]++;\n        }\n    }\n\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    int val = int(10.0 * x[i]);\n    bins[val]++;\n  }\n\n  for (size_t i = 0; i < 10; i++) {\n    std::cout << bins[i] << \" \";\n  }\n  std::cout << std::endl;\n}",
            "#pragma omp parallel\n    {\n        size_t local_count = 0;\n        #pragma omp for\n        for (size_t i = 0; i < x.size(); ++i) {\n            size_t bin_num = x[i] / 10;\n            if (bin_num < 10) {\n                ++local_count;\n            }\n            ++bins[bin_num];\n        }\n        #pragma omp critical\n        {\n            bins[0] += local_count;\n        }\n    }\n}",
            "bins = std::array<size_t, 10>{};\n  #pragma omp parallel for\n  for(size_t i = 0; i < x.size(); i++) {\n    bins.at(static_cast<size_t>(x[i]/10))++;\n  }\n}",
            "#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tdouble bin = floor(x[i]/10);\n\t\tbins[bin]++;\n\t}\n}",
            "}",
            "// TODO: Your code goes here\n\n}",
            "assert(x.size() > 0);\n  assert(x.size() <= bins.size() * 10);\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    size_t idx = static_cast<size_t>(x[i]/10);\n    idx = std::min(idx, bins.size() - 1);\n    #pragma omp atomic\n    bins[idx]++;\n  }\n}",
            "// compute the number of elements in [0, 10), [10, 20), etc.\n    for(int i = 0; i < x.size(); i++) {\n        int bin = int(std::floor(x[i] / 10.0)) * 10;\n        bins[bin / 10]++;\n    }\n}",
            "// TODO: Your code here\n  \n}",
            "bins.fill(0);\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i = 0; i < x.size(); i++)\n            bins[((size_t) x[i]/10)%10]++;\n    }\n}",
            "// Your code here.\n\t#pragma omp parallel\n\t{\n\t\tint thread_id = omp_get_thread_num();\n\t\tint num_of_threads = omp_get_num_threads();\n\t\tsize_t start = x.size()/num_of_threads*thread_id;\n\t\tsize_t end = x.size()/num_of_threads*(thread_id+1);\n\t\tfor(int i=start; i<end; i++){\n\t\t\tif(x[i]<10){\n\t\t\t\tbins[0]++;\n\t\t\t}\n\t\t\telse if(x[i]>=10 && x[i]<20){\n\t\t\t\tbins[1]++;\n\t\t\t}\n\t\t\telse if(x[i]>=20 && x[i]<30){\n\t\t\t\tbins[2]++;\n\t\t\t}\n\t\t\telse if(x[i]>=30 && x[i]<40){\n\t\t\t\tbins[3]++;\n\t\t\t}\n\t\t\telse if(x[i]>=40 && x[i]<50){\n\t\t\t\tbins[4]++;\n\t\t\t}\n\t\t\telse if(x[i]>=50 && x[i]<60){\n\t\t\t\tbins[5]++;\n\t\t\t}\n\t\t\telse if(x[i]>=60 && x[i]<70){\n\t\t\t\tbins[6]++;\n\t\t\t}\n\t\t\telse if(x[i]>=70 && x[i]<80){\n\t\t\t\tbins[7]++;\n\t\t\t}\n\t\t\telse if(x[i]>=80 && x[i]<90){\n\t\t\t\tbins[8]++;\n\t\t\t}\n\t\t\telse{\n\t\t\t\tbins[9]++;\n\t\t\t}\n\t\t}\n\t}\n}",
            "std::array<size_t, 10> counts{};\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    int bin = static_cast<int>((x[i] + 9)/10);\n    counts[bin]++;\n  }\n\n  bins = counts;\n}",
            "int i;\n#pragma omp parallel for private(i) shared(x,bins)\n    for (i=0; i<x.size(); i++)\n    {\n        if ((x[i] >= 0) && (x[i] < 10))\n        {\n            bins[0]++;\n        }\n        else if ((x[i] >= 10) && (x[i] < 20))\n        {\n            bins[1]++;\n        }\n        else if ((x[i] >= 20) && (x[i] < 30))\n        {\n            bins[2]++;\n        }\n        else if ((x[i] >= 30) && (x[i] < 40))\n        {\n            bins[3]++;\n        }\n        else if ((x[i] >= 40) && (x[i] < 50))\n        {\n            bins[4]++;\n        }\n        else if ((x[i] >= 50) && (x[i] < 60))\n        {\n            bins[5]++;\n        }\n        else if ((x[i] >= 60) && (x[i] < 70))\n        {\n            bins[6]++;\n        }\n        else if ((x[i] >= 70) && (x[i] < 80))\n        {\n            bins[7]++;\n        }\n        else if ((x[i] >= 80) && (x[i] < 90))\n        {\n            bins[8]++;\n        }\n        else\n        {\n            bins[9]++;\n        }\n    }\n}",
            "bins.fill(0);\n#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); ++i)\n\t{\n\t\tsize_t bin = int(x[i] / 10);\n\t\tif (bin < 10)\n\t\t\tbins[bin]++;\n\t}\n}",
            "// Your code here.\n  for (int i = 0; i < 10; i++) {\n    bins[i] = 0;\n  }\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    int b = (x[i] / 10);\n    bins[b]++;\n  }\n}",
            "// Your code here\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for\n        for (int i = 0; i < 10; i++)\n            bins[i] = 0;\n    }\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++)\n    {\n        int index = x[i] / 10;\n        if (index < 10)\n            bins[index]++;\n    }\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n    size_t val = x[i] / 10;\n    bins[val]++;\n  }\n}",
            "// TODO: fill this in!\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    size_t index = (x[i]/10);\n    bins[index]++;\n  }\n}",
            "// TODO: Your code here\n    int chunk_size = x.size()/omp_get_max_threads();\n#pragma omp parallel for\n    for(int i = 0; i < omp_get_max_threads(); i++){\n        size_t cnt = 0;\n        int start = i*chunk_size;\n        int end = start + chunk_size;\n        if(end > x.size()){\n            end = x.size();\n        }\n        for(int j = start; j < end; j++){\n            if(x[j] >= j*10 && x[j] < j*10 + 10){\n                cnt++;\n            }\n        }\n        bins[i] = cnt;\n    }\n}",
            "size_t bins_size = sizeof(bins)/sizeof(bins[0]);\n    for(int i=0; i<bins_size; i++)\n        bins[i] = 0;\n\n    #pragma omp parallel for\n    for(int i=0; i<(int)x.size(); i++)\n        bins[int(x[i]/10)]++;\n}",
            "// implement this function using OpenMP\n}",
            "bins.fill(0);\n    const double start = 0.0;\n    const double stop = 10.0;\n    const int n_threads = 4;\n    std::vector<double> x_sorted;\n    x_sorted = x;\n    std::sort(x_sorted.begin(), x_sorted.end());\n\n    int n = (int)x_sorted.size();\n\n    if (n == 0) {\n        bins.fill(0);\n    }\n\n    else {\n        int chunk_size = n / n_threads;\n        if (n % n_threads!= 0) chunk_size++;\n\n        #pragma omp parallel num_threads(n_threads)\n        {\n            int id = omp_get_thread_num();\n            int start_index = id * chunk_size;\n            int end_index = start_index + chunk_size;\n\n            if (id == n_threads - 1) {\n                end_index = n;\n            }\n\n            for (int i = start_index; i < end_index; ++i) {\n\n                double val = x_sorted[i];\n                if (val >= start && val < stop) {\n                    int bin = (int)((val - start) / 10);\n                    bins[bin]++;\n                }\n\n            }\n        }\n    }\n\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t index = (x[i] / 10.0) + 1;\n        if (index < 10) {\n            #pragma omp atomic\n            bins[index]++;\n        }\n    }\n}",
            "// TODO: implement\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i = 0; i < x.size(); i++)\n        {\n            int index = (int)((x[i]/10)*10);\n            if(index <= 9)\n            {\n                bins[index]++;\n            }\n        }\n    }\n}",
            "}",
            "// Hint: You can use bins.fill(0) to set all bins to 0 initially.\n\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        int nth = omp_get_num_threads();\n\n        #pragma omp for\n        for (size_t i = 0; i < x.size(); ++i) {\n            bins[int(x[i]/10.0*nth+tid)] += 1;\n        }\n    }\n\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        bins.at(static_cast<size_t>(x.at(i) / 10.))++;\n    }\n}",
            "#pragma omp parallel for\n    for (int i=0; i<10; ++i)\n    {\n        for (int j=0; j<x.size(); j++)\n            if (x[j] >= i*10 && x[j] < (i+1)*10)\n                bins[i]++;\n    }\n}",
            "#pragma omp parallel\n  {\n    #pragma omp for\n    for (size_t i = 0; i < x.size(); i++) {\n      int value = x[i];\n      value /= 10;\n      value *= 10;\n      if (value >= 0 && value < 10) {\n        #pragma omp atomic\n        bins[value] += 1;\n      }\n    }\n  }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double v = (x[i] - 1) / 10;\n    bins[(int)v]++;\n  }\n}",
            "const size_t N = x.size();\n    // Your code here\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        const double val = x.at(i);\n        const size_t j = val / 10.0;\n        const size_t index = j - 1;\n        #pragma omp atomic\n        bins[index]++;\n    }\n}",
            "size_t nthreads = omp_get_max_threads();\n  size_t nbin = 10;\n  size_t bin_size = x.size() / nbin;\n  std::vector<std::array<size_t, 10> > bins_thread(nthreads);\n\n  #pragma omp parallel\n  {\n    size_t ithread = omp_get_thread_num();\n    size_t istart = ithread * bin_size;\n    size_t istop = std::min(istart + bin_size, x.size());\n    std::array<size_t, 10> &bin = bins_thread[ithread];\n\n    #pragma omp for\n    for (size_t i = istart; i < istop; ++i) {\n      int bin_val = int(x[i] / 10.);\n      bin[bin_val]++;\n    }\n  }\n\n  bins = bins_thread[0];\n  for (int i = 1; i < nthreads; ++i) {\n    for (int j = 0; j < 10; ++j) {\n      bins[j] += bins_thread[i][j];\n    }\n  }\n}",
            "// Fill in code here.\n\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < 10; i++)\n  {\n    bins[i] = 0;\n  }\n\n  for (int i = 0; i < x.size(); i++)\n  {\n    int bin = x[i] / 10;\n    bins[bin]++;\n  }\n}",
            "for (int i=0; i<bins.size(); ++i) {\n        bins[i] = 0;\n    }\n\n    int thread_count = omp_get_max_threads();\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        int bin = (int)(x[i]/10);\n        bins[bin]++;\n    }\n}",
            "for (auto i = 0; i < 10; ++i) bins[i] = 0;\n  #pragma omp parallel for\n  for (auto i = 0; i < x.size(); ++i) {\n    auto val = x[i];\n    if (val < 10) {\n      ++bins[0];\n    } else if (val < 20) {\n      ++bins[1];\n    } else if (val < 30) {\n      ++bins[2];\n    } else if (val < 40) {\n      ++bins[3];\n    } else if (val < 50) {\n      ++bins[4];\n    } else if (val < 60) {\n      ++bins[5];\n    } else if (val < 70) {\n      ++bins[6];\n    } else if (val < 80) {\n      ++bins[7];\n    } else if (val < 90) {\n      ++bins[8];\n    } else if (val < 100) {\n      ++bins[9];\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (int i=0; i<(int)x.size(); i++) {\n        int index = (int)(x[i]/10);\n        if (index < 10) {\n            bins[index]++;\n        }\n    }\n}",
            "// Write your solution here\n\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "serial",
        "prompt": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "size_t k = 0;\n  for (auto const& a : x) {\n    if (a < 10) {\n      bins[k++]++;\n    }\n  }\n}",
            "size_t n = x.size();\n\tstd::array<size_t, 10> counts{};\n\n\tfor (size_t i = 0; i < n; ++i) {\n\t\tif (x[i] >= 0 && x[i] <= 10) counts[0]++;\n\t\tif (x[i] > 10 && x[i] <= 20) counts[1]++;\n\t\tif (x[i] > 20 && x[i] <= 30) counts[2]++;\n\t\tif (x[i] > 30 && x[i] <= 40) counts[3]++;\n\t\tif (x[i] > 40 && x[i] <= 50) counts[4]++;\n\t\tif (x[i] > 50 && x[i] <= 60) counts[5]++;\n\t\tif (x[i] > 60 && x[i] <= 70) counts[6]++;\n\t\tif (x[i] > 70 && x[i] <= 80) counts[7]++;\n\t\tif (x[i] > 80 && x[i] <= 90) counts[8]++;\n\t\tif (x[i] > 90 && x[i] <= 100) counts[9]++;\n\t}\n\n\tbins = counts;\n\n}",
            "// TODO: Your code here\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n    bins[static_cast<size_t>(x[i] / 10.0)] += 1;\n  }\n}",
            "bins.fill(0);\n\tfor (auto val : x) {\n\t\tauto bucket = floor(val / 10.0);\n\t\tif (bucket < 0.0 || bucket > 9.0) {\n\t\t\tthrow std::runtime_error(\"binsBy10Count: bad value\");\n\t\t}\n\t\tbins[bucket]++;\n\t}\n}",
            "for(auto value : x) {\n        size_t bin = (size_t) value / 10;\n        bins[bin]++;\n    }\n}",
            "bins.fill(0);\n    int index = 0;\n    for (auto i : x) {\n        index = static_cast<int>((i / 10) * 10);\n        if (index >= 0 && index < 10) {\n            ++bins[index];\n        }\n    }\n}",
            "bins.fill(0);\n  for (auto xi : x) {\n    bins[(int)xi / 10] += 1;\n  }\n}",
            "bins.fill(0);\n\tfor (auto &v : x) {\n\t\tint bin = std::floor((v - 1) / 10);\n\t\tif (bin < 0) bin = 0;\n\t\tif (bin > 9) bin = 9;\n\t\tbins[bin]++;\n\t}\n}",
            "int const n = 10;\n  for(size_t i = 0; i < n; ++i)\n    bins[i] = 0;\n\n  for(auto const& value : x) {\n    int index = value / 10;\n    if(index >= n)\n      index = n-1;\n    ++bins[index];\n  }\n}",
            "// TODO: Your code goes here\n}",
            "// TODO\n}",
            "for (double value : x) {\n        bins[int(value / 10)] += 1;\n    }\n}",
            "for(int i = 0; i < x.size(); i++) {\n\n\t\tif(x[i] < 10.) {\n\t\t\tbins[0]++;\n\t\t} else if(x[i] < 20.) {\n\t\t\tbins[1]++;\n\t\t} else if(x[i] < 30.) {\n\t\t\tbins[2]++;\n\t\t} else if(x[i] < 40.) {\n\t\t\tbins[3]++;\n\t\t} else if(x[i] < 50.) {\n\t\t\tbins[4]++;\n\t\t} else if(x[i] < 60.) {\n\t\t\tbins[5]++;\n\t\t} else if(x[i] < 70.) {\n\t\t\tbins[6]++;\n\t\t} else if(x[i] < 80.) {\n\t\t\tbins[7]++;\n\t\t} else if(x[i] < 90.) {\n\t\t\tbins[8]++;\n\t\t} else if(x[i] < 100.) {\n\t\t\tbins[9]++;\n\t\t} else {\n\t\t\tthrow std::invalid_argument(\"Value not within range.\");\n\t\t}\n\t}\n}",
            "for (auto val : x) {\n        bins[static_cast<size_t>((val / 10)) % 10]++;\n    }\n}",
            "bins.fill(0);\n    for (auto xi : x) {\n        if (xi < 10.) bins[0]++;\n        else if (xi >= 10. && xi < 20.) bins[1]++;\n        else if (xi >= 20. && xi < 30.) bins[2]++;\n        else if (xi >= 30. && xi < 40.) bins[3]++;\n        else if (xi >= 40. && xi < 50.) bins[4]++;\n        else if (xi >= 50. && xi < 60.) bins[5]++;\n        else if (xi >= 60. && xi < 70.) bins[6]++;\n        else if (xi >= 70. && xi < 80.) bins[7]++;\n        else if (xi >= 80. && xi < 90.) bins[8]++;\n        else bins[9]++;\n    }\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n    double val = x[i];\n    if (val < 10) {\n      bins[0] += 1;\n    }\n    else if (val < 20) {\n      bins[1] += 1;\n    }\n    else if (val < 30) {\n      bins[2] += 1;\n    }\n    else if (val < 40) {\n      bins[3] += 1;\n    }\n    else if (val < 50) {\n      bins[4] += 1;\n    }\n    else if (val < 60) {\n      bins[5] += 1;\n    }\n    else if (val < 70) {\n      bins[6] += 1;\n    }\n    else if (val < 80) {\n      bins[7] += 1;\n    }\n    else if (val < 90) {\n      bins[8] += 1;\n    }\n    else {\n      bins[9] += 1;\n    }\n  }\n}",
            "bins[0] = 0; bins[1] = 0; bins[2] = 0; bins[3] = 0;\n    bins[4] = 0; bins[5] = 0; bins[6] = 0; bins[7] = 0;\n    bins[8] = 0; bins[9] = 0;\n\n    for(double value : x) {\n        if(value <= 10.0) bins[0]++;\n        else if(value <= 20.0) bins[1]++;\n        else if(value <= 30.0) bins[2]++;\n        else if(value <= 40.0) bins[3]++;\n        else if(value <= 50.0) bins[4]++;\n        else if(value <= 60.0) bins[5]++;\n        else if(value <= 70.0) bins[6]++;\n        else if(value <= 80.0) bins[7]++;\n        else if(value <= 90.0) bins[8]++;\n        else bins[9]++;\n    }\n}",
            "bins.fill(0);\n    for (auto x_val : x) {\n        auto rounded = std::round(x_val);\n        if (rounded < 10 || rounded > 100) continue;\n\n        auto digit = rounded % 10;\n        bins[digit]++;\n    }\n}",
            "assert(bins.size() == 10);\n   std::fill(bins.begin(), bins.end(), 0);\n\n   for (auto const& v : x) {\n      auto const i = int(std::floor(v / 10)) * 10;\n      if (i < 0 || i > 100) {\n         std::cerr << \"Invalid value: \" << v << std::endl;\n         continue;\n      }\n      bins[i/10]++;\n   }\n}",
            "for(size_t i = 0; i < x.size(); ++i) {\n    size_t index = static_cast<size_t>(x[i]/10);\n    if(index >= 10) {\n      index = 9;\n    }\n    bins[index] += 1;\n  }\n}",
            "// TODO: Your code here\n}",
            "std::sort(x.begin(), x.end());\n\n    auto maxX = x.back();\n    auto minX = x.front();\n    double stepSize = (maxX - minX) / 10;\n\n    for (size_t i = 0; i < 10; i++) {\n        bins[i] = 0;\n    }\n\n    auto start = minX;\n    for (auto i : x) {\n        auto bin = i / stepSize;\n        auto binFloor = std::floor(bin);\n        auto binCeiling = std::ceil(bin);\n\n        // TODO: Figure out why this works\n        if (i < binCeiling * stepSize) {\n            bins[binFloor]++;\n        } else if (i >= binCeiling * stepSize) {\n            bins[binCeiling]++;\n        }\n\n    }\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n        // TODO\n    }\n}",
            "bins.fill(0);\n    for (double val : x) {\n        bins[static_cast<int>(std::floor(val/10.0)) % 10]++;\n    }\n}",
            "// TODO\n}",
            "std::array<size_t, 10> temp = {0,0,0,0,0,0,0,0,0,0};\n  for (auto v: x) {\n    if (v >= 0 && v <= 10) {\n      temp[0] += 1;\n    } else if (v >= 10 && v <= 20) {\n      temp[1] += 1;\n    } else if (v >= 20 && v <= 30) {\n      temp[2] += 1;\n    } else if (v >= 30 && v <= 40) {\n      temp[3] += 1;\n    } else if (v >= 40 && v <= 50) {\n      temp[4] += 1;\n    } else if (v >= 50 && v <= 60) {\n      temp[5] += 1;\n    } else if (v >= 60 && v <= 70) {\n      temp[6] += 1;\n    } else if (v >= 70 && v <= 80) {\n      temp[7] += 1;\n    } else if (v >= 80 && v <= 90) {\n      temp[8] += 1;\n    } else if (v >= 90 && v <= 100) {\n      temp[9] += 1;\n    }\n  }\n  bins = temp;\n}",
            "for (auto const& val : x) {\n        // cast to integer so it will work for float and double\n        int index = static_cast<int>(val / 10.0);\n        if (index < 0) {\n            index = 0;\n        } else if (index > 9) {\n            index = 9;\n        }\n        ++bins[index];\n    }\n}",
            "// Fill in the bins:\n  bins.fill(0);\n  for (auto i : x) {\n    auto b = i / 10.0;\n    bins[b]++;\n  }\n}",
            "assert(x.size() > 0);\n  assert(bins.size() == 10);\n\n  for (auto const& v : x) {\n    size_t idx = std::floor(v/10.0);\n    if (idx < 10) {\n      bins[idx]++;\n    }\n  }\n}",
            "for (auto i = 0; i < x.size(); i++) {\n      if (x[i] >= 0 && x[i] < 10) {\n         bins[0]++;\n      } else if (x[i] >= 10 && x[i] < 20) {\n         bins[1]++;\n      } else if (x[i] >= 20 && x[i] < 30) {\n         bins[2]++;\n      } else if (x[i] >= 30 && x[i] < 40) {\n         bins[3]++;\n      } else if (x[i] >= 40 && x[i] < 50) {\n         bins[4]++;\n      } else if (x[i] >= 50 && x[i] < 60) {\n         bins[5]++;\n      } else if (x[i] >= 60 && x[i] < 70) {\n         bins[6]++;\n      } else if (x[i] >= 70 && x[i] < 80) {\n         bins[7]++;\n      } else if (x[i] >= 80 && x[i] < 90) {\n         bins[8]++;\n      } else if (x[i] >= 90 && x[i] < 100) {\n         bins[9]++;\n      }\n   }\n}",
            "// implement this function\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (double value : x) {\n    size_t index = static_cast<size_t>(value / 10);\n    if (index >= 10) {\n      index = 9;\n    }\n    ++bins[index];\n  }\n}",
            "// start by initializing all the bins to zero\n    for (auto& bin : bins) {\n        bin = 0;\n    }\n\n    // for each value in x, do:\n    for (double const& value : x) {\n        // figure out which bin the value belongs in\n        // (hint: use floor)\n        size_t bin = floor(value / 10.0);\n        // increment the count in that bin\n        bins[bin] += 1;\n    }\n}",
            "bins.fill(0);\n  for (auto xi: x) {\n    // Checks for xi >= 0\n    if (xi >= 0) {\n      // xi = (int) (10.0 * xi);\n      auto xi10 = (int) (10.0 * xi);\n      ++bins[xi10];\n    }\n  }\n}",
            "std::array<size_t, 10> counts;\n\n    std::transform(x.begin(), x.end(), counts.begin(), [](double x){\n        if(x < 0) return 0;\n        if(x < 10) return 0;\n        if(x < 20) return 1;\n        if(x < 30) return 2;\n        if(x < 40) return 3;\n        if(x < 50) return 4;\n        if(x < 60) return 5;\n        if(x < 70) return 6;\n        if(x < 80) return 7;\n        if(x < 90) return 8;\n        if(x < 100) return 9;\n        return 10;\n    });\n\n    for(size_t i = 0; i < counts.size(); ++i)\n        bins[i] += counts[i];\n}",
            "int bin = 0;\n  for (auto &val : x) {\n    if (val >= 0 && val < 10) {\n      bin = 0;\n    } else if (val >= 10 && val < 20) {\n      bin = 1;\n    } else if (val >= 20 && val < 30) {\n      bin = 2;\n    } else if (val >= 30 && val < 40) {\n      bin = 3;\n    } else if (val >= 40 && val < 50) {\n      bin = 4;\n    } else if (val >= 50 && val < 60) {\n      bin = 5;\n    } else if (val >= 60 && val < 70) {\n      bin = 6;\n    } else if (val >= 70 && val < 80) {\n      bin = 7;\n    } else if (val >= 80 && val < 90) {\n      bin = 8;\n    } else if (val >= 90 && val <= 100) {\n      bin = 9;\n    }\n    bins[bin]++;\n  }\n}",
            "// TODO\n    size_t N = x.size();\n    size_t *result = new size_t[10];\n    for(size_t i = 0; i < 10; i++){\n        result[i] = 0;\n    }\n    for(size_t i = 0; i < N; i++){\n        result[static_cast<size_t>(x[i])/10]++;\n    }\n    bins = std::array<size_t, 10>(result);\n}",
            "for (auto val: x) {\n        if (val < 10) {\n            bins[0]++;\n        } else if (val < 20) {\n            bins[1]++;\n        } else if (val < 30) {\n            bins[2]++;\n        } else if (val < 40) {\n            bins[3]++;\n        } else if (val < 50) {\n            bins[4]++;\n        } else if (val < 60) {\n            bins[5]++;\n        } else if (val < 70) {\n            bins[6]++;\n        } else if (val < 80) {\n            bins[7]++;\n        } else if (val < 90) {\n            bins[8]++;\n        } else {\n            bins[9]++;\n        }\n    }\n}",
            "for (auto i : x) {\n    auto index = static_cast<size_t>(i / 10);\n    if (index == 10) {\n      index = 9;\n    }\n    bins[index]++;\n  }\n}",
            "for (int i=0; i<10; ++i) {\n      bins[i] = 0;\n   }\n   for (double i : x) {\n      if (i<10) bins[i]++;\n      else if (i<20) bins[i-10]++;\n      else if (i<30) bins[i-20]++;\n      else if (i<40) bins[i-30]++;\n      else if (i<50) bins[i-40]++;\n      else if (i<60) bins[i-50]++;\n      else if (i<70) bins[i-60]++;\n      else if (i<80) bins[i-70]++;\n      else if (i<90) bins[i-80]++;\n      else bins[9]++;\n   }\n}",
            "for (auto const& val: x) {\n    auto index = static_cast<size_t>(val);\n    bins[index / 10]++;\n  }\n}",
            "// TODO: implement\n}",
            "for (int i = 0; i < x.size(); i++) {\n    size_t index = x[i]/10;\n    bins[index]++;\n  }\n}",
            "std::sort(x.begin(), x.end());\n    std::fill(bins.begin(), bins.end(), 0);\n    size_t i = 0;\n    size_t j = 0;\n    while(i < x.size() && j < 10) {\n        if (x[i] < 10) {\n            bins[j]++;\n            i++;\n        } else {\n            j++;\n        }\n    }\n}",
            "// 1. Declare a vector of 10 integers that will contain the bins\n  // 2. Go through the vector x and count the number of entries that fall into each bin\n  // 3. Store the counts in the bins vector\n}",
            "for (auto& val : x) {\n        size_t index = val / 10;\n        if (index >= 10) {\n            index = 9;\n        }\n        bins[index]++;\n    }\n}",
            "bins.fill(0);\n    for (auto xi : x) {\n        size_t bin_index = std::floor(10 * xi);\n        if (bin_index < 0)\n            bin_index = 0;\n        else if (bin_index >= 10)\n            bin_index = 9;\n        bins[bin_index]++;\n    }\n}",
            "// Fill the vector with zeros first\n    bins.fill(0);\n\n    // Loop through the vector and increment the corresponding bin counts\n    for(auto i = 0; i < x.size(); i++) {\n        size_t index = static_cast<size_t>(std::floor(x.at(i) / 10));\n        // Check for index out of range\n        if (index >= 10) {\n            index = 9;\n        }\n        bins.at(index)++;\n    }\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] >= 0.0 && x[i] <= 10.0) {\n      bins[0]++;\n    }\n    else if (x[i] > 10.0 && x[i] <= 20.0) {\n      bins[1]++;\n    }\n    else if (x[i] > 20.0 && x[i] <= 30.0) {\n      bins[2]++;\n    }\n    else if (x[i] > 30.0 && x[i] <= 40.0) {\n      bins[3]++;\n    }\n    else if (x[i] > 40.0 && x[i] <= 50.0) {\n      bins[4]++;\n    }\n    else if (x[i] > 50.0 && x[i] <= 60.0) {\n      bins[5]++;\n    }\n    else if (x[i] > 60.0 && x[i] <= 70.0) {\n      bins[6]++;\n    }\n    else if (x[i] > 70.0 && x[i] <= 80.0) {\n      bins[7]++;\n    }\n    else if (x[i] > 80.0 && x[i] <= 90.0) {\n      bins[8]++;\n    }\n    else if (x[i] > 90.0 && x[i] <= 100.0) {\n      bins[9]++;\n    }\n    else {\n      std::cout << \"Error: value out of bounds in binBy10Count\" << std::endl;\n    }\n  }\n}",
            "for (size_t i = 0; i < bins.size(); ++i) {\n    bins[i] = 0;\n  }\n\n  for (auto v : x) {\n    if (v >= 0 && v < 10) {\n      ++bins[0];\n    } else if (v >= 10 && v < 20) {\n      ++bins[1];\n    } else if (v >= 20 && v < 30) {\n      ++bins[2];\n    } else if (v >= 30 && v < 40) {\n      ++bins[3];\n    } else if (v >= 40 && v < 50) {\n      ++bins[4];\n    } else if (v >= 50 && v < 60) {\n      ++bins[5];\n    } else if (v >= 60 && v < 70) {\n      ++bins[6];\n    } else if (v >= 70 && v < 80) {\n      ++bins[7];\n    } else if (v >= 80 && v < 90) {\n      ++bins[8];\n    } else if (v >= 90 && v < 100) {\n      ++bins[9];\n    }\n  }\n}",
            "// TODO: YOUR CODE HERE\n\n}",
            "for (double const& x_i : x) {\n        // find the bucket that x_i belongs to\n        auto index = x_i/10;\n        // check for rounding issues\n        if (index < 0) index = 0;\n        // check for rounding issues\n        if (index >= 10) index = 9;\n        // increment the counter\n        ++bins[index];\n    }\n}",
            "assert(x.size() <= bins.size());\n    for (auto &&i : x)\n        bins[static_cast<size_t>(i / 10.)]++;\n}",
            "// TODO: Your code goes here\n\n  // return the number of elements in the array\n  return bins.size();\n}",
            "std::array<size_t, 10> counts {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n\n  for (auto& d : x) {\n    if (d >= 0 && d < 10) {\n      counts[0]++;\n    } else if (d >= 10 && d < 20) {\n      counts[1]++;\n    } else if (d >= 20 && d < 30) {\n      counts[2]++;\n    } else if (d >= 30 && d < 40) {\n      counts[3]++;\n    } else if (d >= 40 && d < 50) {\n      counts[4]++;\n    } else if (d >= 50 && d < 60) {\n      counts[5]++;\n    } else if (d >= 60 && d < 70) {\n      counts[6]++;\n    } else if (d >= 70 && d < 80) {\n      counts[7]++;\n    } else if (d >= 80 && d < 90) {\n      counts[8]++;\n    } else if (d >= 90 && d < 100) {\n      counts[9]++;\n    }\n  }\n  bins = counts;\n}",
            "for (auto val: x) {\n    size_t index = static_cast<size_t>(val / 10.0);\n    if (index >= 10) index = 9;\n    ++bins[index];\n  }\n}",
            "bins.fill(0);\n  for (double val : x) {\n    if (val < 10) {\n      ++bins[0];\n    } else if (val >= 10 && val < 20) {\n      ++bins[1];\n    } else if (val >= 20 && val < 30) {\n      ++bins[2];\n    } else if (val >= 30 && val < 40) {\n      ++bins[3];\n    } else if (val >= 40 && val < 50) {\n      ++bins[4];\n    } else if (val >= 50 && val < 60) {\n      ++bins[5];\n    } else if (val >= 60 && val < 70) {\n      ++bins[6];\n    } else if (val >= 70 && val < 80) {\n      ++bins[7];\n    } else if (val >= 80 && val < 90) {\n      ++bins[8];\n    } else {\n      ++bins[9];\n    }\n  }\n}",
            "// TODO\n}",
            "std::vector<double> tmp{x.begin(), x.end()};\n    std::sort(tmp.begin(), tmp.end());\n    for(auto& e : tmp) {\n        auto x = int(e);\n        if (x >= 0 && x < 10) {\n            bins[0]++;\n        } else if (x >= 10 && x < 20) {\n            bins[1]++;\n        } else if (x >= 20 && x < 30) {\n            bins[2]++;\n        } else if (x >= 30 && x < 40) {\n            bins[3]++;\n        } else if (x >= 40 && x < 50) {\n            bins[4]++;\n        } else if (x >= 50 && x < 60) {\n            bins[5]++;\n        } else if (x >= 60 && x < 70) {\n            bins[6]++;\n        } else if (x >= 70 && x < 80) {\n            bins[7]++;\n        } else if (x >= 80 && x < 90) {\n            bins[8]++;\n        } else if (x >= 90 && x < 100) {\n            bins[9]++;\n        }\n    }\n}",
            "bins.fill(0);\n\n   for(auto v : x) {\n      size_t bin = static_cast<size_t>(v / 10.0);\n      bins[bin]++;\n   }\n\n}",
            "// fill the array with zeros\n    std::fill(bins.begin(), bins.end(), 0);\n    // iterate through the values, increment the bins where applicable\n    for(auto const& el: x) {\n        if (el >= 0 && el <= 10) {\n            ++bins[0];\n        } else if (el > 10 && el <= 20) {\n            ++bins[1];\n        } else if (el > 20 && el <= 30) {\n            ++bins[2];\n        } else if (el > 30 && el <= 40) {\n            ++bins[3];\n        } else if (el > 40 && el <= 50) {\n            ++bins[4];\n        } else if (el > 50 && el <= 60) {\n            ++bins[5];\n        } else if (el > 60 && el <= 70) {\n            ++bins[6];\n        } else if (el > 70 && el <= 80) {\n            ++bins[7];\n        } else if (el > 80 && el <= 90) {\n            ++bins[8];\n        } else if (el > 90 && el <= 100) {\n            ++bins[9];\n        }\n    }\n}",
            "size_t n = 0;\n    for (auto const& xi : x) {\n        if (n == bins.size()) break;\n        if (xi < 10) bins[n++]++;\n        else if (xi >= 10 && xi < 20) bins[n++]++;\n        else if (xi >= 20 && xi < 30) bins[n++]++;\n        else if (xi >= 30 && xi < 40) bins[n++]++;\n        else if (xi >= 40 && xi < 50) bins[n++]++;\n        else if (xi >= 50 && xi < 60) bins[n++]++;\n        else if (xi >= 60 && xi < 70) bins[n++]++;\n        else if (xi >= 70 && xi < 80) bins[n++]++;\n        else if (xi >= 80 && xi < 90) bins[n++]++;\n        else bins[n++]++;\n    }\n}",
            "size_t count = 0;\n\tfor (double i : x) {\n\t\tif (i < 10 && i > 0) {\n\t\t\tbins[0]++;\n\t\t\tcontinue;\n\t\t}\n\t\tsize_t bin = size_t(i / 10.0);\n\t\tif (bin == 0) {\n\t\t\tbins[0]++;\n\t\t\tcontinue;\n\t\t}\n\t\tif (bin < 10) {\n\t\t\tbins[bin]++;\n\t\t}\n\t}\n\tfor (size_t i = 0; i < 10; i++) {\n\t\tstd::cout << i * 10 << \" - \" << (i + 1) * 10 << \" : \" << bins[i] << \" / \" << x.size() << \"\\n\";\n\t}\n}",
            "// TODO: Your code here\n  int n = x.size();\n  std::array<size_t, 10> result;\n  std::fill_n(result.begin(), 10, 0);\n  for (int i = 0; i < n; ++i) {\n    int index = (x[i] / 10.0);\n    if (index >= 10) {\n      index = 9;\n    }\n    result[index]++;\n  }\n  bins = result;\n}",
            "std::array<size_t, 10> temp{};\n    for (auto val: x) {\n        auto index = static_cast<int>(std::floor(val / 10.0)) % 10;\n        temp[index]++;\n    }\n    bins = temp;\n}",
            "// TODO: your code here\n}",
            "// TODO: fill in this function\n    // TODO: make sure you test it\n}",
            "std::array<size_t, 10> counts{};\n    for (auto const &value : x) {\n        size_t index = static_cast<size_t>((value - 1) / 10);\n        counts[index]++;\n    }\n\n    bins = counts;\n}",
            "std::array<double, 10> cutpoints = {0., 10., 20., 30., 40., 50., 60., 70., 80., 90.};\n    std::vector<int> cutPointIndices = {0, 10, 20, 30, 40, 50, 60, 70, 80, 90};\n    bins.fill(0);\n    for(auto& x_val : x) {\n        for(auto& cutpoint_ind : cutPointIndices) {\n            if(x_val < cutpoints[cutpoint_ind]) {\n                bins[cutpoint_ind]++;\n                break;\n            }\n        }\n    }\n}",
            "std::array<size_t, 10> results;\n  for (auto xi : x) {\n    int i = 10 * int(xi) / 100;\n    results[i]++;\n  }\n  bins = results;\n}",
            "bins.fill(0);\n  for (double d : x) {\n    bins[size_t(std::floor(d/10.)) % 10]++;\n  }\n}",
            "bins = {};\n\tfor (auto const& e : x) {\n\t\tbins[static_cast<size_t>(e/10)] += 1;\n\t}\n}",
            "for (auto i = 0; i < x.size(); i++) {\n    if (x[i] >= 0. && x[i] < 10.)\n      bins[0]++;\n    else if (x[i] >= 10. && x[i] < 20.)\n      bins[1]++;\n    else if (x[i] >= 20. && x[i] < 30.)\n      bins[2]++;\n    else if (x[i] >= 30. && x[i] < 40.)\n      bins[3]++;\n    else if (x[i] >= 40. && x[i] < 50.)\n      bins[4]++;\n    else if (x[i] >= 50. && x[i] < 60.)\n      bins[5]++;\n    else if (x[i] >= 60. && x[i] < 70.)\n      bins[6]++;\n    else if (x[i] >= 70. && x[i] < 80.)\n      bins[7]++;\n    else if (x[i] >= 80. && x[i] < 90.)\n      bins[8]++;\n    else if (x[i] >= 90. && x[i] <= 100.)\n      bins[9]++;\n    else\n      std::cerr << \"Wrong input value in binsBy10Count\\n\";\n  }\n}",
            "auto lo = [](int i) { return 10.0 * i; };\n    auto hi = [](int i) { return 10.0 * i + 10; };\n\n    // Use the accumulate function to do the counting. The first argument to\n    // accumulate is the beginning of the array. The second argument is the\n    // initial value of the accumulated value. The third argument is a\n    // function that is applied to the accumulated value, the current element\n    // of the array, and the index of the current element. In this case, the\n    // index is only used to check for which bin the value belongs.\n    std::fill(bins.begin(), bins.end(), 0);\n    std::accumulate(x.begin(), x.end(), bins.begin(),\n                    [](std::array<size_t, 10> &bins, double x, int i) {\n                        if (x >= lo(i) && x < hi(i)) {\n                            ++bins[i];\n                        }\n                        return bins;\n                    });\n}",
            "assert(x.size() > 0);\n  bins.fill(0);\n  for (double y : x) {\n    assert(y >= 0 && y <= 100);\n    bins[static_cast<size_t>(y/10)]++;\n  }\n}",
            "int i = 0;\n   int j = 0;\n   for(auto n: x){\n       if(n <= 10){\n           bins[i]++;\n       } else {\n           for(; j < 10 && n > j * 10; j++){\n\n           }\n           bins[j]++;\n       }\n   }\n}",
            "for (size_t i=0; i<x.size(); i++) {\n    // TODO: use an inline if-else to determine the bin number for the current\n    // value, then increment the correct count.\n  }\n}",
            "assert(x.size() == bins.size());\n  std::sort(x.begin(), x.end());\n  for (size_t i = 0; i < x.size(); ++i) {\n    size_t j = static_cast<size_t>(x[i]);\n    bins[j / 10] += 1;\n  }\n}",
            "size_t i = 0;\n    for (auto value : x) {\n        int bin = floor(value / 10);\n        bins[bin]++;\n        ++i;\n    }\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin = size_t(x[i] / 10.0);\n        if (bin < 10) {\n            ++bins[bin];\n        }\n    }\n}",
            "for (size_t i = 0; i < 10; i++) {\n        bins[i] = 0;\n    }\n\n    for (double val : x) {\n        size_t idx = (size_t)(val / 10);\n        if (idx > 9) idx = 9;\n        bins[idx] += 1;\n    }\n}",
            "bins.fill(0);\n  double min = x[0];\n  double max = x[0];\n  for (double v : x) {\n    min = std::min(min, v);\n    max = std::max(max, v);\n  }\n  assert(min >= 0);\n  assert(max <= 100);\n  for (double v : x) {\n    bins[int((v / 10) - min)]++;\n  }\n}",
            "// Write your code here\n    size_t count = 0;\n    size_t bin = 0;\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] >= 0 && x[i] < 10) {\n            bin = 0;\n            bins[bin]++;\n        } else if (x[i] >= 10 && x[i] < 20) {\n            bin = 1;\n            bins[bin]++;\n        } else if (x[i] >= 20 && x[i] < 30) {\n            bin = 2;\n            bins[bin]++;\n        } else if (x[i] >= 30 && x[i] < 40) {\n            bin = 3;\n            bins[bin]++;\n        } else if (x[i] >= 40 && x[i] < 50) {\n            bin = 4;\n            bins[bin]++;\n        } else if (x[i] >= 50 && x[i] < 60) {\n            bin = 5;\n            bins[bin]++;\n        } else if (x[i] >= 60 && x[i] < 70) {\n            bin = 6;\n            bins[bin]++;\n        } else if (x[i] >= 70 && x[i] < 80) {\n            bin = 7;\n            bins[bin]++;\n        } else if (x[i] >= 80 && x[i] < 90) {\n            bin = 8;\n            bins[bin]++;\n        } else if (x[i] >= 90 && x[i] < 100) {\n            bin = 9;\n            bins[bin]++;\n        } else {\n            std::cout << \"bin out of range\" << std::endl;\n            throw std::invalid_argument(\"bin out of range\");\n        }\n    }\n}",
            "bins[0] = 0;\n\tbins[1] = 0;\n\tbins[2] = 0;\n\tbins[3] = 0;\n\tbins[4] = 0;\n\tbins[5] = 0;\n\tbins[6] = 0;\n\tbins[7] = 0;\n\tbins[8] = 0;\n\tbins[9] = 0;\n\tfor (int i = 0; i < 10; i++) {\n\t\tif (x[i] < 10) {\n\t\t\tbins[0]++;\n\t\t}\n\t\telse if (x[i] >= 10 && x[i] < 20) {\n\t\t\tbins[1]++;\n\t\t}\n\t\telse if (x[i] >= 20 && x[i] < 30) {\n\t\t\tbins[2]++;\n\t\t}\n\t\telse if (x[i] >= 30 && x[i] < 40) {\n\t\t\tbins[3]++;\n\t\t}\n\t\telse if (x[i] >= 40 && x[i] < 50) {\n\t\t\tbins[4]++;\n\t\t}\n\t\telse if (x[i] >= 50 && x[i] < 60) {\n\t\t\tbins[5]++;\n\t\t}\n\t\telse if (x[i] >= 60 && x[i] < 70) {\n\t\t\tbins[6]++;\n\t\t}\n\t\telse if (x[i] >= 70 && x[i] < 80) {\n\t\t\tbins[7]++;\n\t\t}\n\t\telse if (x[i] >= 80 && x[i] < 90) {\n\t\t\tbins[8]++;\n\t\t}\n\t\telse if (x[i] >= 90 && x[i] < 100) {\n\t\t\tbins[9]++;\n\t\t}\n\t}\n}",
            "std::for_each(x.begin(), x.end(), [&](auto v) {\n      auto idx = size_t(v/10.0);\n      if (idx >= 10) idx = 9;\n      bins[idx]++;\n   });\n}",
            "bins.fill(0);\n  for (double value : x) {\n    if (value >= 0 && value < 10) {\n      bins[0]++;\n    } else if (value >= 10 && value < 20) {\n      bins[1]++;\n    } else if (value >= 20 && value < 30) {\n      bins[2]++;\n    } else if (value >= 30 && value < 40) {\n      bins[3]++;\n    } else if (value >= 40 && value < 50) {\n      bins[4]++;\n    } else if (value >= 50 && value < 60) {\n      bins[5]++;\n    } else if (value >= 60 && value < 70) {\n      bins[6]++;\n    } else if (value >= 70 && value < 80) {\n      bins[7]++;\n    } else if (value >= 80 && value < 90) {\n      bins[8]++;\n    } else if (value >= 90 && value <= 100) {\n      bins[9]++;\n    }\n  }\n}",
            "for (auto const& value : x) {\n        size_t bin = std::floor(value / 10.0);\n        if (bin < 0 || bin >= bins.size()) {\n            throw std::out_of_range(\"invalid input value\");\n        }\n        ++bins[bin];\n    }\n}",
            "for (auto val : x) {\n    if (val >= 0.0 && val < 10.0) {\n      ++bins[0];\n    } else if (val >= 10.0 && val < 20.0) {\n      ++bins[1];\n    } else if (val >= 20.0 && val < 30.0) {\n      ++bins[2];\n    } else if (val >= 30.0 && val < 40.0) {\n      ++bins[3];\n    } else if (val >= 40.0 && val < 50.0) {\n      ++bins[4];\n    } else if (val >= 50.0 && val < 60.0) {\n      ++bins[5];\n    } else if (val >= 60.0 && val < 70.0) {\n      ++bins[6];\n    } else if (val >= 70.0 && val < 80.0) {\n      ++bins[7];\n    } else if (val >= 80.0 && val < 90.0) {\n      ++bins[8];\n    } else if (val >= 90.0 && val <= 100.0) {\n      ++bins[9];\n    }\n  }\n}",
            "for (auto const& e : x) {\n        size_t i = (size_t)e / 10;\n        bins[i]++;\n    }\n}",
            "assert(bins.size() == 10);\n\tbins.fill(0);\n\tfor (auto d : x) {\n\t\tint b = 10 * (d / 10.);\n\t\tif (b < 10) bins[b]++;\n\t}\n}",
            "// Make sure bins are initialized to zero.\n  std::fill(bins.begin(), bins.end(), 0);\n  for (auto xi : x) {\n    // Figure out which bin xi belongs to.\n    // The bins are [0, 10), [10, 20), [20, 30),...\n    int bin = int(10 * xi) / 10;\n    if (bin < 0 || bin >= 10) {\n      throw std::range_error(\"binsBy10Count() : Invalid bin value \" + std::to_string(bin));\n    }\n    ++bins[bin];\n  }\n}",
            "std::array<double, 10> edges = {0.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0};\n    for (double const& value : x) {\n        for (size_t index = 0; index < 10; ++index) {\n            if (value >= edges[index] && value < edges[index + 1]) {\n                ++bins[index];\n            }\n        }\n    }\n}",
            "size_t i = 0;\n    for (double val : x) {\n        double bin = val/10.0;\n        if (bin >= 0 && bin < 1) {\n            bins[i]++;\n        } else if (bin >= 1 && bin < 2) {\n            bins[i+1]++;\n        } else if (bin >= 2 && bin < 3) {\n            bins[i+2]++;\n        } else if (bin >= 3 && bin < 4) {\n            bins[i+3]++;\n        } else if (bin >= 4 && bin < 5) {\n            bins[i+4]++;\n        } else if (bin >= 5 && bin < 6) {\n            bins[i+5]++;\n        } else if (bin >= 6 && bin < 7) {\n            bins[i+6]++;\n        } else if (bin >= 7 && bin < 8) {\n            bins[i+7]++;\n        } else if (bin >= 8 && bin < 9) {\n            bins[i+8]++;\n        } else if (bin >= 9 && bin <= 10) {\n            bins[i+9]++;\n        } else {\n            throw std::runtime_error(\"val=\" + std::to_string(val) + \" is not in [0,10)\");\n        }\n        i++;\n    }\n}",
            "assert(x.size() == bins.size());\n\n    // TODO: Your code here.\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n        // Convert the input to a [0,100) interval\n        size_t idx = static_cast<size_t>(std::floor(10*x[i]));\n        if (idx > 9) {\n            idx = 9;\n        }\n        bins[idx] += 1;\n    }\n}",
            "bins.fill(0);\n\n\tfor (double d : x) {\n\t\tif (d >= 0.0 && d < 10.0) {\n\t\t\tbins[0]++;\n\t\t} else if (d >= 10.0 && d < 20.0) {\n\t\t\tbins[1]++;\n\t\t} else if (d >= 20.0 && d < 30.0) {\n\t\t\tbins[2]++;\n\t\t} else if (d >= 30.0 && d < 40.0) {\n\t\t\tbins[3]++;\n\t\t} else if (d >= 40.0 && d < 50.0) {\n\t\t\tbins[4]++;\n\t\t} else if (d >= 50.0 && d < 60.0) {\n\t\t\tbins[5]++;\n\t\t} else if (d >= 60.0 && d < 70.0) {\n\t\t\tbins[6]++;\n\t\t} else if (d >= 70.0 && d < 80.0) {\n\t\t\tbins[7]++;\n\t\t} else if (d >= 80.0 && d < 90.0) {\n\t\t\tbins[8]++;\n\t\t} else if (d >= 90.0 && d < 100.0) {\n\t\t\tbins[9]++;\n\t\t}\n\t}\n}",
            "// TODO: Your code goes here\n  size_t num = x.size();\n  bins.fill(0);\n  for (size_t i = 0; i < num; i++) {\n    if (x[i] >= 0 && x[i] < 10) {\n      bins[0] += 1;\n    }\n    else if (x[i] >= 10 && x[i] < 20) {\n      bins[1] += 1;\n    }\n    else if (x[i] >= 20 && x[i] < 30) {\n      bins[2] += 1;\n    }\n    else if (x[i] >= 30 && x[i] < 40) {\n      bins[3] += 1;\n    }\n    else if (x[i] >= 40 && x[i] < 50) {\n      bins[4] += 1;\n    }\n    else if (x[i] >= 50 && x[i] < 60) {\n      bins[5] += 1;\n    }\n    else if (x[i] >= 60 && x[i] < 70) {\n      bins[6] += 1;\n    }\n    else if (x[i] >= 70 && x[i] < 80) {\n      bins[7] += 1;\n    }\n    else if (x[i] >= 80 && x[i] < 90) {\n      bins[8] += 1;\n    }\n    else if (x[i] >= 90 && x[i] <= 100) {\n      bins[9] += 1;\n    }\n  }\n}",
            "// TODO: Fill in this function to compute the number of data values\n    // that are in each bin.\n    for (int i = 0; i < 10; i++) {\n        bins[i] = 0;\n    }\n\n    for (double val : x) {\n        if (val >= 0 && val < 10) {\n            bins[0]++;\n        } else if (val >= 10 && val < 20) {\n            bins[1]++;\n        } else if (val >= 20 && val < 30) {\n            bins[2]++;\n        } else if (val >= 30 && val < 40) {\n            bins[3]++;\n        } else if (val >= 40 && val < 50) {\n            bins[4]++;\n        } else if (val >= 50 && val < 60) {\n            bins[5]++;\n        } else if (val >= 60 && val < 70) {\n            bins[6]++;\n        } else if (val >= 70 && val < 80) {\n            bins[7]++;\n        } else if (val >= 80 && val < 90) {\n            bins[8]++;\n        } else if (val >= 90 && val <= 100) {\n            bins[9]++;\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        int idx = (int) (x[i] / 10);\n        if (idx < 0) idx = 0;\n        if (idx > 10) idx = 10;\n        bins[idx]++;\n    }\n}",
            "// your code goes here\n\n}",
            "for (double v: x) {\n        size_t i = v / 10;\n        if (i >= 10)\n            i = 9;\n        bins[i]++;\n    }\n}",
            "for (size_t i=0; i<x.size(); ++i) {\n        auto bin = static_cast<size_t>(x[i]/10);\n        if (bin >= 10) {\n            throw std::logic_error(\"Value out of range in binsBy10Count\");\n        }\n        bins[bin]++;\n    }\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n    int bin = (int) std::floor(x[i] / 10);\n    if (bin >= 10) {\n      bin = 9;\n    }\n    if (bin < 0) {\n      bin = 0;\n    }\n    bins[bin]++;\n  }\n}",
            "int max_bin = 9;\n  int min_bin = 0;\n  int bin_size = 10;\n  int bin_index = 0;\n\n  for (auto elem : x) {\n    if ((elem >= min_bin && elem <= max_bin) || (elem >= 100 - min_bin)) {\n      bin_index = (int)((elem - min_bin) / bin_size);\n    } else if (elem < min_bin) {\n      bin_index = 0;\n    } else {\n      bin_index = 9;\n    }\n    bins[bin_index] += 1;\n  }\n}",
            "// for (auto& el : x)\n    // {\n    //     std::cout << el << \" \";\n    // }\n    // std::cout << std::endl;\n\n    for (auto& el : x) {\n        bins[static_cast<size_t>(std::floor(el / 10.0)) % 10]++;\n    }\n}",
            "// your code goes here\n    size_t n = x.size();\n    for (size_t i = 0; i < n; ++i) {\n        if (x[i] >= 0 && x[i] <= 10)\n            ++bins[0];\n        else if (x[i] > 10 && x[i] <= 20)\n            ++bins[1];\n        else if (x[i] > 20 && x[i] <= 30)\n            ++bins[2];\n        else if (x[i] > 30 && x[i] <= 40)\n            ++bins[3];\n        else if (x[i] > 40 && x[i] <= 50)\n            ++bins[4];\n        else if (x[i] > 50 && x[i] <= 60)\n            ++bins[5];\n        else if (x[i] > 60 && x[i] <= 70)\n            ++bins[6];\n        else if (x[i] > 70 && x[i] <= 80)\n            ++bins[7];\n        else if (x[i] > 80 && x[i] <= 90)\n            ++bins[8];\n        else if (x[i] > 90 && x[i] <= 100)\n            ++bins[9];\n    }\n}",
            "for (size_t i = 0; i < 10; ++i) {\n\t\tbins[i] = 0;\n\t}\n\tsize_t counter = 0;\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tif (x[i] >= 0 && x[i] < 10) {\n\t\t\t++bins[counter];\n\t\t}\n\t\telse if (x[i] >= 10 && x[i] < 20) {\n\t\t\t++bins[counter + 1];\n\t\t}\n\t\telse if (x[i] >= 20 && x[i] < 30) {\n\t\t\t++bins[counter + 2];\n\t\t}\n\t\telse if (x[i] >= 30 && x[i] < 40) {\n\t\t\t++bins[counter + 3];\n\t\t}\n\t\telse if (x[i] >= 40 && x[i] < 50) {\n\t\t\t++bins[counter + 4];\n\t\t}\n\t\telse if (x[i] >= 50 && x[i] < 60) {\n\t\t\t++bins[counter + 5];\n\t\t}\n\t\telse if (x[i] >= 60 && x[i] < 70) {\n\t\t\t++bins[counter + 6];\n\t\t}\n\t\telse if (x[i] >= 70 && x[i] < 80) {\n\t\t\t++bins[counter + 7];\n\t\t}\n\t\telse if (x[i] >= 80 && x[i] < 90) {\n\t\t\t++bins[counter + 8];\n\t\t}\n\t\telse if (x[i] >= 90 && x[i] <= 100) {\n\t\t\t++bins[counter + 9];\n\t\t}\n\t}\n}",
            "for (size_t i=0; i<x.size(); i++) {\n        if (x.at(i) < 10) {\n            bins.at(0) += 1;\n        }\n        else if (x.at(i) < 20) {\n            bins.at(1) += 1;\n        }\n        else if (x.at(i) < 30) {\n            bins.at(2) += 1;\n        }\n        else if (x.at(i) < 40) {\n            bins.at(3) += 1;\n        }\n        else if (x.at(i) < 50) {\n            bins.at(4) += 1;\n        }\n        else if (x.at(i) < 60) {\n            bins.at(5) += 1;\n        }\n        else if (x.at(i) < 70) {\n            bins.at(6) += 1;\n        }\n        else if (x.at(i) < 80) {\n            bins.at(7) += 1;\n        }\n        else if (x.at(i) < 90) {\n            bins.at(8) += 1;\n        }\n        else if (x.at(i) < 100) {\n            bins.at(9) += 1;\n        }\n    }\n}",
            "for (double i : x) {\n        size_t j = std::floor(i/10.0);\n        bins[j] += 1;\n    }\n}",
            "std::array<size_t, 10> counts = {};\n    for (auto xi : x) {\n        if (0 < xi && xi < 10) counts[0] += 1;\n        if (10 <= xi && xi < 20) counts[1] += 1;\n        if (20 <= xi && xi < 30) counts[2] += 1;\n        if (30 <= xi && xi < 40) counts[3] += 1;\n        if (40 <= xi && xi < 50) counts[4] += 1;\n        if (50 <= xi && xi < 60) counts[5] += 1;\n        if (60 <= xi && xi < 70) counts[6] += 1;\n        if (70 <= xi && xi < 80) counts[7] += 1;\n        if (80 <= xi && xi < 90) counts[8] += 1;\n        if (90 <= xi && xi < 100) counts[9] += 1;\n    }\n    bins = counts;\n}",
            "double const min = 0.0;\n  double const max = 100.0;\n  double const step = 10.0;\n  size_t index = 0;\n  for (double val : x) {\n    bins[index++] = std::count_if(x.begin(), x.end(),\n      [=](double v){\n        return v >= min && v < max;\n      });\n  }\n}",
            "// You have to initialize the values in bins before using them.\n  bins.fill(0);\n  size_t index;\n  for (size_t i = 0; i < x.size(); i++) {\n    index = static_cast<size_t>(x[i]/10);\n    if (index < 10) {\n      bins[index]++;\n    }\n  }\n}",
            "bins.fill(0);\n\n    for (size_t i = 0; i < x.size(); i++) {\n        double val = x[i];\n        if (val >= 0 && val < 10) {\n            bins[0]++;\n        } else if (val >= 10 && val < 20) {\n            bins[1]++;\n        } else if (val >= 20 && val < 30) {\n            bins[2]++;\n        } else if (val >= 30 && val < 40) {\n            bins[3]++;\n        } else if (val >= 40 && val < 50) {\n            bins[4]++;\n        } else if (val >= 50 && val < 60) {\n            bins[5]++;\n        } else if (val >= 60 && val < 70) {\n            bins[6]++;\n        } else if (val >= 70 && val < 80) {\n            bins[7]++;\n        } else if (val >= 80 && val < 90) {\n            bins[8]++;\n        } else if (val >= 90 && val <= 100) {\n            bins[9]++;\n        }\n    }\n}",
            "for (double i : x) {\n        if (i >= 0 && i <= 10) bins[0]++;\n        else if (i > 10 && i <= 20) bins[1]++;\n        else if (i > 20 && i <= 30) bins[2]++;\n        else if (i > 30 && i <= 40) bins[3]++;\n        else if (i > 40 && i <= 50) bins[4]++;\n        else if (i > 50 && i <= 60) bins[5]++;\n        else if (i > 60 && i <= 70) bins[6]++;\n        else if (i > 70 && i <= 80) bins[7]++;\n        else if (i > 80 && i <= 90) bins[8]++;\n        else if (i > 90 && i <= 100) bins[9]++;\n    }\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n\t\tint i_floor = int(x[i]);\n\t\tint i_ceil = int(x[i]) + 1;\n\t\tif (i_floor > 99 || i_ceil > 99) {\n\t\t\tstd::cout << \"Error: x[\" << i << \"]=\" << x[i] << \" out of bounds\" << std::endl;\n\t\t}\n\t\telse {\n\t\t\tbins[i_floor]++;\n\t\t\tbins[i_ceil]++;\n\t\t}\n\t}\n\n\treturn;\n}",
            "// TODO\n    return;\n}",
            "bins.fill(0);\n\tfor (double val : x) {\n\t\tint index = static_cast<int>(val / 10);\n\t\tif (index < 0 || index >= 10) {\n\t\t\tcontinue;\n\t\t}\n\t\tbins[index] += 1;\n\t}\n}",
            "bins = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n    std::array<size_t, 10> counts;\n    counts.fill(0);\n\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] < 10) {\n            counts[0]++;\n        } else if (x[i] < 20) {\n            counts[1]++;\n        } else if (x[i] < 30) {\n            counts[2]++;\n        } else if (x[i] < 40) {\n            counts[3]++;\n        } else if (x[i] < 50) {\n            counts[4]++;\n        } else if (x[i] < 60) {\n            counts[5]++;\n        } else if (x[i] < 70) {\n            counts[6]++;\n        } else if (x[i] < 80) {\n            counts[7]++;\n        } else if (x[i] < 90) {\n            counts[8]++;\n        } else if (x[i] < 100) {\n            counts[9]++;\n        }\n    }\n\n    bins[0] = counts[0];\n    bins[1] = counts[1];\n    bins[2] = counts[2];\n    bins[3] = counts[3];\n    bins[4] = counts[4];\n    bins[5] = counts[5];\n    bins[6] = counts[6];\n    bins[7] = counts[7];\n    bins[8] = counts[8];\n    bins[9] = counts[9];\n}",
            "std::array<size_t, 10> counts = {0};\n    for(auto x_element : x){\n        counts[x_element/10]++;\n    }\n    bins = counts;\n}",
            "// Create the bins vector.\n  for(auto& v : bins) v = 0;\n\n  // Go through x and increment the bins.\n  for(auto const& xi : x) {\n\n    // Find the bin.\n    size_t bin = floor(xi/10.0);\n\n    // Increment the bin.\n    ++bins[bin];\n  }\n}",
            "for (size_t i=0; i<bins.size(); ++i)\n    bins[i] = 0;\n\n  for (double d : x) {\n    size_t idx = (size_t)(std::round(d/10.0)) % 10;\n    bins[idx]++;\n  }\n}",
            "// TODO\n}",
            "for (double i : x) {\n        if (i < 10) bins[0]++;\n        else if (i < 20) bins[1]++;\n        else if (i < 30) bins[2]++;\n        else if (i < 40) bins[3]++;\n        else if (i < 50) bins[4]++;\n        else if (i < 60) bins[5]++;\n        else if (i < 70) bins[6]++;\n        else if (i < 80) bins[7]++;\n        else if (i < 90) bins[8]++;\n        else if (i < 100) bins[9]++;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for(size_t i = 0; i < x.size(); i++) {\n        size_t bin = static_cast<size_t>(10 * x[i]);\n        if(bin > 100)\n            bin = 100;\n        if(bin < 0)\n            bin = 0;\n        bins[bin]++;\n    }\n}",
            "for(double const& y : x) {\n        size_t bin = std::floor((y - 0) / (10 + 1));\n        ++bins[bin];\n    }\n}",
            "bins.fill(0);\n\n  for(double const& v : x) {\n    double x_bin = std::floor(v/10);\n    size_t x_bin_index = (size_t)x_bin;\n    if(x_bin_index < 10) {\n      bins[x_bin_index]++;\n    }\n  }\n\n}",
            "std::vector<size_t> counts(10, 0);\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    size_t index = static_cast<size_t>(x[i] / 10);\n    counts[index]++;\n  }\n\n  bins = counts;\n}",
            "double bin_val = 0;\n  size_t bin_idx = 0;\n  for(size_t i=0; i < x.size(); ++i) {\n    bin_val = x[i]/10;\n    bin_idx = (size_t) bin_val;\n    bins[bin_idx]++;\n  }\n}",
            "for (double const& xi : x) {\n    size_t i = floor(10*xi);\n    bins[i]++;\n  }\n}",
            "for (auto i = 0; i < 10; ++i) {\n        bins[i] = std::count_if(x.begin(), x.end(), [i](double v) {\n            return (i*10 < v) && (v < (i+1)*10);\n        });\n    }\n}",
            "double step = 10;\n    size_t count[10] = {0};\n    for (auto it = x.begin(); it!= x.end(); it++) {\n        if (*it >= 0 && *it <= step)\n            count[0]++;\n        else if (*it > step && *it <= step * 2)\n            count[1]++;\n        else if (*it > step * 2 && *it <= step * 3)\n            count[2]++;\n        else if (*it > step * 3 && *it <= step * 4)\n            count[3]++;\n        else if (*it > step * 4 && *it <= step * 5)\n            count[4]++;\n        else if (*it > step * 5 && *it <= step * 6)\n            count[5]++;\n        else if (*it > step * 6 && *it <= step * 7)\n            count[6]++;\n        else if (*it > step * 7 && *it <= step * 8)\n            count[7]++;\n        else if (*it > step * 8 && *it <= step * 9)\n            count[8]++;\n        else if (*it > step * 9 && *it <= step * 10)\n            count[9]++;\n    }\n    for (int i = 0; i < 10; i++)\n        bins[i] = count[i];\n}",
            "bins = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n\n    for (size_t i = 0; i < x.size(); i++) {\n        size_t pos = (x[i] - 1) / 10;\n        bins[pos]++;\n    }\n}",
            "// TODO: Your code here\n  assert(x.size() == 10);\n  int i = 0;\n  int num_bins = 10;\n  for(int j = 0; j < num_bins; j++) {\n    bins[j] = 0;\n  }\n  for(int j = 0; j < x.size(); j++) {\n    bins[i] = bins[i] + 1;\n    if(x[j] <= 10) {\n      i = 0;\n    }\n    else if(x[j] > 10 && x[j] <= 20) {\n      i = 1;\n    }\n    else if(x[j] > 20 && x[j] <= 30) {\n      i = 2;\n    }\n    else if(x[j] > 30 && x[j] <= 40) {\n      i = 3;\n    }\n    else if(x[j] > 40 && x[j] <= 50) {\n      i = 4;\n    }\n    else if(x[j] > 50 && x[j] <= 60) {\n      i = 5;\n    }\n    else if(x[j] > 60 && x[j] <= 70) {\n      i = 6;\n    }\n    else if(x[j] > 70 && x[j] <= 80) {\n      i = 7;\n    }\n    else if(x[j] > 80 && x[j] <= 90) {\n      i = 8;\n    }\n    else if(x[j] > 90 && x[j] <= 100) {\n      i = 9;\n    }\n  }\n}",
            "// Your code here\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tbins[int(x[i] / 10)] += 1;\n\t}\n}",
            "for (double const& value : x) {\n        size_t const i = static_cast<size_t>(value * 0.1);\n        if (i < 10) {\n            bins[i] += 1;\n        }\n    }\n}",
            "// TODO: Your code goes here\n}",
            "std::array<size_t, 10> temp;\n    std::fill(temp.begin(), temp.end(), 0);\n    for (int i=0; i<x.size(); i++) {\n        if ((int)x[i] < 0 || (int)x[i] > 100)\n            std::cerr << \"Out of range error!\" << std::endl;\n        temp[((int)x[i])/10]++;\n    }\n    bins = temp;\n}",
            "size_t i = 0;\n   for (auto xv : x) {\n      if (xv < 10) {\n         bins[0] += 1;\n      } else if (xv < 20) {\n         bins[1] += 1;\n      } else if (xv < 30) {\n         bins[2] += 1;\n      } else if (xv < 40) {\n         bins[3] += 1;\n      } else if (xv < 50) {\n         bins[4] += 1;\n      } else if (xv < 60) {\n         bins[5] += 1;\n      } else if (xv < 70) {\n         bins[6] += 1;\n      } else if (xv < 80) {\n         bins[7] += 1;\n      } else if (xv < 90) {\n         bins[8] += 1;\n      } else {\n         bins[9] += 1;\n      }\n   }\n}",
            "// TODO: Your code here\n    // hint: you may want to use std::for_each()\n\n    std::for_each(x.begin(), x.end(), [&bins](double i) {\n        size_t temp = i / 10;\n        if (temp < 10) bins[temp] += 1;\n    });\n\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for(size_t i = 0; i < x.size(); ++i) {\n        size_t bin = (size_t)(x[i] / 10.0 + 0.5);\n        if(bin > 9) bin = 9;\n        ++bins[bin];\n    }\n}",
            "// write code here\n}",
            "for (auto i = 0; i < x.size(); ++i) {\n        auto index = std::floor(x[i]/10.0);\n        if (index < 0)\n            index = 0;\n        if (index >= 10)\n            index = 9;\n        bins[index]++;\n    }\n}",
            "bins.fill(0);\n    for (double v : x) {\n        size_t binIdx = 10 * floor(v / 10);\n        if (binIdx < 10) {\n            bins[binIdx]++;\n        }\n    }\n}",
            "bins[0] = std::count_if(std::begin(x), std::end(x), [](double i) { return i >= 0 && i < 10; });\n    bins[1] = std::count_if(std::begin(x), std::end(x), [](double i) { return i >= 10 && i < 20; });\n    bins[2] = std::count_if(std::begin(x), std::end(x), [](double i) { return i >= 20 && i < 30; });\n    bins[3] = std::count_if(std::begin(x), std::end(x), [](double i) { return i >= 30 && i < 40; });\n    bins[4] = std::count_if(std::begin(x), std::end(x), [](double i) { return i >= 40 && i < 50; });\n    bins[5] = std::count_if(std::begin(x), std::end(x), [](double i) { return i >= 50 && i < 60; });\n    bins[6] = std::count_if(std::begin(x), std::end(x), [](double i) { return i >= 60 && i < 70; });\n    bins[7] = std::count_if(std::begin(x), std::end(x), [](double i) { return i >= 70 && i < 80; });\n    bins[8] = std::count_if(std::begin(x), std::end(x), [](double i) { return i >= 80 && i < 90; });\n    bins[9] = std::count_if(std::begin(x), std::end(x), [](double i) { return i >= 90 && i <= 100; });\n}",
            "for (size_t i = 0; i < 10; i++) {\n    bins[i] = 0;\n  }\n  for (double const& xi : x) {\n    bins[floor(xi / 10.0)]++;\n  }\n}",
            "for (double xi : x) {\n        size_t bin = 0;\n        if (xi <= 10.0)\n            bin = 0;\n        else if (xi <= 20.0)\n            bin = 1;\n        else if (xi <= 30.0)\n            bin = 2;\n        else if (xi <= 40.0)\n            bin = 3;\n        else if (xi <= 50.0)\n            bin = 4;\n        else if (xi <= 60.0)\n            bin = 5;\n        else if (xi <= 70.0)\n            bin = 6;\n        else if (xi <= 80.0)\n            bin = 7;\n        else if (xi <= 90.0)\n            bin = 8;\n        else\n            bin = 9;\n\n        bins[bin] += 1;\n    }\n}",
            "int i, j;\n    for (i = 0; i < 10; ++i)\n        bins[i] = 0;\n    for (j = 0; j < x.size(); ++j) {\n        int index = floor(x[j]/10);\n        if (index < 0 || index > 9) {\n            std::cout << \"Value \" << x[j] << \" is out of bounds \" << index << \"\\n\";\n            throw \"Value is out of bounds\";\n        }\n        bins[index]++;\n    }\n}",
            "for (auto const& val : x) {\n        if (val < 10) {\n            ++bins[0];\n        } else if (val < 20) {\n            ++bins[1];\n        } else if (val < 30) {\n            ++bins[2];\n        } else if (val < 40) {\n            ++bins[3];\n        } else if (val < 50) {\n            ++bins[4];\n        } else if (val < 60) {\n            ++bins[5];\n        } else if (val < 70) {\n            ++bins[6];\n        } else if (val < 80) {\n            ++bins[7];\n        } else if (val < 90) {\n            ++bins[8];\n        } else {\n            ++bins[9];\n        }\n    }\n}",
            "// Initialize bins to 0\n    std::fill(bins.begin(), bins.end(), 0);\n\n    // Loop through elements in x\n    for(double xval : x) {\n        // For each value, we will check if it is within\n        // [0, 10), [10, 20), etc.\n        int which = xval/10;\n\n        // This is the value in bins that we want to modify\n        if(which >= 0 && which < 10)\n            bins[which]++;\n        else\n            std::cout << \"Error! Value out of range: \" << which << std::endl;\n    }\n}",
            "std::array<size_t, 10> ctr = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };\n   for(double value : x) {\n      size_t idx = size_t(std::floor(value / 10.0));\n      ctr[idx]++;\n   }\n   bins = ctr;\n}",
            "// std::array<size_t, 10> bins{};\n\n  // TODO: Implement this function.\n  for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] < 10) {\n      bins[0]++;\n    } else if (x[i] >= 10 && x[i] < 20) {\n      bins[1]++;\n    } else if (x[i] >= 20 && x[i] < 30) {\n      bins[2]++;\n    } else if (x[i] >= 30 && x[i] < 40) {\n      bins[3]++;\n    } else if (x[i] >= 40 && x[i] < 50) {\n      bins[4]++;\n    } else if (x[i] >= 50 && x[i] < 60) {\n      bins[5]++;\n    } else if (x[i] >= 60 && x[i] < 70) {\n      bins[6]++;\n    } else if (x[i] >= 70 && x[i] < 80) {\n      bins[7]++;\n    } else if (x[i] >= 80 && x[i] < 90) {\n      bins[8]++;\n    } else if (x[i] >= 90 && x[i] < 100) {\n      bins[9]++;\n    } else {\n      throw std::invalid_argument(\"Invalid value for x: \" + std::to_string(x[i]));\n    }\n  }\n}",
            "bins = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] >= 0 && x[i] < 10) {\n            bins[0]++;\n        }\n        else if (x[i] >= 10 && x[i] < 20) {\n            bins[1]++;\n        }\n        else if (x[i] >= 20 && x[i] < 30) {\n            bins[2]++;\n        }\n        else if (x[i] >= 30 && x[i] < 40) {\n            bins[3]++;\n        }\n        else if (x[i] >= 40 && x[i] < 50) {\n            bins[4]++;\n        }\n        else if (x[i] >= 50 && x[i] < 60) {\n            bins[5]++;\n        }\n        else if (x[i] >= 60 && x[i] < 70) {\n            bins[6]++;\n        }\n        else if (x[i] >= 70 && x[i] < 80) {\n            bins[7]++;\n        }\n        else if (x[i] >= 80 && x[i] < 90) {\n            bins[8]++;\n        }\n        else if (x[i] >= 90 && x[i] <= 100) {\n            bins[9]++;\n        }\n    }\n}",
            "bins[0] = 0;\n\tfor (auto i = 0; i < x.size(); ++i) {\n\t\tauto bin = static_cast<size_t>(floor(x[i])/10.0);\n\t\tif (bin >= 10) {\n\t\t\tbin = 9;\n\t\t}\n\t\tbins[bin] += 1;\n\t}\n}",
            "// write code here\n}",
            "size_t n = x.size();\n  for(auto i=0u; i<n; ++i) {\n    int bin_idx = (x[i]/10.0);\n    if(bin_idx<0)\n      bin_idx = 0;\n    else if(bin_idx >= 10)\n      bin_idx = 9;\n    bins[bin_idx]++;\n  }\n}",
            "size_t n = x.size();\n    for (size_t i = 0; i < 10; ++i) {\n        bins[i] = 0;\n    }\n    for (size_t i = 0; i < n; ++i) {\n        bins[std::floor(x[i]/10.0) % 10] += 1;\n    }\n}",
            "for (auto const& xval: x) {\n        if (xval < 0.0) {\n            continue;\n        }\n        if (xval > 100.0) {\n            continue;\n        }\n        size_t xbin = size_t(xval / 10.0);\n        if (xbin >= bins.size()) {\n            xbin = bins.size() - 1;\n        }\n        bins[xbin] += 1;\n    }\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] < 10) bins[0]++;\n        else if (x[i] < 20) bins[1]++;\n        else if (x[i] < 30) bins[2]++;\n        else if (x[i] < 40) bins[3]++;\n        else if (x[i] < 50) bins[4]++;\n        else if (x[i] < 60) bins[5]++;\n        else if (x[i] < 70) bins[6]++;\n        else if (x[i] < 80) bins[7]++;\n        else if (x[i] < 90) bins[8]++;\n        else bins[9]++;\n    }\n}",
            "std::array<double, 10> edges = {0, 10, 20, 30, 40, 50, 60, 70, 80, 90};\n\n   for (size_t i = 0; i < x.size(); ++i) {\n      for (size_t j = 0; j < edges.size() - 1; ++j) {\n         if (x[i] >= edges[j] && x[i] < edges[j + 1]) {\n            ++bins[j];\n            break;\n         }\n      }\n   }\n}",
            "for (int i = 0; i < x.size(); ++i) {\n       size_t bin = (x.at(i) - 10)/10;\n       ++bins[bin];\n   }\n}",
            "std::array<size_t, 10> counts;\n    for (auto xi : x) {\n        int idx = int(xi/10);\n        counts[idx]++;\n    }\n    bins = counts;\n}",
            "assert(x.size() >= 10);\n    //... your code here...\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] >= 0.0 && x[i] < 10.0) {\n            bins[0]++;\n        } else if (x[i] >= 10.0 && x[i] < 20.0) {\n            bins[1]++;\n        } else if (x[i] >= 20.0 && x[i] < 30.0) {\n            bins[2]++;\n        } else if (x[i] >= 30.0 && x[i] < 40.0) {\n            bins[3]++;\n        } else if (x[i] >= 40.0 && x[i] < 50.0) {\n            bins[4]++;\n        } else if (x[i] >= 50.0 && x[i] < 60.0) {\n            bins[5]++;\n        } else if (x[i] >= 60.0 && x[i] < 70.0) {\n            bins[6]++;\n        } else if (x[i] >= 70.0 && x[i] < 80.0) {\n            bins[7]++;\n        } else if (x[i] >= 80.0 && x[i] < 90.0) {\n            bins[8]++;\n        } else {\n            bins[9]++;\n        }\n    }\n}",
            "// TODO: Your code here\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  for (auto const& val : x) {\n    size_t const bin = val / 10;\n    if (bin >= bins.size())\n      continue;\n    bins[bin]++;\n  }\n}",
            "for (auto& val : x) {\n        bins[(int)(val/10)].",
            "size_t ix = 0;\n    size_t iy = 0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        ix = x[i]/10;\n        iy = 9-ix;\n        ++bins[iy];\n    }\n}",
            "// TODO\n}",
            "for(double i : x) {\n\t\tif(i < 10) {\n\t\t\tbins[0]++;\n\t\t}\n\t\telse if(i < 20) {\n\t\t\tbins[1]++;\n\t\t}\n\t\telse if(i < 30) {\n\t\t\tbins[2]++;\n\t\t}\n\t\telse if(i < 40) {\n\t\t\tbins[3]++;\n\t\t}\n\t\telse if(i < 50) {\n\t\t\tbins[4]++;\n\t\t}\n\t\telse if(i < 60) {\n\t\t\tbins[5]++;\n\t\t}\n\t\telse if(i < 70) {\n\t\t\tbins[6]++;\n\t\t}\n\t\telse if(i < 80) {\n\t\t\tbins[7]++;\n\t\t}\n\t\telse if(i < 90) {\n\t\t\tbins[8]++;\n\t\t}\n\t\telse {\n\t\t\tbins[9]++;\n\t\t}\n\t}\n}",
            "bins.fill(0);\n    for (auto value : x) {\n        size_t index = std::floor(value * 10.0 / 100.0);\n        if (index < 10)\n            bins[index] += 1;\n    }\n}",
            "for (double d: x) {\n        if (d >= 0 && d < 10)\n            bins[0]++;\n        else if (d >= 10 && d < 20)\n            bins[1]++;\n        else if (d >= 20 && d < 30)\n            bins[2]++;\n        else if (d >= 30 && d < 40)\n            bins[3]++;\n        else if (d >= 40 && d < 50)\n            bins[4]++;\n        else if (d >= 50 && d < 60)\n            bins[5]++;\n        else if (d >= 60 && d < 70)\n            bins[6]++;\n        else if (d >= 70 && d < 80)\n            bins[7]++;\n        else if (d >= 80 && d < 90)\n            bins[8]++;\n        else if (d >= 90 && d < 100)\n            bins[9]++;\n    }\n}",
            "bins.fill(0);\n\n  for(auto const& xi : x) {\n\n    if(xi >= 0.0 && xi <= 10.0) {\n      bins[0] += 1;\n    } else if(xi > 10.0 && xi <= 20.0) {\n      bins[1] += 1;\n    } else if(xi > 20.0 && xi <= 30.0) {\n      bins[2] += 1;\n    } else if(xi > 30.0 && xi <= 40.0) {\n      bins[3] += 1;\n    } else if(xi > 40.0 && xi <= 50.0) {\n      bins[4] += 1;\n    } else if(xi > 50.0 && xi <= 60.0) {\n      bins[5] += 1;\n    } else if(xi > 60.0 && xi <= 70.0) {\n      bins[6] += 1;\n    } else if(xi > 70.0 && xi <= 80.0) {\n      bins[7] += 1;\n    } else if(xi > 80.0 && xi <= 90.0) {\n      bins[8] += 1;\n    } else if(xi > 90.0) {\n      bins[9] += 1;\n    }\n\n  }\n}",
            "// TODO: Your code goes here\n\n  // Initializing the array\n  for (size_t i = 0; i < bins.size(); i++) {\n    bins[i] = 0;\n  }\n  // Using for loop to count the values from 0 to 10 in 10 different intervals\n  for (size_t i = 0; i < x.size(); i++) {\n    size_t a = floor(x[i] / 10.0);\n    bins[a] += 1;\n  }\n\n  //std::cout << \"The value of bins after the first count: \" << std::endl;\n  //std::cout << bins[0] << \" \" << bins[1] << \" \" << bins[2] << \" \" << bins[3] << \" \" << bins[4] << \" \" << bins[5] << \" \" << bins[6] << \" \" << bins[7] << \" \" << bins[8] << \" \" << bins[9] << std::endl;\n}",
            "bins.fill(0);\n    for (auto const& val : x) {\n        size_t ind = (val / 10);\n        if (ind >= 10)\n            ind = 9;\n        ++bins[ind];\n    }\n}",
            "for (auto const& elem : x) {\n        // TODO\n    }\n}",
            "bins.fill(0);\n    for (double xi : x) {\n        int binId = (int)(10 * xi);\n        if (binId >= 0 && binId < 10) {\n            bins[binId] += 1;\n        }\n    }\n}",
            "std::vector<double> v = x;\n\tstd::sort(v.begin(), v.end());\n\t\n\tsize_t i = 0;\n\tdouble prev = 0;\n\tfor (auto& value : v) {\n\t\tif (value < 10) {\n\t\t\tbins[0]++;\n\t\t}\n\t\telse if (value >= 10 && value < 20) {\n\t\t\tbins[1]++;\n\t\t}\n\t\telse if (value >= 20 && value < 30) {\n\t\t\tbins[2]++;\n\t\t}\n\t\telse if (value >= 30 && value < 40) {\n\t\t\tbins[3]++;\n\t\t}\n\t\telse if (value >= 40 && value < 50) {\n\t\t\tbins[4]++;\n\t\t}\n\t\telse if (value >= 50 && value < 60) {\n\t\t\tbins[5]++;\n\t\t}\n\t\telse if (value >= 60 && value < 70) {\n\t\t\tbins[6]++;\n\t\t}\n\t\telse if (value >= 70 && value < 80) {\n\t\t\tbins[7]++;\n\t\t}\n\t\telse if (value >= 80 && value < 90) {\n\t\t\tbins[8]++;\n\t\t}\n\t\telse if (value >= 90 && value < 100) {\n\t\t\tbins[9]++;\n\t\t}\n\t\telse {\n\t\t\tstd::cerr << \"Something wrong!!!\\n\";\n\t\t}\n\t}\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  // TODO: Your code goes here\n}",
            "// TODO: Fill in your solution here.\n}",
            "std::array<size_t, 10> hist = {0,0,0,0,0,0,0,0,0,0};\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t binId = (size_t) (x[i] / 10.0);\n        hist[binId] += 1;\n    }\n    bins = hist;\n}",
            "// TODO: Your code goes here\n}",
            "for(size_t i = 0; i < x.size(); ++i) {\n        size_t bin = std::floor(x[i]/10);\n        if (bin == 10) {\n            bin = 9;\n        }\n        ++bins[bin];\n    }\n}",
            "// TODO: YOUR CODE HERE\n\n}",
            "bins.fill(0);\n  for (auto const& value : x) {\n    auto bin = (size_t)std::floor(value / 10);\n    bins[bin]++;\n  }\n}",
            "assert(x.size() >= 10);\n    for (int i = 0; i < 10; ++i) {\n        bins[i] = 0;\n    }\n    for (double const& value : x) {\n        ++bins[value / 10];\n    }\n}",
            "for (auto& xi : x) {\n    if (xi < 0)\n      xi = 0;\n    if (xi >= 100)\n      xi = 99.99999;\n    bins[xi/10] += 1;\n  }\n}",
            "size_t N = x.size();\n    for (size_t i = 0; i < N; i++) {\n        size_t bin = static_cast<size_t>(10.0 * x[i]);\n        bins[bin]++;\n    }\n}",
            "auto const& bins_start = [](size_t i) {\n    return (i == 0)? 0.0 : 10.0 * i;\n  };\n  auto const& bins_end = [](size_t i) {\n    return 10.0 * (i + 1);\n  };\n\n  // Initialize bins to all zeros\n  bins.fill(0);\n\n  for(auto const& val : x) {\n    // Find the bin that val belongs in\n    size_t bin_index = 0;\n    for(; bin_index < bins.size() && val >= bins_start(bin_index); ++bin_index) { }\n    // The bin index is one past the last valid bin, so decrement\n    --bin_index;\n\n    // If val falls in this bin, increment the count\n    if(val < bins_end(bin_index)) {\n      ++bins[bin_index];\n    }\n  }\n}",
            "for (double xi : x) {\n        int i = (int)(xi / 10);\n        if (i >= 0 && i < 10) bins[i]++;\n    }\n}",
            "// TODO\n}",
            "assert(x.size() == 10);\n    bins.fill(0);\n    //...\n}",
            "bins.fill(0);\n   for (double val : x) {\n      int bin = static_cast<int>(val/10);\n      bins.at(bin)++;\n   }\n}",
            "// TODO: Your code here\n}",
            "// TODO:\n    //   Complete the function to count values in each of the ten bins\n    //   (0-10, 10-20,... 90-100) and store the counts in the `bins` array.\n    //   Use a `for` loop and `if` statement.\n    // Hint:\n    //   To test a condition, use the `operator<` and `operator<=` on doubles\n    //   to compare two numbers.\n    //   Hint:\n    //     `a < b` is true if `a` is strictly less than `b`\n    //     `a <= b` is true if `a` is less than or equal to `b`\n    //     `a == b` is true if `a` is equal to `b`\n    //     `a!= b` is true if `a` is not equal to `b`\n    //     `a > b` is true if `a` is greater than `b`\n    //     `a >= b` is true if `a` is greater than or equal to `b`\n    //     Note: The comparison operators only work on doubles.\n    for (auto xi : x) {\n        if (xi < 10) {\n            bins[0] += 1;\n        } else if (xi >= 10 && xi < 20) {\n            bins[1] += 1;\n        } else if (xi >= 20 && xi < 30) {\n            bins[2] += 1;\n        } else if (xi >= 30 && xi < 40) {\n            bins[3] += 1;\n        } else if (xi >= 40 && xi < 50) {\n            bins[4] += 1;\n        } else if (xi >= 50 && xi < 60) {\n            bins[5] += 1;\n        } else if (xi >= 60 && xi < 70) {\n            bins[6] += 1;\n        } else if (xi >= 70 && xi < 80) {\n            bins[7] += 1;\n        } else if (xi >= 80 && xi < 90) {\n            bins[8] += 1;\n        } else if (xi >= 90) {\n            bins[9] += 1;\n        }\n    }\n}",
            "double min = *std::min_element(x.begin(), x.end());\n\tdouble max = *std::max_element(x.begin(), x.end());\n\n\tfor (auto i = 0; i < 10; ++i) {\n\t\tdouble start = min + i * 10;\n\t\tdouble end = start + 10;\n\n\t\tsize_t count = 0;\n\t\tfor (auto val : x) {\n\t\t\tif (val >= start && val < end)\n\t\t\t\t++count;\n\t\t}\n\n\t\tbins[i] = count;\n\t}\n}",
            "// HINT: Use a for loop\n  // HINT: Use std::vector<double>::iterator to walk through x\n  // HINT: Use std::array<size_t, 10>::iterator to walk through bins\n  // HINT: Use std::distance(begin, end) to compute the distance between begin and end\n  // HINT: Use std::distance(begin, x.begin() + 10) to compute the distance between begin and x.begin() + 10\n  // HINT: Use std::distance(begin, x.end()) to compute the distance between begin and x.end()\n  // HINT: Use std::distance(x.begin(), x.begin() + 10) to compute the distance between x.begin() and x.begin() + 10\n  // HINT: Use std::distance(x.begin(), x.end()) to compute the distance between x.begin() and x.end()\n  // HINT: Use std::distance(x.begin() + 10, x.end()) to compute the distance between x.begin() + 10 and x.end()\n  // HINT: Use std::distance(x.end() - 10, x.end()) to compute the distance between x.end() - 10 and x.end()\n  // HINT: Use x.begin() + 10 to set the iterator to the 10th value\n  // HINT: Use std::advance(iterator, 10) to advance the iterator 10 steps\n  // HINT: Use std::advance(iterator, -10) to retreat the iterator 10 steps\n\n  for (auto it = x.begin(); it!= x.end(); it++) {\n    if ((*it) >= 0 && (*it) < 10) {\n      bins[0]++;\n    } else if ((*it) >= 10 && (*it) < 20) {\n      bins[1]++;\n    } else if ((*it) >= 20 && (*it) < 30) {\n      bins[2]++;\n    } else if ((*it) >= 30 && (*it) < 40) {\n      bins[3]++;\n    } else if ((*it) >= 40 && (*it) < 50) {\n      bins[4]++;\n    } else if ((*it) >= 50 && (*it) < 60) {\n      bins[5]++;\n    } else if ((*it) >= 60 && (*it) < 70) {\n      bins[6]++;\n    } else if ((*it) >= 70 && (*it) < 80) {\n      bins[7]++;\n    } else if ((*it) >= 80 && (*it) < 90) {\n      bins[8]++;\n    } else {\n      bins[9]++;\n    }\n  }\n  return;\n}",
            "size_t bin_index = 0;\n  for(auto val : x) {\n    if(val < 10) {\n      ++bins[bin_index];\n    } else {\n      ++bin_index;\n    }\n  }\n}",
            "std::vector<double> v10(x.size());\n  for (auto i = 0; i < x.size(); ++i) {\n    v10[i] = floor(x[i]/10.0);\n  }\n  std::sort(v10.begin(), v10.end());\n  size_t j = 0;\n  bins.fill(0);\n  for (auto i = 0; i < x.size(); ++i) {\n    if (i == v10[j] * 10) {\n      ++j;\n    }\n    ++bins[(size_t)v10[j] - 1];\n  }\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n        bins[static_cast<size_t>(x[i]) / 10]++;\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "hip",
        "prompt": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use AMD HIP to compute in parallel. The kernel is initialized with at least as many threads as values in x.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\n__global__ void binsBy10Count(const double *x, size_t N, size_t[10] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// Your code here\n}",
            "for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n    size_t bin = 0;\n    if (x[i] < 10) {\n      bin = 0;\n    } else if (x[i] < 20) {\n      bin = 1;\n    } else if (x[i] < 30) {\n      bin = 2;\n    } else if (x[i] < 40) {\n      bin = 3;\n    } else if (x[i] < 50) {\n      bin = 4;\n    } else if (x[i] < 60) {\n      bin = 5;\n    } else if (x[i] < 70) {\n      bin = 6;\n    } else if (x[i] < 80) {\n      bin = 7;\n    } else if (x[i] < 90) {\n      bin = 8;\n    } else {\n      bin = 9;\n    }\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "__shared__ size_t counts[10];\n  const int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (gid < N) {\n    size_t i = x[gid]/10;\n    atomicAdd(&counts[i], 1);\n  }\n  __syncthreads();\n  for (int i = 0; i < 10; i++) {\n    atomicAdd(&bins[i], counts[i]);\n  }\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    size_t bin = (size_t) ((x[i] - 1) / 10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n   if (i < N) {\n      int bin = (x[i]/10) - 1;\n      if (bin < 0 || bin > 9) {\n         printf(\"WARNING: bin = %d is out of bounds. x[i] = %f\\n\", bin, x[i]);\n         bin = 0;\n      }\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "// Threads 0, 1,... 9 will be responsible for counting elements\n    // in the range [0, 10), [10, 20), [20, 30),... respectively\n    int bin = threadIdx.x;\n    size_t count = 0;\n    for (int i = bin; i < N; i += 10) {\n        if (x[i] >= bin * 10 && x[i] < (bin + 1) * 10) {\n            count++;\n        }\n    }\n    bins[bin] = count;\n}",
            "const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  const int threadCount = blockDim.x * gridDim.x;\n  size_t i = 0;\n  for (; i + tid < N; i += threadCount) {\n    if (x[i] < 10) bins[0]++;\n    else if (x[i] < 20) bins[1]++;\n    else if (x[i] < 30) bins[2]++;\n    else if (x[i] < 40) bins[3]++;\n    else if (x[i] < 50) bins[4]++;\n    else if (x[i] < 60) bins[5]++;\n    else if (x[i] < 70) bins[6]++;\n    else if (x[i] < 80) bins[7]++;\n    else if (x[i] < 90) bins[8]++;\n    else if (x[i] < 100) bins[9]++;\n  }\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n   if (tid < N) {\n      bins[tid / 10]++;\n   }\n}",
            "size_t tid = threadIdx.x;\n   size_t i = blockIdx.x * blockDim.x + tid;\n   if (i < N) {\n      size_t j = (size_t) floor(x[i] / 10.0);\n      atomicAdd(&bins[j], 1);\n   }\n}",
            "const size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n  for (size_t i = tid; i < N; i += gridDim.x * blockDim.x) {\n    const size_t bin = (i % 10) * 2;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// declare your variables here\n  // initialize the bins\n  for (int i = 0; i < 10; i++) {\n    bins[i] = 0;\n  }\n  // compute the bin counts and update the bins\n  // declare your variables here\n  for (int i = threadIdx.x; i < N; i += blockDim.x) {\n    if (x[i] > 0 && x[i] < 10) {\n      atomicAdd(&bins[0], 1);\n    } else if (x[i] > 10 && x[i] < 20) {\n      atomicAdd(&bins[1], 1);\n    } else if (x[i] > 20 && x[i] < 30) {\n      atomicAdd(&bins[2], 1);\n    } else if (x[i] > 30 && x[i] < 40) {\n      atomicAdd(&bins[3], 1);\n    } else if (x[i] > 40 && x[i] < 50) {\n      atomicAdd(&bins[4], 1);\n    } else if (x[i] > 50 && x[i] < 60) {\n      atomicAdd(&bins[5], 1);\n    } else if (x[i] > 60 && x[i] < 70) {\n      atomicAdd(&bins[6], 1);\n    } else if (x[i] > 70 && x[i] < 80) {\n      atomicAdd(&bins[7], 1);\n    } else if (x[i] > 80 && x[i] < 90) {\n      atomicAdd(&bins[8], 1);\n    } else if (x[i] > 90 && x[i] < 100) {\n      atomicAdd(&bins[9], 1);\n    }\n  }\n}",
            "// Fill this in\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n    if (tid < N) {\n        bins[floor(x[tid] / 10.0)]++;\n    }\n}",
            "const size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    bins[i / 10] += 1;\n  }\n}",
            "// TODO\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        size_t idx = (x[i] / 10) % 10;\n        atomicAdd(&bins[idx], 1);\n    }\n}",
            "// This is a stub; fill in code to make it run\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        bins[tid / 10]++;\n    }\n}",
            "size_t idx = threadIdx.x;\n\n    // 1. Declare a local variable to hold the index of the current value.\n    size_t valueIdx = idx;\n\n    // 2. Declare an iterator variable that starts at zero and increments by 10.\n    size_t start = 0;\n\n    // 3. Compute the end of the current block of 10 values.\n    size_t end = min(N, start + 10);\n\n    // 4. Iterate over the block of 10 values.\n    while (start < N) {\n\n        // 5. Increase the start index by 10.\n        start += 10;\n\n        // 6. Decrease the end index by 10.\n        end -= 10;\n\n        // 7. If the value index is between the start and end indices, then increment the current bin.\n        if (valueIdx >= start && valueIdx < end) {\n            atomicAdd(&bins[valueIdx - start], 1);\n        }\n    }\n}",
            "for (int i = threadIdx.x; i < N; i += blockDim.x) {\n        int bin = (int)(floor(x[i]/10));\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "size_t idx = blockIdx.x*blockDim.x + threadIdx.x;\n\n  if (idx < N) {\n    int b = (x[idx] + 10) / 10;\n    atomicAdd(&bins[b], 1);\n  }\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n   if(idx < N) {\n      int val = (int)x[idx];\n      if (val >= 0 && val < 10)\n         atomicAdd(&bins[val], 1);\n   }\n}",
            "//TODO\n}",
            "/* Start your code here */\n  if (threadIdx.x >= N) return;\n  int bin = threadIdx.x / 10;\n  if (x[threadIdx.x] >= 0 && x[threadIdx.x] < 10) {\n    atomicAdd(&bins[bin], 1);\n  }\n  /* End your code here */\n}",
            "int i = threadIdx.x;\n\n  __shared__ double sdata[256];\n\n  double x1 = 0;\n  x1 = x[i];\n\n  // sdata[threadIdx.x] = x1;\n  sdata[threadIdx.x] = 0;\n\n  // Each thread puts its local sum into shared memory\n  __syncthreads();\n\n  for (int s = 1; s < blockDim.x; s *= 2) {\n    if (i % (2 * s) == 0) {\n      sdata[i] = sdata[i] + sdata[i + s];\n    }\n\n    // ensure all threads have finished writing to shared memory\n    __syncthreads();\n  }\n\n  bins[i] = sdata[i];\n}",
            "// initialize thread counter\n    int count = 0;\n\n    for (int i = 0; i < N; i++) {\n\n        // if number is between 0 and 10, increase count\n        if (x[i] >= 0 && x[i] < 10) {\n            count++;\n        }\n    }\n\n    // store count in the corresponding bin\n    bins[0] = count;\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (i < N) {\n        bins[(size_t)((x[i] + 9) / 10)]++;\n    }\n}",
            "for(size_t i=blockIdx.x*blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    // replace this with your code\n    //bins[0] += (x[i] > 100 || x[i] < 0)? 1 : 0;\n    //bins[1] += (x[i] > 10 && x[i] < 20)? 1 : 0;\n    //bins[2] += (x[i] > 20 && x[i] < 30)? 1 : 0;\n    //bins[3] += (x[i] > 30 && x[i] < 40)? 1 : 0;\n    //bins[4] += (x[i] > 40 && x[i] < 50)? 1 : 0;\n    //bins[5] += (x[i] > 50 && x[i] < 60)? 1 : 0;\n    //bins[6] += (x[i] > 60 && x[i] < 70)? 1 : 0;\n    //bins[7] += (x[i] > 70 && x[i] < 80)? 1 : 0;\n    //bins[8] += (x[i] > 80 && x[i] < 90)? 1 : 0;\n    //bins[9] += (x[i] > 90 && x[i] < 100)? 1 : 0;\n    if (x[i] > 100 || x[i] < 0) bins[0]++;\n    else if (x[i] > 10 && x[i] < 20) bins[1]++;\n    else if (x[i] > 20 && x[i] < 30) bins[2]++;\n    else if (x[i] > 30 && x[i] < 40) bins[3]++;\n    else if (x[i] > 40 && x[i] < 50) bins[4]++;\n    else if (x[i] > 50 && x[i] < 60) bins[5]++;\n    else if (x[i] > 60 && x[i] < 70) bins[6]++;\n    else if (x[i] > 70 && x[i] < 80) bins[7]++;\n    else if (x[i] > 80 && x[i] < 90) bins[8]++;\n    else if (x[i] > 90 && x[i] < 100) bins[9]++;\n  }\n}",
            "int i = blockIdx.x*blockDim.x + threadIdx.x;\n  if (i < N) {\n    // Note: x[i] >= 0 because it comes from a uniform distribution.\n    // Note: There is no problem with integer division here,\n    // because 10 is a power of 2 (and 3 is a power of 2), so we can\n    // use bit-masking to extract the lowest-order bits.\n    // This is a useful technique in HIP.\n    size_t bin = (x[i] / 10) & 0x3;  // x[i]%10\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "for (int i = threadIdx.x; i < N; i += blockDim.x) {\n        if (i % 10 < 10) bins[i % 10]++;\n    }\n}",
            "// TODO: Your code here\n}",
            "if (threadIdx.x < N) {\n    // TODO 1 : Implement the kernel\n  }\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    bins[(int)(((double)x[i]/10.0))%10]++;\n  }\n}",
            "__shared__ int counts[10];\n  int i = threadIdx.x;\n  counts[i] = 0;\n  __syncthreads();\n  for (size_t i = 0; i < N; i++) {\n    int bin = (int) (x[i] / 10);\n    atomicAdd(&counts[bin], 1);\n  }\n  __syncthreads();\n  if (i == 0) {\n    bins[0] = counts[0];\n    bins[1] = counts[1];\n    bins[2] = counts[2];\n    bins[3] = counts[3];\n    bins[4] = counts[4];\n    bins[5] = counts[5];\n    bins[6] = counts[6];\n    bins[7] = counts[7];\n    bins[8] = counts[8];\n    bins[9] = counts[9];\n  }\n}",
            "// TODO: implement\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if (idx < N) {\n    bins[min(9, (int)floor(10 * x[idx]))]++;\n  }\n}",
            "__shared__ double sdata[10];\n    // initialize thread private values to zero\n    size_t t = threadIdx.x;\n    size_t bin = t * 10;\n    for (size_t i = 0; i < 10; i++) {\n        bins[i] = 0;\n    }\n\n    if (t < N) {\n        size_t bin = (size_t) (x[t] / 10.0);\n        atomicAdd(&bins[bin], 1);\n    }\n    __syncthreads();\n    if (t < 10) {\n        for (size_t i = 1; i < blockDim.x; i++) {\n            atomicAdd(&bins[t], bins[t + i * 10]);\n        }\n    }\n}",
            "for (size_t idx = threadIdx.x + blockIdx.x * blockDim.x; idx < N; idx += blockDim.x * gridDim.x) {\n    bins[(size_t)((int)floor(x[idx] / 10) + 1)]++;\n  }\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n\n  __shared__ size_t localBins[10];\n\n  if (threadIdx.x < 10) {\n    localBins[threadIdx.x] = 0;\n  }\n\n  __syncthreads();\n\n  for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n    int bin = (int)((x[i] - 0) / 10);\n    atomicAdd(&localBins[bin], 1);\n  }\n\n  __syncthreads();\n\n  for (size_t i = threadIdx.x; i < 10; i += blockDim.x) {\n    atomicAdd(&bins[i], localBins[i]);\n  }\n}",
            "const size_t gtid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (gtid < N) {\n        // The thread id is the number of the current value\n        const double current = x[gtid];\n        // The bin number is the number of the value times 10\n        const double bin = current * 10.0;\n        // The number of the bin is the value of the bin rounded down (we only want the lower 5 decimal digits)\n        const size_t binNumber = (size_t)bin;\n        // We want to set the value in the bin array, so subtract the bin number from the original bin value\n        const size_t binOffset = binNumber - bin;\n        // Use an atomic add to increment the correct value in the array\n        atomicAdd(&bins[binOffset], 1);\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int i = 0;\n    if (tid < N) {\n        i = (int)x[tid];\n        atomicAdd(&bins[i / 10], 1);\n    }\n}",
            "size_t tid = threadIdx.x;\n    if (tid < N) {\n        int v = x[tid];\n        if (v >= 0 && v < 10) {\n            atomicAdd(&bins[v], 1);\n        }\n    }\n}",
            "// compute the bin\n    int bin = (int)(x[threadIdx.x]/10);\n\n    // atomic add\n    atomicAdd(bins + bin, 1);\n}",
            "/*\n   * TODO: Your code here.\n   * Hint: use atomicAdd() to increment a histogram bin counter.\n   */\n\n  double i = 0;\n  double bin = 0;\n  double j = 0;\n  double k = 0;\n\n  double *bins_d;\n\n  hipMalloc((void **)&bins_d, sizeof(double) * 10);\n  hipMemset(bins_d, 0, sizeof(double) * 10);\n\n  i = threadIdx.x + blockDim.x * blockIdx.x;\n  if (i < N) {\n    bin = (x[i] / 10) + 1;\n\n    hipMemcpy(bins_d, bins, sizeof(double) * 10, hipMemcpyHostToDevice);\n    hipDeviceSynchronize();\n\n    j = atomicAdd(bins_d + (int)bin, 1);\n    hipMemcpy(bins, bins_d, sizeof(double) * 10, hipMemcpyDeviceToHost);\n    hipDeviceSynchronize();\n  }\n\n  //  printf(\"bins = %d %d %d %d %d %d %d %d %d %d\\n\", bins[0], bins[1], bins[2], bins[3], bins[4], bins[5], bins[6], bins[7], bins[8], bins[9]);\n  return;\n}",
            "}",
            "if (threadIdx.x >= N) return;\n  int idx = (int) (x[threadIdx.x]/10);\n  atomicAdd(&bins[idx], 1);\n}",
            "// TODO:\n}",
            "// TODO: your code here\n}",
            "for (size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n         i < N;\n         i += blockDim.x * gridDim.x) {\n        double val = x[i] / 10;\n        if (val >= 0 && val < 10)\n            atomicAdd(&bins[0], 1);\n        else if (val >= 10 && val < 20)\n            atomicAdd(&bins[1], 1);\n        else if (val >= 20 && val < 30)\n            atomicAdd(&bins[2], 1);\n        else if (val >= 30 && val < 40)\n            atomicAdd(&bins[3], 1);\n        else if (val >= 40 && val < 50)\n            atomicAdd(&bins[4], 1);\n        else if (val >= 50 && val < 60)\n            atomicAdd(&bins[5], 1);\n        else if (val >= 60 && val < 70)\n            atomicAdd(&bins[6], 1);\n        else if (val >= 70 && val < 80)\n            atomicAdd(&bins[7], 1);\n        else if (val >= 80 && val < 90)\n            atomicAdd(&bins[8], 1);\n        else if (val >= 90 && val <= 100)\n            atomicAdd(&bins[9], 1);\n    }\n}",
            "int idx = threadIdx.x;\n   int stride = blockDim.x;\n   int start = idx;\n   int stop = N;\n   while (start < stop) {\n      int bin = static_cast<int>(x[start] / 10);\n      atomicAdd(&bins[bin], 1);\n      start += stride;\n   }\n}",
            "// TODO: implement this function\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        size_t bin = static_cast<size_t>(x[i] / 10.);\n        bins[bin]++;\n    }\n}",
            "if (threadIdx.x < N) {\n        // TODO: Count the number of values in [0,10), [10, 20), [20, 30),...\n        // and store the count in bins.\n        // Example:\n        // if x[tid] is between 0 and 10, count 1 in bins[0], and so on.\n        // bins[0] += x[tid] >= 0 && x[tid] < 10? 1 : 0;\n        // bins[1] += x[tid] >= 10 && x[tid] < 20? 1 : 0;\n        //...\n        // bins[9] += x[tid] >= 90 && x[tid] <= 100? 1 : 0;\n    }\n}",
            "// TODO\n}",
            "__shared__ size_t sharedBins[10];\n    if (threadIdx.x < 10)\n        sharedBins[threadIdx.x] = 0;\n    __syncthreads();\n\n    for (size_t i = blockIdx.x*blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        if (x[i] > 0. && x[i] < 10.)\n            atomicAdd(&sharedBins[0], 1);\n        else if (x[i] >= 10. && x[i] < 20.)\n            atomicAdd(&sharedBins[1], 1);\n        else if (x[i] >= 20. && x[i] < 30.)\n            atomicAdd(&sharedBins[2], 1);\n        else if (x[i] >= 30. && x[i] < 40.)\n            atomicAdd(&sharedBins[3], 1);\n        else if (x[i] >= 40. && x[i] < 50.)\n            atomicAdd(&sharedBins[4], 1);\n        else if (x[i] >= 50. && x[i] < 60.)\n            atomicAdd(&sharedBins[5], 1);\n        else if (x[i] >= 60. && x[i] < 70.)\n            atomicAdd(&sharedBins[6], 1);\n        else if (x[i] >= 70. && x[i] < 80.)\n            atomicAdd(&sharedBins[7], 1);\n        else if (x[i] >= 80. && x[i] < 90.)\n            atomicAdd(&sharedBins[8], 1);\n        else if (x[i] >= 90. && x[i] < 100.)\n            atomicAdd(&sharedBins[9], 1);\n    }\n    __syncthreads();\n\n    if (threadIdx.x < 10)\n        atomicAdd(&bins[threadIdx.x], sharedBins[threadIdx.x]);\n}",
            "// Use AMD HIP to compute in parallel.\n  // Each thread should handle a chunk of 10 values.\n  // The kernel is initialized with at least as many threads as values in x.\n\n  // compute the index of the first value in this thread's chunk\n  size_t thread_first_index = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // compute the number of values in this thread's chunk\n  size_t thread_chunk_size = (threadIdx.x + 1) * blockDim.x;\n\n  // loop through the chunk, counting the values in each bin\n  for (size_t i = thread_first_index; i < thread_chunk_size; i++) {\n    if (i < N) {\n      size_t bin_number = size_t(std::floor(x[i]/10.0));\n      if (bin_number > 9) {\n        // out of bounds; set to last bin\n        bin_number = 9;\n      }\n      atomicAdd(&bins[bin_number], 1);\n    }\n  }\n}",
            "const int tid = threadIdx.x;\n  const int nt = blockDim.x;\n\n  // each thread is assigned its own bin\n  const size_t i = tid;\n  const size_t start = i * N / nt;\n  const size_t stop = min(start + N / nt, N);\n  size_t bin = 0;\n  for (size_t j = start; j < stop; ++j) {\n    if (x[j] >= 0 && x[j] < 10) {\n      ++bin;\n    }\n  }\n  // store the counts\n  atomicAdd(bins + 0, bin);\n}",
            "size_t my_idx = threadIdx.x + blockIdx.x * blockDim.x;\n    // TODO: fill in\n}",
            "for (int idx = threadIdx.x; idx < N; idx += blockDim.x) {\n        // calculate 10x value of x[idx]\n        double f = floor(x[idx]/10.0);\n        int bin = (f < 10.0)? f : 9;\n        // increment count for bin\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO: Implement\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N)\n        return;\n    int bin = 0;\n    if (x[idx] >= 0.0 && x[idx] < 10.0) {\n        bin = 0;\n    }\n    else if (x[idx] >= 10.0 && x[idx] < 20.0) {\n        bin = 1;\n    }\n    else if (x[idx] >= 20.0 && x[idx] < 30.0) {\n        bin = 2;\n    }\n    else if (x[idx] >= 30.0 && x[idx] < 40.0) {\n        bin = 3;\n    }\n    else if (x[idx] >= 40.0 && x[idx] < 50.0) {\n        bin = 4;\n    }\n    else if (x[idx] >= 50.0 && x[idx] < 60.0) {\n        bin = 5;\n    }\n    else if (x[idx] >= 60.0 && x[idx] < 70.0) {\n        bin = 6;\n    }\n    else if (x[idx] >= 70.0 && x[idx] < 80.0) {\n        bin = 7;\n    }\n    else if (x[idx] >= 80.0 && x[idx] < 90.0) {\n        bin = 8;\n    }\n    else if (x[idx] >= 90.0 && x[idx] < 100.0) {\n        bin = 9;\n    }\n    atomicAdd(&bins[bin], 1);\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// your code here\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i >= N)\n    return;\n  size_t bin = (size_t)((x[i] + 10) / 10);\n  atomicAdd(&bins[bin], 1);\n}",
            "// compute the starting index of the thread\n    size_t start = blockDim.x * blockIdx.x + threadIdx.x;\n    // compute the ending index of the thread\n    size_t end = min(start + blockDim.x, N);\n    // create a loop that increments the start index by 1\n    for (size_t i = start; i < end; ++i) {\n        // compute the bin of the value\n        size_t bin = (size_t)((x[i] / 10.0) + 0.5);\n        // increment the value at the bin\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n        int bin = (int) floor(x[i] / 10.0);\n        assert(bin >= 0);\n        assert(bin < 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "__shared__ double shared[100];\n    __shared__ size_t counts[10];\n    //...\n}",
            "const int tid = blockDim.x*blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    const int idx = (int) floor(x[tid] / 10.0);\n    atomicAdd(&bins[idx], 1);\n  }\n}",
            "const int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  const int stride = blockDim.x * gridDim.x;\n  size_t bin_id = 0;\n\n  for (int i = tid; i < N; i += stride) {\n    bin_id = (x[i] / 10) % 10;\n    atomicAdd(&bins[bin_id], 1);\n  }\n}",
            "__shared__ size_t smem[10];\n  // initialize the shared memory array to 0\n  if (threadIdx.x < 10) smem[threadIdx.x] = 0;\n  __syncthreads();\n\n  const double *ptr = x + threadIdx.x;\n  for (size_t i = 0; i < N; i += blockDim.x) {\n    const double val = ptr[i];\n    if (val < 0.0 || val > 100.0) continue; // ignore out of range values\n    smem[val / 10]++; // store the count for the bin in shared memory\n  }\n  __syncthreads();\n\n  for (size_t i = threadIdx.x; i < 10; i += blockDim.x) {\n    atomicAdd(bins + i, smem[i]); // add the count to the global array\n  }\n}",
            "// start of a thread block\n    int i = threadIdx.x;\n    // blockIdx.x is the index of the block\n    size_t index = blockIdx.x * blockDim.x + i;\n\n    // if we are within the bounds of the array\n    if (index < N) {\n        size_t binIndex = static_cast<size_t>(x[index] / 10.0);\n        // if the value is within the bounds\n        if (binIndex < 10) {\n            atomicAdd(&bins[binIndex], 1);\n        }\n    }\n}",
            "// This kernel counts the number of values in each 10 element interval of the input array.\n  // It is called by the main application function.\n  // x is a pointer to the array of values.\n  // N is the size of the array.\n  // bins is an array to store the results of the counting.\n  // The array must be allocated on the host and passed in as an argument to the kernel.\n  // Each thread in the kernel computes the number of values in the interval corresponding to its index in the array.\n  // Each thread will update its corresponding element in bins.\n\n  // The kernel will be launched with at least as many threads as there are values in the input array.\n  // The grid dimension should be 1, and the block dimension should be the number of threads.\n  // The grid dimension should be 1, and the block dimension should be the number of threads.\n  // There should be no need to pass in the block and grid dimensions.\n  // It will be computed by the CUDA runtime.\n\n  // Get the thread index.\n  // Determine which 10 element interval the thread corresponds to.\n  // Compute the start index and end index of the interval.\n  // Count the number of values in the interval.\n  // Store the result in the corresponding element of bins.\n\n  // This is the index of the thread in the kernel.\n  int tid = threadIdx.x;\n\n  // The thread index is not used in the following example, but is used in the\n  // assignment for the next lab.\n  int index = tid;\n\n  // Each thread will compute the number of values in its interval.\n  // This is the interval size.\n  int size = 10;\n\n  // The intervals in the array are 10 values long.\n  // The number of elements in the array is N.\n  // The number of intervals is N/10.\n  // The final index is N, so the total number of values is N+1.\n  int nintervals = N / size;\n\n  // The last interval will have fewer values.\n  if (index < N % size) {\n    // Add 1 to the number of values in the interval.\n    ++nintervals;\n  }\n\n  // The last interval may have fewer than 10 values.\n  // The start index of the last interval will be N-size.\n  int start_index = index * size;\n\n  // The end index of the last interval will be N.\n  int end_index = min(start_index + size, N);\n\n  // The number of values in the interval.\n  int n = 0;\n\n  // Count the number of values in the interval.\n  for (int i = start_index; i < end_index; ++i) {\n    ++n;\n  }\n\n  // Each thread will update the corresponding element in bins.\n  // This is the thread index in the bins array.\n  int bin_index = index / size;\n\n  // Update the corresponding element in bins with the number of values in the interval.\n  bins[bin_index] = n;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    size_t bin = 0;\n    if (x[i] >= 0 && x[i] < 10)\n      bin = 0;\n    else if (x[i] >= 10 && x[i] < 20)\n      bin = 1;\n    else if (x[i] >= 20 && x[i] < 30)\n      bin = 2;\n    else if (x[i] >= 30 && x[i] < 40)\n      bin = 3;\n    else if (x[i] >= 40 && x[i] < 50)\n      bin = 4;\n    else if (x[i] >= 50 && x[i] < 60)\n      bin = 5;\n    else if (x[i] >= 60 && x[i] < 70)\n      bin = 6;\n    else if (x[i] >= 70 && x[i] < 80)\n      bin = 7;\n    else if (x[i] >= 80 && x[i] < 90)\n      bin = 8;\n    else if (x[i] >= 90 && x[i] <= 100)\n      bin = 9;\n    else\n      bin = 10;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO: Replace 0 with the number of threads in this kernel\n    size_t tid = 0;\n\n    // TODO: Implement\n    for (; tid < N; ++tid) {\n        if (x[tid] > 10 * tid)\n            ++bins[tid];\n    }\n}",
            "// TODO\n}",
            "for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += gridDim.x * blockDim.x) {\n    if (x[i] < 10) {\n      atomicAdd(&bins[0], 1);\n    } else if (x[i] < 20) {\n      atomicAdd(&bins[1], 1);\n    } else if (x[i] < 30) {\n      atomicAdd(&bins[2], 1);\n    } else if (x[i] < 40) {\n      atomicAdd(&bins[3], 1);\n    } else if (x[i] < 50) {\n      atomicAdd(&bins[4], 1);\n    } else if (x[i] < 60) {\n      atomicAdd(&bins[5], 1);\n    } else if (x[i] < 70) {\n      atomicAdd(&bins[6], 1);\n    } else if (x[i] < 80) {\n      atomicAdd(&bins[7], 1);\n    } else if (x[i] < 90) {\n      atomicAdd(&bins[8], 1);\n    } else if (x[i] < 100) {\n      atomicAdd(&bins[9], 1);\n    } else {\n      printf(\"Input value %lf is not between 0 and 100.\\n\", x[i]);\n    }\n  }\n}",
            "// Start with an empty set of bins\n  for (int i = 0; i < 10; i++) bins[i] = 0;\n\n  // Each thread gets a value of x to process\n  size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n\n  // Aggregate the values by counting in the bins\n  while (i < N) {\n    // Find the bin that the value belongs in\n    size_t bin = (x[i] / 10);\n\n    // Add 1 to the bin count\n    atomicAdd(&(bins[bin]), 1);\n    i += blockDim.x * gridDim.x;\n  }\n}",
            "extern __shared__ double shared[];\n\n  size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n\n  size_t t_start = blockIdx.x * blockDim.x;\n  size_t t_end = t_start + blockDim.x;\n  if (t_end > N) t_end = N;\n\n  size_t i = t_start + threadIdx.x;\n  size_t stride = blockDim.x * gridDim.x;\n\n  size_t idx, cnt;\n\n  if (threadIdx.x == 0) {\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    bins[4] = 0;\n    bins[5] = 0;\n    bins[6] = 0;\n    bins[7] = 0;\n    bins[8] = 0;\n    bins[9] = 0;\n  }\n  __syncthreads();\n\n  while (i < t_end) {\n    idx = (size_t)(x[i] / 10);\n    cnt = atomicAdd(&shared[idx], 1);\n    if (cnt == 0) {\n      atomicAdd(&bins[idx], 1);\n    }\n    i += stride;\n  }\n\n  __syncthreads();\n\n  if (threadIdx.x == 0) {\n    atomicAdd(&bins[0], shared[0]);\n    atomicAdd(&bins[1], shared[1]);\n    atomicAdd(&bins[2], shared[2]);\n    atomicAdd(&bins[3], shared[3]);\n    atomicAdd(&bins[4], shared[4]);\n    atomicAdd(&bins[5], shared[5]);\n    atomicAdd(&bins[6], shared[6]);\n    atomicAdd(&bins[7], shared[7]);\n    atomicAdd(&bins[8], shared[8]);\n    atomicAdd(&bins[9], shared[9]);\n  }\n\n  __syncthreads();\n}",
            "size_t threadId = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t stride = blockDim.x * gridDim.x;\n\n  for (size_t i = threadId; i < N; i += stride) {\n    size_t bin = (x[i] / 10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO\n    size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        if (x[i] >= 0 && x[i] < 10)\n            atomicAdd(&bins[0], 1);\n        else if (x[i] >= 10 && x[i] < 20)\n            atomicAdd(&bins[1], 1);\n        else if (x[i] >= 20 && x[i] < 30)\n            atomicAdd(&bins[2], 1);\n        else if (x[i] >= 30 && x[i] < 40)\n            atomicAdd(&bins[3], 1);\n        else if (x[i] >= 40 && x[i] < 50)\n            atomicAdd(&bins[4], 1);\n        else if (x[i] >= 50 && x[i] < 60)\n            atomicAdd(&bins[5], 1);\n        else if (x[i] >= 60 && x[i] < 70)\n            atomicAdd(&bins[6], 1);\n        else if (x[i] >= 70 && x[i] < 80)\n            atomicAdd(&bins[7], 1);\n        else if (x[i] >= 80 && x[i] < 90)\n            atomicAdd(&bins[8], 1);\n        else if (x[i] >= 90 && x[i] < 100)\n            atomicAdd(&bins[9], 1);\n    }\n}",
            "size_t i = threadIdx.x;\n    if (i < N) {\n        bins[(int)(x[i] / 10)] += 1;\n    }\n}",
            "// Each thread gets a number to work on, use the index from the thread to\n    // find the right element in x.\n    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N) {\n        // Find the bin the element should go into and increment the counter.\n        size_t bin = (size_t)(x[i] / 10.0) % 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO: implement me!\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    const int idx = (int) (x[tid] / 10.0);\n    atomicAdd(&bins[idx], 1);\n  }\n}",
            "const size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i >= N) {\n        return;\n    }\n    const size_t bin = (size_t) (i / 10) % 10;\n    atomicAdd(&bins[bin], 1);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    bins[(int)((x[i] / 10.0)) % 10] += 1;\n  }\n}",
            "}",
            "size_t start = (blockIdx.x * blockDim.x) + threadIdx.x;\n  size_t end = min(start + blockDim.x, N);\n  size_t i;\n\n  for (i = start; i < end; ++i) {\n    size_t bin = (size_t)(x[i]/10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "const double start = 0.0;\n    const double end = 10.0;\n    const double binsize = 0.1;\n\n    // TODO\n\n}",
            "// HIP note: threads block is not specified here, so\n  // will run at least as many threads as values in x.\n\n  // HIP note: blockIdx and threadIdx are both 3-D (x, y, z).\n  // They are only 1-D (index) if running in 1-D grid.\n\n  // HIP note: we have to initialize the shared memory in the kernel because it's\n  // a global memory allocation and we don't know if we'll be the first\n  // kernel to run on the device.\n  __shared__ int sharedBins[10];\n  if (threadIdx.x == 0) {\n    for (int i = 0; i < 10; i++) sharedBins[i] = 0;\n  }\n  __syncthreads();\n\n  // HIP note: for loop in kernel body is faster than using CUDA's parallel_reduce()\n  for (int i = threadIdx.x; i < N; i += blockDim.x) {\n    int bin = (int) (x[i] / 10);\n    atomicAdd(&sharedBins[bin], 1);\n  }\n  __syncthreads();\n\n  // HIP note: to copy shared memory to global memory, we\n  // use a reduction to add all the values.\n  for (int i = 1; i < 10; i++) {\n    sharedBins[0] += sharedBins[i];\n  }\n  if (threadIdx.x == 0) {\n    bins[0] = sharedBins[0];\n    for (int i = 1; i < 10; i++) {\n      bins[i] = sharedBins[i];\n    }\n  }\n}",
            "int i = threadIdx.x;\n    if (i >= N) return;\n\n    int bin = i/10; // the range of i is [0,90)\n    bins[bin]++;\n}",
            "// TODO: implement binsBy10Count\n}",
            "unsigned int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    size_t bin = (size_t) floor(x[tid] / 10.0);\n    if (bin >= 10)\n      bin = 9;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t block = (N + blockDim.x - 1) / blockDim.x;\n    size_t start = blockDim.x * blockIdx.x + threadIdx.x;\n    size_t end = block * blockDim.x;\n    for (size_t i = start; i < N; i += blockDim.x * gridDim.x) {\n        int bin = i/10;\n        if (bin < 10) {\n            atomicAdd(&bins[bin], 1);\n        }\n    }\n}",
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int nthreads = blockDim.x;\n    int nblocks = gridDim.x;\n    size_t start = bid * nthreads;\n    size_t stop = (bid + 1) * nthreads;\n    size_t step = nblocks * nthreads;\n    if (start >= N)\n        return;\n    if (stop > N)\n        stop = N;\n\n    for (size_t i = start; i < stop; i += step) {\n        double value = x[i];\n        size_t bin = static_cast<size_t>(value / 10);\n        if (value < 10)\n            bin = 0;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n\n  for (size_t i = idx; i < N; i += stride) {\n    size_t bin = (size_t) (x[i] / 10.0);\n    bins[bin]++;\n  }\n}",
            "// TODO: implement me!\n  // Note: the N passed to this kernel is the number of values in x.\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if (idx >= N)\n    return;\n\n  // your code here\n}",
            "const size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid >= N) {\n        return;\n    }\n\n    size_t bin = (size_t) (floor(x[tid] / 10.0));\n    if (bin > 9) {\n        bin = 9;\n    }\n\n    atomicAdd(&bins[bin], 1);\n}",
            "unsigned int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i >= N) return;\n\n    double value = x[i];\n    size_t bin = (size_t) value / 10;\n    if (bin < 10) {\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO: Your code here\n}",
            "int threadIdx = threadIdx.x + blockDim.x * blockIdx.x;\n\n  if (threadIdx < N) {\n    if (x[threadIdx] >= 0 && x[threadIdx] <= 10)\n      atomicAdd(&bins[0], 1);\n    else if (x[threadIdx] >= 10 && x[threadIdx] <= 20)\n      atomicAdd(&bins[1], 1);\n    else if (x[threadIdx] >= 20 && x[threadIdx] <= 30)\n      atomicAdd(&bins[2], 1);\n    else if (x[threadIdx] >= 30 && x[threadIdx] <= 40)\n      atomicAdd(&bins[3], 1);\n    else if (x[threadIdx] >= 40 && x[threadIdx] <= 50)\n      atomicAdd(&bins[4], 1);\n    else if (x[threadIdx] >= 50 && x[threadIdx] <= 60)\n      atomicAdd(&bins[5], 1);\n    else if (x[threadIdx] >= 60 && x[threadIdx] <= 70)\n      atomicAdd(&bins[6], 1);\n    else if (x[threadIdx] >= 70 && x[threadIdx] <= 80)\n      atomicAdd(&bins[7], 1);\n    else if (x[threadIdx] >= 80 && x[threadIdx] <= 90)\n      atomicAdd(&bins[8], 1);\n    else\n      atomicAdd(&bins[9], 1);\n  }\n}",
            "int tid = threadIdx.x;\n  __shared__ int count[10];\n\n  if (tid < 10) {\n    count[tid] = 0;\n  }\n\n  __syncthreads();\n\n  for (int i = tid; i < N; i += blockDim.x) {\n    int idx = (int) (x[i] / 10.0);\n    atomicAdd(&count[idx], 1);\n  }\n\n  __syncthreads();\n\n  if (tid < 10) {\n    bins[tid] = count[tid];\n  }\n}",
            "// TODO (begin solution)\n    int idx = blockIdx.x*blockDim.x + threadIdx.x;\n    if(idx < N){\n        int bin = (int)(x[idx]/10);\n        atomicAdd(&bins[bin], 1);\n    }\n\n    // TODO (end solution)\n\n}",
            "int idx = threadIdx.x;\n  __shared__ double xBuf[1024];\n  // Copy values from x to shared memory\n  if (idx < N) {\n    xBuf[idx] = x[idx];\n  }\n  __syncthreads();\n  // Perform local bin counts\n  for (size_t i = idx; i < N; i += blockDim.x) {\n    bins[(size_t)(xBuf[i] / 10.0)]++;\n  }\n}",
            "// TODO: Implement me\n  for (int i = 0; i < N; i++) {\n    size_t b = (int)(x[i] / 10);\n    if (b < 0)\n      b = 0;\n    if (b >= 10)\n      b = 9;\n    atomicAdd(&bins[b], 1);\n  }\n}",
            "//... your code goes here\n}",
            "// TODO: write CUDA kernel\n}",
            "size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n   if (i < N) {\n      bins[i / 10] += 1;\n   }\n}",
            "}",
            "extern __shared__ double cache[];\n  size_t *cache2 = (size_t *)cache;\n\n  // fill the cache with values\n  size_t tid = threadIdx.x;\n  size_t cache_offset = threadIdx.x;\n  if (cache_offset + tid < N) {\n    cache[cache_offset] = x[tid];\n  }\n\n  // parallel reduction\n  __syncthreads();\n  for (size_t offset = 1; offset < blockDim.x; offset <<= 1) {\n    if (cache_offset + offset < N) {\n      cache[cache_offset] += cache[cache_offset + offset];\n    }\n    __syncthreads();\n  }\n\n  // write the reduction\n  if (tid == 0) {\n    double sum = cache[0];\n    size_t index = (size_t)sum / 10.0;\n    if (index >= 10) {\n      index = 9;\n    }\n    cache2[index]++;\n  }\n  __syncthreads();\n\n  // final sum\n  size_t global_index = threadIdx.x + blockDim.x * blockIdx.x;\n  if (global_index < 10) {\n    atomicAdd(&bins[global_index], cache2[global_index]);\n  }\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n   if (tid < N) {\n      // Fill in your code here.\n   }\n}",
            "// TODO\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        int idx10 = idx / 10;\n        atomicAdd(&bins[idx10], 1);\n    }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        size_t idx = i / 10;\n        atomicAdd(&bins[idx], 1);\n    }\n}",
            "int idx = threadIdx.x + blockDim.x*blockIdx.x;\n  if (idx < N) {\n    bins[x[idx]/10] += 1;\n  }\n}",
            "__shared__ int shared[1024];\n\n    shared[threadIdx.x] = 0;\n    __syncthreads();\n    int local_count = 0;\n    for (int i = threadIdx.x; i < N; i += blockDim.x) {\n        int bucket = x[i] / 10;\n        if (bucket < 10) {\n            atomicAdd(&shared[bucket], 1);\n            local_count++;\n        }\n    }\n    __syncthreads();\n    if (threadIdx.x == 0) {\n        for (int i = 0; i < 10; i++) {\n            bins[i] = shared[i];\n        }\n    }\n}",
            "// TODO: YOUR CODE HERE\n}",
            "// TODO\n}",
            "// TODO\n}",
            "const int tid = threadIdx.x;\n    const int totalThreads = blockDim.x;\n    const int gtid = threadIdx.x + blockIdx.x * blockDim.x;\n    const int numThreads = blockDim.x * gridDim.x;\n\n    for (size_t i = gtid; i < N; i += numThreads) {\n        int bin = (int) (x[i] / 10);\n        if (bin < 0) {\n            bin = 0;\n        } else if (bin > 10) {\n            bin = 10;\n        }\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += gridDim.x * blockDim.x) {\n    const size_t bin = (size_t)((double)i / 10.0);\n    if (bin < 10) {\n      atomicAdd(bins + bin, 1);\n    }\n  }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        int bin = (int) (x[i] / 10.);\n        if (bin < 10) {\n            atomicAdd(&bins[bin], 1);\n        }\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        int bin = floor((x[idx] - 0) / 10);\n        // printf(\"bin %d\\n\", bin);\n        // atomicAdd(&bins[bin], 1);\n        atomicAdd(&bins[bin], x[idx]);\n    }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    int bin = (int)(x[i] / 10.0);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// Calculate which thread we are in the grid, and what our thread ID is for the block we are in.\n    // The value of threadIdx.x is how many threads there are in a block, so we use that to calculate\n    // which value we are in the vector. We also know how many threads there are in a block, so we can\n    // use that to divide up the work among threads.\n\n    // Determine our thread ID within the block, and the ID of the block we are in.\n    // The variable tid is thread ID and bid is block ID.\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    // Each thread will work on a different element of the array.\n    int i = bid * blockDim.x + tid;\n\n    // For each thread, check if the current element is in the range we want.\n    if (i < N) {\n        if (x[i] >= 0 && x[i] < 10) {\n            bins[0]++;\n        } else if (x[i] >= 10 && x[i] < 20) {\n            bins[1]++;\n        } else if (x[i] >= 20 && x[i] < 30) {\n            bins[2]++;\n        } else if (x[i] >= 30 && x[i] < 40) {\n            bins[3]++;\n        } else if (x[i] >= 40 && x[i] < 50) {\n            bins[4]++;\n        } else if (x[i] >= 50 && x[i] < 60) {\n            bins[5]++;\n        } else if (x[i] >= 60 && x[i] < 70) {\n            bins[6]++;\n        } else if (x[i] >= 70 && x[i] < 80) {\n            bins[7]++;\n        } else if (x[i] >= 80 && x[i] < 90) {\n            bins[8]++;\n        } else {\n            bins[9]++;\n        }\n    }\n\n    // At the end of the kernel, synchronize all threads.\n    //__syncthreads();\n}",
            "size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n   if (i < N) {\n       double val = x[i];\n       if (val >= 0 && val < 10) bins[0]++;\n       else if (val >= 10 && val < 20) bins[1]++;\n       else if (val >= 20 && val < 30) bins[2]++;\n       else if (val >= 30 && val < 40) bins[3]++;\n       else if (val >= 40 && val < 50) bins[4]++;\n       else if (val >= 50 && val < 60) bins[5]++;\n       else if (val >= 60 && val < 70) bins[6]++;\n       else if (val >= 70 && val < 80) bins[7]++;\n       else if (val >= 80 && val < 90) bins[8]++;\n       else if (val >= 90 && val <= 100) bins[9]++;\n   }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = blockDim.x * gridDim.x;\n    for (; idx < N; idx += stride) {\n        if (x[idx] >= 0 && x[idx] < 10) {\n            atomicAdd(&bins[0], 1);\n        } else if (x[idx] >= 10 && x[idx] < 20) {\n            atomicAdd(&bins[1], 1);\n        } else if (x[idx] >= 20 && x[idx] < 30) {\n            atomicAdd(&bins[2], 1);\n        } else if (x[idx] >= 30 && x[idx] < 40) {\n            atomicAdd(&bins[3], 1);\n        } else if (x[idx] >= 40 && x[idx] < 50) {\n            atomicAdd(&bins[4], 1);\n        } else if (x[idx] >= 50 && x[idx] < 60) {\n            atomicAdd(&bins[5], 1);\n        } else if (x[idx] >= 60 && x[idx] < 70) {\n            atomicAdd(&bins[6], 1);\n        } else if (x[idx] >= 70 && x[idx] < 80) {\n            atomicAdd(&bins[7], 1);\n        } else if (x[idx] >= 80 && x[idx] < 90) {\n            atomicAdd(&bins[8], 1);\n        } else if (x[idx] >= 90 && x[idx] <= 100) {\n            atomicAdd(&bins[9], 1);\n        }\n    }\n}",
            "size_t threadId = threadIdx.x + blockIdx.x * blockDim.x;\n    for (size_t i = threadId; i < N; i += blockDim.x * gridDim.x) {\n        int idx = (int)((int)x[i] / 10.0);\n        bins[idx]++;\n    }\n}",
            "size_t thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n    if (thread_id < N) {\n        if (x[thread_id] >= 0 && x[thread_id] < 10) {\n            atomicAdd(&bins[0], 1);\n        } else if (x[thread_id] >= 10 && x[thread_id] < 20) {\n            atomicAdd(&bins[1], 1);\n        } else if (x[thread_id] >= 20 && x[thread_id] < 30) {\n            atomicAdd(&bins[2], 1);\n        } else if (x[thread_id] >= 30 && x[thread_id] < 40) {\n            atomicAdd(&bins[3], 1);\n        } else if (x[thread_id] >= 40 && x[thread_id] < 50) {\n            atomicAdd(&bins[4], 1);\n        } else if (x[thread_id] >= 50 && x[thread_id] < 60) {\n            atomicAdd(&bins[5], 1);\n        } else if (x[thread_id] >= 60 && x[thread_id] < 70) {\n            atomicAdd(&bins[6], 1);\n        } else if (x[thread_id] >= 70 && x[thread_id] < 80) {\n            atomicAdd(&bins[7], 1);\n        } else if (x[thread_id] >= 80 && x[thread_id] < 90) {\n            atomicAdd(&bins[8], 1);\n        } else if (x[thread_id] >= 90 && x[thread_id] <= 100) {\n            atomicAdd(&bins[9], 1);\n        }\n    }\n}",
            "// Use a shared array to store the bins, which can then be summed in a single thread\n  __shared__ size_t thread_bins[10];\n  thread_bins[threadIdx.x] = 0;\n  __syncthreads();\n\n  // Each thread should only increment its local bin\n  if (threadIdx.x < 10) {\n    for (int i = 0; i < N; i++) {\n      if (x[i] < 10 * (threadIdx.x + 1)) {\n        thread_bins[threadIdx.x]++;\n      }\n    }\n  }\n  __syncthreads();\n\n  // Each thread with data should only add its local bins to the overall bins\n  if (threadIdx.x < 10) {\n    for (int i = 0; i < 10; i++) {\n      bins[threadIdx.x] += thread_bins[i];\n    }\n  }\n}",
            "// TODO: implement\n}",
            "// HIP doesn't really need thread id, so we'll just use the index of the\n   // value in x.\n   size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      size_t bin = i / 10;\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "...\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index < N) {\n    double val = x[index];\n    if (val >= 0 && val < 10) {\n      atomicAdd(&bins[0], 1);\n    } else if (val >= 10 && val < 20) {\n      atomicAdd(&bins[1], 1);\n    } else if (val >= 20 && val < 30) {\n      atomicAdd(&bins[2], 1);\n    } else if (val >= 30 && val < 40) {\n      atomicAdd(&bins[3], 1);\n    } else if (val >= 40 && val < 50) {\n      atomicAdd(&bins[4], 1);\n    } else if (val >= 50 && val < 60) {\n      atomicAdd(&bins[5], 1);\n    } else if (val >= 60 && val < 70) {\n      atomicAdd(&bins[6], 1);\n    } else if (val >= 70 && val < 80) {\n      atomicAdd(&bins[7], 1);\n    } else if (val >= 80 && val < 90) {\n      atomicAdd(&bins[8], 1);\n    } else if (val >= 90 && val <= 100) {\n      atomicAdd(&bins[9], 1);\n    }\n  }\n}",
            "// TODO: Your code here\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int gtid = threadIdx.x;\n\n    __shared__ double data[BLOCKSIZE];\n    __shared__ size_t localBins[10];\n    __shared__ size_t offset;\n\n    if (tid < N) {\n        data[gtid] = x[tid];\n    }\n\n    // do a reduction\n    for (int i = 1; i < blockDim.x; i *= 2) {\n        if (gtid % (2 * i) == 0) {\n            data[gtid] += data[gtid + i];\n        }\n        __syncthreads();\n    }\n\n    if (gtid == 0) {\n        // count the elements\n        for (int i = 0; i < 10; i++) {\n            localBins[i] = 0;\n        }\n\n        for (int i = 0; i < N; i++) {\n            int index = (int) data[i] / 10;\n            localBins[index]++;\n        }\n\n        // store the results back to global memory\n        offset = 0;\n        for (int i = 0; i < 10; i++) {\n            bins[i] = localBins[i];\n            offset += localBins[i];\n        }\n    }\n\n    // the final result is stored in bins array\n    if (tid < N) {\n        int index = (int) x[tid] / 10;\n        atomicAdd(&bins[index], 1);\n    }\n}",
            "if (threadIdx.x < N) {\n        int bin = x[threadIdx.x] / 10;\n        if (bin < 10) {\n            atomicAdd(&bins[bin], 1);\n        }\n    }\n}",
            "size_t tid = threadIdx.x + blockIdx.x*blockDim.x;\n  if (tid < N) {\n    bins[(size_t)floor(x[tid]/10.0)]++;\n  }\n\n}",
            "int tx = threadIdx.x;\n  int block = blockIdx.x;\n  int gid = block * blockDim.x + tx;\n  double val = x[gid];\n  int bin = (int) val / 10;\n  bins[bin] += 1;\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i >= N) return;\n    atomicAdd(&bins[min((int)(x[i] / 10.0), 9)], 1);\n}",
            "if (threadIdx.x < N) {\n        const size_t value = static_cast<size_t>(x[threadIdx.x]);\n        if (value < 10) {\n            atomicAdd(&bins[0], 1);\n        } else if (value < 20) {\n            atomicAdd(&bins[1], 1);\n        } else if (value < 30) {\n            atomicAdd(&bins[2], 1);\n        } else if (value < 40) {\n            atomicAdd(&bins[3], 1);\n        } else if (value < 50) {\n            atomicAdd(&bins[4], 1);\n        } else if (value < 60) {\n            atomicAdd(&bins[5], 1);\n        } else if (value < 70) {\n            atomicAdd(&bins[6], 1);\n        } else if (value < 80) {\n            atomicAdd(&bins[7], 1);\n        } else if (value < 90) {\n            atomicAdd(&bins[8], 1);\n        } else {\n            atomicAdd(&bins[9], 1);\n        }\n    }\n}",
            "// TODO: Fill in code here\n\n}",
            "const size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    const size_t bin = (size_t)((x[i] - 1) / 10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// Initialize your GPU kernel here\n  // 1. find a way to find the range of the current threadIdx.x in x\n  // 2. find a way to find the number of threads in the range of x\n  // 3. find a way to count the number of values in [low,high) and store the count in bins\n}",
            "// TODO: Your code here\n}",
            "//TODO\n}",
            "// Initialize bins to 0.\n    // Note that you can only initialize local memory arrays in the kernel.\n    __shared__ int bins_local[10];\n    for (size_t i = 0; i < 10; i++) {\n        bins_local[i] = 0;\n    }\n    for (size_t i = threadIdx.x + blockIdx.x * blockDim.x; i < N; i += blockDim.x * gridDim.x) {\n        size_t index = static_cast<size_t>(x[i] / 10);\n        bins_local[index]++;\n    }\n    // Use __syncthreads() to force threads in the block to finish before adding counts.\n    __syncthreads();\n    for (size_t i = threadIdx.x; i < 10; i += blockDim.x) {\n        atomicAdd(&bins[i], bins_local[i]);\n    }\n}",
            "size_t i = threadIdx.x + blockIdx.x*blockDim.x;\n  if (i < N) {\n    size_t bin = (size_t)(10*x[i] + 0.5);\n    if (bin < 10) {\n      atomicAdd(&bins[bin], 1);\n    }\n  }\n}",
            "int i = threadIdx.x + blockDim.x * blockIdx.x;\n  if (i < N) {\n    int val = (int)x[i];\n    int bin = 10 * (val / 10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "const int tid = threadIdx.x;\n    const int nthreads = blockDim.x;\n    size_t b = 0;\n\n    for (int i = tid; i < N; i += nthreads) {\n        if (x[i] >= 0 && x[i] < 10)\n            b = 0;\n        else if (x[i] >= 10 && x[i] < 20)\n            b = 1;\n        else if (x[i] >= 20 && x[i] < 30)\n            b = 2;\n        else if (x[i] >= 30 && x[i] < 40)\n            b = 3;\n        else if (x[i] >= 40 && x[i] < 50)\n            b = 4;\n        else if (x[i] >= 50 && x[i] < 60)\n            b = 5;\n        else if (x[i] >= 60 && x[i] < 70)\n            b = 6;\n        else if (x[i] >= 70 && x[i] < 80)\n            b = 7;\n        else if (x[i] >= 80 && x[i] < 90)\n            b = 8;\n        else if (x[i] >= 90 && x[i] <= 100)\n            b = 9;\n        __syncthreads();\n        atomicAdd(bins + b, 1);\n    }\n}",
            "// TODO: implement\n  size_t thread_id = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t stride = blockDim.x * gridDim.x;\n  if (thread_id >= N) return;\n  for (int i = thread_id; i < N; i += stride) {\n    if (x[i] >= 0 && x[i] < 10)\n      atomicAdd(&bins[0], 1);\n    else if (x[i] >= 10 && x[i] < 20)\n      atomicAdd(&bins[1], 1);\n    else if (x[i] >= 20 && x[i] < 30)\n      atomicAdd(&bins[2], 1);\n    else if (x[i] >= 30 && x[i] < 40)\n      atomicAdd(&bins[3], 1);\n    else if (x[i] >= 40 && x[i] < 50)\n      atomicAdd(&bins[4], 1);\n    else if (x[i] >= 50 && x[i] < 60)\n      atomicAdd(&bins[5], 1);\n    else if (x[i] >= 60 && x[i] < 70)\n      atomicAdd(&bins[6], 1);\n    else if (x[i] >= 70 && x[i] < 80)\n      atomicAdd(&bins[7], 1);\n    else if (x[i] >= 80 && x[i] < 90)\n      atomicAdd(&bins[8], 1);\n    else if (x[i] >= 90 && x[i] < 100)\n      atomicAdd(&bins[9], 1);\n    else\n      return;\n  }\n}",
            "// TODO\n    // Get the total number of threads (the number of elements in the input array).\n    const size_t num_threads = blockDim.x * gridDim.x;\n    // Get the thread number\n    const size_t thread_idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    for (size_t i = thread_idx; i < num_threads; i += num_threads) {\n        // Get the value of the element on the current thread.\n        const double val = x[i];\n        // Get the index of the bin this element belongs to.\n        const size_t bin_idx = floor(val / 10);\n        // Increment the count of this bin.\n        atomicAdd(&bins[bin_idx], 1);\n    }\n}",
            "}",
            "// TODO\n}",
            "int n = threadIdx.x + blockIdx.x * blockDim.x;\n    if (n < N) {\n        int bin = n / 10;\n        if (bin < 10) {\n            atomicAdd(&bins[bin], 1);\n        }\n    }\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n  if (tid >= N) return;\n\n  int bin = (int)(x[tid] / 10);\n  atomicAdd(&bins[bin], 1);\n}",
            "// Shared memory, indexed by bin number, contains the number of elements in the bin\n  extern __shared__ int sh_count[];\n\n  size_t thread_idx = threadIdx.x;\n  size_t thread_count = blockDim.x;\n\n  for (size_t tid = thread_idx; tid < N; tid += thread_count) {\n    size_t bin = (x[tid] / 10.0);\n    if (bin >= 10) {\n      bin = 9;\n    }\n\n    atomicAdd(&sh_count[bin], 1);\n  }\n\n  // Make sure all thread have finished\n  __syncthreads();\n\n  // Write counts to global memory\n  for (size_t tid = thread_idx; tid < 10; tid += thread_count) {\n    bins[tid] = sh_count[tid];\n  }\n}",
            "// TODO\n    // initialize bins to zero\n\n    // find index of x[i] in [0, 10)\n    // add 1 to bins\n\n    // TODO\n    // finalize\n}",
            "// TODO: add kernel code here\n\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int gid = tid;\n    if (gid >= N)\n        return;\n\n    int bin = (int) (x[gid] / 10);\n    atomicAdd(&bins[bin], 1);\n}",
            "__shared__ size_t sh_bins[10];\n\n  // Zero out shared memory\n  size_t tid = threadIdx.x;\n  if (tid < 10)\n    sh_bins[tid] = 0;\n\n  __syncthreads();\n\n  // Compute index\n  size_t start = (blockDim.x * blockIdx.x + tid) * 10;\n\n  // Process elements in range\n  while (start < N) {\n    int bin = (int)x[start] / 10;\n    sh_bins[bin]++;\n    start += blockDim.x * gridDim.x;\n  }\n\n  // Reduce\n  for (size_t i = 10 / 2; i > 0; i /= 2) {\n    if (tid < i)\n      sh_bins[tid] += sh_bins[tid + i];\n    __syncthreads();\n  }\n\n  if (tid == 0) {\n    bins[0] = sh_bins[0];\n    for (int i = 1; i < 10; ++i)\n      bins[i] = bins[i - 1] + sh_bins[i];\n  }\n}",
            "unsigned int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if (idx < N) {\n    int ix = (int)(10 * x[idx]);\n    if (ix >= 0 && ix < 10) {\n      atomicAdd(&bins[ix], 1);\n    }\n  }\n}",
            "const double BUCKET_WIDTH = 10.0;\n    const double BUCKET_OFFSET = -1.0;\n\n    for (size_t i = 0; i < N; i++) {\n        size_t bin = static_cast<size_t>((x[i] - BUCKET_OFFSET) / BUCKET_WIDTH);\n        bins[bin]++;\n    }\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n  if (tid >= N) return;\n  int bin = (int) (x[tid] / 10);\n  atomicAdd(&bins[bin], 1);\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n\n  if (tid < N) {\n    int index = floor(x[tid] / 10.0);\n    atomicAdd(&bins[index], 1);\n  }\n}",
            "const double step = 10.0;\n\n    const size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        // use modulo to calculate the bin number\n        const size_t bin = floor(x[i]/step);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Your code here\n  return;\n}",
            "int idx = threadIdx.x + blockDim.x * blockIdx.x;\n\n  if (idx < N) {\n    int binIdx = (int)(10 * x[idx] +.5);\n    atomicAdd(&bins[binIdx], 1);\n  }\n}",
            "size_t i = threadIdx.x + blockDim.x*blockIdx.x;\n  if (i >= N) return;\n\n  // TODO: Add code\n}",
            "size_t start = threadIdx.x + blockIdx.x * blockDim.x;\n    for(size_t i=start; i<N; i += blockDim.x*gridDim.x) {\n        size_t bin = (size_t)((x[i]-0)/10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// Thread ID in range [0, N)\n  const auto idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Perform the computations in range [0, N)\n  if (idx < N) {\n    int bin = 0;\n    if (x[idx] >= 0 && x[idx] < 10)\n      bin = 0;\n    else if (x[idx] >= 10 && x[idx] < 20)\n      bin = 1;\n    else if (x[idx] >= 20 && x[idx] < 30)\n      bin = 2;\n    else if (x[idx] >= 30 && x[idx] < 40)\n      bin = 3;\n    else if (x[idx] >= 40 && x[idx] < 50)\n      bin = 4;\n    else if (x[idx] >= 50 && x[idx] < 60)\n      bin = 5;\n    else if (x[idx] >= 60 && x[idx] < 70)\n      bin = 6;\n    else if (x[idx] >= 70 && x[idx] < 80)\n      bin = 7;\n    else if (x[idx] >= 80 && x[idx] < 90)\n      bin = 8;\n    else if (x[idx] >= 90 && x[idx] < 100)\n      bin = 9;\n\n    // Perform the atomic increment of bin\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int gid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (gid < N) {\n        int bin = floor(x[gid]/10.0) * 10;\n        bins[bin/10] += 1;\n    }\n}",
            "// Get the global thread index\n    int idx = threadIdx.x + blockDim.x * blockIdx.x;\n\n    // Get the number of bins\n    int B = 10;\n\n    // Get the number of elements per bin\n    int EB = N / B;\n\n    // Get the index of the bin\n    int bi = idx / EB;\n\n    // Get the index of the element in the bin\n    int ei = idx % EB;\n\n    // Compute the value in the bin\n    double binVal = x[bi * EB + ei];\n\n    // Update the global count in the bin\n    atomicAdd(&bins[bi], 1);\n}",
            "extern __shared__ double values[];\n  int t = threadIdx.x;\n  int b = blockIdx.x;\n  int i = b * blockDim.x + t;\n  if (i < N) {\n    values[t] = x[i];\n    __syncthreads();\n    // Count elements in range [10 * t, 10 * t + 10).\n    int start = 10 * t;\n    int end = min(start + 10, int(N));\n    int count = 0;\n    for (int k = start; k < end; k++) {\n      if (values[k] >= start && values[k] < end) count++;\n    }\n    bins[b] = count;\n  }\n}",
            "size_t tgid = threadIdx.x;\n  size_t blockOffset = blockIdx.x * blockDim.x;\n  size_t gridOffset = blockDim.x * gridDim.x;\n\n  size_t t = tgid;\n  size_t b = 0;\n\n  for (; t < N; t += gridOffset) {\n    size_t bin = floor(x[t] / 10.0);\n    if (bin < 10)\n      atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t threadID = threadIdx.x;\n  size_t blockID = blockIdx.x;\n  size_t blockSize = blockDim.x;\n  if (threadID < N) {\n    size_t binIdx = (x[threadID] - 0) / 10;\n    if (binIdx < 10) {\n      atomicAdd(&bins[binIdx], 1);\n    }\n  }\n}",
            "/*\n        HIP does not provide any way to compute a block index in an\n        arbitrary range. The block index is the same as the thread index\n        within the block, and the thread index can only be in the range\n        [0,blockDim.x). Hence, if we want to support a range larger than\n        the block dimension, we have to divide it into blocks.\n        Each block is assigned a range of values and computes the\n        number of elements in each interval.\n    */\n    size_t threadIdx = threadIdx.x;\n    size_t blockDim = blockDim.x;\n    size_t blockOffset = blockIdx.x * blockDim;\n    size_t i;\n    size_t binIndex;\n\n    for (i = threadIdx + blockOffset; i < N; i += blockDim) {\n        binIndex = (size_t)((x[i] - 0.0) / 10.0);\n        atomicAdd(&bins[binIndex], 1);\n    }\n}",
            "// TODO: Implement the code to count the numbers\n    // between each pair of values.\n    size_t block_id = blockIdx.x;\n    size_t block_size = blockDim.x;\n    size_t g_tid = threadIdx.x;\n    // Get the thread id in the whole block\n    size_t tid = block_id * block_size + g_tid;\n    // Get the start position in the vector x\n    size_t st = (tid * N) / blockDim.x;\n    if (tid < 10) {\n        size_t count = 0;\n        for (size_t i = st; i < st + N / blockDim.x; i++) {\n            if (x[i] >= tid * 10 && x[i] <= (tid + 1) * 10) {\n                count++;\n            }\n        }\n        atomicAdd(&bins[tid], count);\n    }\n}",
            "...\n}",
            "int tid = threadIdx.x;\n  int i = blockIdx.x * blockDim.x + tid;\n  int gtid = threadIdx.x + blockDim.x * blockIdx.x;\n  if (i < N) {\n    double val = x[i];\n    int bin = floor(val / 10);\n    atomicAdd(bins + bin, 1);\n  }\n}",
            "size_t blockSize = blockDim.x;\n    size_t blockIdx = blockIdx.x;\n    size_t threadIdx = threadIdx.x;\n\n    // This loop uses the first index of x to index bins\n    for (size_t i = threadIdx; i < N; i += blockSize) {\n        // Cast to int to get the right behavior for 95.\n        int index = (int) (x[i] / 10.0);\n        // TODO: update the bins[index] here.\n    }\n\n    // TODO: you need to synchronize threads in the block before the kernel\n    // function returns, which is the return statement below.\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N) {\n        int value = (int)(floor(x[idx] / 10.0) * 10.0);\n        atomicAdd(&bins[value], 1);\n    }\n}",
            "}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (tid < N) {\n        size_t digit = (x[tid] / 10.0) - floor(x[tid] / 10.0);\n        if (digit < 10 && digit >= 0)\n            atomicAdd(&bins[digit], 1);\n    }\n}",
            "// Each thread will process one value in [0,100]\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int v = x[tid];\n    if (v >= 0.0 && v < 10.0) {\n      atomicAdd(&bins[0], 1);\n    } else if (v >= 10.0 && v < 20.0) {\n      atomicAdd(&bins[1], 1);\n    } else if (v >= 20.0 && v < 30.0) {\n      atomicAdd(&bins[2], 1);\n    } else if (v >= 30.0 && v < 40.0) {\n      atomicAdd(&bins[3], 1);\n    } else if (v >= 40.0 && v < 50.0) {\n      atomicAdd(&bins[4], 1);\n    } else if (v >= 50.0 && v < 60.0) {\n      atomicAdd(&bins[5], 1);\n    } else if (v >= 60.0 && v < 70.0) {\n      atomicAdd(&bins[6], 1);\n    } else if (v >= 70.0 && v < 80.0) {\n      atomicAdd(&bins[7], 1);\n    } else if (v >= 80.0 && v < 90.0) {\n      atomicAdd(&bins[8], 1);\n    } else if (v >= 90.0 && v <= 100.0) {\n      atomicAdd(&bins[9], 1);\n    } else {\n      printf(\"Bad value %d\\n\", v);\n    }\n  }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        // TODO\n    }\n}",
            "if (threadIdx.x >= N) return;\n    size_t bin = (x[threadIdx.x] - 1) / 10;\n    atomicAdd(&bins[bin], 1);\n}",
            "// TODO: implement and summarize how this works\n    // HINT: use a grid of 10 threads and a block of at least as many threads as in x\n    return;\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= N) return;\n    bins[static_cast<size_t>(x[tid] / 10.0f)] += 1;\n}",
            "// TODO\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    if (x[tid] < 10) bins[tid] = 1;\n    else if (x[tid] < 20) bins[tid] = 2;\n    else if (x[tid] < 30) bins[tid] = 3;\n    else if (x[tid] < 40) bins[tid] = 4;\n    else if (x[tid] < 50) bins[tid] = 5;\n    else if (x[tid] < 60) bins[tid] = 6;\n    else if (x[tid] < 70) bins[tid] = 7;\n    else if (x[tid] < 80) bins[tid] = 8;\n    else if (x[tid] < 90) bins[tid] = 9;\n    else if (x[tid] <= 100) bins[tid] = 10;\n  }\n}",
            "const size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  const size_t stride = gridDim.x * blockDim.x;\n\n  for (size_t i = tid; i < N; i += stride) {\n    const size_t bin = static_cast<size_t>(floor(x[i] / 10.0));\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t thread_id = threadIdx.x + blockDim.x * blockIdx.x;\n    if (thread_id < N) {\n        if (x[thread_id] < 10) {\n            atomicAdd(&bins[0], 1);\n        } else if (x[thread_id] < 20) {\n            atomicAdd(&bins[1], 1);\n        } else if (x[thread_id] < 30) {\n            atomicAdd(&bins[2], 1);\n        } else if (x[thread_id] < 40) {\n            atomicAdd(&bins[3], 1);\n        } else if (x[thread_id] < 50) {\n            atomicAdd(&bins[4], 1);\n        } else if (x[thread_id] < 60) {\n            atomicAdd(&bins[5], 1);\n        } else if (x[thread_id] < 70) {\n            atomicAdd(&bins[6], 1);\n        } else if (x[thread_id] < 80) {\n            atomicAdd(&bins[7], 1);\n        } else if (x[thread_id] < 90) {\n            atomicAdd(&bins[8], 1);\n        } else {\n            atomicAdd(&bins[9], 1);\n        }\n    }\n}",
            "int tx = threadIdx.x;\n  int t = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n\n  for (int i = t; i < N; i += stride) {\n    int bin = (int) (x[i] / 10.0);\n    if (bin < 0) {\n      bin = 0;\n    } else if (bin > 9) {\n      bin = 9;\n    }\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "const size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        const int val = static_cast<int>(x[tid]);\n        atomicAdd(&bins[val / 10], 1);\n    }\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    int stride = blockDim.x * gridDim.x;\n\n    for (size_t i = idx; i < N; i += stride) {\n        double val = x[i];\n\n        if (val > 10 && val <= 20) {\n            atomicAdd(&(bins[0]), 1);\n        } else if (val > 20 && val <= 30) {\n            atomicAdd(&(bins[1]), 1);\n        } else if (val > 30 && val <= 40) {\n            atomicAdd(&(bins[2]), 1);\n        } else if (val > 40 && val <= 50) {\n            atomicAdd(&(bins[3]), 1);\n        } else if (val > 50 && val <= 60) {\n            atomicAdd(&(bins[4]), 1);\n        } else if (val > 60 && val <= 70) {\n            atomicAdd(&(bins[5]), 1);\n        } else if (val > 70 && val <= 80) {\n            atomicAdd(&(bins[6]), 1);\n        } else if (val > 80 && val <= 90) {\n            atomicAdd(&(bins[7]), 1);\n        } else if (val > 90 && val <= 100) {\n            atomicAdd(&(bins[8]), 1);\n        } else if (val <= 10) {\n            atomicAdd(&(bins[9]), 1);\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// YOUR CODE HERE\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t chunk = x.size() / size;\n\n  std::vector<double> x_local(chunk);\n\n  MPI_Scatter(x.data(), chunk, MPI_DOUBLE, x_local.data(), chunk, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (auto &i : x_local) {\n    if (i >= 0 && i < 10)\n      ++bins[0];\n    else if (i >= 10 && i < 20)\n      ++bins[1];\n    else if (i >= 20 && i < 30)\n      ++bins[2];\n    else if (i >= 30 && i < 40)\n      ++bins[3];\n    else if (i >= 40 && i < 50)\n      ++bins[4];\n    else if (i >= 50 && i < 60)\n      ++bins[5];\n    else if (i >= 60 && i < 70)\n      ++bins[6];\n    else if (i >= 70 && i < 80)\n      ++bins[7];\n    else if (i >= 80 && i < 90)\n      ++bins[8];\n    else if (i >= 90 && i <= 100)\n      ++bins[9];\n  }\n\n  std::vector<size_t> bins_local(10);\n\n  MPI_Reduce(bins.data(), bins_local.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i = 0; i < 10; i++)\n      bins[i] = bins_local[i];\n  }\n}",
            "// TODO: write the code here\n  int size = bins.size();\n  size_t total;\n  MPI_Reduce(&size, &total, 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if(total!= x.size()) {\n    printf(\"Error, vector size!= %lu\\n\", total);\n  }\n  if (bins.size()!= x.size()) {\n    printf(\"Error, bins size!= %lu\\n\", bins.size());\n  }\n\n  size_t local_size = x.size()/size;\n  std::vector<size_t> local_bins(size, 0);\n  for(size_t i = 0; i < local_size; i++) {\n    int index = x.at(i);\n    int div = index/10;\n    local_bins.at(div)++;\n  }\n  MPI_Reduce(&local_bins[0], &bins[0], 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "size_t myRank, numProcs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n\n    //determine what data each rank should have.\n    size_t myDataLength = x.size()/numProcs;\n    if (myRank == numProcs - 1)\n        myDataLength = x.size() - (numProcs - 1) * myDataLength;\n    std::vector<double> myData(x.begin() + myDataLength * myRank, x.begin() + myDataLength * (myRank + 1));\n\n    //convert each value to a bin and increment that bin.\n    std::array<size_t, 10> myBins{};\n    for (auto const &el : myData) {\n        int bin = (el / 10);\n        myBins[bin]++;\n    }\n    MPI_Reduce(MPI_IN_PLACE, myBins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (myRank == 0)\n        bins = myBins;\n\n}",
            "//TODO: Your code here\n\n    std::array<size_t, 10> bins_per_proc;\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    //fill the bin_per_proc array with the bins for this process\n    for (size_t i = 0; i < x.size(); i++) {\n        int bin = floor(x[i] / 10.0);\n        bins_per_proc[bin]++;\n    }\n\n    //merge the bins_per_proc arrays from every process\n    std::array<size_t, 10> temp_bins;\n    for (int i = 0; i < size; i++) {\n        if (i == rank) {\n            MPI_Gather(bins_per_proc.data(), 10, MPI_UNSIGNED_LONG, temp_bins.data(), 10, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n        } else {\n            MPI_Gather(bins_per_proc.data(), 10, MPI_UNSIGNED_LONG, temp_bins.data(), 10, MPI_UNSIGNED_LONG, i, MPI_COMM_WORLD);\n        }\n    }\n\n    //fill bins array with the correct values\n    if (rank == 0) {\n        bins[0] = temp_bins[0];\n        bins[1] = temp_bins[1];\n        bins[2] = temp_bins[2];\n        bins[3] = temp_bins[3];\n        bins[4] = temp_bins[4];\n        bins[5] = temp_bins[5];\n        bins[6] = temp_bins[6];\n        bins[7] = temp_bins[7];\n        bins[8] = temp_bins[8];\n        bins[9] = temp_bins[9];\n    }\n}",
            "MPI_Status status;\n    MPI_Request request;\n\n    // Set up the send and receive counts\n    int sendcount = x.size();\n    int recvcount = 1;\n    int sendtype = MPI_DOUBLE;\n    int recvtype = MPI_INT;\n\n    // Set up the send and receive buffer pointers\n    double* sendbuf = x.data();\n    int* recvbuf = (int*)malloc(recvcount * sizeof(int));\n\n    // Determine the number of ranks\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Initialize the bins to zero\n    bins.fill(0);\n\n    // Split the vector into equal parts\n    size_t chunksize = x.size()/size;\n    size_t remainder = x.size()%size;\n\n    // Set the offset for the start of this processes chunk\n    size_t offset = rank*chunksize + (rank < remainder? rank : remainder);\n\n    // Set up the send and receive buffer pointers\n    double* sendbuf = x.data();\n    int* recvbuf = (int*)malloc(recvcount * sizeof(int));\n\n    // Post the non-blocking receive for this process\n    MPI_Irecv(recvbuf, recvcount, recvtype, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &request);\n\n    // Loop over the process chunks\n    for (size_t i = offset; i < x.size(); i+=chunksize) {\n\n        // Find the bin number\n        double bin_start = i/10;\n        double bin_end = (i+chunksize-1)/10;\n        int bin_num = int(bin_start);\n        bins[bin_num] += int(bin_end) - bin_start + 1;\n\n        // Check for overlapping bins\n        if (i+chunksize < x.size() && (i+chunksize)/10 == bin_end) {\n            bin_num = int(bin_end + 1);\n            bins[bin_num] += 1;\n        }\n\n    }\n\n    // Send the bins to the other processes\n    MPI_Isend(bins.data(), 10, MPI_INT, rank > 0? rank-1 : MPI_PROC_NULL, 0, MPI_COMM_WORLD, &request);\n\n    // Wait for the data to arrive\n    MPI_Wait(&request, &status);\n\n    // Check the receive buffer\n    if (status.MPI_SOURCE == rank+1) {\n        memcpy(bins.data() + 1, recvbuf, recvcount*sizeof(int));\n    }\n\n    // Free the receive buffer\n    free(recvbuf);\n\n}",
            "size_t nRanks;\n  MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // count how many numbers are in each rank's section\n  size_t size = x.size();\n  for (size_t i = 0; i < size; ++i) {\n    int r = static_cast<int>(x[i]);\n    r /= 10;\n    r *= 10;\n    bins[r] += 1;\n  }\n\n  // gather all the numbers in each section\n  int *values;\n  std::vector<int> send_values;\n  std::vector<int> recv_values;\n  MPI_Status status;\n  if (rank == 0) {\n    values = new int[100];\n    std::fill(values, values + 100, 0);\n\n    // gather all the counts\n    for (int r = 1; r < nRanks; ++r) {\n      send_values.clear();\n      for (size_t i = 0; i < 10; ++i) {\n        send_values.push_back(bins[i]);\n      }\n      MPI_Gather(&send_values[0], 10, MPI_INT,\n                 &values[0], 10, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n\n    // count the numbers in each section\n    for (int r = 0; r < 100; ++r) {\n      for (int i = 0; i < nRanks; ++i) {\n        if (r >= i * 10 && r < (i + 1) * 10) {\n          bins[r - i * 10] += values[r];\n        }\n      }\n    }\n\n    // gather the counts\n    for (int r = 1; r < nRanks; ++r) {\n      recv_values.clear();\n      MPI_Gather(&bins[0], 10, MPI_INT,\n                 &recv_values[0], 10, MPI_INT, r, MPI_COMM_WORLD);\n      for (int i = 0; i < 10; ++i) {\n        bins[i] += recv_values[i];\n      }\n    }\n\n    delete [] values;\n  } else {\n    values = new int[10];\n    std::fill(values, values + 10, 0);\n\n    // gather all the counts\n    for (int r = 0; r < nRanks; ++r) {\n      send_values.clear();\n      for (size_t i = 0; i < 10; ++i) {\n        send_values.push_back(bins[i]);\n      }\n      MPI_Gather(&send_values[0], 10, MPI_INT,\n                 &values[0], 10, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n\n    // count the numbers in each section\n    for (int r = 0; r < 10; ++r) {\n      for (int i = 0; i < nRanks; ++i) {\n        if (r >= i * 10 && r < (i + 1) * 10) {\n          bins[r - i * 10] += values[r];\n        }\n      }\n    }\n\n    // gather the counts\n    MPI_Gather(&bins[0], 10, MPI_INT,\n               &recv_values[0], 10, MPI_INT, 0, MPI_COMM_WORLD);\n    for (int i = 0; i < 10; ++i) {\n      bins[i] += recv_values[i];\n    }\n\n    delete [] values;\n  }\n}",
            "assert(x.size() > 0);\n  assert(x.size() % 10 == 0);\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  size_t n = x.size() / size;\n  std::array<size_t, 10> counts_per_rank;\n  for (size_t i = 0; i < 10; i++) {\n    counts_per_rank[i] = 0;\n  }\n  std::vector<size_t> indices;\n  if (rank == 0) {\n    for (size_t i = 0; i < x.size(); i++) {\n      if (x[i] >= 10.0 * i && x[i] < 10.0 * (i + 1)) {\n        indices.push_back(i);\n      }\n    }\n  }\n  MPI_Gather(&indices[0], indices.size(), MPI_INT, &bins[0], indices.size(),\n             MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (size_t i = 0; i < indices.size(); i++) {\n      bins[i] = 1;\n      for (size_t j = 1; j < size; j++) {\n        bins[i] += bins_per_rank[j * indices.size() + i];\n      }\n    }\n  }\n}",
            "// TODO\n  // get rank and number of ranks\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int num_procs;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  // initialize bins\n  bins.fill(0);\n  // get size of local vector\n  int local_size = x.size()/num_procs;\n  // get start index for local vector\n  int start_index = rank*local_size;\n  // get end index for local vector\n  int end_index = (rank+1)*local_size;\n  // iterate through local vector\n  for (int i = start_index; i < end_index; ++i) {\n    // get current value\n    double value = x[i];\n    // get current bin index\n    int bin = value/10;\n    // increment bin count\n    ++bins[bin];\n  }\n}",
            "bins.fill(0);\n\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int count;\n  MPI_Status status;\n\n  std::vector<double> send_data;\n  for (size_t i = 0; i < x.size(); i += 10) {\n    if (x[i] > 9.99) {\n      continue;\n    }\n    send_data.push_back(x[i]);\n  }\n\n  std::vector<double> receive_data;\n  for (size_t i = 0; i < 10; i++) {\n    double send_data_temp;\n    for (size_t j = 0; j < send_data.size(); j++) {\n      if (send_data[j] < (i+1)*10 && send_data[j] >= i*10) {\n        send_data_temp = send_data[j];\n        send_data.erase(send_data.begin()+j);\n        receive_data.push_back(send_data_temp);\n        break;\n      }\n    }\n  }\n\n  int root = 0;\n  if (rank == root) {\n    bins.fill(0);\n  }\n  MPI_Gather(&send_data[0], send_data.size(), MPI_DOUBLE, &receive_data[0], receive_data.size(), MPI_DOUBLE, root, MPI_COMM_WORLD);\n\n  if (rank == root) {\n    for (size_t i = 0; i < receive_data.size(); i++) {\n      for (size_t j = 0; j < 10; j++) {\n        if (receive_data[i] >= j*10 && receive_data[i] < (j+1)*10) {\n          bins[j]++;\n          break;\n        }\n      }\n    }\n  }\n\n}",
            "// Your code here\n\n\tstd::vector<int> counts(10, 0);\n\n\tfor (int i = 0; i < x.size(); ++i) {\n\t\tint index = floor(x[i] / 10);\n\t\tif (index < 0 || index > 9)\n\t\t\tcontinue;\n\t\t++counts[index];\n\t}\n\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tfor (int i = 0; i < 10; ++i) {\n\t\tif (rank == 0)\n\t\t\tbins[i] = 0;\n\t\tint total = 0;\n\t\tMPI_Allreduce(&counts[i], &total, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\t\tbins[i] = total;\n\t}\n}",
            "size_t n = x.size();\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    // compute my size and offset\n    size_t my_size = n / world_size;\n    size_t my_offset = my_size * world_rank;\n\n    // set up a MPI_Request for receiving bins\n    MPI_Request req;\n\n    // compute my local counts\n    std::array<size_t, 10> my_bins;\n    for (size_t i = 0; i < my_size; i++) {\n        int i10 = static_cast<int>(x[i + my_offset] / 10.0);\n        my_bins[i10] += 1;\n    }\n\n    // communicate local counts to bins\n    if (world_rank == 0) {\n        MPI_Irecv(bins.data(), 10, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD, &req);\n        MPI_Send(my_bins.data(), 10, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n    else {\n        MPI_Send(my_bins.data(), 10, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n        MPI_Recv(bins.data(), 10, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    MPI_Wait(&req, MPI_STATUS_IGNORE);\n}",
            "// TODO\n    int count = x.size();\n    size_t min = 0;\n    size_t max = 10;\n    size_t n_bins = 10;\n    int rank;\n    int size;\n    int i;\n    MPI_Status status;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Get the value of min and max from every rank and find the rank of min and max value\n    int rank_min = -1;\n    int rank_max = -1;\n    MPI_Allreduce(&min, &rank_min, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    MPI_Allreduce(&max, &rank_max, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);\n\n    // If the rank is the one containing min or max value\n    if(rank == rank_min || rank == rank_max) {\n        // Find the global min and max value\n        double min_val;\n        double max_val;\n        MPI_Allreduce(&min, &min_val, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n        MPI_Allreduce(&max, &max_val, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);\n\n        // Compute the number of bins per rank\n        int nbins = (int)(max_val - min_val)/10;\n\n        // For all ranks\n        for(int i = 0; i < size; i++) {\n            // If the rank contains min and max value\n            if(i == rank_min || i == rank_max) {\n                // Send the value of nbins to all other ranks\n                MPI_Send(&nbins, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n            }\n            // If the rank does not contain min and max value\n            else {\n                // Send the value of -1 to all other ranks\n                MPI_Send(&nbins, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n            }\n        }\n    }\n\n    // If the rank is not the one containing min and max value\n    else {\n        // Receive the number of bins per rank\n        int nbins;\n        MPI_Recv(&nbins, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n        min = (size_t)nbins*rank;\n        max = (size_t)nbins*(rank+1);\n    }\n\n    // For every rank\n    for(int i = 0; i < size; i++) {\n        // If the rank is the one containing min and max value\n        if(i == rank_min || i == rank_max) {\n            // For every bin\n            for(int j = 0; j < nbins; j++) {\n                // Get the min and max value of the bin\n                size_t min_val = (size_t)j*nbins + (size_t)min;\n                size_t max_val = (size_t)j*nbins + (size_t)min + (size_t)nbins;\n                // Count the number of values in the bin and store the value in the bins array\n                size_t count = 0;\n                for(int k = 0; k < count; k++) {\n                    if(x[k] >= min_val && x[k] < max_val) {\n                        count++;\n                    }\n                }\n                bins[j] = count;\n            }\n        }\n        // If the rank is not the one containing min and max value\n        else {\n            // Receive the values of the bin and store in the bins array\n            for(int j = 0; j < nbins; j++) {\n                MPI_Recv(&bins[j], 1, MPI_UINT64_T, i, 0, MPI_COMM_WORLD",
            "std::vector<double> buf;\n    bins = {0};\n    MPI_Allreduce(&x[0], &buf[0], x.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    for (auto i = 0; i < buf.size(); i++) {\n        if (buf[i] == 0) {\n            continue;\n        }\n        if (buf[i] < 10) {\n            bins[0] += 1;\n        } else if (buf[i] >= 10 && buf[i] < 20) {\n            bins[1] += 1;\n        } else if (buf[i] >= 20 && buf[i] < 30) {\n            bins[2] += 1;\n        } else if (buf[i] >= 30 && buf[i] < 40) {\n            bins[3] += 1;\n        } else if (buf[i] >= 40 && buf[i] < 50) {\n            bins[4] += 1;\n        } else if (buf[i] >= 50 && buf[i] < 60) {\n            bins[5] += 1;\n        } else if (buf[i] >= 60 && buf[i] < 70) {\n            bins[6] += 1;\n        } else if (buf[i] >= 70 && buf[i] < 80) {\n            bins[7] += 1;\n        } else if (buf[i] >= 80 && buf[i] < 90) {\n            bins[8] += 1;\n        } else if (buf[i] >= 90 && buf[i] <= 100) {\n            bins[9] += 1;\n        }\n    }\n}",
            "int const size = x.size();\n  int const rank = MPI_Comm_rank(MPI_COMM_WORLD);\n  int const num_ranks = MPI_Comm_size(MPI_COMM_WORLD);\n  int const my_bins = 10 / num_ranks;\n  int const my_bins_start = my_bins * rank;\n  int const my_bins_end = my_bins * (rank + 1);\n  int count[10] = {0};\n  double my_count[10] = {0};\n  double start, end;\n\n  if(rank == 0) {\n    for(int i = 0; i < size; ++i) {\n      int index = x[i];\n      if (index >= 10)\n        index = 9;\n      else if (index < 0)\n        index = 0;\n      count[index] += 1;\n    }\n  } else {\n    for(int i = 0; i < size; ++i) {\n      int index = x[i];\n      if (index >= 10)\n        index = 9;\n      else if (index < 0)\n        index = 0;\n      if (index >= my_bins_start && index < my_bins_end)\n        my_count[index - my_bins_start] += 1;\n    }\n  }\n\n  MPI_Reduce(count, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for(int i = 0; i < 10; ++i) {\n      start = i * my_bins_start;\n      end = start + my_bins_start;\n      if (end >= 10) {\n        end = 10;\n        bins[i] += my_count[9];\n      } else {\n        bins[i] += my_count[i];\n      }\n    }\n  }\n}",
            "int n = x.size();\n    bins.fill(0);\n    size_t size = 10;\n    size_t chunk = n/size;\n    MPI_Status status;\n\n    for(int i = 0; i < size; i++){\n\n        int rank = MPI_Comm_rank(MPI_COMM_WORLD);\n\n        if(rank == i){\n\n            for(int j = 0; j < chunk; j++){\n\n                int idx = chunk * i + j;\n                double value = x[idx];\n\n                if(value >= 0 && value < 10){\n\n                    bins[0] += 1;\n\n                }else if(value >= 10 && value < 20){\n\n                    bins[1] += 1;\n\n                }else if(value >= 20 && value < 30){\n\n                    bins[2] += 1;\n\n                }else if(value >= 30 && value < 40){\n\n                    bins[3] += 1;\n\n                }else if(value >= 40 && value < 50){\n\n                    bins[4] += 1;\n\n                }else if(value >= 50 && value < 60){\n\n                    bins[5] += 1;\n\n                }else if(value >= 60 && value < 70){\n\n                    bins[6] += 1;\n\n                }else if(value >= 70 && value < 80){\n\n                    bins[7] += 1;\n\n                }else if(value >= 80 && value < 90){\n\n                    bins[8] += 1;\n\n                }else if(value >= 90 && value <= 100){\n\n                    bins[9] += 1;\n\n                }else{\n                    std::cerr << \"Warning: No bin for value \" << value << std::endl;\n                }\n            }\n        }\n\n        MPI_Barrier(MPI_COMM_WORLD);\n    }\n}",
            "/* TODO: Your code here */\n\n    /* Your solution should be 4 lines of code. \n       1. Assign the values in x to a vector\n       2. Parallel for\n       3. Get the number of elements in [0, 10)\n       4. Get the number of elements in [10, 20)\n       5. Get the number of elements in [20, 30)\n       6. Get the number of elements in [30, 40)\n       7. Get the number of elements in [40, 50)\n       8. Get the number of elements in [50, 60)\n       9. Get the number of elements in [60, 70)\n       10. Get the number of elements in [70, 80)\n       11. Get the number of elements in [80, 90)\n       12. Get the number of elements in [90, 100)\n       13. Scatter the counts to the appropriate bin\n       14. Assign the counts to bins\n     */\n}",
            "}",
            "MPI_Datatype double_type;\n    MPI_Type_contiguous(sizeof(double), MPI_BYTE, &double_type);\n    MPI_Type_commit(&double_type);\n\n    size_t total_num = x.size();\n    size_t my_num = x.size() / MPI_COMM_WORLD.size();\n    std::vector<double> my_data(my_num);\n    for (size_t i = 0; i < my_num; ++i) {\n        my_data[i] = x[i + MPI_COMM_WORLD.rank() * my_num];\n    }\n\n    // send to root\n    std::vector<double> data_to_root;\n    if (MPI_COMM_WORLD.rank()!= 0) {\n        MPI_Send(my_data.data(), my_num, double_type, 0, 0, MPI_COMM_WORLD);\n    }\n    else {\n        for (int i = 1; i < MPI_COMM_WORLD.size(); ++i) {\n            MPI_Status status;\n            MPI_Probe(i, 0, MPI_COMM_WORLD, &status);\n            int count;\n            MPI_Get_count(&status, double_type, &count);\n            data_to_root.resize(data_to_root.size() + count);\n            MPI_Recv(data_to_root.data() + data_to_root.size() - count, count, double_type, i, 0, MPI_COMM_WORLD, &status);\n        }\n        data_to_root.insert(data_to_root.end(), my_data.begin(), my_data.end());\n    }\n\n    std::vector<size_t> result(10, 0);\n    if (MPI_COMM_WORLD.rank() == 0) {\n        for (auto const& x: data_to_root) {\n            ++result[x / 10];\n        }\n    }\n\n    // recv from root\n    if (MPI_COMM_WORLD.rank()!= 0) {\n        MPI_Recv(result.data(), 10, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    else {\n        for (int i = 1; i < MPI_COMM_WORLD.size(); ++i) {\n            MPI_Status status;\n            MPI_Probe(i, 0, MPI_COMM_WORLD, &status);\n            int count;\n            MPI_Get_count(&status, MPI_INT, &count);\n            MPI_Recv(result.data() + result.size() - count, count, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n        }\n    }\n    MPI_Type_free(&double_type);\n    bins = result;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  if (rank == 0) {\n    for (int i = 0; i < 10; ++i) {\n      bins[i] = 0;\n    }\n  }\n\n  std::array<size_t, 10> counts;\n  for (int i = 0; i < 10; ++i) {\n    counts[i] = 0;\n  }\n  if (rank == 0) {\n    std::cout << \"rank: \" << rank << \" \" << bins << std::endl;\n  }\n\n  // compute the count of values in each bin\n  int bin;\n  double bin_start, bin_end;\n  for (int i = 0; i < x.size(); ++i) {\n    bin = x[i] / 10;\n    bin_start = bin * 10;\n    bin_end = bin_start + 10;\n    if (bin < 0 || bin > 10) {\n      std::cerr << \"Error: the bin number is out of range.\" << std::endl;\n    }\n    if (x[i] >= bin_start && x[i] < bin_end) {\n      counts[bin]++;\n    }\n  }\n  std::cout << \"counts: \" << counts << std::endl;\n  // reduce the count of values in each bin on each rank to bins on rank 0\n  MPI_Reduce(counts.data(), bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0,\n             MPI_COMM_WORLD);\n  if (rank == 0) {\n    std::cout << \"bins: \" << bins << std::endl;\n  }\n}",
            "int rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tsize_t myBins[10] = { 0 };\n\n\t//compute bins on each rank\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tint bin = (int)(x[i] / 10);\n\t\tmyBins[bin]++;\n\t}\n\n\t//get the max count of bins of each rank\n\tint maxBinCount = 0;\n\tfor (int i = 0; i < 10; i++) {\n\t\tif (myBins[i] > maxBinCount) {\n\t\t\tmaxBinCount = myBins[i];\n\t\t}\n\t}\n\n\t//compute the bins on rank 0\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < 10; i++) {\n\t\t\tif (myBins[i] == 0) {\n\t\t\t\tbins[i] = 0;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tbins[i] = myBins[i];\n\t\t\t}\n\t\t}\n\t}\n\telse {\n\t\tfor (int i = 0; i < 10; i++) {\n\t\t\tMPI_Send(myBins, 10, MPI_INT, 0, i, MPI_COMM_WORLD);\n\t\t}\n\t}\n\n\tint recv_buf[10];\n\tint numProcs;\n\tMPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n\n\t//compute the bins on rank 0\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < numProcs - 1; i++) {\n\t\t\tfor (int j = 0; j < 10; j++) {\n\t\t\t\tMPI_Recv(recv_buf, 10, MPI_INT, i + 1, j, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t\t\tbins[j] += recv_buf[j];\n\t\t\t}\n\t\t}\n\t}\n\telse {\n\t\tMPI_Recv(recv_buf, 10, MPI_INT, 0, rank - 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t}\n\n\t//print the results\n\tfor (int i = 0; i < 10; i++) {\n\t\tstd::cout << \"bin[\" << i << \"]: \" << bins[i] << std::endl;\n\t}\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (size < 2) {\n\t\tthrow std::runtime_error(\"binsBy10Count: at least two processes required\");\n\t}\n\n\t// if (rank == 0) {\n\t// \tstd::cout << \"binsBy10Count\" << std::endl;\n\t// }\n\n\tsize_t count = x.size();\n\tsize_t step = 10 / size;\n\n\t// if (rank == 0) {\n\t// \tstd::cout << \"count: \" << count << std::endl;\n\t// }\n\n\tsize_t start = step * rank;\n\n\tstd::vector<int> counts(10);\n\n\tfor (size_t i = 0; i < step; i++) {\n\t\tint value = x[i + start];\n\t\tif (value < 10) {\n\t\t\tcounts[0]++;\n\t\t} else if (value < 20) {\n\t\t\tcounts[1]++;\n\t\t} else if (value < 30) {\n\t\t\tcounts[2]++;\n\t\t} else if (value < 40) {\n\t\t\tcounts[3]++;\n\t\t} else if (value < 50) {\n\t\t\tcounts[4]++;\n\t\t} else if (value < 60) {\n\t\t\tcounts[5]++;\n\t\t} else if (value < 70) {\n\t\t\tcounts[6]++;\n\t\t} else if (value < 80) {\n\t\t\tcounts[7]++;\n\t\t} else if (value < 90) {\n\t\t\tcounts[8]++;\n\t\t} else if (value < 100) {\n\t\t\tcounts[9]++;\n\t\t}\n\t}\n\n\t// if (rank == 0) {\n\t// \tstd::cout << \"start: \" << start << std::endl;\n\t// \tstd::cout << \"step: \" << step << std::endl;\n\t// }\n\n\t// std::cout << \"counts\" << std::endl;\n\t// std::cout << counts[0] << std::endl;\n\t// std::cout << counts[1] << std::endl;\n\t// std::cout << counts[2] << std::endl;\n\t// std::cout << counts[3] << std::endl;\n\t// std::cout << counts[4] << std::endl;\n\t// std::cout << counts[5] << std::endl;\n\t// std::cout << counts[6] << std::endl;\n\t// std::cout << counts[7] << std::endl;\n\t// std::cout << counts[8] << std::endl;\n\t// std::cout << counts[9] << std::endl;\n\n\tMPI_Allreduce(counts.data(), bins.data(), 10, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n\t// if (rank == 0) {\n\t// \tstd::cout << \"bins\" << std::endl;\n\t// \tstd::cout << bins[0] << std::endl;\n\t// \tstd::cout << bins[1] << std::endl;\n\t// \tstd::cout << bins[2] << std::endl;\n\t// \tstd::cout << bins[3] << std::endl;\n\t// \tstd::cout << bins[4] << std::endl;\n\t// \tstd::cout << bins[5] << std::endl;\n\t// \tstd::cout << bins[6] << std::endl;\n\t// \tstd::cout << bins[7] << std::endl;\n\t// \tstd::cout << bins[8] << std::endl;\n\t// \tstd::cout << bins[9] << std::endl;\n\t// }\n\n\t// if (rank == 0) {\n\t// \tstd::cout << \"end binsBy10Count\" << std::endl;",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    /*\n     * Step 1: get the size of the array and calculate the length of each bin\n     *         for each rank.\n     */\n\n    size_t N = x.size();\n    size_t len = N/size;\n    size_t left = N%size;\n    size_t start = rank*len;\n    size_t end = start + len;\n    if (rank < left) {\n        end += 1;\n    }\n\n    /*\n     * Step 2: calculate the number of values in each bin for each rank.\n     */\n    for (size_t i = start; i < end; i++) {\n        double val = x[i];\n        if (val < 10) bins[0]++;\n        else if (val < 20) bins[1]++;\n        else if (val < 30) bins[2]++;\n        else if (val < 40) bins[3]++;\n        else if (val < 50) bins[4]++;\n        else if (val < 60) bins[5]++;\n        else if (val < 70) bins[6]++;\n        else if (val < 80) bins[7]++;\n        else if (val < 90) bins[8]++;\n        else if (val <= 100) bins[9]++;\n    }\n\n    /*\n     * Step 3: calculate the sum of the number of values in each bin of each\n     *         rank and send it to rank 0.\n     */\n\n    size_t sum = 0;\n    for (size_t i = 0; i < 10; i++) {\n        sum += bins[i];\n    }\n\n    MPI_Allreduce(&sum, &sum, 1, MPI_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    /*\n     * Step 4: send the number of values in each bin of each rank to rank 0.\n     */\n\n    if (rank == 0) {\n        for (size_t i = 0; i < size; i++) {\n            std::vector<size_t> v;\n            MPI_Recv(&v[0], 10, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < 10; j++) {\n                bins[j] += v[j];\n            }\n        }\n    }\n    else {\n        MPI_Send(&bins[0], 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // Create a data type for a double precision float\n  MPI_Datatype MPI_DBL = MPI_DOUBLE;\n\n  // Create an array of doubles to send\n  std::vector<double> sendbuf(x.size());\n  for(size_t i=0; i < x.size(); ++i) {\n    sendbuf[i] = x[i];\n  }\n\n  // Create a vector of MPI_DBL to recieve\n  std::vector<MPI_DBL> recvbuf(bins.size(), 0);\n\n  // Get the start of the rank's portion of the data\n  size_t start = rank * sendbuf.size()/size;\n\n  // Get the number of elements to receive\n  size_t recvcount = sendbuf.size()/size;\n\n  // Send the data\n  MPI_Scatterv(sendbuf.data(), sendbuf.data()+start, sendbuf.data()+start+recvcount,\n    MPI_DBL, recvbuf.data(), recvcount, MPI_DBL, 0, MPI_COMM_WORLD);\n\n  // Now compute the bins\n  for(size_t i=0; i < recvbuf.size(); ++i) {\n    if(recvbuf[i] >= 0 && recvbuf[i] < 10) {\n      bins[0] += 1;\n    }\n    else if(recvbuf[i] >= 10 && recvbuf[i] < 20) {\n      bins[1] += 1;\n    }\n    else if(recvbuf[i] >= 20 && recvbuf[i] < 30) {\n      bins[2] += 1;\n    }\n    else if(recvbuf[i] >= 30 && recvbuf[i] < 40) {\n      bins[3] += 1;\n    }\n    else if(recvbuf[i] >= 40 && recvbuf[i] < 50) {\n      bins[4] += 1;\n    }\n    else if(recvbuf[i] >= 50 && recvbuf[i] < 60) {\n      bins[5] += 1;\n    }\n    else if(recvbuf[i] >= 60 && recvbuf[i] < 70) {\n      bins[6] += 1;\n    }\n    else if(recvbuf[i] >= 70 && recvbuf[i] < 80) {\n      bins[7] += 1;\n    }\n    else if(recvbuf[i] >= 80 && recvbuf[i] < 90) {\n      bins[8] += 1;\n    }\n    else if(recvbuf[i] >= 90 && recvbuf[i] <= 100) {\n      bins[9] += 1;\n    }\n  }\n\n  // Gather the data\n  MPI_Gather(bins.data(), bins.size(), MPI_INT, bins.data(), bins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "bins.fill(0);\n\n    // TODO: Count the number of values in [0,10), [10, 20), [20, 30),...\n    //... and store the counts in `bins`\n\n\n}",
            "int nranks, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // find start and end of local chunk\n    int local_start = rank * x.size() / nranks;\n    int local_end = (rank + 1) * x.size() / nranks;\n\n    // find start and end of global chunk\n    int global_start = nranks * rank;\n    int global_end = global_start + x.size();\n\n    // initialize bins to 0\n    for (int i = 0; i < 10; ++i) {\n        bins[i] = 0;\n    }\n\n    // count local elements\n    for (int i = local_start; i < local_end; ++i) {\n        int digit = std::floor(x[i] / 10);\n        bins[digit]++;\n    }\n\n    // create a temporary array on rank 0\n    std::array<size_t, 10> local_bins = bins;\n    std::array<size_t, 10> global_bins;\n\n    // sum local bins\n    if (rank == 0) {\n        for (int i = 0; i < 10; ++i) {\n            global_bins[i] = 0;\n            for (int j = 0; j < nranks; ++j) {\n                global_bins[i] += local_bins[i];\n            }\n        }\n    }\n\n    // print global counts\n    if (rank == 0) {\n        for (int i = 0; i < 10; ++i) {\n            std::cout << global_bins[i] << std::endl;\n        }\n    }\n}",
            "// TODO: your code goes here\n    // TODO: allocate the array on rank 0\n    // TODO: fill in the array on rank 0\n    // TODO: use MPI to sum the bins from each rank\n    // TODO: save the result on rank 0\n}",
            "// TODO\n    //return bins;\n}",
            "size_t mpi_rank, mpi_size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\n  double x_size = x.size();\n  size_t bins_size = bins.size();\n\n  // get the index of the first element of the current rank\n  // so you only process the relevant elements\n  size_t start_index = (x_size / mpi_size) * mpi_rank;\n  size_t end_index = start_index + x_size / mpi_size;\n\n  // count the number of elements in each bin\n  for (auto x_itr = x.begin() + start_index; x_itr!= x.begin() + end_index;\n       x_itr++) {\n    size_t index = (size_t)((*x_itr) / 10.0);\n\n    if (index >= bins_size) {\n      std::cerr << \"bin index out of range\" << std::endl;\n      return;\n    }\n    bins[index]++;\n  }\n\n  // now gather the bins\n  std::vector<size_t> total_bins(bins_size, 0);\n  if (mpi_rank == 0) {\n    MPI_Reduce(MPI_IN_PLACE, &total_bins[0], bins_size, MPI_UNSIGNED_LONG_LONG,\n               MPI_SUM, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Reduce(&bins[0], &total_bins[0], bins_size, MPI_UNSIGNED_LONG_LONG,\n               MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n\n  // send results back to rank 0\n  if (mpi_rank == 0) {\n    for (int i = 0; i < bins_size; i++) {\n      std::cout << i * 10 << \"-\" << (i + 1) * 10 << \": \" << total_bins[i]\n                << std::endl;\n    }\n  }\n}",
            "int world_size;\n\tint world_rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n\tint chunk_size = x.size() / world_size;\n\tint remainder = x.size() % world_size;\n\tint start = chunk_size * world_rank;\n\tint end = chunk_size * (world_rank + 1);\n\tif (world_rank == world_size - 1)\n\t{\n\t\tend += remainder;\n\t}\n\n\tstd::vector<size_t> my_bins(10);\n\tfor (size_t i = start; i < end; i++)\n\t{\n\t\tmy_bins[x[i] / 10]++;\n\t}\n\n\tstd::vector<size_t> all_bins(10 * world_size);\n\tMPI_Gather(my_bins.data(), my_bins.size(), MPI_LONG_LONG, all_bins.data(), my_bins.size(), MPI_LONG_LONG, 0, MPI_COMM_WORLD);\n\n\tif (world_rank == 0)\n\t{\n\t\tfor (int i = 0; i < 10; i++)\n\t\t{\n\t\t\tfor (int j = 0; j < world_size; j++)\n\t\t\t{\n\t\t\t\tbins[i] += all_bins[i + j * 10];\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: implement\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // Split the vector x into equally-sized chunks for each rank\n  std::vector<double> x_local;\n  for(size_t i=rank*10; i < std::min<size_t>((rank+1)*10, x.size()); ++i) {\n    x_local.push_back(x[i]);\n  }\n\n  // Compute the count of elements in each 10 bins\n  std::vector<size_t> counts;\n  for(auto const& v : x_local) {\n    size_t idx = floor(v/10.0);\n    ++counts[idx];\n  }\n\n  // Collect the counts in each rank\n  std::vector<size_t> counts_total(10, 0);\n  MPI_Allgather(&counts[0], 10, MPI_UNSIGNED_LONG_LONG, &counts_total[0], 10, MPI_UNSIGNED_LONG_LONG, MPI_COMM_WORLD);\n\n  // Set the bins value\n  for(size_t i=0; i<10; ++i) {\n    bins[i] = counts_total[i];\n  }\n}",
            "//TODO: implement me\n    const size_t size = x.size();\n    std::vector<size_t> tmp(size);\n\n    for(size_t i = 0; i < size; i++)\n        tmp[i] = x[i] / 10;\n\n    const int rank = mpi::rank(mpi::comm());\n    MPI_Allgather(tmp.data(), size, MPI_INT, bins.data(), size, MPI_INT, mpi::comm());\n\n    //mpi::allgather(mpi::comm(), x, tmp);\n    //for(size_t i = 0; i < size; i++)\n    //    std::cout << tmp[i] << \" \";\n    //std::cout << std::endl;\n\n    if (rank == 0) {\n        for (size_t i = 0; i < size; i++)\n            std::cout << tmp[i] << \" \";\n        std::cout << std::endl;\n\n        for (size_t i = 0; i < 10; i++) {\n            for (size_t j = 0; j < bins[i]; j++) {\n                std::cout << x[i] << \" \";\n            }\n            std::cout << std::endl;\n        }\n        std::cout << std::endl;\n    }\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int nrank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nrank);\n\n    int* count = new int[10];\n    for (int i = 0; i < 10; i++)\n    {\n        count[i] = 0;\n    }\n\n    size_t bsize = x.size() / nrank;\n\n    for (int i = 0; i < bsize; i++)\n    {\n        int index = x[rank * bsize + i] / 10;\n        count[index]++;\n    }\n\n    if (rank == 0)\n    {\n        for (int i = 0; i < 10; i++)\n        {\n            bins[i] = count[i];\n        }\n    }\n\n    for (int i = 0; i < 10; i++)\n    {\n        MPI_Reduce(&count[i], &bins[i], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0)\n    {\n        for (int i = 0; i < 10; i++)\n        {\n            bins[i] = (bins[i] / nrank);\n        }\n    }\n\n    delete[] count;\n}",
            "int rank, nRanks;\n    MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int remainder = nRanks%10;\n    int numToProcess = (nRanks/10) + ((rank < remainder)? 1: 0);\n    if (rank < remainder) {\n        numToProcess++;\n    }\n    std::vector<double> my_x(numToProcess);\n    std::vector<size_t> my_bins(numToProcess);\n    int index = 0;\n    for (int i = rank; i < x.size(); i+= nRanks) {\n        if (i%10 < numToProcess) {\n            my_x[index] = x[i];\n            index++;\n        }\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n    MPI_Allreduce(MPI_IN_PLACE, &my_x[0], numToProcess, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    MPI_Allreduce(MPI_IN_PLACE, &my_bins[0], numToProcess, MPI_SIZE_T, MPI_SUM, MPI_COMM_WORLD);\n    bins[0] = my_bins[0];\n    for (int i = 1; i < 10; i++) {\n        bins[i] = my_bins[i-1] + my_bins[i];\n    }\n}",
            "size_t my_total = 0;\n  for(size_t i=0; i<x.size(); i++) {\n    my_total += (x[i]/10.0);\n  }\n\n  std::vector<size_t> bin_counts(10, 0);\n\n  // get the MPI rank of the calling process\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // get the number of MPI processes\n  int nproc;\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  // determine the work each process will do\n  size_t work_per_proc = my_total/nproc;\n  size_t remaining_work = my_total%nproc;\n\n  // determine where the work begins for this rank\n  size_t work_begin = work_per_proc*rank + std::min(rank, remaining_work);\n\n  // determine where the work ends for this rank\n  size_t work_end = work_begin + work_per_proc;\n  if(remaining_work>rank) work_end++;\n\n  // populate my_work with the values of x between the beginning and end\n  std::vector<double> my_work;\n  for(size_t i=work_begin; i<work_end; i++) {\n    my_work.push_back(x[i]);\n  }\n\n  // now each rank has a range of values of x. Use this range to populate\n  // the array of counts\n  for(size_t i=0; i<my_work.size(); i++) {\n    int bin = my_work[i]/10;\n    bin_counts[bin]++;\n  }\n\n  // reduce all the counts to rank 0\n  MPI_Allreduce(MPI_IN_PLACE, &bin_counts, 10, MPI_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  // now store the counts in the bins\n  for(size_t i=0; i<10; i++) {\n    bins[i] = bin_counts[i];\n  }\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &bins.size());\n  MPI_Comm_rank(MPI_COMM_WORLD, &bins.size());\n\n  MPI_Scatter(x.data(), 1, MPI_DOUBLE, &bins, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  int j = 0;\n  for(int i = 0; i < 10; i++){\n    if(i < 9)\n      j = (i+1)*10;\n    else\n      j = 100;\n    bins[i] = std::count(x.begin(), x.end(), [&j](double d){return d < j;});\n  }\n\n  MPI_Gather(bins.data(), 10, MPI_UNSIGNED, &bins, 10, MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Implement this function.\n}",
            "int numproc;\n    MPI_Comm_size(MPI_COMM_WORLD, &numproc);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    size_t binIndex;\n    double binSize;\n    double start;\n    if (rank == 0) {\n        start = 0.0;\n        binSize = 10.0;\n    }\n    else {\n        start = rank * (10.0 * x.size() / numproc);\n        binSize = 10.0 / numproc;\n    }\n    // std::cout << \"Bin Size: \" << binSize << \" Start: \" << start << std::endl;\n    for (auto &xValue : x) {\n        binIndex = static_cast<size_t>(floor((xValue - start) / binSize));\n        ++bins[binIndex];\n    }\n\n    std::vector<size_t> sendcounts(numproc, 0);\n    std::vector<size_t> recvcounts(numproc, 0);\n    std::vector<size_t> displs(numproc, 0);\n\n    for (int i = 0; i < bins.size(); ++i) {\n        size_t rankIndex = static_cast<size_t>(floor(i * numproc / bins.size()));\n        if (rankIndex < numproc) {\n            ++sendcounts[rankIndex];\n        }\n    }\n\n    for (int i = 0; i < numproc; ++i) {\n        if (i > 0) {\n            displs[i] = displs[i-1] + sendcounts[i-1];\n        }\n        recvcounts[i] = sendcounts[i];\n    }\n\n    std::vector<size_t> receiveBins(recvcounts[rank]);\n\n    if (rank == 0) {\n        std::vector<size_t> sendBins(sendcounts[0]);\n        for (size_t i = 0; i < bins.size(); ++i) {\n            if (i < sendcounts[0]) {\n                sendBins[i] = bins[i];\n            }\n        }\n\n        MPI_Gatherv(&sendBins[0], sendcounts[0], MPI_UNSIGNED_LONG_LONG, &receiveBins[0], &recvcounts[0], &displs[0], MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n    }\n    else {\n        MPI_Gatherv(&bins[0], sendcounts[rank], MPI_UNSIGNED_LONG_LONG, &receiveBins[0], &recvcounts[0], &displs[0], MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        for (size_t i = 0; i < receiveBins.size(); ++i) {\n            bins[i] = receiveBins[i];\n        }\n    }\n}",
            "constexpr int rank = 0;\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int log2_size = 0;\n  while (size >>= 1)\n    ++log2_size;\n\n  size_t local_bins[10] = {0};\n  for (auto v : x) {\n    auto index = std::floor(v/10.0)*10.0;\n    if (index >= 0 && index < 10)\n      local_bins[index]++;\n  }\n\n  size_t global_bins[10] = {0};\n  MPI_Allreduce(local_bins, global_bins, 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  for (int i = 0; i < 10; i++)\n    bins[i] = global_bins[i];\n}",
            "auto const& size = x.size();\n\tif (size == 0) {\n\t\treturn;\n\t}\n\n\t// divide the vector x into 10 partitions\n\tauto const sizePerRank = size / 10;\n\tauto const leftOver = size - sizePerRank * 10;\n\tif (leftOver) {\n\t\t// add left over to one of the 10 ranks\n\t\tsize_t offset = size - leftOver;\n\t\tauto& rank = (x.at(offset) / 10) % 10;\n\t\tbins[rank] += leftOver;\n\t}\n\n\tMPI_Status status;\n\tint const rank = MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tint const size = MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint const start = rank * sizePerRank;\n\tint const end = start + sizePerRank;\n\tint const myBins[10] = { 0 };\n\n\tfor (int i = start; i < end; ++i) {\n\t\tauto const& val = x.at(i);\n\t\tbins[val / 10]++;\n\t}\n\n\tMPI_Reduce(myBins, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  if (rank == 0) {\n    size_t myCount = 0;\n    for (size_t i=0; i<10; i++) {\n      bins[i] = 0;\n    }\n    for (size_t i=0; i<x.size(); i++) {\n      double v = x[i];\n      if (v >= 0.0 && v < 10.0) {\n        bins[0] += 1;\n      } else if (v >= 10.0 && v < 20.0) {\n        bins[1] += 1;\n      } else if (v >= 20.0 && v < 30.0) {\n        bins[2] += 1;\n      } else if (v >= 30.0 && v < 40.0) {\n        bins[3] += 1;\n      } else if (v >= 40.0 && v < 50.0) {\n        bins[4] += 1;\n      } else if (v >= 50.0 && v < 60.0) {\n        bins[5] += 1;\n      } else if (v >= 60.0 && v < 70.0) {\n        bins[6] += 1;\n      } else if (v >= 70.0 && v < 80.0) {\n        bins[7] += 1;\n      } else if (v >= 80.0 && v < 90.0) {\n        bins[8] += 1;\n      } else if (v >= 90.0 && v <= 100.0) {\n        bins[9] += 1;\n      }\n    }\n  } else {\n    size_t myCount = 0;\n    for (size_t i=0; i<x.size(); i++) {\n      double v = x[i];\n      if (v >= 0.0 && v < 10.0) {\n        myCount += 1;\n      } else if (v >= 10.0 && v < 20.0) {\n        myCount += 1;\n      } else if (v >= 20.0 && v < 30.0) {\n        myCount += 1;\n      } else if (v >= 30.0 && v < 40.0) {\n        myCount += 1;\n      } else if (v >= 40.0 && v < 50.0) {\n        myCount += 1;\n      } else if (v >= 50.0 && v < 60.0) {\n        myCount += 1;\n      } else if (v >= 60.0 && v < 70.0) {\n        myCount += 1;\n      } else if (v >= 70.0 && v < 80.0) {\n        myCount += 1;\n      } else if (v >= 80.0 && v < 90.0) {\n        myCount += 1;\n      } else if (v >= 90.0 && v <= 100.0) {\n        myCount += 1;\n      }\n    }\n  }\n}",
            "int const rank = MPI_Comm_rank(MPI_COMM_WORLD);\n    int const size = MPI_Comm_size(MPI_COMM_WORLD);\n    // TODO: Implement\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  if (rank == 0) {\n    for (auto i = 0; i < 10; ++i) {\n      bins[i] = 0;\n    }\n  }\n\n  auto n = x.size();\n  std::vector<double> local_bins(10, 0);\n\n  for (auto i = 0; i < n; ++i) {\n    int bin = x[i] / 10;\n    local_bins[bin] += 1;\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "auto my_rank = MPI::COMM_WORLD.Get_rank();\n  int size = MPI::COMM_WORLD.Get_size();\n\n  if (size == 1) {\n    for (auto const &val : x) {\n      if (val >= 0 && val <= 9)\n        ++bins[0];\n      else if (val >= 10 && val <= 19)\n        ++bins[1];\n      else if (val >= 20 && val <= 29)\n        ++bins[2];\n      else if (val >= 30 && val <= 39)\n        ++bins[3];\n      else if (val >= 40 && val <= 49)\n        ++bins[4];\n      else if (val >= 50 && val <= 59)\n        ++bins[5];\n      else if (val >= 60 && val <= 69)\n        ++bins[6];\n      else if (val >= 70 && val <= 79)\n        ++bins[7];\n      else if (val >= 80 && val <= 89)\n        ++bins[8];\n      else if (val >= 90 && val <= 99)\n        ++bins[9];\n    }\n  }\n  else {\n    size_t chunk_size = x.size() / size;\n    std::vector<size_t> send_counts;\n    std::vector<size_t> recv_counts;\n    for (int i = 0; i < size; ++i)\n      send_counts.push_back(chunk_size);\n    MPI::COMM_WORLD.Alltoall(&send_counts[0], 1, MPI::INT, &recv_counts[0], 1, MPI::INT);\n    std::vector<size_t> displacements;\n    displacements.push_back(0);\n    for (int i = 1; i < size; ++i)\n      displacements.push_back(displacements[i - 1] + send_counts[i - 1]);\n\n    std::vector<double> send_buffer;\n    std::vector<double> recv_buffer;\n    for (int i = 0; i < size; ++i)\n      send_buffer.insert(send_buffer.end(), x.begin() + displacements[i], x.begin() + displacements[i] + send_counts[i]);\n    MPI::COMM_WORLD.Alltoallv(&send_buffer[0], &send_counts[0], &displacements[0], MPI::DOUBLE, &recv_buffer[0], &recv_counts[0], &displacements[0], MPI::DOUBLE);\n    std::vector<size_t> bin_counts;\n    for (int i = 0; i < size; ++i)\n      bin_counts.insert(bin_counts.end(), bins.begin(), bins.begin() + recv_counts[i]);\n    for (auto const &val : recv_buffer) {\n      if (val >= 0 && val <= 9)\n        ++bin_counts[0];\n      else if (val >= 10 && val <= 19)\n        ++bin_counts[1];\n      else if (val >= 20 && val <= 29)\n        ++bin_counts[2];\n      else if (val >= 30 && val <= 39)\n        ++bin_counts[3];\n      else if (val >= 40 && val <= 49)\n        ++bin_counts[4];\n      else if (val >= 50 && val <= 59)\n        ++bin_counts[5];\n      else if (val >= 60 && val <= 69)\n        ++bin_counts[6];\n      else if (val >= 70 && val <= 79)\n        ++bin_counts[7];\n      else if (val >= 80 && val <= 89)\n        ++bin_counts[8];\n      else if (val >= 90 && val <= 99)",
            "constexpr int num_procs = 2;\n  if(x.size() % num_procs!= 0) {\n    std::cerr << \"Vector size must be divisible by \" << num_procs << '\\n';\n    exit(1);\n  }\n\n  std::array<size_t, 10> partial_bins{};\n\n  for(int rank = 0; rank < num_procs; rank++) {\n    const size_t start = x.size()/num_procs * rank;\n    const size_t end = x.size()/num_procs * (rank+1);\n    size_t bins_count{};\n    for(size_t i = start; i < end; i++) {\n      const int bin = x[i]/10;\n      partial_bins[bin]++;\n    }\n  }\n\n  MPI_Reduce(MPI_IN_PLACE, partial_bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if(rank == 0) {\n    for(size_t i = 0; i < 10; i++) {\n      bins[i] = partial_bins[i];\n    }\n  }\n}",
            "size_t start = x[0];\n    size_t end = x[0] + 10;\n    int rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Compute the start and end of each rank's region\n    // This is the first element in x, rounded up to the nearest 10\n    start = (start + 9) / 10 * 10;\n    // This is the last element in x, rounded down to the nearest 10\n    end = (end - 1) / 10 * 10;\n    // Adjust end if it is now above 100\n    if (end > 100) {\n        end = 100;\n    }\n    // Set the start and end of this rank's region\n    bins[0] = start;\n    bins[9] = end;\n    // Use MPI to compute the rest of the bins\n    int nprocs = 1;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    for (int i = 1; i < nprocs; ++i) {\n        MPI_Send(&bins[0], 10, MPI_LONG_LONG_INT, i, 0, MPI_COMM_WORLD);\n        MPI_Recv(&bins[0], 10, MPI_LONG_LONG_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    // Now loop over each element in x, bumping up or down the appropriate bin\n    // as necessary.\n    for (size_t i = 0; i < x.size(); ++i) {\n        int bin = x[i] / 10;\n        if (bin < bins[0]) {\n            bins[0]--;\n            bin = 0;\n        }\n        else if (bin >= bins[9]) {\n            bins[9]++;\n            bin = 9;\n        }\n        bins[bin]++;\n    }\n    // Compute the sum on all ranks\n    MPI_Allreduce(MPI_IN_PLACE, &bins[0], 10, MPI_LONG_LONG_INT, MPI_SUM, MPI_COMM_WORLD);\n}",
            "// Fill this in\n}",
            "// Fill this in\n\n}",
            "const int N_RANKS = 3;\n  int rank = 0;\n  int nprocs = 1;\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  if (nprocs!= N_RANKS) {\n    throw std::invalid_argument(\"There must be exactly 3 ranks.\");\n  }\n  if (x.size() % nprocs!= 0) {\n    throw std::invalid_argument(\"The size of the input vector must be divisible by the number of ranks.\");\n  }\n\n  size_t elementsPerRank = x.size() / nprocs;\n  size_t offset = rank * elementsPerRank;\n\n  size_t binIndex = 0;\n  size_t binCount = 0;\n  size_t binSize = 10;\n  for (size_t i = offset; i < offset + elementsPerRank; i++) {\n    binIndex = i / binSize;\n    binCount = x[i] / binSize;\n    bins[binIndex] += binCount;\n  }\n}",
            "int mpi_size = 0;\n    int mpi_rank = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n    std::vector<double> v = x;\n    size_t v_length = x.size();\n    std::vector<double> partial_results(mpi_size, 0);\n    int p_rank = 0;\n\n    for (int i = 0; i < 10; i++) {\n        int partial_result = 0;\n        for (int k = 0; k < v_length; k++) {\n            if (i == 0 && v[k] >= 0 && v[k] < 10) {\n                partial_result++;\n            }\n            else if (i == 1 && v[k] >= 10 && v[k] < 20) {\n                partial_result++;\n            }\n            else if (i == 2 && v[k] >= 20 && v[k] < 30) {\n                partial_result++;\n            }\n            else if (i == 3 && v[k] >= 30 && v[k] < 40) {\n                partial_result++;\n            }\n            else if (i == 4 && v[k] >= 40 && v[k] < 50) {\n                partial_result++;\n            }\n            else if (i == 5 && v[k] >= 50 && v[k] < 60) {\n                partial_result++;\n            }\n            else if (i == 6 && v[k] >= 60 && v[k] < 70) {\n                partial_result++;\n            }\n            else if (i == 7 && v[k] >= 70 && v[k] < 80) {\n                partial_result++;\n            }\n            else if (i == 8 && v[k] >= 80 && v[k] < 90) {\n                partial_result++;\n            }\n            else if (i == 9 && v[k] >= 90 && v[k] < 100) {\n                partial_result++;\n            }\n        }\n        MPI_Reduce(&partial_result, &partial_results[p_rank], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n        p_rank++;\n    }\n    if (mpi_rank == 0) {\n        bins[0] = partial_results[0];\n        bins[1] = partial_results[1];\n        bins[2] = partial_results[2];\n        bins[3] = partial_results[3];\n        bins[4] = partial_results[4];\n        bins[5] = partial_results[5];\n        bins[6] = partial_results[6];\n        bins[7] = partial_results[7];\n        bins[8] = partial_results[8];\n        bins[9] = partial_results[9];\n    }\n    return;\n}",
            "int rank, n_procs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &n_procs);\n\n    int n = x.size();\n\n    int bins_per_rank = 10;\n    int n_bins = 10;\n    int n_bins_per_proc = n_bins / n_procs;\n    std::vector<size_t> bins_local(n_bins_per_proc);\n\n    int i, i0, i1, i2;\n    for (i = 0; i < n; i++) {\n        i0 = i / (bins_per_rank * n_procs);\n        i1 = (i / bins_per_rank) % n_procs;\n        i2 = i % bins_per_rank;\n\n        if (i0 * bins_per_rank + i2 >= 10 && i0 * bins_per_rank + i2 <= 19) {\n            if (rank == i1) bins_local[i0 * n_bins_per_proc + i2 - 10]++;\n        }\n    }\n\n    std::vector<size_t> bins_total(n_bins);\n    MPI_Allreduce(bins_local.data(), bins_total.data(), n_bins, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    bins = std::array<size_t, 10>();\n    for (size_t i = 0; i < bins.size(); i++) {\n        bins[i] = bins_total[i];\n    }\n\n}",
            "int num_ranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n  size_t const N = x.size();\n  size_t const rank = MPI_Comm_rank(MPI_COMM_WORLD);\n  size_t const B = N / num_ranks;\n  size_t const b = N % num_ranks;\n\n  size_t const n = rank < b? B + 1 : B;\n\n  // get the vector of values in our range\n  std::vector<double> v(n);\n  for(size_t i = 0; i < n; ++i) {\n    v[i] = x[rank * B + i];\n  }\n\n  // sort the vector\n  std::sort(v.begin(), v.end());\n\n  // compute the number of entries in each bin\n  bins.fill(0);\n  for(size_t i = 0; i < n; ++i) {\n    ++bins[i / 10];\n  }\n\n  // add the partial counts together\n  for(size_t i = 1; i < 10; ++i) {\n    bins[i] += bins[i - 1];\n  }\n\n  // broadcast the result to all ranks\n  MPI_Bcast(bins.data(), 10, MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n\n  // gather the counts from all ranks\n  std::array<size_t, 10> counts;\n  counts.fill(0);\n  MPI_Allgather(bins.data(), 10, MPI_UNSIGNED, counts.data(), 10, MPI_UNSIGNED, MPI_COMM_WORLD);\n\n  // compute the cumulative sum to get the bins\n  for(size_t i = 1; i < 10; ++i) {\n    bins[i] += bins[i - 1];\n  }\n\n  // scatter the bins to all ranks\n  std::array<size_t, 10> send;\n  send.fill(0);\n  MPI_Scatter(bins.data(), 10, MPI_UNSIGNED, send.data(), 10, MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n\n  // add the counts to the bins to get the final result\n  for(size_t i = 0; i < n; ++i) {\n    --bins[i / 10];\n    --bins[i / 10];\n    --bins[i / 10];\n    --bins[i / 10];\n    --bins[i / 10];\n    --bins[i / 10];\n    --bins[i / 10];\n    --bins[i / 10];\n    --bins[i / 10];\n    --bins[i / 10];\n    ++bins[send[i]];\n  }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, size;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &size);\n    int n = (int) x.size();\n    int localN = n / size;\n    int localI = rank * localN;\n    int offset = 0;\n    std::array<size_t, 10> localBins;\n    for (int i = 0; i < 10; i++) {\n        int k = i * localN + localI;\n        if (k >= localN) {\n            break;\n        }\n        int v = (int) x[k];\n        v /= 10;\n        v += 1;\n        localBins[i] += v;\n    }\n    for (int i = 0; i < 10; i++) {\n        localBins[i] += offset;\n    }\n    std::vector<size_t> bins(10, 0);\n    MPI_Reduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, comm);\n    if (rank == 0) {\n        for (int i = 0; i < 10; i++) {\n            std::cout << bins[i] << \" \";\n        }\n        std::cout << std::endl;\n    }\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  std::array<size_t, 10> localBins;\n  std::array<double, 10> localX;\n\n  // for every 10 values, calculate which bin it belongs to and increment that bin count\n  for (int i = 0; i < 10; i++) {\n    localBins[i] = 0;\n  }\n\n  int elements = x.size() / size;\n  int extra = x.size() % size;\n\n  // create local x and local bins\n  if (rank == 0) {\n    for (int i = 0; i < elements; i++) {\n      localX[i] = x[i];\n    }\n    for (int i = 0; i < extra; i++) {\n      localX[i + elements] = x[i + elements];\n    }\n  }\n\n  // send the local x to the right rank\n  if (rank < elements) {\n    MPI_Send(&localX[rank], 1, MPI_DOUBLE, rank, 0, MPI_COMM_WORLD);\n  }\n\n  if (rank >= elements) {\n    MPI_Send(&localX[rank - elements], 1, MPI_DOUBLE, elements, 0, MPI_COMM_WORLD);\n  }\n\n  // receive the local x from the left rank\n  if (rank == 0) {\n    for (int i = 0; i < elements; i++) {\n      MPI_Recv(&localX[i], 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    for (int i = 0; i < extra; i++) {\n      MPI_Recv(&localX[elements + i], 1, MPI_DOUBLE, elements + i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  }\n\n  // calculate the bin counts and send it to the left rank\n  if (rank == 0) {\n    for (int i = 0; i < 10; i++) {\n      if (localX[i] < 10) {\n        localBins[0]++;\n      }\n      else if (localX[i] < 20) {\n        localBins[1]++;\n      }\n      else if (localX[i] < 30) {\n        localBins[2]++;\n      }\n      else if (localX[i] < 40) {\n        localBins[3]++;\n      }\n      else if (localX[i] < 50) {\n        localBins[4]++;\n      }\n      else if (localX[i] < 60) {\n        localBins[5]++;\n      }\n      else if (localX[i] < 70) {\n        localBins[6]++;\n      }\n      else if (localX[i] < 80) {\n        localBins[7]++;\n      }\n      else if (localX[i] < 90) {\n        localBins[8]++;\n      }\n      else if (localX[i] < 100) {\n        localBins[9]++;\n      }\n    }\n  }\n\n  // receive the local bin counts from the left rank\n  if (rank == 0) {\n    for (int i = 0; i < 10; i++) {\n      MPI_Recv(&localBins[i], 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  }\n\n  // distribute the local bin counts to the right rank\n  if (rank < elements) {\n    MPI_Send(&localBins[rank], 1, MPI_INT, rank, 0, MPI_COMM_WORLD);\n  }\n\n  if (rank >= elements) {",
            "auto count = [&](double x) {\n    auto i = static_cast<size_t>(x / 10.0) % bins.size();\n    return i;\n  };\n  std::fill(bins.begin(), bins.end(), 0);\n  auto n_ranks = mpi::get_size();\n  auto n_tasks = n_ranks * x.size();\n  std::vector<size_t> bins_local(bins.size());\n  for (size_t i = 0; i < x.size(); ++i) {\n    auto k = count(x[i]);\n    bins_local[k]++;\n  }\n  std::vector<size_t> bins_sum(bins.size());\n  mpi::all_reduce(bins_local.data(), bins_sum.data(), bins.size(), mpi::plus<>());\n  std::copy(bins_sum.begin(), bins_sum.end(), bins.begin());\n}",
            "size_t rank = 0;\n  int mpiSize = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &mpiSize);\n  assert(mpiSize > 0);\n  assert(x.size() % mpiSize == 0);\n  size_t n = x.size() / mpiSize;\n  assert(n > 0);\n  assert(n % 10 == 0);\n\n  // rank 0\n  if (rank == 0) {\n    for (size_t r = 1; r < mpiSize; r++) {\n      std::vector<double> tmp;\n      tmp.reserve(n);\n      for (size_t i = r * n; i < (r + 1) * n; i++) {\n        tmp.push_back(x[i]);\n      }\n      MPI_Send(&tmp[0], tmp.size(), MPI_DOUBLE, r, 0, MPI_COMM_WORLD);\n    }\n  }\n  // rank!= 0\n  else {\n    std::vector<double> tmp;\n    tmp.reserve(n);\n    for (size_t i = rank * n; i < (rank + 1) * n; i++) {\n      tmp.push_back(x[i]);\n    }\n    MPI_Recv(&tmp[0], tmp.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    binsBy10CountLocal(tmp, bins);\n  }\n}",
            "// TODO: Your code here\n\n  return;\n}",
            "int rank;\n    int size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int n = 10;\n    int nb = n / size;\n    int b = nb * rank;\n    int e = b + nb;\n    if (e > n) e = n;\n\n    // Compute local counts\n    for (int i = b; i < e; ++i) {\n        int bin = (int) (x[i] / 10);\n        if (bin < 0) bin = 0;\n        if (bin >= 10) bin = 9;\n        bins[bin]++;\n    }\n\n    // Sum local counts on rank 0\n    if (rank == 0) {\n        std::array<size_t, 10> tally;\n        for (int r = 1; r < size; ++r) {\n            MPI_Recv(tally.data(), 10, MPI_UNSIGNED_LONG, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int i = 0; i < 10; ++i)\n                bins[i] += tally[i];\n        }\n    }\n    else if (rank > 0) {\n        MPI_Send(bins.data(), 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO\n}",
            "for (int i = 0; i < 10; ++i) {\n    bins[i] = 0;\n  }\n  for (int i = 0; i < x.size(); ++i) {\n    int bin = x[i]/10;\n    bins[bin] += 1;\n  }\n}",
            "size_t local_count = 0;\n  for (auto const& i : x) {\n    if (i >= 0 && i < 10)\n      local_count++;\n  }\n\n  MPI_Allreduce(&local_count, &bins[0], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  local_count = 0;\n  for (auto const& i : x) {\n    if (i >= 10 && i < 20)\n      local_count++;\n  }\n  MPI_Allreduce(&local_count, &bins[1], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  local_count = 0;\n  for (auto const& i : x) {\n    if (i >= 20 && i < 30)\n      local_count++;\n  }\n  MPI_Allreduce(&local_count, &bins[2], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  local_count = 0;\n  for (auto const& i : x) {\n    if (i >= 30 && i < 40)\n      local_count++;\n  }\n  MPI_Allreduce(&local_count, &bins[3], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  local_count = 0;\n  for (auto const& i : x) {\n    if (i >= 40 && i < 50)\n      local_count++;\n  }\n  MPI_Allreduce(&local_count, &bins[4], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  local_count = 0;\n  for (auto const& i : x) {\n    if (i >= 50 && i < 60)\n      local_count++;\n  }\n  MPI_Allreduce(&local_count, &bins[5], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  local_count = 0;\n  for (auto const& i : x) {\n    if (i >= 60 && i < 70)\n      local_count++;\n  }\n  MPI_Allreduce(&local_count, &bins[6], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  local_count = 0;\n  for (auto const& i : x) {\n    if (i >= 70 && i < 80)\n      local_count++;\n  }\n  MPI_Allreduce(&local_count, &bins[7], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  local_count = 0;\n  for (auto const& i : x) {\n    if (i >= 80 && i < 90)\n      local_count++;\n  }\n  MPI_Allreduce(&local_count, &bins[8], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  local_count = 0;\n  for (auto const& i : x) {\n    if (i >= 90 && i < 101)\n      local_count++;\n  }\n  MPI_Allreduce(&local_count, &bins[9], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "/* your code goes here */\n}",
            "// TODO: Your code here\n}",
            "// TODO\n  int rank = 0;\n  int size = 1;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int chunk_size = x.size()/size;\n  int start = rank*chunk_size;\n  int end = start+chunk_size;\n\n  std::vector<double> send_x(chunk_size);\n  std::array<size_t, 10> send_bins;\n\n  for(int i = 0; i<chunk_size; i++){\n    send_x[i] = x[i+start];\n  }\n  for(int i = 0; i<10; i++){\n    send_bins[i] = 0;\n  }\n  int step = 10;\n  for(int i = 0; i<chunk_size; i++){\n    if((send_x[i]/10) < 1.0){\n      send_bins[0] += 1;\n    }else if((send_x[i]/10) < 2.0){\n      send_bins[1] += 1;\n    }else if((send_x[i]/10) < 3.0){\n      send_bins[2] += 1;\n    }else if((send_x[i]/10) < 4.0){\n      send_bins[3] += 1;\n    }else if((send_x[i]/10) < 5.0){\n      send_bins[4] += 1;\n    }else if((send_x[i]/10) < 6.0){\n      send_bins[5] += 1;\n    }else if((send_x[i]/10) < 7.0){\n      send_bins[6] += 1;\n    }else if((send_x[i]/10) < 8.0){\n      send_bins[7] += 1;\n    }else if((send_x[i]/10) < 9.0){\n      send_bins[8] += 1;\n    }else if((send_x[i]/10) < 10.0){\n      send_bins[9] += 1;\n    }\n  }\n  std::vector<double> recv_x(0);\n  std::array<size_t, 10> recv_bins = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n  MPI_Gather(&send_bins, 10, MPI_UNSIGNED_LONG, &recv_bins, 10, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n  if(rank == 0){\n    bins = recv_bins;\n  }\n}",
            "bins.fill(0);\n    for (double val : x) {\n        size_t bin = size_t(val / 10);\n        if (bin > 9) bin = 9;\n        bins[bin] += 1;\n    }\n}",
            "std::array<size_t, 10> counts {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n    size_t n = x.size();\n    size_t n_per_rank = n / MPI_Comm_size(MPI_COMM_WORLD);\n    for (size_t i = 0; i < n_per_rank; i++) {\n        size_t val = x[i];\n        size_t bin = floor(val / 10.0);\n        counts[bin]++;\n    }\n    bins = counts;\n}",
            "}",
            "size_t rank, nprocs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    // Make sure x is divisible by nprocs\n    if(nprocs * (x.size() / nprocs)!= x.size()) {\n        std::cerr << \"Input size is not divisible by the number of processes\" << std::endl;\n        return;\n    }\n\n    size_t my_start = x.size() / nprocs * rank;\n    size_t my_end = x.size() / nprocs * (rank + 1);\n\n    std::array<size_t, 10> my_bins;\n    for(int i = 0; i < my_bins.size(); i++) {\n        my_bins[i] = 0;\n    }\n\n    for(int i = my_start; i < my_end; i++) {\n        size_t bin_index = (size_t)floor(x[i] / 10.0);\n        my_bins[bin_index]++;\n    }\n\n    std::array<size_t, 10> total_bins;\n    total_bins.fill(0);\n\n    // Sum the bin counts for all processes\n    for(int i = 0; i < my_bins.size(); i++) {\n        MPI_Allreduce(&my_bins[i], &total_bins[i], 1, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n    }\n\n    // Copy the resulting bins to the output array.\n    for(int i = 0; i < total_bins.size(); i++) {\n        bins[i] = total_bins[i];\n    }\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tint num_bins = 10;\n\tbins.fill(0);\n\n\tif (rank == 0)\n\t{\n\t\tint start = 0;\n\t\tint stride = size;\n\t\tint count = num_bins;\n\t\tMPI_Scatter(x.data(), stride, MPI_DOUBLE, bins.data(), count, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\t\tfor (int i = 0; i < num_bins; i++)\n\t\t{\n\t\t\tif (bins[i] == 0)\n\t\t\t\tcontinue;\n\t\t\tstd::cout << \"Bin \" << i << \" has \" << bins[i] << \" values.\" << std::endl;\n\t\t}\n\t}\n\telse\n\t{\n\t\tint start = rank;\n\t\tint stride = size;\n\t\tint count = num_bins;\n\t\tMPI_Scatter(x.data(), stride, MPI_DOUBLE, bins.data(), count, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\t}\n}",
            "bins.fill(0);\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (rank == 0) {\n    for (int i = 0; i < world_size; i++) {\n      size_t x0 = (x.size() / world_size) * i;\n      size_t x1 = (x.size() / world_size) * (i + 1);\n      size_t offset = x0 / 10;\n\n      for (size_t j = x0; j < x1; j++) {\n        bins[((x[j] - 1) / 10 - offset) % 10] += 1;\n      }\n    }\n  }\n\n  std::vector<size_t> bins_local(10, 0);\n\n  if (rank!= 0) {\n    size_t x0 = (x.size() / world_size) * rank;\n    size_t x1 = (x.size() / world_size) * (rank + 1);\n    size_t offset = x0 / 10;\n\n    for (size_t i = x0; i < x1; i++) {\n      bins_local[((x[i] - 1) / 10 - offset) % 10] += 1;\n    }\n  }\n\n  std::vector<size_t> bins_global(10, 0);\n  MPI_Allgather(&bins_local[0], 10, MPI_UNSIGNED_LONG_LONG, &bins_global[0],\n                10, MPI_UNSIGNED_LONG_LONG, MPI_COMM_WORLD);\n\n  for (size_t i = 0; i < 10; i++) {\n    bins[i] = bins[i] + bins_global[i];\n  }\n}",
            "int nproc, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    \n    //Get the number of values between 0-10\n    size_t x10 = 0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] >= 0 && x[i] <= 10) {\n            ++x10;\n        }\n    }\n    \n    //Split the array into nproc pieces and put each piece in an array\n    std::vector<double> x10_array(x10);\n    size_t idx = 0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] >= 0 && x[i] <= 10) {\n            x10_array[idx] = x[i];\n            ++idx;\n        }\n    }\n    \n    //Send to every rank and receive from rank 0\n    std::array<size_t, 10> result;\n    if (rank == 0) {\n        for (int i = 1; i < nproc; ++i) {\n            MPI_Recv(&result, 10, MPI_UNSIGNED_LONG_LONG, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 10; ++j) {\n                bins[j] += result[j];\n            }\n        }\n    }\n    else {\n        MPI_Send(&x10_array, x10, MPI_DOUBLE, 0, 1, MPI_COMM_WORLD);\n        MPI_Recv(&result, 10, MPI_UNSIGNED_LONG_LONG, 0, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n}",
            "// fill in code here\n    // don't forget to call MPI_Barrier() and MPI_Finalize()!\n}",
            "int my_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n    int num_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    // Determine the number of values for each process to work on\n    // and the starting index of each process\n    size_t const per_proc = x.size() / num_ranks;\n    size_t const start_idx = my_rank * per_proc;\n    size_t const end_idx = my_rank == num_ranks - 1? x.size() : start_idx + per_proc;\n\n    for (size_t i = start_idx; i < end_idx; i++) {\n        size_t const value = (x[i] / 10) * 10;\n        bins[value]++;\n    }\n\n    // TODO: Implement parallel reduction to rank 0\n    //       to compute the final counts\n\n    if (my_rank == 0) {\n        for (size_t i = 1; i < num_ranks; i++) {\n            MPI_Recv(&bins[0], 10, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&bins[0], 10, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t block_size = x.size() / size;\n    size_t mod_size = x.size() % size;\n\n    size_t my_offset = rank * block_size + (rank < mod_size? rank : mod_size);\n    size_t my_size = block_size + (rank < mod_size? 1 : 0);\n\n    std::vector<double> x_local(x.begin() + my_offset, x.begin() + my_offset + my_size);\n\n    bins.fill(0);\n    for (auto& v : x_local) {\n        if (v >= 0 && v < 10) bins[0] += 1;\n        else if (v >= 10 && v < 20) bins[1] += 1;\n        else if (v >= 20 && v < 30) bins[2] += 1;\n        else if (v >= 30 && v < 40) bins[3] += 1;\n        else if (v >= 40 && v < 50) bins[4] += 1;\n        else if (v >= 50 && v < 60) bins[5] += 1;\n        else if (v >= 60 && v < 70) bins[6] += 1;\n        else if (v >= 70 && v < 80) bins[7] += 1;\n        else if (v >= 80 && v < 90) bins[8] += 1;\n        else if (v >= 90 && v <= 100) bins[9] += 1;\n    }\n\n    MPI_Reduce(&bins[0], &bins[0], bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "std::vector<int> result(10);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    //calculating number of values in each interval\n    int bins_per_rank = 10;\n    int bin_size = x.size() / bins_per_rank;\n    for (int i = rank * bin_size; i < (rank + 1) * bin_size; i++) {\n        if (i >= 100) {\n            continue;\n        }\n        int bin = (int) x[i] / 10;\n        result[bin]++;\n    }\n\n    //summing the results\n    MPI_Allreduce(&result[0], &bins[0], 10, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, size;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &size);\n    auto nperrank = x.size() / size;\n    if (x.size() % size > rank) {\n        nperrank += 1;\n    }\n\n    std::vector<double> local_x(nperrank);\n    for (size_t i = 0; i < nperrank; ++i) {\n        local_x[i] = x[rank * nperrank + i];\n    }\n    std::vector<int> local_bins(10, 0);\n    for (size_t i = 0; i < nperrank; ++i) {\n        int bin = static_cast<int>(local_x[i] / 10);\n        if (bin < 0) {\n            bin = 0;\n        }\n        if (bin > 9) {\n            bin = 9;\n        }\n        local_bins[bin] += 1;\n    }\n\n    std::vector<int> global_bins(10, 0);\n    MPI_Allreduce(&local_bins[0], &global_bins[0], 10, MPI_INT, MPI_SUM, comm);\n    bins = std::array<size_t, 10>{global_bins[0], global_bins[1], global_bins[2], global_bins[3],\n                                  global_bins[4], global_bins[5], global_bins[6], global_bins[7],\n                                  global_bins[8], global_bins[9]};\n\n    return;\n}",
            "bins.fill(0);\n\n\tfor (auto const& val : x)\n\t{\n\t\tsize_t bin = (val / 10);\n\t\t\n\t\tif (bin < 10)\n\t\t\tbins[bin]++;\n\t}\n}",
            "const size_t n = x.size();\n  const size_t nbins = 10;\n  std::array<size_t, 10> localBins;\n  MPI_Comm comm = MPI_COMM_WORLD;\n  MPI_Status stat;\n  int rank;\n  int nproc;\n\n  MPI_Comm_size(comm, &nproc);\n  MPI_Comm_rank(comm, &rank);\n\n  // for each rank, figure out which bins it contains\n  for (int i = 0; i < n; i++) {\n    localBins[x[i] / 10]++;\n  }\n\n  // create a vector of empty bins, one for each rank\n  std::array<std::vector<size_t>, 10> sendBins;\n  std::array<std::vector<size_t>, 10> recvBins;\n  for (int i = 0; i < nbins; i++) {\n    sendBins[i] = std::vector<size_t>(nproc, 0);\n    recvBins[i] = std::vector<size_t>(nproc, 0);\n  }\n\n  // send and receive\n  for (int i = 0; i < nbins; i++) {\n    sendBins[i][rank] = localBins[i];\n  }\n\n  MPI_Alltoall(sendBins.data(), 1, MPI_INT, recvBins.data(), 1, MPI_INT, comm);\n\n  // add the ranks together\n  for (int i = 0; i < nbins; i++) {\n    for (int j = 0; j < nproc; j++) {\n      bins[i] += recvBins[i][j];\n    }\n  }\n}",
            "//TODO\n}",
            "// TODO\n\n\n    // ***** COMMENT *****\n    //\n    // For this problem you will need to understand the MPI_Gather() function\n    // - this is a collective operation where every rank has a copy of the\n    //   same data\n    //\n    // You need to determine which value each rank should be using to\n    // determine which bin to put the element into.\n    // - If the number is less than 10 use 0\n    // - If it is greater than 10 and less than 20, use 1\n    // - And so on\n    //\n    // You then need to use a custom data type that will store the bins in a\n    // way that MPI can understand. I suggest you use a vector of size_t.\n    //\n    // You will need to determine the total size of the array (using MPI_Allreduce)\n    // and then use MPI_Gather to gather the individual ranks data into a\n    // large array that will be stored on the root rank\n\n    //\n    // You will need to use MPI_Allreduce() to determine the total size of the\n    // array. You will also need to use MPI_Gather() to gather the data into a\n    // large array on rank 0\n    //\n    // You can assume the following:\n    // - Each rank has a complete copy of the data.\n    // - The array bins is already initialized.\n    // - The array bins on rank 0 is empty\n    //\n    // The number of elements in x is equal to the number of elements in\n    // bins.\n    //\n    // You should return bins on rank 0 after this function.\n\n    //\n    // You should use the MPI_Gather() function to do this. The MPI_Gather()\n    // function has the following parameters:\n    // - Send buffer: A pointer to the values that will be sent. In this case\n    //   the value will be the index of the bin the element belongs in.\n    // - Send count: The number of values to be sent.\n    // - Send type: The MPI datatype of the values to be sent.\n    // - Receive buffer: A pointer to the values that will be stored on each\n    //   rank.\n    // - Receive count: The number of values to be stored.\n    // - Receive type: The MPI datatype of the values to be stored.\n    // - Root: The rank to send to and receive from.\n    //\n    //\n\n}",
            "// Your code goes here\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n    int x_size = x.size();\n    int chunk_size = x_size/n_ranks;\n    int left_over = x_size%n_ranks;\n    MPI_Status status;\n    std::vector<size_t> local_bins(10);\n    local_bins.fill(0);\n\n    if(rank == 0)\n    {\n        bins.fill(0);\n        for(int i = 1; i < n_ranks; i++)\n        {\n            MPI_Recv(local_bins.data(), 10, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n            for(int j = 0; j < 10; j++)\n            {\n                bins[j] += local_bins[j];\n            }\n        }\n    }\n    else\n    {\n        if(left_over > 0)\n        {\n            chunk_size += 1;\n        }\n        for(int i = rank*chunk_size; i < rank*chunk_size+chunk_size; i++)\n        {\n            if(i >= x_size)\n                break;\n            int idx = x[i]/10;\n            local_bins[idx]++;\n        }\n        MPI_Send(local_bins.data(), 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t index = static_cast<size_t> (x[i] / 10);\n        bins[index] += 1;\n    }\n}",
            "size_t num_procs = 1;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    size_t rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size_t n = x.size();\n\n    if (n > 0) {\n        size_t block_size = n / num_procs;\n        size_t remainder = n % num_procs;\n        size_t start = rank * block_size;\n        size_t end = start + block_size;\n\n        if (rank < remainder) {\n            end++;\n        }\n        // std::cout << \"Start: \" << start << std::endl;\n        // std::cout << \"End: \" << end << std::endl;\n        if (rank == 0) {\n            bins.fill(0);\n        }\n\n        for (size_t i = start; i < end; i++) {\n            bins[static_cast<size_t>((x[i] / 10.0))].fetch_add(1);\n        }\n    }\n    std::array<size_t, 10> total_bins;\n    total_bins.fill(0);\n\n    MPI_Reduce(&bins[0], &total_bins[0], 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (int i = 0; i < 10; i++) {\n            std::cout << \"Bin [\" << i * 10 << \", \" << (i + 1) * 10 << \"] = \" << total_bins[i] << std::endl;\n        }\n    }\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    size_t localSize = x.size();\n    size_t globalSize;\n\n    MPI_Allreduce(&localSize, &globalSize, 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    // std::array<size_t, 10> bins(0);\n\n    int localBin;\n    for(int i = 0; i < x.size(); i++) {\n\n        // Compute local bin number\n        localBin = (int)x[i]/10;\n        if(localBin >= 10)\n            localBin = 9;\n\n        bins[localBin]++;\n\n    }\n\n    // Collect all bins on rank 0\n    // if(rank == 0) {\n    //     std::vector<size_t> allBins(10*size);\n    //     MPI_Gather(bins.data(), 10, MPI_UNSIGNED_LONG_LONG, allBins.data(), 10, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n    //     for(int i = 0; i < 10; i++) {\n    //         for(int j = 0; j < size; j++) {\n    //             if(allBins[i*size + j] > 0) {\n    //                 std::cout << \"Bin [\" << i*10 << \", \" << (i+1)*10 << \"): \" << allBins[i*size + j] << std::endl;\n    //             }\n    //         }\n    //     }\n    // }\n    // else {\n    //     MPI_Gather(bins.data(), 10, MPI_UNSIGNED_LONG_LONG, NULL, 10, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n    // }\n\n    // std::cout << \"Bin [0,10): \" << bins[0] << std::endl;\n    // std::cout << \"Bin [10,20): \" << bins[1] << std::endl;\n    // std::cout << \"Bin [20,30): \" << bins[2] << std::endl;\n    // std::cout << \"Bin [30,40): \" << bins[3] << std::endl;\n    // std::cout << \"Bin [40,50): \" << bins[4] << std::endl;\n    // std::cout << \"Bin [50,60): \" << bins[5] << std::endl;\n    // std::cout << \"Bin [60,70): \" << bins[6] << std::endl;\n    // std::cout << \"Bin [70,80): \" << bins[7] << std::endl;\n    // std::cout << \"Bin [80,90): \" << bins[8] << std::endl;\n    // std::cout << \"Bin [90,100): \" << bins[9] << std::endl;\n\n\n    // std::cout << \"localSize: \" << localSize << std::endl;\n\n}",
            "std::vector<double> allx(x);\n\tfor(int rank = 1; rank < MPI_COMM_WORLD.size(); rank++) {\n\t\tstd::vector<double> morex;\n\t\tMPI_Recv(morex.data(), 10, MPI_DOUBLE, rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\tallx.insert(allx.end(), morex.begin(), morex.end());\n\t}\n\tstd::sort(allx.begin(), allx.end());\n\tstd::vector<double>::iterator first = allx.begin();\n\tstd::vector<double>::iterator last = allx.begin();\n\tfor (size_t i = 0; i < allx.size(); i++) {\n\t\tlast++;\n\t\tif (i == 9 || last == allx.end() || *last - *first >= 10) {\n\t\t\tif (last - first > 9) last = first + 9;\n\t\t\tfor(std::vector<double>::iterator it = first; it < last; it++)\n\t\t\t\tbins[std::distance(allx.begin(), it) / 10]++;\n\t\t\tfirst = last;\n\t\t}\n\t}\n\tfor(int rank = 1; rank < MPI_COMM_WORLD.size(); rank++) {\n\t\tstd::vector<double> lessx(bins.begin(), bins.begin() + (bins.size() / 2));\n\t\tMPI_Send(lessx.data(), 5, MPI_DOUBLE, rank, 0, MPI_COMM_WORLD);\n\t\tstd::vector<double> morex(bins.begin() + (bins.size() / 2), bins.end());\n\t\tMPI_Send(morex.data(), 5, MPI_DOUBLE, rank, 0, MPI_COMM_WORLD);\n\t}\n\tMPI_Status status;\n\tstd::vector<double> allbins(bins.begin(), bins.begin() + (bins.size() / 2));\n\tif(MPI_COMM_WORLD.rank() == 0)\n\t\tallbins.insert(allbins.end(), bins.begin() + (bins.size() / 2), bins.end());\n\tif(MPI_COMM_WORLD.rank() > 0) {\n\t\tMPI_Recv(bins.data(), bins.size() / 2, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);\n\t\tstd::vector<double> morebins(bins.begin() + (bins.size() / 2), bins.end());\n\t\tMPI_Recv(morebins.data(), bins.size() / 2, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);\n\t\tbins = std::vector<double>(bins.begin(), bins.end());\n\t\tbins.insert(bins.end(), morebins.begin(), morebins.end());\n\t\tbins.insert(bins.end(), allbins.begin(), allbins.end());\n\t}\n}",
            "int rank, nprocs;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int bin = 0;\n    int maxbin = 100 / 10;\n    int blocksize = x.size() / nprocs;\n    int rem = x.size() % nprocs;\n    int start = rank * blocksize;\n    int end = start + blocksize;\n    int total = 0;\n    if(rank == 0) {\n        for(int i = 0; i < maxbin; ++i) {\n            bins[i] = 0;\n        }\n    }\n\n    if(rank == nprocs-1) {\n        end = x.size() - 1;\n    }\n    else {\n        end = start + blocksize;\n    }\n    if(rank == 0) {\n        for(int i = start; i <= end; ++i) {\n            bin = floor(x[i]/10);\n            bins[bin]++;\n        }\n    }\n    else {\n        for(int i = start; i < start + blocksize; ++i) {\n            bin = floor(x[i]/10);\n            bins[bin]++;\n        }\n    }\n\n    MPI_Gather(&bins[0], 10, MPI_INT, &bins[0], 10, MPI_INT, 0, MPI_COMM_WORLD);\n    if(rank == 0) {\n        for(int i = 0; i < maxbin; ++i) {\n            total += bins[i];\n            printf(\"%d \", bins[i]);\n        }\n        printf(\"\\n\");\n        printf(\"Total: %d\\n\", total);\n    }\n}",
            "const int rank = MPI_Comm_rank(MPI_COMM_WORLD);\n  const int size = MPI_Comm_size(MPI_COMM_WORLD);\n  const double low = rank * 10.0 / size;\n  const double high = (rank + 1) * 10.0 / size;\n  MPI_Request req[size];\n  MPI_Status stat[size];\n\n  // Initialize local bins\n  std::fill(bins.begin(), bins.end(), 0);\n  for (auto &it: x) {\n    if (it >= low && it < high) {\n      auto index = static_cast<size_t>(std::floor(it / 10));\n      ++bins[index];\n    }\n  }\n\n  // Gather counts from each rank\n  for (int i = 0; i < size; ++i) {\n    MPI_Irecv(&bins[0], bins.size(), MPI_UNSIGNED, i, 0, MPI_COMM_WORLD, &req[i]);\n  }\n  for (int i = 0; i < size; ++i) {\n    MPI_Wait(&req[i], &stat[i]);\n  }\n\n  if (rank == 0) {\n    // Print the bins\n    for (auto &it: bins) {\n      std::cout << it << \" \";\n    }\n    std::cout << std::endl;\n  }\n}",
            "assert(x.size() > 0);\n  assert(bins.size() == 10);\n\n  auto f = [](double x) { return int(x/10); };\n\n  auto f_1 = [](int i){ return i*10; };\n\n  std::vector<int> indices(x.size());\n\n  int n = x.size();\n  int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  std::vector<int> x_temp(x.size());\n  std::vector<int> counts(size);\n  std::vector<int> bins_temp(bins.size());\n\n  auto sendcount = x.size() / size;\n\n  if(rank == 0) {\n    bins.fill(0);\n  }\n\n  //send the portion of the vector to the appropriate rank\n  MPI_Scatter(x.data(), sendcount, MPI_DOUBLE, x_temp.data(), sendcount, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for(auto i = 0; i < x_temp.size(); i++) {\n    indices[i] = f(x_temp[i]);\n  }\n\n  //now we have the indices\n  for(auto i = 0; i < indices.size(); i++) {\n    bins_temp[indices[i]]++;\n  }\n\n  MPI_Gather(bins_temp.data(), bins.size(), MPI_INT, counts.data(), bins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n  if(rank == 0) {\n    //if we are rank 0, we have all the data\n    for(auto i = 0; i < counts.size(); i++) {\n      for(auto j = 0; j < counts[i]; j++) {\n        bins[f_1(i)]++;\n      }\n    }\n\n  }\n\n\n}",
            "std::vector<size_t> r(bins.size(), 0);\n    MPI_Allreduce(&x[0], &r[0], x.size(), MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n    size_t count = 0;\n    for(size_t i = 0; i < r.size(); ++i) {\n        count += r[i];\n        bins[i] = count;\n    }\n}",
            "// TODO: Fill in this function\n}",
            "size_t n = x.size();\n  size_t bins_per_rank = (n/10);\n\n  MPI_Comm comm = MPI_COMM_WORLD;\n  int nranks, rank;\n  MPI_Comm_size(comm, &nranks);\n  MPI_Comm_rank(comm, &rank);\n\n  if (n % nranks!= 0) {\n    std::cerr << \"n not a multiple of nranks\" << std::endl;\n    std::exit(1);\n  }\n\n  size_t counts_per_rank = n/nranks;\n\n  if (rank == 0) {\n    for (int i = 0; i < nranks; ++i) {\n      size_t start = counts_per_rank*i;\n      size_t end = counts_per_rank*i + counts_per_rank;\n      std::array<size_t, 10> bins_in_range;\n      for (int j = 0; j < 10; ++j) {\n        bins_in_range[j] = 0;\n      }\n\n      for (int j = start; j < end; ++j) {\n        int bin = (x[j]/10)*10;\n        bins_in_range[bin] += 1;\n      }\n\n      MPI_Send(&bins_in_range[0], 10, MPI_UNSIGNED_LONG_LONG, i, 100, comm);\n    }\n  } else {\n    size_t start = counts_per_rank*rank;\n    size_t end = counts_per_rank*rank + counts_per_rank;\n    std::array<size_t, 10> bins_in_range;\n    for (int j = 0; j < 10; ++j) {\n      bins_in_range[j] = 0;\n    }\n\n    for (int j = start; j < end; ++j) {\n      int bin = (x[j]/10)*10;\n      bins_in_range[bin] += 1;\n    }\n    MPI_Status status;\n    MPI_Recv(&bins[0], 10, MPI_UNSIGNED_LONG_LONG, 0, 100, comm, &status);\n\n    for (int j = 0; j < 10; ++j) {\n      bins[j] += bins_in_range[j];\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 1; i < nranks; ++i) {\n      MPI_Recv(&bins[0], 10, MPI_UNSIGNED_LONG_LONG, i, 100, comm, &status);\n    }\n  }\n}",
            "// TODO: add code here\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    //calculate how many elements does each process have\n    size_t local_size = x.size() / size;\n    //add 1 to last process, if it has not an even number of elements\n    if(size*local_size!= x.size())\n        local_size++;\n\n    //calculate the start and end of every process\n    size_t local_start = rank * local_size;\n    size_t local_end = local_start + local_size - 1;\n\n    //set the beginning of the local array\n    std::vector<double> local_vec(x.begin() + local_start, x.begin() + local_end + 1);\n\n    std::array<size_t, 10> local_bins;\n\n    for(int i = 0; i < 10; i++)\n        local_bins[i] = 0;\n\n    for(int i = 0; i < local_vec.size(); i++)\n    {\n        //local_vec[i] >= 10*i, and local_vec[i] < 10*(i+1)\n        if(local_vec[i] >= 10*i && local_vec[i] < 10*(i+1))\n            local_bins[i] = local_bins[i] + 1;\n    }\n\n    //the rank 0 receives the bins from every process\n    if(rank == 0)\n    {\n        for(int i = 0; i < size; i++)\n        {\n            MPI_Recv(&bins[0], 10, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n\n    //every process sends the local bins to the rank 0\n    if(rank!= 0)\n    {\n        MPI_Send(&local_bins[0], 10, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int rank, num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    // Determine start and end indices for the ranks that will compute bins.\n    // This is done by computing the global index of each rank's first and last\n    // bin, and determining which ranks are responsible for each bin.\n    int firstBin = rank * 10;\n    int lastBin = firstBin + 10;\n    if (rank == num_ranks - 1) {\n        lastBin = 100;\n    }\n\n    // For all bin intervals, figure out which ranks are responsible for\n    // calculating the count.\n    std::vector<int> ranks;\n    for (int i = firstBin; i < lastBin; i++) {\n        int binRank = (i * num_ranks) / 100;\n        ranks.push_back(binRank);\n    }\n\n    // Initialize bins to 0.\n    std::fill(bins.begin(), bins.end(), 0);\n\n    // Perform the calculations for each bin interval.\n    for (int i = firstBin; i < lastBin; i++) {\n        int binRank = ranks[i];\n        int binIndex = i - firstBin;\n\n        // If the rank owns the bin, perform the calculation.\n        if (binRank == rank) {\n            for (int j = i; j < lastBin; j++) {\n                if (x[j] >= i && x[j] < i + 10) {\n                    bins[binIndex] += 1;\n                }\n            }\n        }\n\n        // Otherwise, communicate with the rank that owns the bin.\n        else {\n            MPI_Request request;\n            MPI_Irecv(&bins[binIndex], 1, MPI_LONG_LONG, binRank, 0, MPI_COMM_WORLD, &request);\n\n            int sendRank = binRank;\n            MPI_Send(&sendRank, 1, MPI_INT, binRank, 0, MPI_COMM_WORLD);\n\n            int sendCount = bins[binIndex];\n            MPI_Send(&sendCount, 1, MPI_LONG_LONG, binRank, 0, MPI_COMM_WORLD);\n\n            MPI_Wait(&request, MPI_STATUS_IGNORE);\n        }\n    }\n\n    // If this rank is responsible for a bin, communicate with the rank that owns\n    // the bin.\n    if (rank == firstBin / 10) {\n        for (int i = 0; i < 10; i++) {\n            int binRank = i;\n\n            MPI_Request request;\n            MPI_Irecv(&bins[i], 1, MPI_LONG_LONG, binRank, 0, MPI_COMM_WORLD, &request);\n\n            int sendRank = binRank;\n            MPI_Send(&sendRank, 1, MPI_INT, binRank, 0, MPI_COMM_WORLD);\n\n            int sendCount = bins[i];\n            MPI_Send(&sendCount, 1, MPI_LONG_LONG, binRank, 0, MPI_COMM_WORLD);\n\n            MPI_Wait(&request, MPI_STATUS_IGNORE);\n        }\n    }\n}",
            "// TODO\n}",
            "int rank = 0;\n    int size = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_n = x.size();\n    size_t global_n = local_n * size;\n\n    std::vector<double> local_x(local_n);\n\n    if (rank == 0) {\n        for (int i = 0; i < size; ++i) {\n            std::copy(x.begin() + i * local_n,\n                      x.begin() + (i + 1) * local_n,\n                      local_x.begin());\n        }\n    } else {\n        std::copy(x.begin() + rank * local_n,\n                  x.begin() + (rank + 1) * local_n,\n                  local_x.begin());\n    }\n\n    std::array<size_t, 10> local_bins{};\n    for (size_t i = 0; i < local_n; ++i) {\n        auto v = local_x[i];\n        local_bins[v / 10] += 1;\n    }\n\n    std::array<size_t, 10> global_bins{};\n    MPI_Reduce(local_bins.data(), global_bins.data(), 10, MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}",
            "std::vector<double> x_copy(x.begin(), x.end());\n  std::sort(x_copy.begin(), x_copy.end());\n\n  size_t N = x.size();\n  size_t num_of_bins = 10;\n  size_t binsize = N / num_of_bins;\n  size_t remainder = N % num_of_bins;\n\n  std::vector<size_t> counts(num_of_bins);\n  counts.assign(num_of_bins, 0);\n  size_t start = 0;\n  size_t stop = binsize;\n  size_t i = 0;\n  for(; start < N; i++) {\n    while(i!= remainder && start < N) {\n      if(x_copy[start] <= 10) {\n        counts[i]++;\n        start += binsize;\n      } else {\n        start++;\n      }\n    }\n    if(i == remainder) {\n      while(start < N) {\n        if(x_copy[start] <= 10) {\n          counts[i]++;\n          start += binsize;\n        } else {\n          start++;\n        }\n      }\n    }\n  }\n\n  size_t displs[num_of_bins];\n  displs[0] = 0;\n  for(int i = 1; i < num_of_bins; i++) {\n    displs[i] = displs[i - 1] + counts[i - 1];\n  }\n  MPI_Allgatherv(&counts[0], num_of_bins, MPI_INT, &bins[0], &counts[0], &displs[0], MPI_INT, MPI_COMM_WORLD);\n}",
            "int myRank;\n  int mySize;\n  MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n  MPI_Comm_size(MPI_COMM_WORLD, &mySize);\n\n  // determine the chunk of work this rank will do\n  int stride = x.size() / mySize;\n  int start = myRank * stride;\n  int end = start + stride;\n  if (myRank == mySize - 1) end = x.size();\n\n  // do the work\n  size_t localBins[10] = {0};\n  for (int i = start; i < end; ++i) {\n    int bin = (int) (x[i] / 10.0);\n    if (bin >= 10) bin = 9;\n    localBins[bin]++;\n  }\n\n  // combine the results\n  std::array<size_t, 10> globalBins;\n  if (myRank == 0) {\n    for (int i = 0; i < mySize; ++i) {\n      MPI_Recv(&globalBins[0], 10, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD,\n               MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&localBins[0], 10, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n  for (int i = 0; i < mySize; ++i) {\n    if (i!= 0) {\n      for (int j = 0; j < 10; ++j) {\n        bins[j] += globalBins[j];\n      }\n    }\n  }\n}",
            "const size_t rank = MPI::COMM_WORLD.Get_rank();\n  const size_t size = MPI::COMM_WORLD.Get_size();\n\n  // get my part of x\n  std::vector<double> my_x(x.begin() + rank * size, x.begin() + (rank + 1) * size);\n  std::vector<size_t> my_bins(10);\n\n  // get my count\n  for(int i = 0; i < 10; i++) {\n    auto it = std::lower_bound(my_x.begin(), my_x.end(), i * 10);\n    auto count = std::count_if(it, my_x.end(), [i](double x){return (x >= i * 10) && (x < (i + 1) * 10);});\n    my_bins[i] = count;\n  }\n\n  // get sum of counts\n  std::vector<size_t> all_bins(10 * size);\n  MPI::COMM_WORLD.Allgather(&my_bins[0], 10, MPI::UNSIGNED, &all_bins[0], 10, MPI::UNSIGNED);\n\n  // compute the global counts\n  for(int i = 0; i < 10; i++) {\n    bins[i] = 0;\n    for(int j = 0; j < size; j++) {\n      bins[i] += all_bins[i * size + j];\n    }\n  }\n}",
            "// TODO: Your code here\n    //int rank, size;\n    //MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    //MPI_Comm_size(MPI_COMM_WORLD, &size);\n    size_t numX = x.size();\n    std::vector<double> vx(numX);\n    //int chunk = numX/size;\n    //if(rank == 0)\n    //{\n    //    std::cout<<\"chunk: \"<<chunk<<std::endl;\n    //}\n    int chunk;\n    if(rank == 0)\n    {\n        chunk = numX/size;\n        if(numX%size!= 0)\n            chunk++;\n        std::cout<<\"chunk: \"<<chunk<<std::endl;\n        for(int i = 0; i < size; i++)\n        {\n            MPI_Send(&x[i*chunk], chunk, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n        }\n    }\n    else\n    {\n        MPI_Status status;\n        MPI_Recv(&vx[0], chunk, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);\n    }\n    std::vector<int> vbin(10);\n    for(int i = 0; i < vx.size(); i++)\n    {\n        int bin = std::floor(vx[i]/10.0)*10;\n        vbin[bin]++;\n    }\n    MPI_Gather( &vbin[0], 10, MPI_INT, &bins[0], 10, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// determine how many values are in x (assuming all ranks have same data)\n  // then determine the number of values that belong to each bin\n  //\n  size_t n_values = x.size();\n  size_t n_per_bin = n_values/bins.size();\n\n  for(size_t i=0; i<n_values; i+=n_per_bin) {\n    size_t bin_index = (size_t)(x[i]/10.0);\n    bins[bin_index] += 1;\n  }\n\n  return;\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double *data = new double[x.size()];\n    std::copy(x.begin(), x.end(), data);\n\n    double *results = new double[10];\n    memset(results, 0, sizeof(results));\n\n    // for (int i = 0; i < 10; ++i) {\n    //     int start = i * (size / 10);\n    //     int end = (i+1) * (size / 10) - 1;\n    //     for (int j = 0; j < x.size(); ++j) {\n    //         if (data[j] >= start && data[j] <= end) {\n    //             results[i] += 1;\n    //         }\n    //     }\n    // }\n\n    int *indices = new int[10];\n    memset(indices, 0, sizeof(indices));\n    int *displs = new int[size];\n    MPI_Allgather(&rank, 1, MPI_INT, indices, 1, MPI_INT, MPI_COMM_WORLD);\n\n    int st = 0, ed = 0;\n    for (int i = 0; i < size; ++i) {\n        st = indices[i] * (size / 10);\n        ed = indices[i] * (size / 10) + (size / 10) - 1;\n        for (int j = 0; j < x.size(); ++j) {\n            if (data[j] >= st && data[j] <= ed) {\n                results[i] += 1;\n            }\n        }\n        displs[i] = ed;\n    }\n\n    MPI_Reduce(results, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < size; ++i) {\n        if (i!= rank) {\n            MPI_Get(&results[i], 1, MPI_INT, i, displs[i], 1, MPI_INT, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    delete[] data;\n    delete[] results;\n    delete[] indices;\n    delete[] displs;\n}",
            "size_t n = x.size();\n  // Your code goes here\n  int myRank = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n  int size = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int chunk = n / size;\n  int remainder = n % size;\n  int start, end;\n  if (myRank == 0) {\n    start = 0;\n    end = chunk + remainder;\n  } else {\n    start = myRank * chunk + remainder;\n    end = (myRank + 1) * chunk + remainder;\n  }\n\n  int* counts = new int[10]{0};\n  for (int i = start; i < end; ++i) {\n    int idx = (int)(x[i] / 10);\n    counts[idx] += 1;\n  }\n\n  // Reduce sum to rank 0\n  for (int i = 1; i < size; ++i) {\n    MPI_Status status;\n    MPI_Recv(counts, 10, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n    for (int j = 0; j < 10; ++j) {\n      counts[j] += counts[j];\n    }\n  }\n\n  // Copy counts to bins\n  if (myRank == 0) {\n    for (int i = 0; i < 10; ++i) {\n      bins[i] = counts[i];\n    }\n  }\n\n  delete[] counts;\n}",
            "if(x.size()==0) {\n    std::cerr << \"Vector x is empty\" << std::endl;\n    std::exit(1);\n  }\n\n  int world_rank;\n  int world_size;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  size_t x_min = 0;\n  size_t x_max = 100;\n  size_t x_bin_size = 10;\n\n  // divide the x-values into bins\n  if(world_rank == 0) {\n    for(int i=1; i<world_size; i++) {\n      double x_div = x_max / world_size;\n      size_t x_div_start = i * x_div;\n      size_t x_div_end = std::min(x_div * (i+1), x_max);\n      MPI_Send(&x_div_start, 1, MPI_LONG, i, 0, MPI_COMM_WORLD);\n      MPI_Send(&x_div_end, 1, MPI_LONG, i, 0, MPI_COMM_WORLD);\n    }\n  } else {\n    size_t x_div_start;\n    size_t x_div_end;\n    MPI_Recv(&x_div_start, 1, MPI_LONG, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    MPI_Recv(&x_div_end, 1, MPI_LONG, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    for(auto it = x.begin(); it!= x.end(); it++) {\n      if(*it >= x_div_start && *it < x_div_end) {\n        for(int j=0; j<x_bin_size; j++) {\n          if(*it == x_div_start + j*10) {\n            bins[j]++;\n          }\n        }\n      }\n    }\n  }\n}",
            "for (auto i = 0; i < bins.size(); i++) {\n    auto min = i*10;\n    auto max = min + 10;\n    size_t count = 0;\n    for (auto it = x.begin(); it!= x.end(); ++it) {\n      auto val = *it;\n      if (val >= min && val < max) {\n        ++count;\n      }\n    }\n    bins[i] = count;\n  }\n}",
            "//TODO\n\t//Create a vector and count the number of elements in each interval\n\n\treturn;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // compute the count\n    int count = 0;\n    for (double val : x) {\n        if (val >= 0 && val <= 10) {\n            ++count;\n        }\n    }\n\n    // allocate the counts\n    size_t *counts = new size_t[size];\n    memset(counts, 0, sizeof(size_t) * size);\n\n    // alltoall to distribute the counts\n    MPI_Alltoall(counts, 1, MPI_INT, counts, 1, MPI_INT, MPI_COMM_WORLD);\n\n    // compute the partial results\n    size_t sum = 0;\n    size_t step = 10.0 / size;\n    for (size_t i = 0; i < size; ++i) {\n        if (rank < (counts[i] / step)) {\n            sum += 1;\n        }\n    }\n\n    // allocate the partial results\n    size_t *partial = new size_t[size];\n    memset(partial, 0, sizeof(size_t) * size);\n\n    // allreduce to get the total counts\n    MPI_Allreduce(MPI_IN_PLACE, partial, size, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    // compute the result\n    for (size_t i = 0; i < 10; ++i) {\n        bins[i] = (partial[i] + sum) / 10;\n    }\n}",
            "//TODO\n}",
            "// TODO: Fill in your implementation here\n}",
            "size_t const numprocs = 10;\n    size_t const offset = 10 * MPI_Rank();\n    size_t const numelem = x.size();\n\n    for (size_t i = 0; i < numelem; i++) {\n        size_t bin = std::floor(x[i]/10);\n        if (bin >= numprocs) bin = numprocs - 1;\n        bins[bin]++;\n    }\n\n    /* Sum all the bins in each process */\n    MPI_Allreduce(&bins[0], &bins[0], 10, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    /* Output all the bins to the screen */\n    if (MPI_Rank() == 0) {\n        for (int i = 0; i < 10; i++) {\n            std::cout << bins[i] << \" \";\n        }\n        std::cout << \"\\n\";\n    }\n}",
            "// TODO: Fill this in\n}",
            "size_t N = x.size();\n    // Use MPI_Allreduce to sum counts on each rank\n    std::vector<size_t> counts(10, 0);\n    for (size_t i = 0; i < N; ++i) {\n        size_t bin = x[i] / 10.0;\n        assert(bin < 10);\n        ++counts[bin];\n    }\n    // Use MPI_Reduce to sum the counts on rank 0\n    MPI_Reduce(counts.data(), bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    // Note that if you use MPI_ALLREDUCE, then the counts for each rank must be\n    // divided by the number of ranks.\n}",
            "//TODO: Your code here\n    size_t n = x.size();\n\n    std::vector<int> local_bins(10, 0);\n    for (int i = 0; i < n; ++i) {\n        local_bins[floor(x[i]/10)] += 1;\n    }\n\n    int mpi_bins[10];\n    MPI_Allreduce(local_bins.data(), mpi_bins, 10, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    for (int i = 0; i < 10; ++i) {\n        bins[i] = mpi_bins[i];\n    }\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: Your code here\n}",
            "/* You fill in here.\n     *\n     * Hints:\n     *\n     * - You can compute the index of the bin as (x - 10*i)/10.\n     *   (10*i is 10 times i, so 10*i/10 is just i.\n     *   If x is in the range [10*i, 10*i+10), then 10*i/10 is the bin index.\n     *\n     * - There are several ways to increment a bins in a loop.\n     *   You can use ++bins[i] (which is bins[i]++), or bins[i] += 1, or\n     *   bins[i] = bins[i] + 1.\n     *   Any of these should work.\n     *\n     * - This code assumes that there are 10 processes.\n     *   You should think about how you can make it general.\n     *\n     */\n}",
            "int rank, nproc;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n\tint size = x.size();\n\tint stride = (size + nproc - 1) / nproc;\n\tint remainder = (size - stride * nproc);\n\tint start = stride * rank;\n\tint end = start + stride - 1;\n\tif (rank < remainder) {\n\t\tend += 1;\n\t}\n\n\tstd::array<size_t, 10> local_bins = { 0 };\n\tfor (size_t i = 0; i < x.size(); i++) {\n\t\tif (i < end && i >= start) {\n\t\t\tsize_t num = std::floor(x[i] / 10.0);\n\t\t\tlocal_bins[num] += 1;\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tbins = local_bins;\n\t}\n\tMPI_Reduce(&local_bins[0], &bins[0], 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "// TODO: Your code here\n}",
            "auto N = x.size();\n\tstd::vector<int> bins_int(10, 0);\n\n\tfor (int i = 0; i < N; i++) {\n\t\tint bin = static_cast<int>(x[i] / 10);\n\t\tbins_int[bin]++;\n\t}\n\t// convert to output type\n\tfor (int i = 0; i < 10; i++) {\n\t\tbins[i] = static_cast<size_t>(bins_int[i]);\n\t}\n}",
            "// TODO: Your code here\n\n}",
            "//TODO\n}",
            "size_t count = x.size();\n    MPI_Allreduce(MPI_IN_PLACE, &count, 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n    for(auto &bin : bins) bin = 0;\n\n    MPI_Status status;\n    int rank, worldSize;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n\n    std::vector<double> buf(1000);\n    int num_to_send = count/worldSize;\n    int start_from = rank * num_to_send;\n    int end_at = start_from + num_to_send;\n    if(rank == worldSize - 1) {\n        end_at = count;\n    }\n    std::copy(x.begin() + start_from, x.begin() + end_at, buf.begin());\n\n    std::vector<double> recv_buf(1000);\n\n    MPI_Bcast(&buf[0], buf.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    MPI_Allreduce(buf.data(), recv_buf.data(), buf.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    for(int i = 0; i < 10; ++i) {\n        bins[i] = 0;\n        for(int j = 10 * i; j < 10 * i + 10; ++j) {\n            if(recv_buf[j]!= 0) {\n                bins[i] += 1;\n            }\n        }\n    }\n}",
            "int my_rank;\n  int n_ranks;\n  int n_per_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  n_per_rank = x.size()/n_ranks;\n  int i=0;\n  for (int rank=0; rank<n_ranks; rank++) {\n    int s=rank*n_per_rank;\n    int e=s+n_per_rank;\n    for (int j=s; j<e; j++) {\n      if (x[j] >= i*10 && x[j] < (i+1)*10) {\n        bins[i]++;\n      }\n    }\n    i++;\n  }\n}",
            "/* YOUR CODE HERE */\n\n  /*\n  // MPI_Init(NULL, NULL);\n\n  // Get number of processors\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  // Get current process ID\n  int world_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  // Create a vector of size 10 to store counts of values in each 10-bin\n  std::vector<size_t> local_counts(10, 0);\n\n  // Get size of vector\n  size_t n_entries = x.size();\n\n  // Loop through all entries in vector\n  for(size_t i = 0; i < n_entries; i++) {\n    // Loop through all bins\n    for(int j = 0; j < 10; j++) {\n      if(x[i] >= j * 10 && x[i] < (j+1) * 10) {\n        local_counts[j]++;\n      }\n    }\n  }\n\n  // Combine counts from all processors\n  size_t global_counts[10] = {0};\n  MPI_Reduce(local_counts.data(), global_counts, 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // Copy result into bins\n  for(int j = 0; j < 10; j++) {\n    bins[j] = global_counts[j];\n  }\n\n  // MPI_Finalize();\n  */\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int num_proc;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n    double *local_x = new double[x.size()];\n    for(size_t i = 0; i < x.size(); i++) {\n        local_x[i] = x[i];\n    }\n\n    MPI_Bcast(local_x, x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for(size_t i = 0; i < x.size(); i++) {\n        int bin = int(local_x[i]/10.0);\n        bins[bin] += 1;\n    }\n\n    delete [] local_x;\n}",
            "// Fill this in.\n  //\n  // Use MPI_Bcast to send the values from x to rank 0. Use MPI_Reduce\n  // to count the values in each bin.\n  //\n  // You can use a for loop to count the values in each bin.\n  // The values in x are distributed among the ranks, so you\n  // may have to use MPI_Reduce to combine the values in bins\n  // across all ranks.\n  //\n  // You might find std::partition useful. It can be used to\n  // partition the values of x into bins. For example, you\n  // might want to partition the values between [0, 10) using\n  // the code:\n  //   std::partition(x.begin(), x.end(), [](double d){ return d < 10; })\n\n  MPI_Bcast(&x, x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  MPI_Reduce(&bins, &bins, 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "//TODO: YOUR CODE HERE\n    return;\n}",
            "// TODO\n}",
            "// TODO\n\n}",
            "// Your code here\n}",
            "size_t n = x.size();\n    size_t n_per_rank = n / mpi::nproc();\n    size_t start = n_per_rank * mpi::rank();\n    size_t end = start + n_per_rank;\n    std::vector<double> x_rank(end - start);\n    for (size_t i = start; i < end; ++i) {\n        x_rank[i - start] = x[i];\n    }\n    std::sort(x_rank.begin(), x_rank.end());\n\n    std::array<size_t, 10> bins_rank{};\n    size_t i = 0;\n    for (auto x_i : x_rank) {\n        size_t bin = x_i / 10;\n        bins_rank[bin]++;\n        i++;\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, bins_rank.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    for (auto i = 0; i < 10; i++) {\n        bins[i] += bins_rank[i];\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  auto const numPerBin = size / 10;\n\n  // create local bins\n  std::array<size_t, 10> local_bins = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n\n  // count in local bins\n  for (double const& val : x) {\n    if (val < 10) {\n      ++local_bins[0];\n    } else if (val < 20) {\n      ++local_bins[1];\n    } else if (val < 30) {\n      ++local_bins[2];\n    } else if (val < 40) {\n      ++local_bins[3];\n    } else if (val < 50) {\n      ++local_bins[4];\n    } else if (val < 60) {\n      ++local_bins[5];\n    } else if (val < 70) {\n      ++local_bins[6];\n    } else if (val < 80) {\n      ++local_bins[7];\n    } else if (val < 90) {\n      ++local_bins[8];\n    } else {\n      ++local_bins[9];\n    }\n  }\n\n  // create a send buffer\n  std::vector<size_t> send_buffer;\n  send_buffer.reserve(10);\n\n  // create a receive buffer\n  std::vector<size_t> recv_buffer;\n  recv_buffer.reserve(10);\n\n  // send local bins to the next rank\n  for (int i = 0; i < 10; ++i) {\n    if (rank % numPerBin == i) {\n      send_buffer.push_back(local_bins[i]);\n    } else if (rank % numPerBin == i + 1) {\n      recv_buffer.push_back(local_bins[i]);\n    }\n  }\n\n  // receive data\n  MPI_Status status;\n  int next_rank = rank + 1;\n  MPI_Sendrecv(send_buffer.data(), 10, MPI_UNSIGNED_LONG, next_rank, 0,\n               recv_buffer.data(), 10, MPI_UNSIGNED_LONG, next_rank, 0,\n               MPI_COMM_WORLD, &status);\n\n  // merge the received bins\n  for (int i = 0; i < 10; ++i) {\n    if (rank % numPerBin == i) {\n      local_bins[i] = send_buffer[i];\n    } else if (rank % numPerBin == i + 1) {\n      local_bins[i] += recv_buffer[i];\n    }\n  }\n\n  // sum to get global bins\n  for (int i = 0; i < 10; ++i) {\n    bins[i] = local_bins[i];\n  }\n\n  MPI_Allreduce(MPI_IN_PLACE, bins.data(), 10, MPI_UNSIGNED_LONG,\n                MPI_SUM, MPI_COMM_WORLD);\n}",
            "// TODO: write code to fill bins\n\n    // The value of the MPI rank determines what range of the data will be\n    // processed by the current MPI rank\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // The number of MPI ranks determines the number of values to be processed\n    int numRanks;\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n    // Every rank processes its own subvector of the vector of values x\n    std::vector<double> subVector;\n    size_t n = x.size();\n    size_t offset = rank * (n / numRanks);\n    size_t chunkSize = n / numRanks;\n    subVector.resize(chunkSize);\n\n    // Loop over the subvector, and for each element in it, determine which\n    // bin it belongs to. This is achieved by using modular arithmetic, where\n    // the value of an element x_i is assigned to the bin with the lowest\n    // index for which the expression (x_i - offset) % chunkSize <= 10 holds\n    for (int i = 0; i < chunkSize; i++) {\n        subVector[i] = x[offset + i];\n    }\n    for (int i = 0; i < chunkSize; i++) {\n        int value = subVector[i];\n        int binIndex = (value - offset) % chunkSize;\n        if (binIndex <= 10) {\n            bins[0]++;\n        } else if (binIndex > 10 && binIndex <= 20) {\n            bins[1]++;\n        } else if (binIndex > 20 && binIndex <= 30) {\n            bins[2]++;\n        } else if (binIndex > 30 && binIndex <= 40) {\n            bins[3]++;\n        } else if (binIndex > 40 && binIndex <= 50) {\n            bins[4]++;\n        } else if (binIndex > 50 && binIndex <= 60) {\n            bins[5]++;\n        } else if (binIndex > 60 && binIndex <= 70) {\n            bins[6]++;\n        } else if (binIndex > 70 && binIndex <= 80) {\n            bins[7]++;\n        } else if (binIndex > 80 && binIndex <= 90) {\n            bins[8]++;\n        } else if (binIndex > 90 && binIndex <= 100) {\n            bins[9]++;\n        }\n    }\n\n    // After all the ranks have finished processing their subvector, use MPI\n    // to sum the counts from all the ranks.\n    // This is achieved by using MPI_Reduce with operation MPI_SUM\n    int size = bins.size();\n    int status;\n    MPI_Reduce(bins.data(), bins.data(), size, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // In case the rank of this MPI task is 0, print the counts for each bin\n    if (rank == 0) {\n        std::cout << \"bin 0: \" << bins[0] << std::endl;\n        std::cout << \"bin 1: \" << bins[1] << std::endl;\n        std::cout << \"bin 2: \" << bins[2] << std::endl;\n        std::cout << \"bin 3: \" << bins[3] << std::endl;\n        std::cout << \"bin 4: \" << bins[4] << std::endl;\n        std::cout << \"bin 5: \" << bins[5] << std::endl;\n        std::cout << \"bin 6: \" << bins[6] << std::endl;\n        std::cout << \"bin 7: \" << bins[7] << std::endl;\n        std::cout << \"bin 8: \" << bins[8] << std::endl;\n        std::cout << \"bin 9: \" << bins[9] << std::endl;\n    }\n\n    // MPI_Finalize must",
            "//TODO\n}",
            "bins.fill(0);\n    MPI_Barrier(MPI_COMM_WORLD);\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if (rank == 0) {\n        int chunk = 10;\n        int count = 0;\n        for (double val: x) {\n            if (count % chunk == 0) {\n                std::cout << std::endl << std::endl;\n            }\n            std::cout << val << \" \";\n            bins.at(val/10)++;\n            count++;\n        }\n        std::cout << std::endl;\n    } else {\n        int chunk = 10;\n        for (double val: x) {\n            if (val % 10 == rank * chunk) {\n                bins.at(val/10)++;\n            }\n        }\n    }\n}",
            "// TODO: Your code here\n  return;\n}",
            "int world_size;\n    int world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    size_t n_per_rank = x.size()/world_size;\n\n    size_t begin = world_rank*n_per_rank;\n    size_t end = begin+n_per_rank;\n\n    std::vector<size_t> vec_bins(10, 0);\n\n    for (size_t i = begin; i < end; ++i) {\n        size_t val = x.at(i) / 10;\n        vec_bins.at(val)++;\n    }\n\n    bins.fill(0);\n\n    if(world_rank == 0) {\n        for (size_t i = 0; i < 10; ++i) {\n            MPI_Reduce(&vec_bins[i], &bins[i], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        }\n    }\n    else {\n        MPI_Reduce(vec_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "std::vector<double> newx;\n    for (auto& i: x) {\n        int x1 = i/10;\n        if (x1>=0 && x1<10) newx.push_back(x1);\n    }\n    // use a hash_map\n    int nprocs = 1;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    if (nprocs == 1) {\n        std::unordered_map<int, int> m;\n        for (auto& i: newx) {\n            m[i]++;\n        }\n        for (auto& i: m) {\n            bins[i.first] = i.second;\n        }\n        return;\n    }\n    int rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int newrank = rank;\n    int newnprocs = nprocs;\n    int newsize = newx.size();\n    if (newnprocs % 2 == 1 && newrank == 0) {\n        newrank = nprocs - 1;\n        newnprocs--;\n        newsize--;\n    }\n    std::vector<double> newx1, newx2;\n    newx1.resize(newsize);\n    MPI_Scatter(newx.data(), newsize, MPI_DOUBLE, newx1.data(), newsize, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    newx2.resize(newsize);\n    MPI_Scatter(newx.data(), newsize, MPI_DOUBLE, newx2.data(), newsize, MPI_DOUBLE, 1, MPI_COMM_WORLD);\n    std::unordered_map<int, int> m;\n    for (auto& i: newx1) {\n        m[i]++;\n    }\n    for (auto& i: newx2) {\n        m[i]++;\n    }\n    std::vector<int> m1, m2;\n    m1.resize(10);\n    m2.resize(10);\n    for (auto& i: m) {\n        m1[i.first] = i.second;\n    }\n    MPI_Gather(m1.data(), 10, MPI_INT, m2.data(), 10, MPI_INT, 0, MPI_COMM_WORLD);\n    if (newrank == 0) {\n        for (auto& i: m2) {\n            bins[i] = i;\n        }\n    }\n    return;\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  if (rank == 0) {\n    bins.fill(0);\n  }\n\n  // calculate how many elements per rank to process\n  int elementsPerRank = x.size() / size;\n\n  // calculate the first and last elements of each rank to process\n  int rankStart = rank * elementsPerRank;\n  int rankEnd = rankStart + elementsPerRank;\n  if (rank == size-1) {\n    rankEnd = x.size();\n  }\n\n  // calculate which bins the elements in this rank fall into\n  for (int i = rankStart; i < rankEnd; i++) {\n    int index = (x[i] / 10) - 1;\n    bins[index]++;\n  }\n\n  // reduce the bins on all ranks to rank 0\n  MPI_Reduce(MPI_IN_PLACE, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "std::array<size_t, 10> localBins = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n  for (size_t i = 0; i < x.size(); i++) {\n    int rank = (int)x[i] / 10;\n    localBins[rank]++;\n  }\n\n  bins = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n\n  MPI_Allreduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: Compute my local range\n  size_t begin = rank * (x.size() / size);\n  size_t end = begin + (x.size() / size);\n  std::vector<double> x_local;\n  for(int i = begin; i < end; ++i){\n    x_local.push_back(x[i]);\n  }\n\n  // TODO: Compute number of values in each range\n  int index = 0;\n  std::vector<size_t> count;\n  for(double value : x_local){\n    if(value < 10){\n      ++count[0];\n    }else if(value < 20){\n      ++count[1];\n    }else if(value < 30){\n      ++count[2];\n    }else if(value < 40){\n      ++count[3];\n    }else if(value < 50){\n      ++count[4];\n    }else if(value < 60){\n      ++count[5];\n    }else if(value < 70){\n      ++count[6];\n    }else if(value < 80){\n      ++count[7];\n    }else if(value < 90){\n      ++count[8];\n    }else{\n      ++count[9];\n    }\n  }\n\n  // TODO: Reduce my count to bins on rank 0\n  if(rank == 0){\n    MPI_Reduce(MPI_IN_PLACE, &bins[0], 10, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n  else{\n    MPI_Reduce(&count[0], &bins[0], 10, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n\n}",
            "int rank, n;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &n);\n  int remainder = x.size() % n;\n  int chunk = x.size() / n;\n  size_t begin = rank * chunk;\n  size_t end = begin + chunk - 1;\n  if (remainder > 0 && rank == n-1) {\n    end = x.size()-1;\n  }\n\n  //std::cout << \"rank \" << rank << \" has bins \" << bins.size() << std::endl;\n  //for (auto i=bins.begin(); i!=bins.end(); i++) {\n  //  std::cout << *i << \" \";\n  //}\n  //std::cout << std::endl;\n\n  size_t count = 0;\n  for (auto i=begin; i<=end; i++) {\n    int bin = x[i] / 10;\n    if (x[i] % 10 > 5) {\n      bin++;\n    }\n    if (bin >= 10) {\n      bin = 9;\n    }\n    count += (x[i] % 10 > 5);\n    bins[bin] += count;\n  }\n}",
            "// Your code here\n}",
            "// get MPI rank and size\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // compute rank range for x\n    double rank_size = (double) size;\n    double start = (double) rank * (100.0 / rank_size);\n    double end = start + (100.0 / rank_size);\n    double bin_size = 10.0 / rank_size;\n    // get local count\n    size_t local_count = 0;\n    for (double x_val : x) {\n        if (x_val >= start && x_val < end) {\n            local_count++;\n        }\n    }\n\n    // get the global count\n    // initialize a recieve buffer\n    std::array<size_t, 10> recv_buffer;\n    // create a request for recieve\n    MPI_Request request;\n    // get the global count and send/recieve the data\n    MPI_Irecv(recv_buffer.data(), 10, MPI_UNSIGNED_LONG_LONG, 0, 0,\n              MPI_COMM_WORLD, &request);\n    MPI_Send(&local_count, 1, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    MPI_Wait(&request, MPI_STATUS_IGNORE);\n\n    for (int i = 0; i < 10; i++) {\n        bins[i] += recv_buffer[i];\n    }\n}",
            "size_t constexpr N = 10;\n\n    // Each rank sends the local size of its segment to rank 0.\n    int n = x.size() / N;\n    MPI_Gather(&n, 1, MPI_INT, bins.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // Each rank computes the local count.\n    size_t localCount = 0;\n    size_t const start = 10*MPI_Comm_rank(MPI_COMM_WORLD);\n    for(size_t i = start; i < start + n; i++)\n        if(x[i] < 10 || x[i] >= 20)\n            localCount++;\n\n    // Each rank sends its local count to rank 0.\n    MPI_Gather(&localCount, 1, MPI_LONG, bins.data(), 1, MPI_LONG, 0, MPI_COMM_WORLD);\n\n    // Rank 0 accumulates the local counts.\n    if(MPI_Comm_rank(MPI_COMM_WORLD) == 0) {\n        for(size_t i = 1; i < N; i++)\n            bins[i] += bins[i-1];\n    }\n\n    // Rank 0 distributes the bins to the other ranks.\n    if(MPI_Comm_rank(MPI_COMM_WORLD) == 0) {\n        for(int i = 1; i < N; i++)\n            MPI_Send(&bins[i], 1, MPI_LONG, i, 0, MPI_COMM_WORLD);\n    }\n    else {\n        MPI_Status status;\n        MPI_Recv(&bins[0], 1, MPI_LONG, 0, 0, MPI_COMM_WORLD, &status);\n    }\n}",
            "int rank, nproc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    // split the work among processes\n    size_t start = rank * 10;\n    size_t end = start + 10;\n\n    // compute the sum in [start, end)\n    size_t sum = 0;\n    for(size_t i = start; i < end; i++) {\n        size_t bin = (size_t) (x[i]/10);\n        sum += bin;\n    }\n\n    // allgather the results\n    std::vector<size_t> sums(nproc);\n    MPI_Allgather(&sum, 1, MPI_UNSIGNED_LONG, &sums[0], 1, MPI_UNSIGNED_LONG, MPI_COMM_WORLD);\n\n    // store the result\n    for(int i = 0; i < nproc; i++) {\n        size_t index = 10*i;\n        for(size_t j = 0; j < 10; j++) {\n            bins[j] += sums[i];\n        }\n    }\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  size_t bins_size = bins.size();\n\n  // The number of elements per process\n  int elements_per_process = x.size() / MPI_COMM_WORLD_SIZE;\n\n  // The start and end index of x that this process owns\n  int start_idx = rank * elements_per_process;\n  int end_idx = start_idx + elements_per_process;\n\n  // The rank of the process that holds the data we're interested in.\n  int bin_process = 0;\n\n  // If this process holds no data, there's nothing to do\n  if (start_idx >= end_idx)\n    return;\n\n  // If this process holds a number in [0,10) the bin_process is 0\n  if (x[start_idx] < 10)\n    bin_process = 0;\n\n  // If this process holds a number in [10,20) the bin_process is 1\n  else if (x[start_idx] < 20)\n    bin_process = 1;\n\n  // If this process holds a number in [20,30) the bin_process is 2\n  else if (x[start_idx] < 30)\n    bin_process = 2;\n\n  // If this process holds a number in [30,40) the bin_process is 3\n  else if (x[start_idx] < 40)\n    bin_process = 3;\n\n  // If this process holds a number in [40,50) the bin_process is 4\n  else if (x[start_idx] < 50)\n    bin_process = 4;\n\n  // If this process holds a number in [50,60) the bin_process is 5\n  else if (x[start_idx] < 60)\n    bin_process = 5;\n\n  // If this process holds a number in [60,70) the bin_process is 6\n  else if (x[start_idx] < 70)\n    bin_process = 6;\n\n  // If this process holds a number in [70,80) the bin_process is 7\n  else if (x[start_idx] < 80)\n    bin_process = 7;\n\n  // If this process holds a number in [80,90) the bin_process is 8\n  else if (x[start_idx] < 90)\n    bin_process = 8;\n\n  // If this process holds a number in [90,100) the bin_process is 9\n  else if (x[start_idx] < 100)\n    bin_process = 9;\n\n  // Now that we know which process we're interested in, use the bin_process\n  // to find the global process number in the communicator\n  MPI_Comm split_comm;\n  MPI_Comm_split(MPI_COMM_WORLD, bin_process, rank, &split_comm);\n\n  MPI_Request send_req;\n  MPI_Request recv_req;\n\n  size_t total_elements = x.size();\n\n  // The number of elements owned by the process that we're interested in\n  size_t count = end_idx - start_idx;\n\n  // The number of elements on the process that we're interested in\n  size_t local_count = 0;\n\n  // The sum of elements on the process that we're interested in\n  double local_sum = 0;\n\n  // Count the elements owned by this process\n  for (int i = start_idx; i < end_idx; ++i) {\n    if (x[i] < 10)\n      local_sum += 1;\n    else if (x[i] < 20)\n      local_sum += 2;\n    else if (x[i] < 30)\n      local_sum += 3;\n    else if (x[i] < 40)\n      local_sum += 4;\n    else if (x[i] < 50)",
            "// TODO\n}",
            "// Your code here\n  //...\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    // TODO\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int countPerRank = x.size()/size;\n    std::vector<double> xCopy(x.begin()+rank*countPerRank, x.begin()+(rank+1)*countPerRank);\n    for (auto& d : xCopy) {\n        auto idx = (int)(d/10);\n        bins[idx]++;\n    }\n}",
            "// your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int localSize = x.size() / size;\n  int remainder = x.size() % size;\n  int start = rank * localSize;\n  if (rank < remainder) start += rank;\n  else start += remainder;\n  int end = start + localSize;\n\n  for (int i = start; i < end; ++i) {\n    bins[std::floor(x[i] / 10)] += 1;\n  }\n}",
            "auto comm = MPI_COMM_WORLD;\n  auto rank = MPI_Comm_rank(comm);\n  auto size = MPI_Comm_size(comm);\n\n  auto start = x.begin() + rank * (x.size() / size);\n  auto stop = start + (x.size() / size);\n\n  auto binsLocal = std::array<size_t, 10> {0};\n\n  // Count values in range [0, 10), [10, 20),... and store in `binsLocal`\n  for (auto it = start; it!= stop; ++it) {\n    auto value = *it;\n    auto bin = static_cast<size_t>((value / 10) * 10);\n    ++binsLocal[bin];\n  }\n\n  // Add local counts to global bins\n  std::fill(bins.begin(), bins.end(), 0);\n  MPI_Reduce(binsLocal.data(), bins.data(), 10, MPI_LONG_LONG_INT, MPI_SUM, 0, comm);\n}",
            "bins.fill(0);\n  for (auto const& v : x) {\n    size_t idx = v / 10;\n    idx *= 10;\n    bins[idx]++;\n  }\n}",
            "// TODO\n}",
            "// TODO: Your code here\n\n  // MPI_Barrier(MPI_COMM_WORLD);\n  // if (rank == 0) {\n  //   std::cout << \"Rank 0: \" << bins[0] << std::endl;\n  // }\n}",
            "bins.fill(0);\n  // TODO\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t count = 0;\n    std::vector<double> send_buf;\n    if (rank == 0) {\n        for (size_t i = 0; i < x.size(); i++) {\n            if (x[i] < 10) {\n                ++bins[0];\n            } else if (x[i] < 20) {\n                ++bins[1];\n            } else if (x[i] < 30) {\n                ++bins[2];\n            } else if (x[i] < 40) {\n                ++bins[3];\n            } else if (x[i] < 50) {\n                ++bins[4];\n            } else if (x[i] < 60) {\n                ++bins[5];\n            } else if (x[i] < 70) {\n                ++bins[6];\n            } else if (x[i] < 80) {\n                ++bins[7];\n            } else if (x[i] < 90) {\n                ++bins[8];\n            } else if (x[i] < 100) {\n                ++bins[9];\n            } else {\n                assert(false);\n            }\n        }\n    } else {\n        for (size_t i = 0; i < x.size(); i++) {\n            if (x[i] < 10) {\n                ++count;\n            } else if (x[i] < 20) {\n                ++count;\n            } else if (x[i] < 30) {\n                ++count;\n            } else if (x[i] < 40) {\n                ++count;\n            } else if (x[i] < 50) {\n                ++count;\n            } else if (x[i] < 60) {\n                ++count;\n            } else if (x[i] < 70) {\n                ++count;\n            } else if (x[i] < 80) {\n                ++count;\n            } else if (x[i] < 90) {\n                ++count;\n            } else if (x[i] < 100) {\n                ++count;\n            } else {\n                assert(false);\n            }\n        }\n\n        send_buf.resize(count);\n        for (size_t i = 0; i < count; i++) {\n            send_buf[i] = x[i];\n        }\n        count = 0;\n    }\n\n    if (rank == 0) {\n        return;\n    }\n    std::vector<double> recv_buf;\n    MPI_Status status;\n    MPI_Recv(recv_buf.data(), count, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);\n\n    for (size_t i = 0; i < recv_buf.size(); i++) {\n        if (recv_buf[i] < 10) {\n            ++bins[0];\n        } else if (recv_buf[i] < 20) {\n            ++bins[1];\n        } else if (recv_buf[i] < 30) {\n            ++bins[2];\n        } else if (recv_buf[i] < 40) {\n            ++bins[3];\n        } else if (recv_buf[i] < 50) {\n            ++bins[4];\n        } else if (recv_buf[i] < 60) {\n            ++bins[5];\n        } else if (recv_buf[i] < 70) {\n            ++bins[6];\n        } else if (recv_buf[i] < 80) {\n            ++bins[7];\n        } else if (recv_buf[i] < 90) {\n            ++bins[8];\n        } else if (recv_buf",
            "auto rank = MPI_Comm_rank(MPI_COMM_WORLD);\n  auto size = MPI_Comm_size(MPI_COMM_WORLD);\n\n  std::vector<double> x10(x.size());\n  for (size_t i = 0; i < x.size(); ++i) {\n    x10[i] = std::floor(x[i] / 10);\n  }\n\n  std::vector<size_t> counts(10);\n  counts.assign(10, 0);\n  for (size_t i = 0; i < x10.size(); ++i) {\n    ++counts[x10[i]];\n  }\n\n  std::vector<size_t> temp(size);\n  MPI_Allreduce(MPI_IN_PLACE, counts.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (size_t i = 0; i < 10; ++i) {\n      bins[i] = counts[i];\n    }\n  }\n}",
            "// You code here\n    size_t size = x.size();\n    size_t local_count[10];\n    size_t global_count[10];\n    double local_sum[10];\n    double global_sum[10];\n    for (size_t i = 0; i < 10; i++) {\n        local_count[i] = 0;\n        local_sum[i] = 0;\n    }\n    for (size_t i = 0; i < size; i++) {\n        if (x[i] >= 0 && x[i] < 10) {\n            local_count[0]++;\n            local_sum[0] = local_sum[0] + x[i];\n        }\n        if (x[i] >= 10 && x[i] < 20) {\n            local_count[1]++;\n            local_sum[1] = local_sum[1] + x[i];\n        }\n        if (x[i] >= 20 && x[i] < 30) {\n            local_count[2]++;\n            local_sum[2] = local_sum[2] + x[i];\n        }\n        if (x[i] >= 30 && x[i] < 40) {\n            local_count[3]++;\n            local_sum[3] = local_sum[3] + x[i];\n        }\n        if (x[i] >= 40 && x[i] < 50) {\n            local_count[4]++;\n            local_sum[4] = local_sum[4] + x[i];\n        }\n        if (x[i] >= 50 && x[i] < 60) {\n            local_count[5]++;\n            local_sum[5] = local_sum[5] + x[i];\n        }\n        if (x[i] >= 60 && x[i] < 70) {\n            local_count[6]++;\n            local_sum[6] = local_sum[6] + x[i];\n        }\n        if (x[i] >= 70 && x[i] < 80) {\n            local_count[7]++;\n            local_sum[7] = local_sum[7] + x[i];\n        }\n        if (x[i] >= 80 && x[i] < 90) {\n            local_count[8]++;\n            local_sum[8] = local_sum[8] + x[i];\n        }\n        if (x[i] >= 90 && x[i] <= 100) {\n            local_count[9]++;\n            local_sum[9] = local_sum[9] + x[i];\n        }\n    }\n    for (size_t i = 0; i < 10; i++) {\n        MPI_Reduce(&local_count[i], &global_count[i], 1, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        MPI_Reduce(&local_sum[i], &global_sum[i], 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n    if (rank == 0) {\n        for (size_t i = 0; i < 10; i++) {\n            bins[i] = global_count[i];\n        }\n    }\n}",
            "int my_rank, num_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    // TODO: your code goes here\n    MPI_Bcast(&bins[0], bins.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    int block = (x.size() + num_ranks - 1) / num_ranks;\n    int local_count = (my_rank == num_ranks - 1)? x.size() % num_ranks : block;\n    if (my_rank == 0) {\n        MPI_Scatter(x.data(), local_count, MPI_DOUBLE, bins.data(), block, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Scatter(x.data(), local_count, MPI_DOUBLE, bins.data(), block, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n    for (int i = 0; i < local_count; ++i) {\n        size_t id = floor(x[i] / 10) % 10;\n        bins[id] += 1;\n    }\n    if (my_rank == 0) {\n        MPI_Reduce(bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Reduce(bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Compute the start index and the count of data for each rank\n    size_t n = x.size();\n    size_t index = rank * n / size;\n    size_t count = n / size;\n\n    std::vector<double> bins_loc(10, 0.0);\n\n    for (size_t i = index; i < index + count; ++i) {\n        double x_i = x[i];\n        if (x_i > 10.0) {\n            std::cout << \"Warning: x value above 10: \" << x_i << std::endl;\n        } else {\n            int bin = x_i / 10;\n            bins_loc[bin] += 1.0;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 0; r < size; r++) {\n            std::vector<double> bins_tmp;\n            MPI_Recv(&bins_tmp, 10, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int i = 0; i < 10; i++) {\n                bins[i] += bins_tmp[i];\n            }\n        }\n    } else {\n        MPI_Send(&bins_loc, 10, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "for (auto& bin : bins)\n        bin = 0;\n\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, size;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &size);\n\n    int rank_bins[10];\n    for (int i = 0; i < 10; i++)\n        rank_bins[i] = 0;\n\n    for (auto it = x.begin(); it!= x.end(); it++) {\n        int value = (*it) * 10;\n        int bin = value % 10;\n        rank_bins[bin]++;\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, rank_bins, 10, MPI_INT, MPI_SUM, comm);\n    for (int i = 0; i < 10; i++)\n        bins[i] = rank_bins[i];\n}",
            "bins.fill(0);\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tbins[x[i] / 10] += 1;\n\t}\n\n\t// MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tint nproc;\n\tMPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n\tint rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tif (nproc > 1) {\n\t\tsize_t *counts = new size_t[nproc];\n\n\t\t// allgather to gather counts of each value\n\t\tMPI_Allgather(bins.data(), bins.size(), MPI_UNSIGNED_LONG, counts, bins.size(), MPI_UNSIGNED_LONG, MPI_COMM_WORLD);\n\n\t\t// sum the counts of each value\n\t\tsize_t sum = 0;\n\t\tfor (int i = 0; i < nproc; ++i) {\n\t\t\tsum += counts[i];\n\t\t}\n\n\t\tsize_t *pSum = new size_t[nproc];\n\t\tpSum[rank] = sum;\n\n\t\tMPI_Allgather(pSum, nproc, MPI_UNSIGNED_LONG, pSum, nproc, MPI_UNSIGNED_LONG, MPI_COMM_WORLD);\n\n\t\t// now we can recalculate the bins\n\t\tsize_t pos = 0;\n\t\tfor (int i = 0; i < nproc; ++i) {\n\t\t\tfor (int j = 0; j < bins.size(); ++j) {\n\t\t\t\tif (i == 0) {\n\t\t\t\t\tbins[j] = counts[i];\n\t\t\t\t\tpos = j;\n\t\t\t\t} else {\n\t\t\t\t\tbins[pos] += counts[i];\n\t\t\t\t\tpos = (pos + 1) % bins.size();\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tdelete[] bins.data();\n\tdelete[] counts;\n}",
            "int rank, nproc;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  if (rank == 0) {\n    std::fill(bins.begin(), bins.end(), 0);\n    int index = 0;\n    double val = x[index];\n    int bin = (int)(val/10.0);\n\n    for (int i = 1; i < x.size(); i++) {\n      int next_bin = (int)(x[i]/10.0);\n      if (next_bin!= bin) {\n        bins[bin]++;\n        bin = next_bin;\n      }\n    }\n    bins[bin]++;\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < nproc; i++) {\n      if (i == 0) {\n        for (int j = 0; j < 10; j++) {\n          bins[j] += bins[j + 10];\n          std::cout << bins[j] << \" \";\n        }\n        std::cout << \"\\n\";\n      } else {\n        MPI_Status status;\n        MPI_Recv(&bins[0], 10, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n        for (int j = 0; j < 10; j++) {\n          std::cout << bins[j] << \" \";\n        }\n        std::cout << \"\\n\";\n      }\n    }\n  } else {\n    MPI_Send(&bins[0], 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "// TODO: Your code goes here\n\n}",
            "std::array<size_t, 10> counts{0};\n\n  /* TODO: insert code to compute the bins */\n  MPI_Comm_size(MPI_COMM_WORLD, &counts.size());\n  std::vector<double> local_x(x.begin() + counts[rank()], x.begin() + counts[rank()] + counts[rank() + 1]);\n  size_t local_count = 0;\n  for(double i : local_x) {\n    if(i <= 10)\n      local_count++;\n  }\n  counts[rank()] = local_count;\n  MPI_Allreduce(counts.data(), bins.data(), counts.size(), MPI_UNSIGNED, MPI_SUM, MPI_COMM_WORLD);\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    std::vector<double> local_x;\n    if (rank == 0) {\n        local_x = x;\n    } else {\n        local_x.resize(x.size() / size);\n    }\n    int bins_per_rank = 10;\n    std::array<int, 10> local_bins;\n    local_bins.fill(0);\n    for (auto&& v : local_x) {\n        local_bins[v / 10]++;\n    }\n    MPI_Gather(&local_bins, 10, MPI_INT, &bins, 10, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "for (auto &&el : x) {\n        if (el < 10) bins[0]++;\n        else if (el >= 10 && el < 20) bins[1]++;\n        else if (el >= 20 && el < 30) bins[2]++;\n        else if (el >= 30 && el < 40) bins[3]++;\n        else if (el >= 40 && el < 50) bins[4]++;\n        else if (el >= 50 && el < 60) bins[5]++;\n        else if (el >= 60 && el < 70) bins[6]++;\n        else if (el >= 70 && el < 80) bins[7]++;\n        else if (el >= 80 && el < 90) bins[8]++;\n        else if (el >= 90) bins[9]++;\n    }\n}",
            "int nproc, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  std::array<size_t, 10> bincount;\n  for (size_t i=0; i < 10; i++)\n    bincount[i] = 0;\n\n  if (rank == 0) {\n    int size = (int)x.size();\n    int local = size/nproc;\n    int rem = size%nproc;\n    int offset = 0;\n\n    if (rem > 0) {\n      if (rank < rem) {\n        local++;\n        offset = rank * (local + 1);\n      }\n      else {\n        offset = rank * local + rem;\n      }\n    }\n    else {\n      offset = rank * local;\n    }\n\n    for (int i=0; i < local; i++) {\n      size_t bin = (size_t) (x[offset + i] / 10);\n      bincount[bin]++;\n    }\n\n    for (int i=0; i < 10; i++) {\n      MPI_Send(bincount.data() + i, 1, MPI_UNSIGNED_LONG, i, 1, MPI_COMM_WORLD);\n    }\n  }\n  else {\n    MPI_Status status;\n    int local = 0;\n    int rem = 0;\n    int offset = 0;\n\n    if (rank < rem) {\n      local++;\n      offset = rank * (local + 1);\n    }\n    else {\n      offset = rank * local + rem;\n    }\n\n    for (int i=0; i < local; i++) {\n      size_t bin = (size_t) (x[offset + i] / 10);\n      bincount[bin]++;\n    }\n\n    MPI_Recv(bins.data(), 10, MPI_UNSIGNED_LONG, 0, 1, MPI_COMM_WORLD, &status);\n  }\n}",
            "for(auto xval : x) {\n        int bin_num = (xval/10);\n        bins[bin_num]++;\n    }\n}",
            "size_t rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint x_size = x.size();\n\tint x_part = x_size / size;\n\tint x_extra = x_size % size;\n\tint x_from = rank * x_part + std::min(rank, x_extra);\n\tint x_to = x_from + x_part + (rank < x_extra? 1 : 0);\n\tint x_part_size = x_to - x_from;\n\n\tfor (int i = 0; i < 10; i++) {\n\t\tbins[i] = 0;\n\t}\n\n\tfor (int i = x_from; i < x_to; i++) {\n\t\tbins[static_cast<int>(std::floor(x[i] / 10.0))]++;\n\t}\n\n\tMPI_Reduce(&bins[0], &bins[0], 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < 10; i++) {\n\t\t\tstd::cout << bins[i] << \" \";\n\t\t}\n\t}\n}",
            "// TODO\n  MPI_Status status;\n  MPI_Request request;\n\n  size_t rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  size_t offset = rank * x.size() / size;\n  size_t my_size = x.size() / size;\n  for (int i = 0; i < 10; i++) {\n    MPI_Ireduce(&x[i*my_size], &bins[i], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD, &request);\n    MPI_Wait(&request, &status);\n  }\n}",
            "// TODO: Your code here\n\n  // The following code works well, but it is not parallel.\n  //std::vector<double> x2(x);\n  //std::sort(x2.begin(), x2.end());\n  //size_t start = 0;\n  //size_t end = 10;\n  //for (int i=0; i < 10; i++) {\n  //  bins[i] = std::count_if(x2.begin(), x2.end(),\n  //                          [&start, &end](double i) { return start <= i && i < end; });\n  //  start += 10;\n  //  end += 10;\n  //}\n\n  // We will use MPI to split the vector into equally sized chunks.\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  std::vector<double> x2(x);\n  int chunk_size = x2.size() / size;\n  int remainder = x2.size() % size;\n  if (rank < remainder) {\n    chunk_size++;\n  }\n  std::vector<double> x_rank(chunk_size);\n  std::copy_n(x2.begin() + rank * chunk_size, chunk_size, x_rank.begin());\n  std::sort(x_rank.begin(), x_rank.end());\n  size_t start = 0;\n  size_t end = 10;\n  for (int i=0; i < 10; i++) {\n    bins[i] = std::count_if(x_rank.begin(), x_rank.end(),\n                            [&start, &end](double i) { return start <= i && i < end; });\n    start += 10;\n    end += 10;\n  }\n\n  // We will use MPI to reduce results across the ranks.\n  std::vector<size_t> bins_reduced(10, 0);\n  MPI_Reduce(bins.data(), bins_reduced.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    std::cout << \"bins: \";\n    for (size_t i=0; i < 10; i++) {\n      std::cout << i << \": \" << bins_reduced[i] << \" \";\n    }\n    std::cout << std::endl;\n  }\n}",
            "bins.fill(0);\n\n  /* TODO: write your solution here */\n\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  size_t n = x.size();\n  std::vector<size_t> local_bins(10);\n\n  if (n > 0) {\n    int k = std::min(10, (int)n);\n    double min = x[0];\n    double max = x[0];\n    for (size_t i = 1; i < n; ++i) {\n      min = std::min(min, x[i]);\n      max = std::max(max, x[i]);\n    }\n    double step = (max - min) / 10.0;\n    int my_bins[10];\n    for (int i = 0; i < k; ++i) {\n      double xi = x[i];\n      int bin = (xi - min) / step;\n      if (bin < 0) {\n        bin = 0;\n      } else if (bin >= 10) {\n        bin = 9;\n      }\n      my_bins[bin] += 1;\n    }\n    for (int i = 0; i < 10; ++i) {\n      local_bins[i] = my_bins[i];\n    }\n  }\n\n  // Reduce local counts\n  std::vector<size_t> global_bins(10, 0);\n  MPI_Allreduce(local_bins.data(), global_bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  // Gather global counts on rank 0\n  if (rank == 0) {\n    for (int i = 0; i < 10; ++i) {\n      bins[i] = global_bins[i];\n    }\n  }\n}",
            "auto size = x.size();\n\tint my_rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\tint num_ranks;\n\tMPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\tsize_t binsize = size/num_ranks;\n\tstd::array<size_t, 10> partial_bins = {};\n\tfor(int i=0; i<binsize; i++) {\n\t\tif(x[my_rank*binsize + i] >= 0 and x[my_rank*binsize + i] <= 10)\n\t\t\tpartial_bins[0] += 1;\n\t\tif(x[my_rank*binsize + i] >= 10 and x[my_rank*binsize + i] <= 20)\n\t\t\tpartial_bins[1] += 1;\n\t\tif(x[my_rank*binsize + i] >= 20 and x[my_rank*binsize + i] <= 30)\n\t\t\tpartial_bins[2] += 1;\n\t\tif(x[my_rank*binsize + i] >= 30 and x[my_rank*binsize + i] <= 40)\n\t\t\tpartial_bins[3] += 1;\n\t\tif(x[my_rank*binsize + i] >= 40 and x[my_rank*binsize + i] <= 50)\n\t\t\tpartial_bins[4] += 1;\n\t\tif(x[my_rank*binsize + i] >= 50 and x[my_rank*binsize + i] <= 60)\n\t\t\tpartial_bins[5] += 1;\n\t\tif(x[my_rank*binsize + i] >= 60 and x[my_rank*binsize + i] <= 70)\n\t\t\tpartial_bins[6] += 1;\n\t\tif(x[my_rank*binsize + i] >= 70 and x[my_rank*binsize + i] <= 80)\n\t\t\tpartial_bins[7] += 1;\n\t\tif(x[my_rank*binsize + i] >= 80 and x[my_rank*binsize + i] <= 90)\n\t\t\tpartial_bins[8] += 1;\n\t\tif(x[my_rank*binsize + i] >= 90 and x[my_rank*binsize + i] <= 100)\n\t\t\tpartial_bins[9] += 1;\n\t}\n\tMPI_Reduce(MPI_IN_PLACE, partial_bins.data(), 10, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\tbins = partial_bins;\n}",
            "int rank, nproc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    std::vector<double> sub_x;\n    if (rank == 0) {\n        sub_x = x;\n    } else {\n        sub_x.resize(x.size()/nproc);\n        MPI_Scatter(x.data(), sub_x.size(), MPI_DOUBLE, sub_x.data(), sub_x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n\n    std::vector<size_t> sub_bins(10);\n    for (int i=0; i<10; i++) {\n        sub_bins[i] = std::count_if(sub_x.begin(), sub_x.end(), [i](double a){return a/10.0 == i;});\n    }\n\n    bins = std::array<size_t, 10>();\n    if (rank == 0) {\n        MPI_Gather(sub_bins.data(), 10, MPI_UNSIGNED_LONG_LONG, bins.data(), 10, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Gather(sub_bins.data(), 10, MPI_UNSIGNED_LONG_LONG, NULL, 10, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n    }\n}",
            "int rank, nranks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n  // get the local size and local offset\n  int nlocal = x.size() / nranks;\n  int nremaining = x.size() % nranks;\n\n  // make sure the data is evenly distributed\n  if (nranks > x.size()) {\n    nlocal = nremaining;\n  }\n\n  int loffset = rank * nlocal;\n  int nlocal_full = nlocal;\n  if (rank < nremaining) {\n    nlocal_full++;\n  }\n  std::vector<double> xlocal(x.begin() + loffset, x.begin() + loffset + nlocal_full);\n\n  std::array<size_t, 10> local_bins{0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n  for (auto &val : xlocal) {\n    int bin = static_cast<int>(val / 10);\n    bin += bin < 0;\n    bin += bin > 9;\n    local_bins[bin]++;\n  }\n\n  MPI_Allreduce(MPI_IN_PLACE, &local_bins, 10, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  bins = local_bins;\n}",
            "size_t size_x = x.size();\n    int size_mpi = MPI_Comm_size(MPI_COMM_WORLD);\n    int rank = MPI_Comm_rank(MPI_COMM_WORLD);\n    int size_mpi_remainder = size_mpi % 10;\n    int size_mpi_quotient = size_mpi / 10;\n    int size_mpi_per_rank = size_mpi / size_mpi_quotient;\n    int remainder = rank % size_mpi_remainder;\n    int start = rank * size_mpi_per_rank;\n    int end = start + size_mpi_per_rank;\n    int bins_rank[10];\n    for(int i = 0; i < 10; i++){\n        bins_rank[i] = 0;\n    }\n    for(int i = start; i < end; i++){\n        if(x[i] < 10){\n            bins_rank[0]++;\n        }\n        else if(x[i] >= 10 && x[i] < 20){\n            bins_rank[1]++;\n        }\n        else if(x[i] >= 20 && x[i] < 30){\n            bins_rank[2]++;\n        }\n        else if(x[i] >= 30 && x[i] < 40){\n            bins_rank[3]++;\n        }\n        else if(x[i] >= 40 && x[i] < 50){\n            bins_rank[4]++;\n        }\n        else if(x[i] >= 50 && x[i] < 60){\n            bins_rank[5]++;\n        }\n        else if(x[i] >= 60 && x[i] < 70){\n            bins_rank[6]++;\n        }\n        else if(x[i] >= 70 && x[i] < 80){\n            bins_rank[7]++;\n        }\n        else if(x[i] >= 80 && x[i] < 90){\n            bins_rank[8]++;\n        }\n        else if(x[i] >= 90 && x[i] <= 100){\n            bins_rank[9]++;\n        }\n    }\n    if(rank == 0){\n        for(int i = 0; i < 10; i++){\n            for(int j = 0; j < size_mpi_quotient; j++){\n                bins[i] += bins_rank[i];\n            }\n        }\n    }\n}",
            "int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_size = x.size()/world_size;\n\n    // Create a vector of counts for each local vector\n    std::vector<int> local_bins(10);\n\n    // Loop over the local vector and keep count in local_bins\n    for (size_t i = 0; i < local_size; ++i) {\n        int index = std::floor(x[i]/10.0);\n        ++local_bins[index];\n    }\n\n    // Create an MPI_Datatype for the array of counts\n    MPI_Datatype count_type;\n    MPI_Type_contiguous(10, MPI_INT, &count_type);\n    MPI_Type_commit(&count_type);\n\n    // Send the local counts to rank 0\n    if (rank == 0) {\n        for (int r = 1; r < world_size; ++r) {\n            MPI_Recv(bins.data(), 10, count_type, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n\n    // If rank is not 0, send local counts to rank 0\n    else {\n        MPI_Send(local_bins.data(), 10, count_type, 0, 0, MPI_COMM_WORLD);\n    }\n\n    // Free the type\n    MPI_Type_free(&count_type);\n\n    // If rank is 0, add the counts of each rank\n    if (rank == 0) {\n        for (int r = 1; r < world_size; ++r) {\n            MPI_Send(bins.data(), 10, count_type, r, 0, MPI_COMM_WORLD);\n        }\n        for (int r = 1; r < world_size; ++r) {\n            MPI_Recv(bins.data(), 10, count_type, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}",
            "auto count = [](double val, double start, double end) {\n        return (val >= start) && (val < end);\n    };\n\n    size_t count_per_bin = x.size() / bins.size();\n    size_t start = 0;\n    size_t end = count_per_bin;\n    for (int i = 0; i < bins.size(); ++i) {\n        bins[i] = std::count_if(x.begin() + start, x.begin() + end, [&count, start](double val) {\n            return count(val, start, start + 1);\n        });\n        start = end;\n        end += count_per_bin;\n    }\n}",
            "int n = x.size();\n\n    // TODO: Use MPI to count in 10 bins\n    // bins[0] contains the number of values in [0, 10]\n    // bins[1] contains the number of values in [10, 20]\n    // etc.\n\n    MPI_Finalize();\n}",
            "// TODO: Your code here\n}",
            "const int rank = mpi::getRank();\n  const int nranks = mpi::getNumProcesses();\n  const int chunk = (int)x.size() / nranks;\n  int start = rank * chunk;\n  int end = start + chunk;\n  if (rank == nranks - 1) end = x.size();\n\n  // Count\n  for (int i = start; i < end; i++) {\n    int index = (int)x[i] / 10;\n    bins[index]++;\n  }\n}",
            "// Your code here\n  int rank,size;\n  double max_val,min_val;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Reduce(&x[0], &max_val, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);\n  MPI_Reduce(&x[0], &min_val, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n  double range = max_val - min_val;\n  double step = range / 10;\n  double start = min_val;\n\n  //std::cout<<\"[min,max]: [\"<<min_val<<\",\"<<max_val<<\"]\"<<std::endl;\n  for(int i = 0; i < 10; i++) {\n    double end = start + step;\n    //std::cout<<\"[\"<<start<<\",\"<<end<<\"]\"<<std::endl;\n    bins[i] = 0;\n    for(int j = 0; j < x.size(); j++) {\n      if(start <= x[j] && x[j] < end) {\n        //std::cout<<\"x[\"<<j<<\"] in [\"<<start<<\",\"<<end<<\"]\"<<std::endl;\n        bins[i]++;\n      }\n    }\n    start = end;\n  }\n\n}",
            "int nranks, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // If rank is 0, compute the total number of values\n    int totalNum;\n    if (rank == 0) {\n        totalNum = x.size();\n    }\n    // Broadcast the total number of values\n    MPI_Bcast(&totalNum, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // Compute the starting and ending values for each rank\n    int start;\n    int end;\n    if (rank == 0) {\n        start = 0;\n        end = (totalNum / nranks) * (rank + 1);\n    } else {\n        start = (totalNum / nranks) * rank;\n        end = (totalNum / nranks) * (rank + 1);\n    }\n\n    // Count the number of values in each bin\n    for (int i = start; i < end; i++) {\n        if (x[i] >= 0 && x[i] <= 10) {\n            bins[0]++;\n        } else if (x[i] > 10 && x[i] <= 20) {\n            bins[1]++;\n        } else if (x[i] > 20 && x[i] <= 30) {\n            bins[2]++;\n        } else if (x[i] > 30 && x[i] <= 40) {\n            bins[3]++;\n        } else if (x[i] > 40 && x[i] <= 50) {\n            bins[4]++;\n        } else if (x[i] > 50 && x[i] <= 60) {\n            bins[5]++;\n        } else if (x[i] > 60 && x[i] <= 70) {\n            bins[6]++;\n        } else if (x[i] > 70 && x[i] <= 80) {\n            bins[7]++;\n        } else if (x[i] > 80 && x[i] <= 90) {\n            bins[8]++;\n        } else if (x[i] > 90 && x[i] <= 100) {\n            bins[9]++;\n        } else {\n            printf(\"ERROR: value %f is not in [0, 100]\\n\", x[i]);\n            exit(1);\n        }\n    }\n}",
            "if (MPI_Comm_rank(MPI_COMM_WORLD, &rank)!= MPI_SUCCESS) {\n    std::cout << \"MPI_Comm_rank call failed!\" << std::endl;\n  }\n  if (MPI_Comm_size(MPI_COMM_WORLD, &size)!= MPI_SUCCESS) {\n    std::cout << \"MPI_Comm_size call failed!\" << std::endl;\n  }\n  int bin_size = 10;\n  double bin_limit = 10.0;\n  bins.fill(0);\n  if (rank == 0) {\n    std::cout << \"Number of elements in bins: \" << bins.size() << std::endl;\n    std::cout << \"Bin limit: \" << bin_limit << std::endl;\n  }\n  int bin_index = 0;\n\n  // get bin index from x value\n  auto getBinIndex = [&](double val) -> int {\n    return (val / bin_limit) + 1;\n  };\n\n  // determine if x value is within bin range\n  auto isBinRange = [&](double val) -> bool {\n    return (val >= 0.0) && (val < bin_limit);\n  };\n\n  // determine if x value is within bin range\n  auto binIncrement = [&](int index) -> void {\n    if (index >= 0 && index < 10) {\n      bins[index] += 1;\n    } else {\n      std::cout << \"Bin index out of range.\" << std::endl;\n    }\n  };\n\n  if (rank == 0) {\n    for (double d : x) {\n      if (isBinRange(d)) {\n        bin_index = getBinIndex(d);\n        binIncrement(bin_index);\n      }\n    }\n  } else {\n    for (double d : x) {\n      if (isBinRange(d)) {\n        bin_index = getBinIndex(d);\n        MPI_Gather(\n            &bin_index,\n            1,\n            MPI_INT,\n            &bin_index,\n            1,\n            MPI_INT,\n            0,\n            MPI_COMM_WORLD\n        );\n        if (rank == 0) {\n          binIncrement(bin_index);\n        }\n      }\n    }\n  }\n}",
            "int num_procs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        MPI_Status status;\n        int i, j;\n        double val;\n        size_t count;\n        for (i = 0; i < x.size(); i++) {\n            val = x[i] / 10;\n            j = (int)val;\n            if (val - j < 0.1) {\n                MPI_Recv(&count, 1, MPI_UNSIGNED_LONG_LONG, MPI_ANY_SOURCE,\n                         MPI_ANY_TAG, MPI_COMM_WORLD, &status);\n                bins[j] += count;\n            } else {\n                count = 0;\n                MPI_Send(&count, 1, MPI_UNSIGNED_LONG_LONG, j, MPI_ANY_TAG,\n                         MPI_COMM_WORLD);\n            }\n        }\n    } else {\n        size_t count = 0;\n        for (auto e: x) {\n            if (e / 10 == rank) {\n                count++;\n            }\n        }\n        MPI_Send(&count, 1, MPI_UNSIGNED_LONG_LONG, 0, MPI_ANY_TAG, MPI_COMM_WORLD);\n    }\n}",
            "// TODO: your code here\n  bins.fill(0);\n  int nprocs, my_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n  int xSize = x.size();\n  int xPerProc = xSize/nprocs;\n  int extra = xSize%nprocs;\n\n  for (int i = my_rank*xPerProc; i < (my_rank*xPerProc+xPerProc+extra); i++) {\n    if (i >= xSize) break;\n    int index = int((x[i]*10)/100);\n    bins[index]++;\n  }\n\n  MPI_Reduce(bins.data(), bins.data(), 10, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if(my_rank == 0) {\n    std::cout << \"Bins: \";\n    for (size_t i = 0; i < bins.size(); i++) {\n      std::cout << bins[i] << \" \";\n    }\n  }\n}",
            "int rank, num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    int total = x.size();\n    int rank_size = total / num_ranks;\n    std::vector<double> local_x;\n\n    if (rank == num_ranks - 1)\n    {\n        local_x.insert(local_x.begin(), x.begin() + rank_size * rank, x.end());\n    }\n    else\n    {\n        local_x.insert(local_x.begin(), x.begin() + rank_size * rank, x.begin() + rank_size * (rank + 1));\n    }\n\n    for (int i = 0; i < local_x.size(); i++)\n    {\n        int div = local_x[i] / 10;\n        bins[div]++;\n    }\n\n    MPI_Reduce(MPI_IN_PLACE, bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank;\n    int num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    const double range_start = 0;\n    const double range_end = 10;\n    const size_t bin_count = 10;\n\n    if(rank == 0) {\n        //fill in array with 0's\n        for (size_t i = 0; i < bin_count; i++) {\n            bins[i] = 0;\n        }\n    }\n\n    //each rank handles one of the 10 bin ranges\n    for (size_t i = rank; i < x.size(); i += num_ranks) {\n        size_t bin = (size_t)((x[i] - range_start) * bin_count / (range_end - range_start));\n        //std::cout << \"bin: \" << bin << std::endl;\n        if (bin > bin_count - 1) {\n            bin = bin_count - 1;\n        }\n        //std::cout << \"value: \" << bin << std::endl;\n        bins[bin]++;\n    }\n}",
            "int const n = x.size();\n  std::vector<double> x_local(n);\n  std::array<size_t, 10> bins_local = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n  MPI_Status status;\n\n  MPI_Scatter(x.data(), n, MPI_DOUBLE, x_local.data(), n, MPI_DOUBLE, 0,\n              MPI_COMM_WORLD);\n\n  for (int i = 0; i < n; ++i) {\n    if (x_local[i] < 10) {\n      ++bins_local[0];\n    } else if (x_local[i] < 20) {\n      ++bins_local[1];\n    } else if (x_local[i] < 30) {\n      ++bins_local[2];\n    } else if (x_local[i] < 40) {\n      ++bins_local[3];\n    } else if (x_local[i] < 50) {\n      ++bins_local[4];\n    } else if (x_local[i] < 60) {\n      ++bins_local[5];\n    } else if (x_local[i] < 70) {\n      ++bins_local[6];\n    } else if (x_local[i] < 80) {\n      ++bins_local[7];\n    } else if (x_local[i] < 90) {\n      ++bins_local[8];\n    } else {\n      ++bins_local[9];\n    }\n  }\n\n  MPI_Reduce(bins_local.data(), bins.data(), 10, MPI_LONG_LONG_INT, MPI_SUM, 0,\n             MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "// TODO\n\n    // return;\n\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, num_procs;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &num_procs);\n\n    double *local_x = new double[x.size()];\n\n    MPI_Scatter(&x[0], x.size(), MPI_DOUBLE, local_x, x.size(), MPI_DOUBLE, 0, comm);\n\n    std::vector<double> local_vector(local_x, local_x + x.size());\n    int count = 0;\n    for (int i = 0; i < local_vector.size(); i++)\n        if (local_vector[i] >= rank * 10 && local_vector[i] < (rank + 1) * 10)\n            count++;\n    bins[rank] = count;\n\n    MPI_Gather(&bins[0], 10, MPI_INT, &bins[0], 10, MPI_INT, 0, comm);\n\n    if (rank == 0) {\n        for (int i = 1; i < num_procs; i++)\n            for (int j = 0; j < 10; j++)\n                bins[j] += bins[i * 10 + j];\n    }\n\n    delete[] local_x;\n\n}",
            "// Your code here\n}",
            "int num_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    size_t n = x.size();\n    size_t m = n / num_ranks;\n    size_t r = n % num_ranks;\n    size_t i = 0;\n    std::fill(bins.begin(), bins.end(), 0);\n\n    std::vector<double> x_proc(m + (r? 1 : 0));\n\n    // Receive data from other ranks\n    for (int rank = 0; rank < num_ranks; rank++) {\n        if (rank == num_ranks - 1) {\n            MPI_Status status;\n            MPI_Recv(&x_proc[0], x_proc.size(), MPI_DOUBLE, rank, 0,\n                    MPI_COMM_WORLD, &status);\n        } else {\n            MPI_Status status;\n            MPI_Recv(&x_proc[0], x_proc.size(), MPI_DOUBLE, rank, 0,\n                    MPI_COMM_WORLD, &status);\n        }\n\n        for (size_t j = 0; j < m + (r? 1 : 0); j++) {\n            size_t bin = static_cast<size_t>((x_proc[j] / 10) + 1);\n            bins[bin - 1]++;\n        }\n\n        i += x_proc.size();\n    }\n\n    // Send data to other ranks\n    for (int rank = 0; rank < num_ranks; rank++) {\n        if (rank == num_ranks - 1) {\n            MPI_Status status;\n            MPI_Send(&x[i], n - i, MPI_DOUBLE, rank, 0, MPI_COMM_WORLD);\n        } else {\n            MPI_Status status;\n            MPI_Send(&x[i], m, MPI_DOUBLE, rank, 0, MPI_COMM_WORLD);\n        }\n        i += m;\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "cuda",
        "prompt": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use CUDA to compute in parallel. The kernel is initialized with at least as many threads as values in x.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\n__global__ void binsBy10Count(const double *x, size_t N, size_t[10] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO: Your code goes here\n  int idx = threadIdx.x;\n\n  if (idx < N) {\n    int i = (int) (x[idx] / 10);\n    i = (i < 0? 0 : i);\n    i = (i >= 10? 9 : i);\n    atomicAdd(&bins[i], 1);\n  }\n}",
            "size_t index = threadIdx.x + blockIdx.x * blockDim.x;\n    if (index < N) {\n        bins[int(index/10)]++;\n    }\n}",
            "int thread_num = blockIdx.x * blockDim.x + threadIdx.x;\n  if (thread_num >= N) return;\n  int bin = (int) floor(x[thread_num] / 10);\n  atomicAdd(&bins[bin], 1);\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  // your code here\n\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  bins[4] = 0;\n  bins[5] = 0;\n  bins[6] = 0;\n  bins[7] = 0;\n  bins[8] = 0;\n  bins[9] = 0;\n\n  if(i < N) {\n    if(x[i] < 10) {\n      bins[0]++;\n    }\n    else if(x[i] > 10 && x[i] <= 20) {\n      bins[1]++;\n    }\n    else if(x[i] > 20 && x[i] <= 30) {\n      bins[2]++;\n    }\n    else if(x[i] > 30 && x[i] <= 40) {\n      bins[3]++;\n    }\n    else if(x[i] > 40 && x[i] <= 50) {\n      bins[4]++;\n    }\n    else if(x[i] > 50 && x[i] <= 60) {\n      bins[5]++;\n    }\n    else if(x[i] > 60 && x[i] <= 70) {\n      bins[6]++;\n    }\n    else if(x[i] > 70 && x[i] <= 80) {\n      bins[7]++;\n    }\n    else if(x[i] > 80 && x[i] <= 90) {\n      bins[8]++;\n    }\n    else if(x[i] > 90 && x[i] <= 100) {\n      bins[9]++;\n    }\n  }\n}",
            "const int tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (tid < N) {\n        int bin = (int) (x[tid] / 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "const size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  const size_t totalThreads = blockDim.x * gridDim.x;\n\n  // TODO: Your code here\n  //...\n  //...\n  //...\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int stride = blockDim.x * gridDim.x;\n\n    for (int i = tid; i < N; i += stride) {\n        int bin = (int)(x[i] / 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "for (int i = blockIdx.x*blockDim.x + threadIdx.x; i < N; i += blockDim.x*gridDim.x) {\n    size_t index = (size_t)floor(x[i]/10.);\n    atomicAdd(&bins[index], 1);\n  }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: YOUR CODE HERE\n}",
            "// TODO\n}",
            "int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  size_t i;\n\n  for (i = bid * blockDim.x + tid; i < N; i += blockDim.x * gridDim.x) {\n    size_t bin = floor(x[i] / 10.0);\n    if (bin < 10) {\n      atomicAdd(&bins[bin], 1);\n    }\n  }\n}",
            "__shared__ double shared[BLOCKSIZE];\n    shared[threadIdx.x] = 0;\n    // TODO: implement\n}",
            "for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    if (x[i] >= 0 && x[i] <= 10) {\n      atomicAdd(&bins[0], 1);\n    } else if (x[i] > 10 && x[i] <= 20) {\n      atomicAdd(&bins[1], 1);\n    } else if (x[i] > 20 && x[i] <= 30) {\n      atomicAdd(&bins[2], 1);\n    } else if (x[i] > 30 && x[i] <= 40) {\n      atomicAdd(&bins[3], 1);\n    } else if (x[i] > 40 && x[i] <= 50) {\n      atomicAdd(&bins[4], 1);\n    } else if (x[i] > 50 && x[i] <= 60) {\n      atomicAdd(&bins[5], 1);\n    } else if (x[i] > 60 && x[i] <= 70) {\n      atomicAdd(&bins[6], 1);\n    } else if (x[i] > 70 && x[i] <= 80) {\n      atomicAdd(&bins[7], 1);\n    } else if (x[i] > 80 && x[i] <= 90) {\n      atomicAdd(&bins[8], 1);\n    } else if (x[i] > 90 && x[i] <= 100) {\n      atomicAdd(&bins[9], 1);\n    }\n  }\n}",
            "int thread_idx = threadIdx.x;\n  // Initialize bins to 0\n  for (int i = 0; i < 10; i++) {\n    bins[i] = 0;\n  }\n\n  // Calculate the interval each thread will work on\n  // TODO: calculate the interval\n  int interval = 0;\n  // Start at the beginning of the thread's interval\n  int begin = thread_idx * interval;\n  // Make sure you don't go over the number of values\n  begin = begin < N? begin : N;\n  int end = begin + interval;\n  end = end < N? end : N;\n\n  // For each value in the interval\n  for (int i = begin; i < end; i++) {\n    // TODO: increment the appropriate bin for x[i]\n    int bin = floor(x[i]/10);\n    atomicAdd(&(bins[bin]), 1);\n  }\n}",
            "extern __shared__ double values[];\n  if (threadIdx.x >= N) return;\n\n  double value = x[threadIdx.x];\n  size_t bin = value / 10;\n\n  // Fill the shared memory.\n  for (size_t i = threadIdx.x + 1; i < N; i += blockDim.x)\n    values[i] = x[i] / 10;\n  __syncthreads();\n\n  // Reduce shared memory.\n  for (size_t i = threadIdx.x / 2; i > 0; i /= 2)\n    if (threadIdx.x % (2 * i) == 0)\n      values[threadIdx.x] += values[threadIdx.x + i];\n  __syncthreads();\n\n  // Save the result.\n  if (threadIdx.x == 0)\n    bins[bin] += values[0];\n}",
            "// Fill this in\n}",
            "/* TODO: Your solution here */\n}",
            "int thread_id = threadIdx.x;\n    int block_id = blockIdx.x;\n    int blockDim = blockDim.x;\n\n    // compute the starting and ending index of the thread in the input array\n    // this loop takes care of the case when there are more threads than\n    // elements\n    int thread_start_idx = block_id * blockDim + thread_id;\n    int thread_end_idx = (block_id + 1) * blockDim;\n    if (thread_end_idx > N) {\n        thread_end_idx = N;\n    }\n\n    int counter = 0;\n    for (int i = thread_start_idx; i < thread_end_idx; i++) {\n        if (x[i] >= 0 && x[i] < 10) {\n            counter++;\n        } else if (x[i] >= 10 && x[i] < 20) {\n            counter++;\n        } else if (x[i] >= 20 && x[i] < 30) {\n            counter++;\n        } else if (x[i] >= 30 && x[i] < 40) {\n            counter++;\n        } else if (x[i] >= 40 && x[i] < 50) {\n            counter++;\n        } else if (x[i] >= 50 && x[i] < 60) {\n            counter++;\n        } else if (x[i] >= 60 && x[i] < 70) {\n            counter++;\n        } else if (x[i] >= 70 && x[i] < 80) {\n            counter++;\n        } else if (x[i] >= 80 && x[i] < 90) {\n            counter++;\n        } else if (x[i] >= 90 && x[i] < 100) {\n            counter++;\n        }\n    }\n\n    // store the counter at the appropriate place in the bins array\n    bins[thread_id] = counter;\n}",
            "// TODO: fill in your code here\n    // use the size_t N to compute the threadIdx and blockIdx\n    // initialize the bins to 0\n    // then in each block, count the elements less than the value of the blockIdx and write the result to bins[blockIdx]\n}",
            "int i = threadIdx.x;\n  if (i < N) {\n    int bin = (int) floor(x[i] / 10.0);\n    bins[bin]++;\n  }\n}",
            "}",
            "// TODO\n}",
            "/*\n    Your code here\n  */\n}",
            "// TODO\n}",
            "for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n    int bin = int(x[i] / 10.0);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int tid = threadIdx.x;\n  int stride = blockDim.x;\n  int start_i = blockIdx.x * stride;\n\n  __shared__ double s_sum[10];\n\n  // initialize s_sum array with 0's\n  for (int i = 0; i < 10; i++) {\n    s_sum[i] = 0;\n  }\n\n  for (int i = start_i; i < N; i += stride) {\n    int bin = x[i] / 10;\n    s_sum[bin] += 1;\n  }\n\n  // atomicAdd to accumulate shared memory\n  for (int i = 0; i < 10; i++) {\n    atomicAdd(&bins[i], s_sum[i]);\n  }\n}",
            "// TODO: Add your code here\n}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n\n    int bin = (int) (floor(index / 10));\n    if(bin > 9) bin = 9;\n\n    bins[bin]++;\n}",
            "// TODO:\n  return;\n}",
            "// TODO\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n\tint bin = 0;\n\tif(i < N) {\n\t\tfor(int j = 1; j < 10; j++) {\n\t\t\tif(i < j * (N/10)) {\n\t\t\t\tbin = j - 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tbins[bin]++;\n}",
            "__shared__ double data[512];\n  int tid = threadIdx.x;\n  int blockId = blockIdx.x;\n  int blockSize = blockDim.x;\n  int i = tid + blockSize * blockId;\n\n  if (i < N)\n    data[tid] = x[i];\n  __syncthreads();\n\n  int start = blockId * blockSize;\n  int stop = blockSize * (blockId + 1);\n  if (start < stop) {\n    int bin = (int)data[tid] / 10;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int i = threadIdx.x;\n\n  if (i < N) {\n    int bin = i / 10;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    size_t bin = (size_t)(x[i] / 10.0);\n    bins[bin] += 1;\n  }\n}",
            "}",
            "/*\n       Initialize the shared memory.\n       The number of threads per block must be an even multiple of 10.\n    */\n    __shared__ size_t values[10];\n\n    /*\n       Determine the index of the first thread that is in this block.\n    */\n    size_t blockFirst = blockIdx.x * blockDim.x + threadIdx.x;\n\n    /*\n       Use only the first 10 values in the thread block.\n    */\n    if (blockFirst < N) {\n        size_t bin = (size_t) ((x[blockFirst] + 10) / 10);\n\n        /*\n           Add this thread's count to the shared memory.\n        */\n        atomicAdd(&values[bin], 1);\n    }\n\n    /*\n       Wait for all threads in the thread block to reach this point before proceeding.\n    */\n    __syncthreads();\n\n    /*\n       Copy the shared memory to the output array.\n    */\n    for (size_t i = threadIdx.x; i < 10; i += blockDim.x) {\n        bins[i] = values[i];\n    }\n}",
            "// TODO\n}",
            "//...\n}",
            "// TODO\n    // Hint: Use atomicAdd to update the counter bins[index]\n\n    const int tid = threadIdx.x;\n\n    if (tid < N) {\n        int index = floor(x[tid] / 10.0);\n        atomicAdd(&bins[index], 1);\n    }\n}",
            "// TODO: use CUDA to compute in parallel\n  // TODO: use a loop to compute each of the 10 bins\n  // TODO: loop over the input range\n  // TODO: count the values in each bin\n  // TODO: store the counts in the output\n}",
            "// TODO: YOUR CODE HERE\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: implement\n\n    int j = threadIdx.x;\n    if (j < N) {\n        int bin = (int)(floor(x[j]/10.0));\n        if (bin > 9) {\n            bin = 9;\n        }\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "if (threadIdx.x < N) {\n        bins[floor(x[threadIdx.x]/10)] += 1;\n    }\n}",
            "__shared__ double buf[1024];\n\n  size_t tx = threadIdx.x;\n  size_t b = blockIdx.x;\n  size_t g = gridDim.x;\n  size_t stride = blockDim.x * g;\n\n  size_t i = b * blockDim.x + tx;\n\n  // copy values to shared memory\n  if (i < N) {\n    buf[tx] = x[i];\n  }\n  __syncthreads();\n\n  if (i < N) {\n    // 10 buckets\n    bins[buf[tx] / 10]++;\n  }\n}",
            "// TODO: insert code here\n}",
            "size_t n = blockIdx.x * blockDim.x + threadIdx.x;\n    if (n < N) {\n        size_t bin = (size_t)((x[n]-1) / 10.0);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO\n}",
            "for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n    if (x[i] < 10) {\n      atomicAdd(&bins[0], 1);\n    }\n    else if (x[i] < 20) {\n      atomicAdd(&bins[1], 1);\n    }\n    else if (x[i] < 30) {\n      atomicAdd(&bins[2], 1);\n    }\n    else if (x[i] < 40) {\n      atomicAdd(&bins[3], 1);\n    }\n    else if (x[i] < 50) {\n      atomicAdd(&bins[4], 1);\n    }\n    else if (x[i] < 60) {\n      atomicAdd(&bins[5], 1);\n    }\n    else if (x[i] < 70) {\n      atomicAdd(&bins[6], 1);\n    }\n    else if (x[i] < 80) {\n      atomicAdd(&bins[7], 1);\n    }\n    else if (x[i] < 90) {\n      atomicAdd(&bins[8], 1);\n    }\n    else {\n      atomicAdd(&bins[9], 1);\n    }\n  }\n}",
            "// TODO: Implement\n}",
            "// TODO: Fill bins with the correct number of values in each range\n\t// Hint: Consider using an if/else if/else if/else construct\n\n\tconst int tid = threadIdx.x;\n\tif (tid < N) {\n\t\tint i = x[tid];\n\t\tif (i >= 0 && i < 10) {\n\t\t\tatomicAdd(&bins[0], 1);\n\t\t}\n\t\telse if (i >= 10 && i < 20) {\n\t\t\tatomicAdd(&bins[1], 1);\n\t\t}\n\t\telse if (i >= 20 && i < 30) {\n\t\t\tatomicAdd(&bins[2], 1);\n\t\t}\n\t\telse if (i >= 30 && i < 40) {\n\t\t\tatomicAdd(&bins[3], 1);\n\t\t}\n\t\telse if (i >= 40 && i < 50) {\n\t\t\tatomicAdd(&bins[4], 1);\n\t\t}\n\t\telse if (i >= 50 && i < 60) {\n\t\t\tatomicAdd(&bins[5], 1);\n\t\t}\n\t\telse if (i >= 60 && i < 70) {\n\t\t\tatomicAdd(&bins[6], 1);\n\t\t}\n\t\telse if (i >= 70 && i < 80) {\n\t\t\tatomicAdd(&bins[7], 1);\n\t\t}\n\t\telse if (i >= 80 && i < 90) {\n\t\t\tatomicAdd(&bins[8], 1);\n\t\t}\n\t\telse if (i >= 90 && i < 100) {\n\t\t\tatomicAdd(&bins[9], 1);\n\t\t}\n\t}\n}",
            "int idx = threadIdx.x;\n    if (idx >= N) return;\n    int i = (int) (x[idx]/10);\n    if (i >= 0 && i <= 9) {\n        atomicAdd(&bins[i], 1);\n    }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (i < N) {\n        size_t index = (size_t) (x[i] / 10);\n        atomicAdd(&(bins[index]), 1);\n    }\n}",
            "// your code goes here\n}",
            "extern __shared__ double data[];\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int nb = blockDim.x;\n  data[tid] = 0;\n  for (int i = bid * nb + tid; i < N; i += nb * gridDim.x) {\n    if (x[i] >= 10 * bid && x[i] < 10 * (bid + 1)) {\n      data[tid]++;\n    }\n  }\n  __syncthreads();\n  // TODO: reduce\n  // TODO: write to output\n  __syncthreads();\n}",
            "// TODO:\n  // Write a loop over the array x. In each iteration, count the number of\n  // elements in x that fall in a particular range (e.g. 0-9, 10-19, 20-29, etc.)\n  // Store the count in bins, which should be passed to the kernel from the host\n  // in the function call.\n  // The kernel should be initialized with at least as many threads as values\n  // in x.\n  //\n  // For example, when N == 10, the kernel should initialize 10 threads\n  // and the array bins should be initialized with 10 elements.\n  // The kernel must store the count of each range in the array bins.\n\n  // The following commented code is an example of how to get the correct count\n  // of the number of elements that fall in a range of 0-9.\n  //\n  // if (threadIdx.x < N)\n  // {\n  //   int k = 0;\n  //   while (k < N)\n  //   {\n  //     if (x[k] >= 0 && x[k] < 10)\n  //       atomicAdd(&bins[0], 1);\n  //     k++;\n  //   }\n  // }\n\n  // This code is an example of how to use atomics to increment the correct bin.\n  // This example is not a full solution to the problem.\n  //\n  // if (threadIdx.x < N)\n  // {\n  //   for (int k = 0; k < N; k++)\n  //   {\n  //     if (x[k] >= 0 && x[k] < 10)\n  //     {\n  //       int tmp = 0;\n  //       tmp = atomicAdd(&bins[0], 1);\n  //       printf(\"x[k] = %d, bins[0] = %d\\n\", x[k], tmp);\n  //     }\n  //   }\n  // }\n}",
            "// TODO\n}",
            "const int numThreads = blockDim.x * gridDim.x;\n    const int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n\n    for (size_t i = threadId; i < N; i += numThreads) {\n\n        int val = (int) (10 * (x[i] - 1) + 0.5);\n        bins[val]++;\n    }\n}",
            "// TODO: Your code here\n  size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    if (x[i] >= 0 && x[i] < 10) {\n      bins[0]++;\n    } else if (x[i] >= 10 && x[i] < 20) {\n      bins[1]++;\n    } else if (x[i] >= 20 && x[i] < 30) {\n      bins[2]++;\n    } else if (x[i] >= 30 && x[i] < 40) {\n      bins[3]++;\n    } else if (x[i] >= 40 && x[i] < 50) {\n      bins[4]++;\n    } else if (x[i] >= 50 && x[i] < 60) {\n      bins[5]++;\n    } else if (x[i] >= 60 && x[i] < 70) {\n      bins[6]++;\n    } else if (x[i] >= 70 && x[i] < 80) {\n      bins[7]++;\n    } else if (x[i] >= 80 && x[i] < 90) {\n      bins[8]++;\n    } else if (x[i] >= 90 && x[i] <= 100) {\n      bins[9]++;\n    }\n  }\n}",
            "// TODO: Your code here\n\n\tbins[0] = 0;\n\tbins[1] = 0;\n\tbins[2] = 0;\n\tbins[3] = 0;\n\tbins[4] = 0;\n\tbins[5] = 0;\n\tbins[6] = 0;\n\tbins[7] = 0;\n\tbins[8] = 0;\n\tbins[9] = 0;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        bins[floor(x[i] / 10.0)] += 1;\n    }\n}",
            "size_t threadId = threadIdx.x + blockDim.x * blockIdx.x;\n  size_t threadCount = blockDim.x * gridDim.x;\n\n  size_t b;\n  for (int i = threadId; i < N; i += threadCount) {\n    if (i % 10 < 10) {\n      b = i % 10;\n    }\n    bins[b]++;\n  }\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N) {\n        size_t idx_10 = (size_t) (x[idx] / 10);\n        if (idx_10 > 10) {\n            idx_10 = 10;\n        }\n        atomicAdd(&bins[idx_10], 1);\n    }\n}",
            "// TODO\n}",
            "// TODO: write implementation\n}",
            "// Use this array to store the 10 counts for each of the 10 bins\n  __shared__ size_t counts[10];\n  // Fill this array with 0's.\n  // NOTE: This is not a thread-safe operation, but since the array is shared,\n  //       this is OK.\n  for (size_t i = 0; i < 10; i++) {\n    counts[i] = 0;\n  }\n  // Fill in the array by count\n  for (size_t i = threadIdx.x + blockDim.x * blockIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    size_t index = (size_t) (x[i] / 10);\n    atomicAdd(&counts[index], 1);\n  }\n  // Store in the global array\n  for (size_t i = threadIdx.x + blockDim.x * blockIdx.x; i < 10; i += blockDim.x * gridDim.x) {\n    bins[i] = counts[i];\n  }\n}",
            "size_t b = blockIdx.x * blockDim.x + threadIdx.x;\n  if (b < N) {\n    size_t bucket = (size_t) (x[b] / 10);\n    if (bucket < 10) {\n      bins[bucket]++;\n    }\n  }\n}",
            "size_t index = threadIdx.x + blockIdx.x * blockDim.x;\n    size_t stride = blockDim.x * gridDim.x;\n    size_t bucket = 0;\n    for (size_t i = index; i < N; i += stride) {\n        // TODO\n        bucket = (x[i] - 0) / 10;\n        if (bucket >= 10) {\n            bucket = 9;\n        }\n        bins[bucket] = bins[bucket] + 1;\n    }\n}",
            "// TODO: implement\n}",
            "/*\n    Initialize bins with 0\n    */\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    bins[4] = 0;\n    bins[5] = 0;\n    bins[6] = 0;\n    bins[7] = 0;\n    bins[8] = 0;\n    bins[9] = 0;\n\n    /*\n    Loop over the values in x\n    */\n    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N;\n         i += blockDim.x * gridDim.x) {\n        /*\n        Count the number of elements in each of the 10 bins\n        */\n        if (x[i] < 10) {\n            bins[0] += 1;\n        } else if (x[i] < 20) {\n            bins[1] += 1;\n        } else if (x[i] < 30) {\n            bins[2] += 1;\n        } else if (x[i] < 40) {\n            bins[3] += 1;\n        } else if (x[i] < 50) {\n            bins[4] += 1;\n        } else if (x[i] < 60) {\n            bins[5] += 1;\n        } else if (x[i] < 70) {\n            bins[6] += 1;\n        } else if (x[i] < 80) {\n            bins[7] += 1;\n        } else if (x[i] < 90) {\n            bins[8] += 1;\n        } else {\n            bins[9] += 1;\n        }\n    }\n}",
            "for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n\t\tint index = (int)(x[i] / 10);\n\t\tatomicAdd(&bins[index], 1);\n\t}\n}",
            "if (threadIdx.x < N) {\n        int bin = int(10 * x[threadIdx.x]);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO: implement the body of this function\n    // TODO: you should not use threads for the loop\n}",
            "int tId = threadIdx.x;\n  int bId = blockIdx.x;\n  int i = bId * blockDim.x + tId;\n  if (i < N) {\n    size_t j = floor(x[i] / 10.0);\n    bins[j] += 1;\n  }\n}",
            "int thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n  if (thread_id < N) {\n    if (x[thread_id] < 10) {\n      atomicAdd(bins + threadIdx.x, 1);\n    } else {\n      atomicAdd(bins + threadIdx.x + 10, 1);\n    }\n  }\n}",
            "// TODO\n}",
            "/* TODO */\n}",
            "const int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n    if (threadId < N) {\n        int bin = (int)(x[threadId] / 10.0);\n        bins[bin] += 1;\n    }\n}",
            "// TODO: your code here.\n}",
            "// Compute which range x[i] is in.\n    if (threadIdx.x < N) {\n        int idx = int(x[threadIdx.x] / 10);\n        // Increase the count of that range.\n        atomicAdd(&bins[idx], 1);\n    }\n}",
            "//TODO\n}",
            "// TODO\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i >= N)\n        return;\n    bins[int(x[i]/10.0)] += 1;\n}",
            "int i = threadIdx.x;\n    if (i < N) {\n        int idx = (int)(floor(x[i] / 10.0));\n        if (idx < 0) idx = 0;\n        if (idx > 9) idx = 9;\n        atomicAdd(&bins[idx], 1);\n    }\n}",
            "// Fill in start and end values for each of the 10 bins\n  const int binStart[10] = {0, 10, 20, 30, 40, 50, 60, 70, 80, 90};\n  const int binEnd[10] = {10, 20, 30, 40, 50, 60, 70, 80, 90, 100};\n\n  // Each thread computes a single bin. Each bin is responsible for computing the\n  // bin count for a range of values, inclusive of the start and end values.\n  int start = threadIdx.x * 10;\n  int end = start + 10;\n  int bin = 0;\n  int count = 0;\n  for (int i = start; i < end; i++) {\n    if (i == N) {\n      break;\n    }\n\n    if (x[i] >= binStart[bin] && x[i] <= binEnd[bin]) {\n      count++;\n    } else if (x[i] > binEnd[bin] && bin < 9) {\n      bin++;\n    }\n  }\n\n  bins[threadIdx.x] = count;\n}",
            "// TODO: Implement me\n}",
            "if(blockIdx.x == 0 && threadIdx.x < N) {\n        double v = x[threadIdx.x];\n        int i = floor(v/10.0);\n        atomicAdd(&bins[i], 1);\n    }\n}",
            "for (int i = 0; i < N; ++i) {\n    int bin = floor(x[i] / 10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO: implement\n}",
            "/*\n  TODO:\n    - Determine which thread in the block is responsible for the i'th value of x, 0 <= i < N.\n    - If that thread is responsible for the value of x[i], then count the number of values in [0,10), [10, 20), [20, 30),...\n      and store the counts in bins.\n    - Use CUDA's built-in atomicAdd() function to increment the counts.\n    - NOTE: You cannot use a traditional loop to increment the counts, because you have to increment the count for one value of x\n      before you increment the count for the next value of x.\n\n    - If you try to use a traditional loop, your program will likely run into a \"race condition\" where two or more threads\n      will attempt to increment the same value of bins at the same time. The result will be that the final value of bins\n      will be incorrect.\n\n    - When you increment an element of bins, do so using atomicAdd() because multiple threads may be attempting to increment\n      the same value.\n  */\n}",
            "int tid = threadIdx.x;\n    int blockSize = blockDim.x;\n    int blockCount = gridDim.x;\n\n    for (int i = tid; i < N; i += blockSize * blockCount) {\n        int idx = (int) (x[i] / 10);\n        atomicAdd(&bins[idx], 1);\n    }\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n    size_t stride = blockDim.x * gridDim.x;\n    for (size_t i = tid; i < N; i += stride) {\n        bins[static_cast<size_t>((x[i] / 10.0)) % 10] += 1;\n    }\n}",
            "int idx = threadIdx.x;\n  bins[idx] = 0;\n  if (idx >= N) return;\n  double value = x[idx];\n  if (value < 10) {\n    bins[0]++;\n  } else if (value < 20) {\n    bins[1]++;\n  } else if (value < 30) {\n    bins[2]++;\n  } else if (value < 40) {\n    bins[3]++;\n  } else if (value < 50) {\n    bins[4]++;\n  } else if (value < 60) {\n    bins[5]++;\n  } else if (value < 70) {\n    bins[6]++;\n  } else if (value < 80) {\n    bins[7]++;\n  } else if (value < 90) {\n    bins[8]++;\n  } else if (value < 100) {\n    bins[9]++;\n  }\n}",
            "// start with one thread per value in x\n    int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // the value at position threadId in x\n    double value = x[threadId];\n\n    // check if threadId is in range of x and it's not the last value in x\n    if(threadId < N && threadId < N - 1){\n\n        // check if value is in between [0, 10)\n        if(value > 0 && value <= 10){\n\n            // add 1 to the count for bin 1\n            atomicAdd(&bins[0], 1);\n        }\n\n        // check if value is in between [10, 20)\n        else if(value > 10 && value <= 20){\n\n            // add 1 to the count for bin 2\n            atomicAdd(&bins[1], 1);\n        }\n\n        // check if value is in between [20, 30)\n        else if(value > 20 && value <= 30){\n\n            // add 1 to the count for bin 3\n            atomicAdd(&bins[2], 1);\n        }\n\n        // check if value is in between [30, 40)\n        else if(value > 30 && value <= 40){\n\n            // add 1 to the count for bin 4\n            atomicAdd(&bins[3], 1);\n        }\n\n        // check if value is in between [40, 50)\n        else if(value > 40 && value <= 50){\n\n            // add 1 to the count for bin 5\n            atomicAdd(&bins[4], 1);\n        }\n\n        // check if value is in between [50, 60)\n        else if(value > 50 && value <= 60){\n\n            // add 1 to the count for bin 6\n            atomicAdd(&bins[5], 1);\n        }\n\n        // check if value is in between [60, 70)\n        else if(value > 60 && value <= 70){\n\n            // add 1 to the count for bin 7\n            atomicAdd(&bins[6], 1);\n        }\n\n        // check if value is in between [70, 80)\n        else if(value > 70 && value <= 80){\n\n            // add 1 to the count for bin 8\n            atomicAdd(&bins[7], 1);\n        }\n\n        // check if value is in between [80, 90)\n        else if(value > 80 && value <= 90){\n\n            // add 1 to the count for bin 9\n            atomicAdd(&bins[8], 1);\n        }\n\n        // check if value is in between [90, 100)\n        else if(value > 90 && value <= 100){\n\n            // add 1 to the count for bin 10\n            atomicAdd(&bins[9], 1);\n        }\n    }\n\n}",
            "size_t index = threadIdx.x + blockIdx.x * blockDim.x;\n  if (index < N) {\n    bins[(size_t) (x[index] / 10.0)]++;\n  }\n}",
            "size_t i;\n    for (i=blockIdx.x*blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        size_t bin = floor((x[i] - 1) / 10.0);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO\n\n}",
            "// TODO: your code here\n}",
            "// TODO\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (i >= N)\n    return;\n\n  size_t bin = (x[i] / 10);\n  bins[bin]++;\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= N) { return; }\n    double val = x[tid];\n    int bin = (int) (val / 10.);\n    atomicAdd(bins + bin, 1);\n}",
            "size_t bin = 0;\n    for (int i = threadIdx.x; i < N; i += blockDim.x) {\n        bin = (x[i] + 5) / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n    int bin = (int)(floor(x[i]/10.0));\n    bins[bin]++;\n  }\n}",
            "__shared__ double x10[1024];\n\n  size_t i, t;\n  int b;\n\n  t = threadIdx.x;\n  if (t < N)\n    x10[t] = x[t];\n  __syncthreads();\n\n  i = t / 10 * 10;\n  b = (t % 10) + 1;\n\n  while (i < N) {\n    if (x10[i] >= b && x10[i] < b + 10) {\n      atomicAdd(&bins[b], 1);\n    }\n    i += blockDim.x;\n  }\n}",
            "// This is the CUDA kernel code. The kernel is called with at least as many threads as\n    // values in x. The threadIdx.x variable is the thread number, and the blockIdx.x\n    // variable is the block number.\n    int bin = (int)(x[threadIdx.x] / 10);\n    bins[bin]++;\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n  for(size_t i = 0; i < 10; i++)\n    bins[i] = 0;\n  if(idx < N) {\n    size_t bin = (size_t)floor(x[idx] / 10.0);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO\n  // Your code here\n  // **************\n}",
            "size_t bin = 0;\n    for (size_t i = 0; i < N; i++) {\n        if (x[i] > 0 && x[i] <= 10) {\n            bin = 0;\n        } else if (x[i] > 10 && x[i] <= 20) {\n            bin = 1;\n        } else if (x[i] > 20 && x[i] <= 30) {\n            bin = 2;\n        } else if (x[i] > 30 && x[i] <= 40) {\n            bin = 3;\n        } else if (x[i] > 40 && x[i] <= 50) {\n            bin = 4;\n        } else if (x[i] > 50 && x[i] <= 60) {\n            bin = 5;\n        } else if (x[i] > 60 && x[i] <= 70) {\n            bin = 6;\n        } else if (x[i] > 70 && x[i] <= 80) {\n            bin = 7;\n        } else if (x[i] > 80 && x[i] <= 90) {\n            bin = 8;\n        } else if (x[i] > 90 && x[i] <= 100) {\n            bin = 9;\n        }\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO\n  //   for (size_t i = 0; i < N; i++) {\n  //     double val = x[i];\n  //     int index = (int) (val / 10);\n  //     bins[index]++;\n  //   }\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index < N) {\n    int val = (int) (x[index] / 10);\n    atomicAdd(&bins[val], 1);\n  }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        size_t bin = x[tid] / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// Initialize a shared memory array containing zeros\n  extern __shared__ double smem[];\n  for (int i = threadIdx.x; i < 10; i += blockDim.x) {\n    smem[i] = 0;\n  }\n  // Compute the index of the bins\n  int thread_index = threadIdx.x + (blockDim.x * blockIdx.x);\n  int index = thread_index;\n  int i = 0;\n  while (index < N) {\n    int b = (int)fmod(((int)x[index]), 10);\n    smem[b]++;\n    index += blockDim.x * gridDim.x;\n  }\n  __syncthreads();\n  // Store the values in the bins in global memory\n  for (int i = threadIdx.x; i < 10; i += blockDim.x) {\n    bins[i] = (size_t)smem[i];\n  }\n}",
            "int gtid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (gtid < N) {\n    int val = (int)floor(x[gtid]);\n    if (val >= 0 && val < 10) {\n      atomicAdd(&bins[val], 1);\n    }\n  }\n}",
            "int idx = threadIdx.x + blockDim.x * blockIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  int nBlocks = gridDim.x;\n\n  for (int i = idx; i < N; i += stride) {\n    int ibin = (int)(x[i] / 10.0);\n    atomicAdd(&bins[ibin], 1);\n  }\n}",
            "/*\n       YOUR CODE HERE\n    */\n}",
            "/*\n    I'm not sure how you want to initialize bins, but this is how you\n    can get the thread number (index) and the number of threads\n  */\n  int threadIndex = blockIdx.x*blockDim.x + threadIdx.x;\n  int totalThreads = blockDim.x * gridDim.x;\n\n  // start at the beginning of the array\n  int start = 0;\n  for (int i = 0; i < 10; i++) {\n    start += bins[i];\n  }\n\n  // work on 10 intervals in parallel\n  for (int i = 0; i < 10; i++) {\n    for (int j = start; j < start + bins[i]; j++) {\n      // I have a solution for this part\n      if (x[j] >= 10*i && x[j] < 10*(i+1)) {\n        // increment the bin\n        // this code is a mess and probably doesn't work...\n        int index = threadIndex % bins[i];\n        int val = atomicAdd(&bins[i], index);\n      }\n    }\n    start += bins[i];\n  }\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n    int bin = 0;\n    if (i < N) {\n        if (x[i] < 10) {\n            bin = 0;\n        } else if (x[i] < 20) {\n            bin = 1;\n        } else if (x[i] < 30) {\n            bin = 2;\n        } else if (x[i] < 40) {\n            bin = 3;\n        } else if (x[i] < 50) {\n            bin = 4;\n        } else if (x[i] < 60) {\n            bin = 5;\n        } else if (x[i] < 70) {\n            bin = 6;\n        } else if (x[i] < 80) {\n            bin = 7;\n        } else if (x[i] < 90) {\n            bin = 8;\n        } else {\n            bin = 9;\n        }\n        bins[bin] = bins[bin] + 1;\n    }\n}",
            "// TO BE IMPLEMENTED\n}",
            "// TODO: Implement this function!\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (i < N) {\n        bins[(int) (x[i] / 10)]++;\n    }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    bins[i/10] += 1;\n  }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// Fill this in\n}",
            "//... add code\n}",
            "// Your code here\n}",
            "// TODO\n}",
            "//TODO\n}",
            "// your code here\n  //__shared__ size_t *shared;\n  //shared[threadIdx.x] = x[threadIdx.x];\n  //__syncthreads();\n  //for (int i = 0; i < 10; i++) {\n  //  if (shared[i] == 0) {\n  //    bins[i] = 0;\n  //  }\n  //}\n  //bins[threadIdx.x] = 0;\n}",
            "}",
            "unsigned int tg_id = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (tg_id < N) {\n    bins[ (unsigned int)((x[tg_id]/10)) ]++;\n  }\n}",
            "// TODO: Implement this\n}",
            "//TODO: implement\n    int thread = threadIdx.x;\n    for (size_t i = thread; i < N; i += blockDim.x) {\n        bins[floor((x[i] - 1) / 10)]++;\n    }\n}",
            "// TODO\n}",
            "// insert kernel code here\n}",
            "size_t id = threadIdx.x + blockIdx.x * blockDim.x;\n    if (id >= N) return;\n    size_t bin = id / 10;\n    atomicAdd(&bins[bin], 1);\n}",
            "// TODO\n}",
            "size_t tid = threadIdx.x;\n    size_t stride = blockDim.x;\n    // Loop over input array with threadIndex\n    for (size_t i = tid; i < N; i += stride) {\n        // Convert to index in bins (between 0 and 10)\n        size_t bin = (size_t)((x[i] - 1) / 10.0);\n        // Add to count for this bin\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO\n  int tid = threadIdx.x;\n  if (tid < N) {\n    int i = (int)(x[tid] / 10);\n    atomicAdd(&bins[i], 1);\n  }\n}",
            "size_t t = threadIdx.x + blockIdx.x * blockDim.x;\n    if (t < N) {\n        int bin = (int) (x[t] / 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO: your code here\n\n    // TODO: your code here\n    return;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N)\n    return;\n  int val = (int)x[i] / 10;\n  atomicAdd(&bins[val], 1);\n}",
            "// TODO\n}",
            "// TODO\n}",
            "}",
            "/* IMPLEMENTATION HERE */\n}",
            "int threadIdx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (threadIdx >= N) return;\n    double value = x[threadIdx];\n    int index = value / 10.0;\n    index = (int) floor(index);\n    index = index * 10;\n    atomicAdd(&bins[index], 1);\n}",
            "size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n  if (i >= N) {\n    return;\n  }\n  double val = x[i];\n  // make sure val is between 0 and 100\n  val = (val > 100)? 100 : val;\n  val = (val < 0)? 0 : val;\n  int binIdx = (int)floor((val - 0.0) / 10.0);\n  atomicAdd(&bins[binIdx], 1);\n}",
            "for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n    if (x[i] >= 0 && x[i] < 10) {\n      atomicAdd(bins, 1);\n    }\n  }\n}",
            "const size_t i = threadIdx.x;\n  if (i >= N) {\n    return;\n  }\n  const double val = x[i];\n  const double x0 = (val / 10);\n  const size_t idx = (x0 > 0)? static_cast<size_t>(x0) : 0;\n  bins[idx] += 1;\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  for (int i = tid; i < N; i += blockDim.x * gridDim.x) {\n    int bin = (int) (floor(x[i]/10.0));\n    if (bin < 0) {\n      bin = 0;\n    } else if (bin > 9) {\n      bin = 9;\n    }\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "const size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    // TODO: Implement me!\n  }\n}",
            "// TODO\n}",
            "int tid = threadIdx.x;\n    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (gid >= N) return;\n    int bin = 0;\n    if (x[gid] < 10)\n        bin = 0;\n    else if (x[gid] < 20)\n        bin = 1;\n    else if (x[gid] < 30)\n        bin = 2;\n    else if (x[gid] < 40)\n        bin = 3;\n    else if (x[gid] < 50)\n        bin = 4;\n    else if (x[gid] < 60)\n        bin = 5;\n    else if (x[gid] < 70)\n        bin = 6;\n    else if (x[gid] < 80)\n        bin = 7;\n    else if (x[gid] < 90)\n        bin = 8;\n    else if (x[gid] < 100)\n        bin = 9;\n\n    atomicAdd(&bins[bin], 1);\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= N)\n        return;\n    int count = 0;\n    if (x[tid] > 0 && x[tid] < 10)\n        count++;\n    if (x[tid] > 10 && x[tid] < 20)\n        count++;\n    if (x[tid] > 20 && x[tid] < 30)\n        count++;\n    if (x[tid] > 30 && x[tid] < 40)\n        count++;\n    if (x[tid] > 40 && x[tid] < 50)\n        count++;\n    if (x[tid] > 50 && x[tid] < 60)\n        count++;\n    if (x[tid] > 60 && x[tid] < 70)\n        count++;\n    if (x[tid] > 70 && x[tid] < 80)\n        count++;\n    if (x[tid] > 80 && x[tid] < 90)\n        count++;\n    if (x[tid] > 90 && x[tid] < 100)\n        count++;\n    if (x[tid] >= 100)\n        count++;\n    bins[tid] = count;\n}",
            "const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n\n  for (int i = idx; i < N; i += stride) {\n    int bucket = (int)x[i] / 10;\n    if (bucket > 10) {\n      bucket = 10;\n    }\n    bins[bucket] += 1;\n  }\n}",
            "// TODO: Add code\n}",
            "int bin = 0;\n\n    // calculate bin\n    if (threadIdx.x <= 9) {\n        bin = threadIdx.x;\n    }\n\n    // do atomic add for each thread\n    atomicAdd(&(bins[bin]), 1);\n}",
            "// Compute the first bin that x will belong to.\n    // The bin is determined by the first digit of the input number.\n    // E.g. 1, 2, 3, 4, 5, 6, 7, 8, 9, 0.\n    // 1 is first digit, so x will go in the first bin.\n    // 95 goes in the 9th bin.\n    // 12 will go in the 1st bin.\n    // 71 will go in the 7th bin.\n\n    // We can use shared memory to store the bins.\n    // Then we can update them atomically using atomicAdd.\n    extern __shared__ size_t sharedBins[];\n    // Zero the bins.\n    for (int i = threadIdx.x; i < 10; i += blockDim.x) {\n        sharedBins[i] = 0;\n    }\n    __syncthreads();\n\n    // For each value in x, compute which bin it goes to.\n    // Each thread will process one value.\n    for (int i = threadIdx.x; i < N; i += blockDim.x) {\n        // Floor divide (round down) to get the first digit.\n        int firstDigit = (int) floor(x[i] / 10.0);\n        // Compute the bin index.\n        int binIndex = firstDigit - 1;\n        // Increment the bin.\n        atomicAdd(&sharedBins[binIndex], 1);\n    }\n    __syncthreads();\n    // Store the bins in the output array.\n    for (int i = threadIdx.x; i < 10; i += blockDim.x) {\n        bins[i] = sharedBins[i];\n    }\n}",
            "// TODO:\n}",
            "// Fill in code here.\n\n}",
            "// Initialize all threads to zero\n    for (int i = 0; i < 10; i++) {\n        bins[i] = 0;\n    }\n    __syncthreads();\n\n    if (threadIdx.x < N) {\n        // Get x value\n        double val = x[threadIdx.x];\n        // Determine which bin we are in\n        double bin = (val / 10) - floor(val / 10);\n        // Get the index of the bin\n        size_t bin_index = (size_t) bin;\n        // Increment that bin's value by 1\n        atomicAdd(&(bins[bin_index]), 1);\n    }\n}",
            "// TODO\n}",
            "// TODO: implement\n\tint threadIdx = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (threadIdx < N) {\n\t\tsize_t idx = (size_t)((x[threadIdx] / 10.0));\n\t\tatomicAdd(&bins[idx], 1);\n\t}\n}",
            "// TODO: YOUR CODE HERE\n}",
            "int thread_idx = threadIdx.x;\n    int block_idx = blockIdx.x;\n    int block_num = gridDim.x;\n\n    if (block_idx < N) {\n        // block_idx is the index of the current thread\n        // we want the index of the number we are currently counting\n        size_t num = x[block_idx];\n        // num is between 0 and 100, so it should be easy to map to the\n        // correct range\n        // convert to float for math, then back to int for math\n        int i = (int)(num / 10.0) * 10;\n\n        // now add one to the correct bin\n        atomicAdd(&bins[i], 1);\n    }\n\n    __syncthreads();\n\n    // at the end of the kernel, each thread has the sum of its range in bins\n    // so we have to add the sums of each range to the previous range in\n    // order to get a full range of counts from 0 to 100\n\n    // we need to do this with a simple for loop because the threads are\n    // not guaranteed to be in the same order\n    for (int i = 0; i < 9; ++i) {\n        atomicAdd(&bins[i], bins[i + 1]);\n    }\n\n    __syncthreads();\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index >= N) return;\n\n    size_t bin = 0;\n    if (x[index] >= 0 && x[index] <= 10) bin = 0;\n    else if (x[index] > 10 && x[index] <= 20) bin = 1;\n    else if (x[index] > 20 && x[index] <= 30) bin = 2;\n    else if (x[index] > 30 && x[index] <= 40) bin = 3;\n    else if (x[index] > 40 && x[index] <= 50) bin = 4;\n    else if (x[index] > 50 && x[index] <= 60) bin = 5;\n    else if (x[index] > 60 && x[index] <= 70) bin = 6;\n    else if (x[index] > 70 && x[index] <= 80) bin = 7;\n    else if (x[index] > 80 && x[index] <= 90) bin = 8;\n    else if (x[index] > 90 && x[index] <= 100) bin = 9;\n    atomicAdd(bins + bin, 1);\n}",
            "__shared__ size_t shared[10];\n\tint t = threadIdx.x;\n\tif (t < 10) {\n\t\tshared[t] = 0;\n\t}\n\t__syncthreads();\n\tfor (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n\t\tif (x[i] >= t * 10 && x[i] < (t + 1) * 10) {\n\t\t\tatomicAdd(&shared[t], 1);\n\t\t}\n\t}\n\t__syncthreads();\n\tif (t < 10) {\n\t\tatomicAdd(&bins[t], shared[t]);\n\t}\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        size_t bin = i % 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "int gtid = blockDim.x * blockIdx.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  int blockSum = 0;\n\n  for (size_t i = gtid; i < N; i += stride) {\n    size_t b = (size_t)x[i] / 10;\n    blockSum += (int)((i < N) && (b < 10));\n  }\n  // Each block adds the blockSum to the blockSum in the same location.\n  for (int offset = blockDim.x / 2; offset > 0; offset /= 2) {\n    blockSum += __shfl_down_sync(0xFFFFFFFF, blockSum, offset);\n  }\n  if (threadIdx.x == 0) {\n    atomicAdd(&bins[blockIdx.x % 10], blockSum);\n  }\n}",
            "// TODO: Compute the total number of threads, the number of blocks, and the number of elements per thread\n  int total_num_threads = 0;\n  int num_blocks = 0;\n  int num_threads_per_block = 0;\n\n  // TODO: Initialize the bins array to zero\n  for (int i = 0; i < 10; i++) {\n    bins[i] = 0;\n  }\n\n  // TODO: Implement the kernel\n\n}",
            "// Insert code here\n}",
            "__shared__ double s[10];\n    for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        int j = floor(x[i] / 10.0);\n        atomicAdd(&s[j], 1.0);\n    }\n    __syncthreads();\n    for (int i = threadIdx.x; i < 10; i += blockDim.x) {\n        atomicAdd(&bins[i], s[i]);\n    }\n}",
            "// Write your code here\n}",
            "// 1. Create a global thread index variable\n    // 2. Create a global thread count variable\n    // 3. Create a shared thread index variable\n    // 4. Create a shared thread count variable\n    // 5. In the kernel, have a global thread index iterate over the values\n    //    (0 <= i < N) and update the bins. The value should be divided by 10\n    //    and the index in the bins array should be incremented.\n    // 6. After the kernel is complete, the bins array should contain the\n    //    counts of values in [0, 10), [10, 20), [20, 30),...\n    //    (i.e. bins[0] = 1, bins[1] = 2, bins[2] = 3, bins[3] = 0, bins[4] = 0)\n    // 7. Update the main function to print the bins array after the kernel is\n    //    complete.\n}",
            "}",
            "int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int gid = bid * blockDim.x + tid;\n  if (gid < N) {\n    int index = x[gid] / 10;\n    atomicAdd(&bins[index], 1);\n  }\n}",
            "// Start with some code to initialize the kernel.\n  // Use CUDA to initialize a vector `bins` that contains 0s.\n  // Loop through the values in `x` and count how many values in the bins \n  // in [0,10), [10, 20), [20, 30),... lie in each range.\n  // For example, if the value 32 is in `x`, then we should increment bins[1],\n  // and if the value 11 is in `x`, then we should increment bins[3].\n\n  // HINT: You can use the modulo operator `%` to calculate the bin index.\n  // You can initialize the bins to zero using this kernel:\n  // __global__ void initializeBins(size_t[10] bins) {\n  //   size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n  //   if (idx < 10) bins[idx] = 0;\n  // }\n\n  // You can call this kernel by using this line:\n  // initializeBins<<<1, 10>>>(bins);\n  // This is a special case of the more general line that follows it.\n\n  // HINT: You can compute the index into `bins` as follows:\n  // size_t index = int(x[i] / 10)\n  // The modulo operator is applied to the division in the index calculation,\n  // so that 12 becomes 1, and 13 becomes 0, etc.\n  //\n  // In this kernel, you will have to loop through the values in x, and the\n  // number of iterations will depend on the value of N.\n  // You can use a loop such as the following:\n  // for (size_t i = threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n  //   // Your code goes here\n  // }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    int bin = (int)floorf(x[idx]/10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int idx = threadIdx.x + blockDim.x * blockIdx.x;\n    if (idx < N) {\n        int bin = (int) ((x[idx] - 1) / 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "//TODO\n}",
            "// TODO\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n  if (tid >= N) return;\n  size_t bin = x[tid] / 10;\n  atomicAdd(&bins[bin], 1);\n}",
            "int idx = threadIdx.x + blockIdx.x*blockDim.x;\n  if (idx >= N) {\n    return;\n  }\n  bins[(int)fmin((int)floor(x[idx]/10.0), 9)]++;\n}",
            "// TODO: fill in this function\n\n  __shared__ double x_shared[1024];\n\n  // TODO: write this\n  int tid = threadIdx.x;\n  for (int i = tid; i < N; i += blockDim.x) {\n    x_shared[i] = x[i];\n  }\n\n  __syncthreads();\n\n  if (tid == 0) {\n    for (int i = 0; i < 10; i++) {\n      bins[i] = 0;\n    }\n  }\n\n  for (int i = tid; i < N; i += blockDim.x) {\n    if (x_shared[i] >= 0.0 && x_shared[i] < 10.0) {\n      bins[0]++;\n    }\n    if (x_shared[i] >= 10.0 && x_shared[i] < 20.0) {\n      bins[1]++;\n    }\n    if (x_shared[i] >= 20.0 && x_shared[i] < 30.0) {\n      bins[2]++;\n    }\n    if (x_shared[i] >= 30.0 && x_shared[i] < 40.0) {\n      bins[3]++;\n    }\n    if (x_shared[i] >= 40.0 && x_shared[i] < 50.0) {\n      bins[4]++;\n    }\n    if (x_shared[i] >= 50.0 && x_shared[i] < 60.0) {\n      bins[5]++;\n    }\n    if (x_shared[i] >= 60.0 && x_shared[i] < 70.0) {\n      bins[6]++;\n    }\n    if (x_shared[i] >= 70.0 && x_shared[i] < 80.0) {\n      bins[7]++;\n    }\n    if (x_shared[i] >= 80.0 && x_shared[i] < 90.0) {\n      bins[8]++;\n    }\n    if (x_shared[i] >= 90.0 && x_shared[i] <= 100.0) {\n      bins[9]++;\n    }\n  }\n\n  __syncthreads();\n\n  for (int i = 1024 / 2; i > 0; i /= 2) {\n    if (tid < i) {\n      bins[tid] += bins[tid + i];\n    }\n    __syncthreads();\n  }\n\n  if (tid == 0) {\n    for (int i = 0; i < 10; i++) {\n      bins[i] = bins[i] / N;\n    }\n  }\n\n  __syncthreads();\n}",
            "// TODO: implement using CUDA\n}",
            "/*\n      Your code here.\n    */\n    int idx = threadIdx.x;\n    int s = 0;\n    if (idx < N) {\n        s = (int)(x[idx] / 10);\n        s = s <= 9? s : 9;\n    }\n    __shared__ size_t counts[10];\n    if (threadIdx.x == 0) {\n        for (int i = 0; i < 10; i++) {\n            counts[i] = 0;\n        }\n    }\n    __syncthreads();\n    atomicAdd(&(counts[s]), 1);\n    __syncthreads();\n    if (idx == 0) {\n        for (int i = 0; i < 10; i++) {\n            bins[i] = counts[i];\n        }\n    }\n}",
            "int threadId = blockIdx.x*blockDim.x + threadIdx.x;\n\n    if(threadId < N) {\n        int value = (int)(x[threadId]/10);\n        atomicAdd(&bins[value], 1);\n    }\n}",
            "// TODO: Replace with real CUDA code that counts values of x in the bins.\n    // (It's OK to use an `if` statement and hard-code values into bins.)\n\n    for (int i = 0; i < N; ++i) {\n        if (x[i] <= 10)\n            bins[0] += 1;\n        else if (x[i] <= 20)\n            bins[1] += 1;\n        else if (x[i] <= 30)\n            bins[2] += 1;\n        else if (x[i] <= 40)\n            bins[3] += 1;\n        else if (x[i] <= 50)\n            bins[4] += 1;\n        else if (x[i] <= 60)\n            bins[5] += 1;\n        else if (x[i] <= 70)\n            bins[6] += 1;\n        else if (x[i] <= 80)\n            bins[7] += 1;\n        else if (x[i] <= 90)\n            bins[8] += 1;\n        else\n            bins[9] += 1;\n    }\n}",
            "//TODO\n  int gtid = blockIdx.x * blockDim.x + threadIdx.x;\n  for (int i = 0; i < N; i++) {\n    int bin = (int) (x[i] / 10.0);\n    if (bin >= 10 || bin < 0)\n      continue;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO\n}",
            "// Start your code here\n    // end your code here\n}",
            "for (size_t i = 0; i < 10; i++)\n        bins[i] = 0;\n\n    for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n        size_t bin = x[i] / 10;\n        bins[bin] += 1;\n    }\n}",
            "// implement\n}",
            "// TODO: YOUR CODE HERE\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO\n}",
            "// Fill this in\n    int nProcs = 0;\n    int myRank = 0;\n\n    // Compute the number of bins per MPI process\n    int numBinsPerProc = 10;\n    int totalBins = 10 * nProcs;\n    int myOffset = myRank * numBinsPerProc;\n\n    // Create an array to hold the bin counts on each MPI process\n    std::array<size_t, 10> myBins{};\n\n    // Create an array to hold the sum of bin counts on each MPI process\n    std::array<size_t, 10> allBins{};\n\n    // Loop over the input array and count the values in each bin\n    for (size_t i = 0; i < x.size(); i++) {\n        // Compute the index for the bin. Note that the index starts from 0\n        int binIndex = x[i] / 10;\n\n        // Increment the count for the bin\n        myBins[binIndex]++;\n    }\n\n    // Sum up the bin counts on each MPI process\n    MPI_Allreduce(myBins.data(), allBins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    // Store the bins on rank 0\n    if (myRank == 0) {\n        std::copy(allBins.begin(), allBins.end(), bins.begin());\n    }\n}",
            "// TODO\n  //int const n = 10;\n  //int const r = x.size();\n  //int const n1 = n/r;\n  //int const rem = n%r;\n\n  //int const b = n1 + 1;\n  //int const rem_count = rem > 0? 1 : 0;\n  //int const sum_count = b - rem_count;\n\n  //int const b = n1 + rem_count;\n  //int const sum_count = sum_count + n;\n\n  //int const rem_count = n - rem_count;\n\n  //int const b = n1;\n  //int const rem_count = rem;\n\n  //int const b = n1;\n  //int const sum_count = n;\n\n  //int const b = n;\n  //int const sum_count = n;\n\n  //int const b = 1;\n  //int const sum_count = r;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = n;\n  //int const sum_count = r;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = r;\n  //int const sum_count = 1;\n\n  //int const b = r;\n  //int const sum_count = 1;\n\n  //int const b = r;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const sum_count = 1;\n\n  //int const b = 1;\n  //int const",
            "size_t num_procs, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int step = 10;\n  int div = 10;\n  int size = 10;\n\n  std::array<int, 10> local_bins;\n  for (int i = 0; i < 10; i++) {\n    local_bins[i] = 0;\n  }\n\n  if (rank == 0) {\n    for (size_t i = 0; i < x.size(); i++) {\n      int index = x[i] / step;\n      local_bins[index]++;\n    }\n  }\n\n  std::array<int, 10> global_bins;\n  for (int i = 0; i < 10; i++) {\n    global_bins[i] = 0;\n  }\n\n  MPI_Reduce(&local_bins[0], &global_bins[0], size, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  for (int i = 0; i < 10; i++) {\n    if (rank == 0) {\n      bins[i] = global_bins[i];\n    }\n  }\n\n  std::array<int, 10> local_bins_sum;\n  for (int i = 0; i < 10; i++) {\n    local_bins_sum[i] = 0;\n  }\n\n  MPI_Reduce(&local_bins[0], &local_bins_sum[0], size, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  MPI_Bcast(&bins[0], size, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int nproc, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double start, end;\n    if (rank == 0) {\n        start = MPI_Wtime();\n    }\n\n    std::vector<size_t> bins_sums(10, 0);\n    for (size_t i = 0; i < x.size(); i++) {\n        int bin = (x[i] + 1) / 10; // 0 -> 0, 10 -> 1,..., 90 -> 9, 91 -> 0\n        bins[bin] += 1;\n    }\n\n    if (rank == 0) {\n        end = MPI_Wtime();\n        std::cout << \"MPI time: \" << (end - start) << std::endl;\n    }\n}",
            "// TODO\n}",
            "// Your code here\n\n}",
            "// YOUR CODE HERE\n    int numRanks, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int numElemPerRank = x.size()/numRanks;\n\n    std::vector<int> vecToBin(x.size());\n\n    #pragma omp parallel for\n    for (int i=0; i < x.size(); ++i) {\n        int binNum = int(x[i]/10);\n        vecToBin[i] = binNum;\n    }\n\n    // binning\n    // rank 0 will store the result\n    std::vector<int> sendBin(numRanks);\n    int idx = 0;\n    for (int i = 0; i < numRanks; ++i) {\n        for (int j = 0; j < numElemPerRank; ++j) {\n            sendBin[i] += vecToBin[idx];\n            ++idx;\n        }\n    }\n\n    // reduce the sum\n    MPI_Allreduce(sendBin.data(), bins.data(), numRanks, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    // reset the sum\n    sendBin.assign(numRanks, 0);\n\n    // rank 0 will store the result\n    for (int i = 0; i < numRanks; ++i) {\n        for (int j = 0; j < numElemPerRank; ++j) {\n            sendBin[i] += vecToBin[idx];\n            ++idx;\n        }\n    }\n    // reduce the sum\n    MPI_Allreduce(sendBin.data(), bins.data(), numRanks, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n}",
            "std::vector<double> x_local = x;\n    std::vector<int> bins_local(10, 0);\n    int n_per_thread = x.size() / omp_get_max_threads();\n    for (int i = 0; i < omp_get_max_threads(); ++i) {\n        int k = 0;\n        for (int j = i * n_per_thread; j < (i + 1) * n_per_thread && j < x.size(); ++j) {\n            int index = (int)std::round(x_local[j] / 10);\n            if (index < 10) {\n                bins_local[index]++;\n            }\n        }\n    }\n\n    std::vector<int> bins_global(10, 0);\n    MPI_Allreduce(MPI_IN_PLACE, bins_local.data(), 10, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    for (int i = 0; i < 10; ++i) {\n        bins[i] += bins_local[i];\n    }\n}",
            "// Write your code here\n\n  return;\n}",
            "// TODO: Your code here\n\n\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        bins[static_cast<size_t>(x[i]/10.0)]++;\n    }\n}",
            "int size = omp_get_max_threads();\n\n    // Get the minimum and maximum of the input vector\n    double max = x[0];\n    double min = x[0];\n    for (auto i = 1; i < x.size(); i++) {\n        if (x[i] > max) {\n            max = x[i];\n        }\n        if (x[i] < min) {\n            min = x[i];\n        }\n    }\n\n    // Determine the range of the input\n    double range = max - min;\n\n    // Determine the number of bins to use\n    double num_bins = range/10;\n\n    // Determine the size of each bin\n    double bin_size = range/num_bins;\n\n    // Determine the starting value of each bin\n    double bin_start = min;\n\n    // Create the bins\n    std::vector<double> bins_local(size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < size; i++) {\n        bins_local[i] = (bin_start + (i + 1)*bin_size);\n    }\n\n    // Initialize all elements of the bins array to 0\n    for (int i = 0; i < 10; i++) {\n        bins[i] = 0;\n    }\n\n    // Scatter the bins to all MPI ranks\n    int bins_size = sizeof(bins_local[0])*bins_local.size();\n    MPI_Scatter(bins_local.data(), bins_size, MPI_CHAR, bins.data(), bins_size, MPI_CHAR, 0, MPI_COMM_WORLD);\n\n    // Count the number of elements that fall in each bin\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = 0; j < 10; j++) {\n            if (x[i] >= bins[j] && x[i] < bins[j] + bin_size) {\n                bins[j]++;\n            }\n        }\n    }\n\n    // Gather all bins from all MPI ranks\n    MPI_Gather(bins.data(), 10*sizeof(size_t), MPI_CHAR, bins.data(), 10*sizeof(size_t), MPI_CHAR, 0, MPI_COMM_WORLD);\n}",
            "int num_procs, proc_id;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &proc_id);\n    int num_per_proc = x.size() / num_procs;\n\n    std::array<size_t, 10> local_bins;\n    for (int i = 0; i < 10; i++) {\n        local_bins[i] = 0;\n    }\n    int rank_sum = 0;\n    for (int i = proc_id * num_per_proc; i < (proc_id + 1) * num_per_proc; i++) {\n        int bin = (int) (x[i] / 10);\n        local_bins[bin]++;\n    }\n    int rank_total = 0;\n    for (int i = 0; i < num_procs; i++) {\n        rank_total += local_bins[i];\n    }\n    int bin_offset = 0;\n    if (proc_id == 0) {\n        bin_offset = rank_total;\n    }\n    MPI_Allreduce(&local_bins, &bins, 10, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    for (int i = 0; i < 10; i++) {\n        bins[i] += bin_offset;\n    }\n}",
            "size_t N = x.size();\n    if (N == 0) return;\n    std::vector<double> bin_sizes(10, 0.0);\n    for (size_t i=0; i<N; ++i) {\n        int bin = std::floor(x[i]/10.0);\n        if (bin < 0 || bin > 9) continue;\n        bin_sizes[bin]++;\n    }\n    int nprocs = 1;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    int rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int nbins = 10;\n    std::vector<size_t> partial_bin_sizes(nbins);\n    std::vector<size_t> partial_totals(nbins);\n    for (int i=0; i<nbins; ++i) {\n        partial_totals[i] = std::floor(bin_sizes[i]/nprocs);\n        partial_bin_sizes[i] = bin_sizes[i] - (partial_totals[i] * (nprocs-1));\n    }\n    size_t bin_start = partial_totals[rank];\n    size_t bin_end = partial_totals[rank] + partial_bin_sizes[rank];\n    size_t total = 0;\n    for (int i=0; i<nbins; ++i) {\n        if (i >= bin_start && i < bin_end) {\n            bins[i] += bin_sizes[i];\n            total += bin_sizes[i];\n        }\n    }\n    std::vector<size_t> all_totals(nprocs);\n    MPI_Allgather(&total, 1, MPI_UNSIGNED_LONG_LONG, all_totals.data(), 1, MPI_UNSIGNED_LONG_LONG, MPI_COMM_WORLD);\n    for (int i=0; i<nprocs; ++i) {\n        size_t sum = 0;\n        for (int j=0; j<i; ++j) {\n            sum += all_totals[j];\n        }\n        for (int j=0; j<nbins; ++j) {\n            if (i!= rank && j >= bin_start && j < bin_end) {\n                bins[j] += sum;\n            }\n        }\n    }\n    return;\n}",
            "// \n  // This is a start on the implementation. You need to add code to partition x\n  // and call binsBy10Sum, and to add code to store the results in bins.\n  // \n  std::vector<std::vector<double>> vecs;\n  int rank;\n  int nrank;\n  MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n  MPI_Comm_size(MPI_COMM_WORLD,&nrank);\n\n  vecs.resize(nrank);\n\n  int size = x.size();\n  int part = size/nrank;\n  int rem = size%nrank;\n\n  if(rank < rem){\n    part = part + 1;\n  }\n\n  MPI_Gather(&x[rank*(part+1)], part+1, MPI_DOUBLE, &vecs[rank], part+1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  if(rank == 0){\n    int i,j,k;\n    int count = 0;\n    double sum;\n    std::vector<double> result(10);\n    std::fill(result.begin(),result.end(),0.0);\n    for(i = 0; i < nrank; i++){\n      for(j = 0; j < part; j++){\n        if(vecs[i][j] >= 0 && vecs[i][j] < 10){\n          count++;\n          sum += vecs[i][j];\n        }\n        else if(vecs[i][j] >= 10 && vecs[i][j] < 20){\n          count++;\n          sum += vecs[i][j];\n        }\n        else if(vecs[i][j] >= 20 && vecs[i][j] < 30){\n          count++;\n          sum += vecs[i][j];\n        }\n        else if(vecs[i][j] >= 30 && vecs[i][j] < 40){\n          count++;\n          sum += vecs[i][j];\n        }\n        else if(vecs[i][j] >= 40 && vecs[i][j] < 50){\n          count++;\n          sum += vecs[i][j];\n        }\n        else if(vecs[i][j] >= 50 && vecs[i][j] < 60){\n          count++;\n          sum += vecs[i][j];\n        }\n        else if(vecs[i][j] >= 60 && vecs[i][j] < 70){\n          count++;\n          sum += vecs[i][j];\n        }\n        else if(vecs[i][j] >= 70 && vecs[i][j] < 80){\n          count++;\n          sum += vecs[i][j];\n        }\n        else if(vecs[i][j] >= 80 && vecs[i][j] < 90){\n          count++;\n          sum += vecs[i][j];\n        }\n        else if(vecs[i][j] >= 90 && vecs[i][j] <= 100){\n          count++;\n          sum += vecs[i][j];\n        }\n      }\n      for(k = 0; k < 10; k++){\n        if(count!= 0){\n          result[k] = sum/count;\n        }\n      }\n    }\n    bins = result;\n  }\n  else{\n    for(int i = 0; i < part; i++){\n      if(vecs[rank][i] >= 0 && vecs[rank][i] < 10){\n        count++;\n        sum += vecs[rank][i];\n      }\n      else if(vecs[rank][i] >= 10 && vecs[rank][i] < 20){\n        count++;\n        sum += vecs[rank][i];\n      }\n      else if(vecs[rank][i] >= 20 && vecs[rank][i] < 30){\n        count++;\n        sum += vecs[rank][i];\n      }\n      else if",
            "// Your code here\n  for (int i = 0; i < x.size(); i++) {\n    int bin = x[i] / 10;\n    if (bin >= 0 && bin < 10) {\n      bins[bin] += 1;\n    }\n  }\n}",
            "assert(x.size() % 10 == 0);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int myBinStart = (rank * x.size()) / size;\n    int myBinStop = ((rank + 1) * x.size()) / size;\n\n    int count = 0;\n    for(int i = myBinStart; i < myBinStop; i++){\n        if(i % 10 < 10 && x[i] >= 0 && x[i] < 10){\n            count++;\n        }\n        else if(i % 10 < 20 && x[i] >= 10 && x[i] < 20){\n            count++;\n        }\n        else if(i % 10 < 30 && x[i] >= 20 && x[i] < 30){\n            count++;\n        }\n        else if(i % 10 < 40 && x[i] >= 30 && x[i] < 40){\n            count++;\n        }\n        else if(i % 10 < 50 && x[i] >= 40 && x[i] < 50){\n            count++;\n        }\n        else if(i % 10 < 60 && x[i] >= 50 && x[i] < 60){\n            count++;\n        }\n        else if(i % 10 < 70 && x[i] >= 60 && x[i] < 70){\n            count++;\n        }\n        else if(i % 10 < 80 && x[i] >= 70 && x[i] < 80){\n            count++;\n        }\n        else if(i % 10 < 90 && x[i] >= 80 && x[i] < 90){\n            count++;\n        }\n        else if(i % 10 < 100 && x[i] >= 90 && x[i] <= 100){\n            count++;\n        }\n    }\n    bins[rank] = count;\n}",
            "// TODO: your code here\n\n    // get rank and size\n    int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // get local size and index\n    int iStart, iEnd, iLocalSize;\n    int numProcess = size;\n    iStart = rank * (x.size() / numProcess);\n    iEnd = (rank + 1) * (x.size() / numProcess);\n    iLocalSize = iEnd - iStart;\n\n    // parallel for\n#pragma omp parallel for\n    for (int i = 0; i < iLocalSize; i++) {\n        int bin = floor(x[iStart + i] / 10);\n        bins[bin]++;\n    }\n}",
            "// TODO: YOUR CODE HERE\n  size_t rank;\n  size_t size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int count = 0;\n  size_t len = x.size();\n\n#pragma omp parallel for\n  for (size_t i = 0; i < len; i++) {\n    int bin = (int) (x[i]/10.);\n    bins[bin]++;\n  }\n  // reduce\n  std::array<size_t, 10> rbins;\n\n  MPI_Reduce(bins.data(), rbins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    std::fill(bins.begin(), bins.end(), 0);\n    for (int i = 0; i < size; i++) {\n      std::copy_n(rbins.begin() + i * 10, 10, bins.begin() + i * 10);\n    }\n  }\n\n  MPI_Bcast(bins.data(), 10, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// Fill in the implementation below\n  size_t rank, commsize;\n  MPI_Comm_size(MPI_COMM_WORLD, &commsize);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  size_t maxval = x.size();\n\n  if(rank == 0){\n    std::fill(bins.begin(), bins.end(), 0);\n  }\n  MPI_Bcast(&bins, 10, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n  // OpenMP\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    int bin = (int)((x[i]/10) + 1);\n    if (rank == 0) {\n      bins[bin - 1]++;\n    }\n  }\n}",
            "int rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tint size;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\t//TODO: your code here\n\t#pragma omp parallel for\n\tfor(int i = 0; i < x.size(); i++) {\n\t\tint index = (x[i] / 10) + 1;\n\t\t#pragma omp atomic\n\t\tbins[index]++;\n\t}\n\tMPI_Reduce(MPI_IN_PLACE, bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "const size_t N = x.size();\n    size_t* bins_ptr = bins.data();\n\n#pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        int nthreads = omp_get_num_threads();\n        size_t rank_start = (N / nthreads) * tid;\n        size_t rank_end = (N / nthreads) * (tid + 1);\n        if (tid == nthreads - 1) {\n            rank_end = N;\n        }\n\n        // if tid is in range, count the values in that range and store in bins_ptr\n        if (rank_start < rank_end) {\n            // #pragma omp atomic\n            // bins_ptr[floor(x[i] / 10)]++;\n            for (size_t i = rank_start; i < rank_end; i++) {\n                bins_ptr[floor(x[i] / 10)]++;\n            }\n        }\n    }\n}",
            "// YOUR CODE HERE\n    // Make sure to use MPI and OpenMP\n}",
            "// Your code here.\n\n}",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int sizePerRank = 100 / size;\n    int startPoint = rank * sizePerRank;\n    int endPoint = (rank+1) * sizePerRank;\n    if(rank == 0) {\n        bins.fill(0);\n    }\n    #pragma omp parallel for\n    for(int i = startPoint; i < endPoint; i++) {\n        int bin = static_cast<int>(x[i] / 10);\n        #pragma omp atomic\n        bins[bin]++;\n    }\n    int binsPerRank = bins.size() / size;\n    int binsOffset = rank * binsPerRank;\n    if(rank!= 0) {\n        MPI_Status status;\n        MPI_Recv(bins.data(), binsPerRank, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n    } else {\n        int sum = bins[0];\n        for(int i = 1; i < bins.size(); i++) {\n            sum += bins[i];\n            bins[i] = sum;\n        }\n    }\n    if(rank!= 0) {\n        MPI_Status status;\n        MPI_Send(bins.data() + binsOffset, binsPerRank, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO: Your code here\n}",
            "int rank, num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    // TODO\n\n    std::cout << \"rank \" << rank << \" count:\" << std::endl;\n    for (auto &c : bins) {\n        std::cout << c << \" \";\n    }\n    std::cout << std::endl;\n    std::cout << \"rank \" << rank << \" sum:\" << std::endl;\n    size_t total = 0;\n    for (auto &c : bins) {\n        total += c;\n        std::cout << c << \" \";\n    }\n    std::cout << std::endl;\n\n    // TODO\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    if (rank == 0) {\n        std::cout << \"sum:\" << total << std::endl;\n    }\n}",
            "if (x.empty()) {\n        return;\n    }\n\n    size_t N = x.size();\n\n    std::vector<size_t> offsets(bins.size());\n    offsets.back() = N;\n    for (size_t i = 1; i < offsets.size(); i++) {\n        offsets[i] = (offsets[i-1] + bins[i-1]) % N;\n    }\n\n    // TODO: compute bins[i] in parallel\n    #pragma omp parallel for\n    for (size_t i = 0; i < bins.size(); i++) {\n        bins[i] = 0;\n        for (size_t j = offsets[i]; j < offsets[i+1]; j++) {\n            size_t v = static_cast<size_t>(10.0 * x[j]);\n            if (v < 10) {\n                bins[i] += 1;\n            }\n        }\n    }\n}",
            "constexpr double DENOMINATOR = 10.0;\n  constexpr double INITIAL_COUNT = 0.0;\n  constexpr size_t NUM_BINS = bins.size();\n\n  // Compute rank of each value in `x`\n  std::vector<int> ranks(x.size());\n#pragma omp parallel for\n  for (size_t i = 0; i < ranks.size(); i++) {\n    ranks[i] = omp_get_thread_num() + i * omp_get_num_threads();\n  }\n\n  // Compute which process will hold the value for a given bin index\n  std::vector<int> bin_owners(NUM_BINS);\n  for (size_t i = 0; i < NUM_BINS; i++) {\n    int bin_rank = i * omp_get_num_threads() / NUM_BINS;\n    bin_owners[i] = bin_rank;\n  }\n\n  // Compute the sum of counts for each bin\n  std::vector<double> bin_counts(NUM_BINS, 0);\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    int rank = ranks[i];\n    double bin_index = x[i] / DENOMINATOR;\n    int bin_owner = bin_owners[(size_t) bin_index];\n    if (rank == bin_owner) {\n      bin_counts[i] = INITIAL_COUNT;\n    }\n  }\n  MPI_Allreduce(MPI_IN_PLACE, bin_counts.data(), NUM_BINS, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  // Redistribute bin counts to bins\n  bins.fill(0);\n  for (size_t i = 0; i < NUM_BINS; i++) {\n    int bin_rank = bin_owners[i];\n    if (bin_rank == 0) {\n      bins[i] = bin_counts[i];\n    }\n  }\n\n  // Collect the bins on rank 0\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  if (rank == 0) {\n    std::vector<size_t> gathered_bins(NUM_BINS * omp_get_num_threads());\n    MPI_Allgather(bins.data(), NUM_BINS, MPI_LONG_LONG, gathered_bins.data(), NUM_BINS, MPI_LONG_LONG, MPI_COMM_WORLD);\n    for (size_t i = 0; i < NUM_BINS; i++) {\n      size_t sum = 0;\n      for (size_t j = 0; j < omp_get_num_threads(); j++) {\n        sum += gathered_bins[i * omp_get_num_threads() + j];\n      }\n      bins[i] = sum;\n    }\n  }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, num_ranks;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &num_ranks);\n    if (num_ranks > 1) {\n        // parallel code\n        int chunk_size = x.size() / num_ranks;\n        int extra_elements = x.size() % num_ranks;\n        std::vector<double> local_x(chunk_size + extra_elements);\n        if (rank < extra_elements) {\n            local_x[rank] = x[rank];\n        } else {\n            local_x[rank] = x[rank + extra_elements];\n        }\n\n        int num_groups = chunk_size / 10 + 1;\n        int group_size = chunk_size / num_groups;\n        std::vector<size_t> local_bins(num_groups, 0);\n        #pragma omp parallel for\n        for (int i = 0; i < num_groups; i++) {\n            for (int j = i * group_size; j < i * group_size + group_size; j++) {\n                if (local_x[j] >= 0 && local_x[j] < 10) {\n                    local_bins[i]++;\n                } else if (local_x[j] >= 10 && local_x[j] < 20) {\n                    local_bins[i + 1]++;\n                } else if (local_x[j] >= 20 && local_x[j] < 30) {\n                    local_bins[i + 2]++;\n                } else if (local_x[j] >= 30 && local_x[j] < 40) {\n                    local_bins[i + 3]++;\n                } else if (local_x[j] >= 40 && local_x[j] < 50) {\n                    local_bins[i + 4]++;\n                } else if (local_x[j] >= 50 && local_x[j] < 60) {\n                    local_bins[i + 5]++;\n                } else if (local_x[j] >= 60 && local_x[j] < 70) {\n                    local_bins[i + 6]++;\n                } else if (local_x[j] >= 70 && local_x[j] < 80) {\n                    local_bins[i + 7]++;\n                } else if (local_x[j] >= 80 && local_x[j] < 90) {\n                    local_bins[i + 8]++;\n                } else if (local_x[j] >= 90 && local_x[j] <= 100) {\n                    local_bins[i + 9]++;\n                }\n            }\n        }\n        std::vector<size_t> bins_reduced(10, 0);\n        MPI_Reduce(&local_bins[0], &bins_reduced[0], 10, MPI_INT, MPI_SUM, 0, comm);\n        bins = bins_reduced;\n    } else {\n        // sequential code\n        int num_groups = x.size() / 10 + 1;\n        int group_size = x.size() / num_groups;\n        std::vector<size_t> bins(num_groups, 0);\n        for (int i = 0; i < num_groups; i++) {\n            for (int j = i * group_size; j < i * group_size + group_size; j++) {\n                if (x[j] >= 0 && x[j] < 10) {\n                    bins[i]++;\n                } else if (x[j] >= 10 && x[j] < 20) {\n                    bins[i + 1]++;\n                } else if (x[j] >= 20 && x[j] < 30) {\n                    bins[i + 2]++;\n                } else if",
            "// Your code goes here\n}",
            "if(rank == 0) {\n        size_t i, j, k;\n        std::vector<double> buffer(size);\n        std::vector<size_t> send_count(size);\n        std::vector<size_t> recv_count(size);\n        std::vector<size_t> displ(size);\n\n        for(i = 0; i < x.size(); i++) {\n            k = (size_t) floor(x[i] / 10.0);\n            j = (size_t) (k * 10);\n            buffer[i] = x[i] - j;\n            send_count[i] = 1;\n        }\n\n        MPI_Alltoall(send_count.data(), 1, MPI_INT, recv_count.data(), 1, MPI_INT, MPI_COMM_WORLD);\n\n        displ[0] = 0;\n        for(i = 1; i < size; i++)\n            displ[i] = displ[i - 1] + recv_count[i - 1];\n\n        std::vector<double> recv_buffer(displ[size - 1] + recv_count[size - 1]);\n\n        MPI_Alltoallv(buffer.data(), send_count.data(), displ.data(), MPI_DOUBLE,\n                      recv_buffer.data(), recv_count.data(), displ.data(), MPI_DOUBLE, MPI_COMM_WORLD);\n\n        #pragma omp parallel for\n        for(i = 0; i < displ[size - 1] + recv_count[size - 1]; i++) {\n            j = (size_t) floor(recv_buffer[i] / 10.0);\n            k = (size_t) (j * 10);\n            bins[i] += 1;\n        }\n    } else {\n        std::vector<double> buffer(x.size());\n\n        #pragma omp parallel for\n        for(size_t i = 0; i < x.size(); i++) {\n            size_t k = (size_t) floor(x[i] / 10.0);\n            buffer[i] = x[i] - k * 10;\n        }\n\n        MPI_Gatherv(buffer.data(), x.size(), MPI_DOUBLE, NULL, NULL, NULL, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n}",
            "int size = omp_get_num_threads();\n  int rank = omp_get_thread_num();\n\n  int local_size = x.size() / size;\n  int local_offset = local_size * rank;\n\n  bins.fill(0);\n\n#pragma omp for\n  for (int i = 0; i < local_size; i++) {\n    int value = static_cast<int>(x[i + local_offset]);\n    if (value <= 10) {\n      bins[0]++;\n    } else if (value > 10 && value <= 20) {\n      bins[1]++;\n    } else if (value > 20 && value <= 30) {\n      bins[2]++;\n    } else if (value > 30 && value <= 40) {\n      bins[3]++;\n    } else if (value > 40 && value <= 50) {\n      bins[4]++;\n    } else if (value > 50 && value <= 60) {\n      bins[5]++;\n    } else if (value > 60 && value <= 70) {\n      bins[6]++;\n    } else if (value > 70 && value <= 80) {\n      bins[7]++;\n    } else if (value > 80 && value <= 90) {\n      bins[8]++;\n    } else if (value > 90 && value <= 100) {\n      bins[9]++;\n    } else {\n      throw std::runtime_error(\"Value not between 0 and 100, inclusive.\");\n    }\n  }\n\n  MPI_Allreduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_LONG_LONG_INT, MPI_SUM, MPI_COMM_WORLD);\n}",
            "assert(x.size() == 10);\n\n    bins.fill(0);\n\n    // 1. Split x into 10 equal parts, and get the size of each part\n    std::vector<size_t> part_sizes(10);\n    size_t const n = x.size();\n    size_t const n_per_part = n / 10;\n    size_t const remainder = n % 10;\n    for (size_t i = 0; i < 10; i++) {\n        part_sizes[i] = n_per_part + (i < remainder? 1 : 0);\n    }\n\n    // 2. Distribute the parts among the processes\n    std::vector<std::vector<double>> parts(10);\n    for (size_t i = 0; i < 10; i++) {\n        parts[i].reserve(part_sizes[i]);\n    }\n    for (size_t i = 0; i < n; i++) {\n        parts[i % 10].push_back(x[i]);\n    }\n\n    // 3. Each process counts the values within its part\n    for (size_t i = 0; i < 10; i++) {\n        for (auto value : parts[i]) {\n            if (value >= i * 10.0 && value < (i + 1) * 10.0) {\n                bins[i]++;\n            }\n        }\n    }\n\n    // 4. Collect the counts from all processes\n    // For each count, add it to the corresponding element in bins on rank 0.\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int num_ranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n  int chunk_size = x.size() / num_ranks;\n  int offset = rank * chunk_size;\n  int count = 0;\n  int bin_index;\n  for (int i = offset; i < offset + chunk_size; ++i) {\n    bin_index = int(x[i] / 10);\n    if (bin_index < 10) count++;\n  }\n\n  bins[rank] = count;\n  // sum bins\n  int sum = 0;\n  for (auto bin : bins) sum += bin;\n\n  // get the total sum for each bin\n  MPI_Allreduce(&sum, &bins[0], 10, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n}",
            "std::vector<double> tmp(x.size());\n  for(size_t i = 0; i < x.size(); ++i) {\n    tmp[i] = x[i] / 10;\n  }\n  int mpi_size, mpi_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n  if(mpi_rank == 0) {\n    for(int i = 0; i < mpi_size; ++i) {\n      int flag = 0;\n      int j = 0;\n      while(flag == 0) {\n        if(i == tmp[j]) {\n          ++bins[j];\n          flag = 1;\n        }\n        else {\n          ++j;\n        }\n      }\n    }\n  }\n  else {\n    int flag = 0;\n    int j = 0;\n    while(flag == 0) {\n      if(mpi_rank == tmp[j]) {\n        ++bins[j];\n        flag = 1;\n      }\n      else {\n        ++j;\n      }\n    }\n  }\n}",
            "// TODO: Your code here\n  int n_threads = omp_get_max_threads();\n  int rank = 0;\n  int size = 1;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  std::vector<int> local_bins;\n  local_bins.resize(10);\n  for (int i = 0; i < 10; i++) {\n    local_bins[i] = 0;\n  }\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    int bin = x[i]/10;\n    local_bins[bin]++;\n  }\n  //std::cout<<\"rank: \"<<rank<<\" local: \"<<local_bins[0]<<\" \"<<local_bins[1]<<\" \"<<local_bins[2]<<\" \"<<local_bins[3]<<std::endl;\n  //std::cout<<\"rank: \"<<rank<<\" local: \"<<local_bins[4]<<\" \"<<local_bins[5]<<\" \"<<local_bins[6]<<\" \"<<local_bins[7]<<std::endl;\n  //std::cout<<\"rank: \"<<rank<<\" local: \"<<local_bins[8]<<\" \"<<local_bins[9]<<\" \"<<local_bins[10]<<\" \"<<local_bins[11]<<std::endl;\n  //std::cout<<\"rank: \"<<rank<<\" local: \"<<local_bins[12]<<\" \"<<local_bins[13]<<\" \"<<local_bins[14]<<\" \"<<local_bins[15]<<std::endl;\n  std::vector<int> global_bins;\n  global_bins.resize(10*size);\n  for (int i = 0; i < 10; i++) {\n    global_bins[i*size + rank] = local_bins[i];\n  }\n\n  //MPI_Allreduce(MPI_IN_PLACE, global_bins.data(), 10, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n  MPI_Allreduce(MPI_IN_PLACE, global_bins.data(), 10, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n  //std::cout<<\"rank: \"<<rank<<\" global: \"<<global_bins[0]<<\" \"<<global_bins[1]<<\" \"<<global_bins[2]<<\" \"<<global_bins[3]<<std::endl;\n  //std::cout<<\"rank: \"<<rank<<\" global: \"<<global_bins[4]<<\" \"<<global_bins[5]<<\" \"<<global_bins[6]<<\" \"<<global_bins[7]<<std::endl;\n  //std::cout<<\"rank: \"<<rank<<\" global: \"<<global_bins[8]<<\" \"<<global_bins[9]<<\" \"<<global_bins[10]<<\" \"<<global_bins[11]<<std::endl;\n  //std::cout<<\"rank: \"<<rank<<\" global: \"<<global_bins[12]<<\" \"<<global_bins[13]<<\" \"<<global_bins[14]<<\" \"<<global_bins[15]<<std::endl;\n  //std::cout<<\"rank: \"<<rank<<\" global: \"<<global_bins[16]<<\" \"<<global_bins[17]<<\" \"<<global_bins[18]<<\" \"<<global_bins[19]<<std::endl;\n\n  if (rank == 0) {\n    //std::cout<<\"rank: \"<<rank<<\" global: \"<<global_bins[0]<<\" \"<<global_bins[1]<<\" \"<<global_bins[2]<<\" \"<<global_bins[3]<<std::",
            "int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  std::vector<int> bin_id(x.size());\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++)\n    bin_id[i] = int(x[i]/10.);\n\n  // Create a data type for the bin ID\n  MPI_Datatype bin_type;\n  MPI_Type_contiguous(1, MPI_INT, &bin_type);\n  MPI_Type_commit(&bin_type);\n\n  // Create a data type for the counts\n  MPI_Datatype count_type;\n  MPI_Type_contiguous(10, MPI_INT, &count_type);\n  MPI_Type_commit(&count_type);\n\n  // Reduce-scatter to compute the number of values in each bin\n  MPI_Op op;\n  MPI_Op_create(plus_op, false, &op);\n  MPI_Reduce_scatter_block(bin_id.data(), bins.data(), 10, bin_type, op, MPI_COMM_WORLD);\n  MPI_Op_free(&op);\n\n  // Clean up the MPI datatypes\n  MPI_Type_free(&bin_type);\n  MPI_Type_free(&count_type);\n}",
            "// Your code here\n}",
            "// TODO: your code here\n\n    return;\n}",
            "int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  //... Your code here...\n}",
            "int rank = 0;\n    int size = 0;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0)\n        std::cout << \"binsBy10Count \" << std::endl;\n\n    if (rank == 0)\n        std::cout << \"binsBy10Count \" << std::endl;\n\n    // calculate the length of the vector\n    int length = x.size();\n\n    // calculate the number of bins\n    int num_bins = bins.size();\n\n    // calculate the size of the local vector\n    int local_length = length / size;\n\n    // calculate the size of the local bins array\n    int local_num_bins = num_bins / size;\n\n    // initialize the bins\n    for (int i = 0; i < num_bins; i++)\n        bins[i] = 0;\n\n    // initialize the local bins\n    for (int i = 0; i < local_num_bins; i++)\n        bins[i] = 0;\n\n    // get the local values\n    std::vector<double> local_vector(local_length);\n    for (int i = 0; i < local_length; i++)\n        local_vector[i] = x[i];\n\n    // calculate the bins for the local values\n#pragma omp parallel\n    {\n        int thread_id = omp_get_thread_num();\n        int thread_count = omp_get_num_threads();\n\n        int start_value = local_length * rank / size;\n        int end_value = local_length * (rank + 1) / size;\n\n        for (int i = start_value; i < end_value; i++) {\n            int j = 10 * ((int) local_vector[i]);\n            if (j > num_bins - 1)\n                j = num_bins - 1;\n            bins[j] += 1;\n        }\n    }\n\n    // reduce the bins of the local vectors\n    std::vector<double> tmp(num_bins);\n    MPI_Allreduce(&bins[0], &tmp[0], num_bins, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n    for (int i = 0; i < num_bins; i++)\n        bins[i] = tmp[i];\n}",
            "}",
            "int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int nbins = 10;\n\n  // TODO: parallelize the outer loop\n  for (size_t i = 0; i < x.size(); i++) {\n    int idx = (x[i] - 1) / 10;\n    if (idx < nbins) {\n      bins[idx]++;\n    }\n  }\n\n  MPI_Reduce(MPI_IN_PLACE, bins.data(), nbins, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    std::cout << \"Bins: \";\n    for (auto bin : bins) {\n      std::cout << bin << \" \";\n    }\n    std::cout << std::endl;\n  }\n}",
            "/* Your code here */\n\n    int num_procs;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n    int num_threads = omp_get_max_threads();\n\n    int rank, proc_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &proc_size);\n\n    int chunk_size = x.size() / num_procs;\n    int extra_elems = x.size() % num_procs;\n\n    std::vector<double> my_x(chunk_size + extra_elems);\n\n    MPI_Scatter(x.data(), chunk_size + extra_elems, MPI_DOUBLE, my_x.data(), chunk_size + extra_elems, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel\n    {\n        int num_threads = omp_get_num_threads();\n        int thread_id = omp_get_thread_num();\n        std::vector<double> my_bins(num_threads);\n        std::array<size_t, 10> bins_sum = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < my_x.size(); ++i)\n        {\n            int bin_index = static_cast<int>(floor(my_x[i] / 10));\n            my_bins[thread_id] += ++bins_sum[bin_index];\n        }\n\n        #pragma omp critical\n        {\n            for (size_t i = 0; i < num_threads; ++i)\n            {\n                bins[i] += my_bins[i];\n            }\n        }\n    }\n\n    MPI_Gather(bins.data(), 10, MPI_LONG_LONG, bins.data(), 10, MPI_LONG_LONG, 0, MPI_COMM_WORLD);\n\n}",
            "// TODO: Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int chunk_size = x.size()/size;\n    int remainder = x.size()%size;\n    int bins_size = 10;\n    std::vector<size_t> bin_count(bins_size);\n    #pragma omp parallel for\n    for (int i = 0; i < chunk_size; i++) {\n        int v = x[rank*chunk_size+i];\n        int bin = (v/10);\n        bin_count[bin] += 1;\n    }\n    if (rank == 0) {\n        for (int i = 0; i < remainder; i++) {\n            int v = x[rank*chunk_size+i];\n            int bin = (v/10);\n            bin_count[bin] += 1;\n        }\n        for (int i = 0; i < bins_size; i++) {\n            bins[i] += bin_count[i];\n        }\n    }\n    else {\n        for (int i = 0; i < chunk_size; i++) {\n            int v = x[rank*chunk_size+i];\n            int bin = (v/10);\n            bin_count[bin] += 1;\n        }\n        if (rank == (size-1)) {\n            for (int i = 0; i < remainder; i++) {\n                int v = x[rank*chunk_size+i];\n                int bin = (v/10);\n                bin_count[bin] += 1;\n            }\n            for (int i = 0; i < bins_size; i++) {\n                bins[i] += bin_count[i];\n            }\n        }\n    }\n}",
            "#pragma omp parallel\n\t{\n\t\tint myrank, np;\n\t\tMPI_Comm_size(MPI_COMM_WORLD, &np);\n\t\tMPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n\t\tsize_t const my_local_size = x.size() / np;\n\t\tsize_t const my_start = myrank * my_local_size;\n\t\tsize_t const my_end = my_start + my_local_size;\n\n\t\t#pragma omp for\n\t\tfor (size_t i = my_start; i < my_end; ++i) {\n\t\t\tint bin = (x[i] / 10) % 10;\n\t\t\tbins[bin]++;\n\t\t}\n\t}\n}",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    // TODO: Your code here\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int block_size = x.size() / size;\n\n    if (rank == 0)\n        std::fill(bins.begin(), bins.end(), 0);\n\n    int my_block_start = rank * block_size;\n    int my_block_end = (rank + 1) * block_size;\n\n    #pragma omp parallel for schedule(dynamic)\n    for (int i = my_block_start; i < my_block_end; ++i) {\n        size_t bin = x[i] / 10;\n        #pragma omp atomic\n        bins[bin]++;\n    }\n\n    if (rank == 0) {\n        // We must reduce the partial bins into one bins on the root\n        std::array<size_t, 10> all_bins;\n        MPI_Allreduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n        for (size_t i = 0; i < 10; ++i)\n            std::cout << bins[i] << \"\\t\";\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n_bins = 10;\n    int x_per_rank = x.size() / size;\n    int x_overlap = x.size() % size;\n    int x_start = rank * x_per_rank + std::min(rank, x_overlap);\n    int x_end = x_start + x_per_rank;\n    if (rank < x_overlap) {\n        x_end += 1;\n    }\n\n    std::vector<double> x_local(x.begin() + x_start, x.begin() + x_end);\n    std::array<size_t, 10> bins_local = {0};\n    #pragma omp parallel for\n    for (int i = 0; i < x_local.size(); i++) {\n        bins_local[int(x_local[i] / 10.0)] += 1;\n    }\n\n    std::vector<size_t> bins_reduce;\n    for (int i = 0; i < n_bins; i++) {\n        bins_reduce.push_back(bins_local[i]);\n    }\n    MPI_Reduce(&bins_reduce[0], &bins[0], 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "// TODO: Your code here\n}",
            "// FIXME: your code here\n}",
            "assert(x.size() % 10 == 0);\n  MPI_Comm comm = MPI_COMM_WORLD;\n  size_t comm_size;\n  MPI_Comm_size(comm, &comm_size);\n  size_t n = x.size() / 10;\n  std::vector<size_t> count_per_rank(n);\n  size_t begin = n * comm_rank;\n  size_t end = n * (comm_rank + 1);\n#pragma omp parallel for\n  for (size_t i = 0; i < n; i++) {\n    size_t count = 0;\n    for (size_t j = begin + i * 10; j < begin + (i + 1) * 10; j++) {\n      if (x[j] < 10) {\n        count += 1;\n      }\n    }\n    count_per_rank[i] = count;\n  }\n  std::vector<size_t> counts_total(n);\n  std::vector<size_t> recvcounts(comm_size, n);\n  MPI_Allreduce(MPI_IN_PLACE, count_per_rank.data(), n, MPI_UNSIGNED_LONG_LONG, MPI_SUM, comm);\n  MPI_Allgather(count_per_rank.data(), n, MPI_UNSIGNED_LONG_LONG, counts_total.data(), n, MPI_UNSIGNED_LONG_LONG, comm);\n  for (int i = 0; i < n; i++) {\n    for (int j = 0; j < comm_size; j++) {\n      bins[i] += counts_total[i + j * n];\n    }\n  }\n}",
            "// TODO: your code here\n  MPI_Comm_size(MPI_COMM_WORLD, &mpiSize);\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpiRank);\n  int threadCount;\n  #pragma omp parallel\n  {\n    threadCount = omp_get_num_threads();\n  }\n  std::vector<double> xCopy(x.size());\n  MPI_Scatter(x.data(), x.size(), MPI_DOUBLE, xCopy.data(), x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  std::vector<double> binsCopy(bins.size());\n  MPI_Gather(xCopy.data(), bins.size(), MPI_DOUBLE, binsCopy.data(), bins.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  #pragma omp parallel for\n  for (int i = 0; i < bins.size(); i++) {\n    double count = 0;\n    for (int j = i*10; j < i*10 + 10; j++) {\n      if (j < x.size()) {\n        count++;\n      }\n    }\n    bins[i] = count;\n  }\n  MPI_Reduce(binsCopy.data(), bins.data(), bins.size(), MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  MPI_Bcast(bins.data(), bins.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "size_t rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if (rank == 0) {\n        std::fill(bins.begin(), bins.end(), 0);\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    auto const& x10 = x;\n    #pragma omp parallel for reduction(+:bins[0])\n    for (int i = 0; i < x10.size(); i++) {\n        if (x10[i] < 10) {\n            bins[0]++;\n        }\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n    for (int i = 1; i < 10; i++) {\n        MPI_Reduce(&bins[i - 1], &bins[i], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, i, MPI_COMM_WORLD);\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (int i = 0; i < bins.size(); i++) {\n            std::cout << \"Rank 0: \" << i << \" - \" << bins[i] << std::endl;\n        }\n    }\n}",
            "// TODO: implement me\n\n\n    //for (int i = 0; i < bins.size(); i++) {\n    //    bins[i] = 0;\n    //}\n    int rank;\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int binsize = x.size() / world_size;\n    int start, end;\n    start = rank * binsize;\n    end = start + binsize;\n    int i;\n    int* local = new int[10];\n\n    for (i = 0; i < 10; i++) {\n        local[i] = 0;\n    }\n\n    if (rank!= world_size - 1) {\n        for (i = start; i < end; i++) {\n            int bin = int(x[i] / 10);\n            if (bin < 10) {\n                local[bin]++;\n            }\n        }\n    }\n    else {\n        for (i = start; i < x.size(); i++) {\n            int bin = int(x[i] / 10);\n            if (bin < 10) {\n                local[bin]++;\n            }\n        }\n    }\n\n    int* local_sums = new int[10];\n\n    MPI_Reduce(local, local_sums, 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    for (i = 0; i < 10; i++) {\n        bins[i] = 0;\n    }\n\n    if (rank == 0) {\n        for (i = 0; i < 10; i++) {\n            bins[i] = local_sums[i];\n        }\n    }\n}",
            "int rank, nproc;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  size_t chunkSize = x.size()/nproc;\n  size_t localStart = rank*chunkSize;\n  size_t localEnd = localStart+chunkSize-1;\n\n  bins.fill(0);\n\n#pragma omp parallel for\n  for(size_t i=0; i<x.size(); i++)\n  {\n      if(x[i] >= localStart && x[i] <= localEnd)\n      {\n          int binIndex = (x[i] - localStart)/10;\n          bins[binIndex]++;\n      }\n  }\n\n  if(rank == 0)\n  {\n      size_t globalBins[10];\n      MPI_Reduce(bins.data(), globalBins, 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n      for(int i=0; i<10; i++)\n      {\n          bins[i] = globalBins[i];\n      }\n  }\n  else\n  {\n      MPI_Reduce(bins.data(), nullptr, 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n}",
            "// Your code goes here\n\n}",
            "assert(x.size() % 10 == 0);\n\n    // 10 bins, each containing 10 values\n    #pragma omp parallel\n    {\n        // each thread works on a different 10-bin chunk\n        int rank = omp_get_thread_num();\n        int numThreads = omp_get_num_threads();\n        int numRanks = omp_get_num_procs();\n\n        // compute offsets for each thread\n        size_t i = x.size() / numThreads * rank;\n        size_t end = i + x.size() / numThreads;\n\n        std::array<size_t, 10> counts = {};\n        for (; i < end; ++i) {\n            // 10 values per bin, hence a division by 10\n            int index = x[i] / 10;\n            assert(index < 10);\n            ++counts[index];\n        }\n\n        // collect counts from all ranks\n        std::array<size_t, 10> countsTotal;\n        MPI_Allreduce(counts.data(), countsTotal.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n        if (rank == 0) {\n            // merge counts\n            for (int i = 0; i < 10; ++i) {\n                bins[i] += countsTotal[i];\n            }\n        }\n    }\n}",
            "const size_t n = x.size();\n    std::vector<size_t> xbins(n);\n    const size_t block_size = n / size_t(MPI_Comm_size(MPI_COMM_WORLD));\n\n#pragma omp parallel\n    {\n        int rank, size;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n        size_t b = block_size * size_t(rank);\n        size_t e = block_size * size_t(rank + 1);\n\n        if (rank == size - 1)\n            e = n;\n\n        for (size_t i = b; i < e; i++)\n            xbins[i] = int(x[i] / 10);\n\n        MPI_Gatherv(&xbins[b], e - b, MPI_INT, &bins[0], &bins, MPI_INT, 0, MPI_COMM_WORLD);\n\n    }\n}",
            "size_t i;\n    size_t r = x.size();\n    size_t my_counts[10];\n\n    //initialize counts to 0\n    for (i = 0; i < 10; i++)\n        my_counts[i] = 0;\n\n    #pragma omp parallel for\n    for (i = 0; i < r; i++) {\n        size_t index = x[i]/10;\n        my_counts[index]++;\n    }\n\n    //collect all counts\n    MPI_Allreduce(my_counts, bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "int num_ranks = 0;\n    int rank_id = 0;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank_id);\n\n    auto n = x.size();\n    std::vector<size_t> counts(10);\n\n    // each rank takes a portion of the data\n    auto start = n / num_ranks * rank_id;\n    auto stop = n / num_ranks * (rank_id + 1);\n\n#pragma omp parallel for\n    for (int i = start; i < stop; i++) {\n        // the index is the floor of x[i] / 10\n        int index = x[i] / 10;\n        // update counts[index]\n        counts[index] += 1;\n    }\n\n    // send and receive counts to the correct rank\n    for (int i = 0; i < 10; i++) {\n        int rank_id_index = i / 10;\n        int rank_id_remainder = i % 10;\n\n        if (rank_id == rank_id_index) {\n            // send to the correct rank\n            MPI_Send(&counts[i], 1, MPI_UNSIGNED_LONG, rank_id_remainder, i, MPI_COMM_WORLD);\n        } else if (rank_id == rank_id_remainder) {\n            // receive from the correct rank\n            MPI_Recv(&bins[i], 1, MPI_UNSIGNED_LONG, rank_id_index, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n  int rank = 0;\n  int nproc = 1;\n  MPI_Comm_rank(comm, &rank);\n  MPI_Comm_size(comm, &nproc);\n\n  // Make sure that x contains at least 10 elements.\n  size_t nx = x.size();\n  if(nx < 10) {\n    std::cerr << \"Error: x should contain at least 10 elements\" << std::endl;\n    MPI_Abort(comm, 1);\n  }\n\n  // Divide the vector x into nproc pieces.\n  size_t nlocal = nx / nproc;\n  size_t left = 10 * rank * nlocal;\n  size_t right = left + nlocal;\n  std::vector<double> xlocal(x.begin() + left, x.begin() + right);\n\n  // Compute bins for xlocal.\n  binsBy10CountLocal(xlocal, bins);\n\n  // Gather bins on rank 0.\n  if(rank == 0) {\n    std::vector<size_t> binsGlobal(bins.size());\n    MPI_Gather(&bins[0], bins.size(), MPI_LONG_LONG, &binsGlobal[0], bins.size(), MPI_LONG_LONG, 0, comm);\n\n    // Compute total number of elements per bin.\n    for(int i = 0; i < 10; ++i) {\n      size_t total = 0;\n      for(int j = 0; j < nproc; ++j) {\n        total += binsGlobal[i + 10 * j];\n      }\n      std::cout << \"bin \" << i << \": \" << total << std::endl;\n    }\n  } else {\n    MPI_Gather(&bins[0], bins.size(), MPI_LONG_LONG, NULL, bins.size(), MPI_LONG_LONG, 0, comm);\n  }\n}",
            "}",
            "// Fill this in\n}",
            "int nprocs, myrank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n    int nperproc = x.size() / nprocs;\n    int rem = x.size() - nperproc * nprocs;\n\n    std::vector<double> local_x(nperproc);\n\n    if (myrank == 0) {\n        for (int i = 1; i < nprocs; ++i) {\n            MPI_Send(&x[i * nperproc], nperproc, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n        }\n    } else {\n        MPI_Recv(&local_x[0], nperproc, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    for (int i = 0; i < local_x.size(); ++i) {\n        local_x[i] = std::floor(local_x[i]/10);\n    }\n\n    std::vector<std::array<size_t, 10>> counts(nprocs);\n    if (myrank == 0) {\n        std::fill(counts[0].begin(), counts[0].end(), 0);\n    }\n    #pragma omp parallel num_threads(nprocs)\n    {\n        if (myrank == 0) {\n            int rank = omp_get_thread_num();\n            std::array<size_t, 10> &bincounts = counts[rank];\n            for (int i = 0; i < local_x.size(); ++i) {\n                ++bincounts[static_cast<int>(local_x[i])];\n            }\n        } else {\n            int rank = omp_get_thread_num();\n            std::array<size_t, 10> &bincounts = counts[rank];\n            for (int i = 0; i < local_x.size(); ++i) {\n                ++bincounts[static_cast<int>(local_x[i])];\n            }\n            MPI_Send(&bincounts[0], 10, MPI_INT, 0, rank, MPI_COMM_WORLD);\n        }\n    }\n\n    if (myrank == 0) {\n        std::fill(bins.begin(), bins.end(), 0);\n        for (int i = 0; i < nprocs; ++i) {\n            std::array<size_t, 10> &bincounts = counts[i];\n            for (int j = 0; j < 10; ++j) {\n                bins[j] += bincounts[j];\n            }\n        }\n    } else {\n        MPI_Recv(&bins[0], 10, MPI_INT, 0, myrank, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    if (myrank == 0) {\n        int total = 0;\n        for (int i = 0; i < 10; ++i) {\n            total += bins[i];\n        }\n        std::cout << \"Total count is \" << total << std::endl;\n    }\n}",
            "}",
            "//TODO\n\n}",
            "const int num_ranks = omp_get_num_threads();\n\tconst int rank = omp_get_thread_num();\n\tconst size_t N = x.size();\n\tconst size_t chunk_size = N / num_ranks;\n\tconst size_t remainder = N % num_ranks;\n\tsize_t my_chunk_size = chunk_size + ((rank < remainder)? 1 : 0);\n\tstd::vector<double> my_x(my_chunk_size);\n\tfor (int i = 0; i < my_chunk_size; i++) {\n\t\tmy_x[i] = x[rank * chunk_size + i];\n\t}\n\tstd::vector<double> my_bins(10);\n\tint remainder = N % num_ranks;\n\tstd::fill(my_bins.begin(), my_bins.end(), 0);\n#pragma omp parallel for reduction(+:my_bins)\n\tfor (int i = 0; i < my_chunk_size; i++) {\n\t\tint idx = (int)(my_x[i] / 10);\n\t\tmy_bins[idx] += 1;\n\t}\n\tstd::vector<size_t> all_bins(10);\n\tMPI_Reduce(&my_bins[0], &all_bins[0], 10, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < 10; i++) {\n\t\t\tbins[i] = all_bins[i];\n\t\t}\n\t}\n\treturn;\n}",
            "int mpiRank = 0;\n    int mpiSize = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpiSize);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpiRank);\n\n    size_t myNumElements = x.size() / mpiSize;\n    std::vector<size_t> myBins(10);\n\n    #pragma omp parallel for\n    for (int i = mpiRank * myNumElements; i < (mpiRank + 1) * myNumElements; i++) {\n        int bin = int(x[i] / 10);\n        myBins[bin]++;\n    }\n\n    std::vector<size_t> allBins(10);\n    MPI_Allreduce(myBins.data(), allBins.data(), 10, MPI_LONG_LONG_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    for (int i = 0; i < 10; i++) {\n        bins[i] = allBins[i];\n    }\n}",
            "/* TODO: Your code here */\n}",
            "assert(x.size() >= 10);\n  bins.fill(0);\n\n  // compute number of bins across all ranks\n  int num_ranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  int num_per_rank = (x.size() + num_ranks - 1) / num_ranks;\n  int start = num_per_rank * omp_get_thread_num();\n  int end = std::min(start + num_per_rank, x.size());\n\n  // for each bin, count values\n  for (int i = start; i < end; ++i) {\n    int index = std::floor(x[i] / 10.0);\n    bins[index]++;\n  }\n}",
            "//YOUR CODE HERE\n}",
            "// YOUR CODE HERE\n\n  // if (mpi_rank == 0) {\n  //   for (int i = 0; i < bins.size(); ++i) {\n  //     std::cout << i << \" \" << bins[i] << std::endl;\n  //   }\n  // }\n}",
            "std::vector<double> local_bins(10);\n  for (auto& val : local_bins) {\n    val = 0;\n  }\n  // your code here\n\n  int size;\n  int rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  #pragma omp parallel\n  {\n    int thread_id = omp_get_thread_num();\n    #pragma omp for\n    for (int i = 0; i < x.size(); i++) {\n      //if (x[i] <= 10) {\n      //  local_bins[0]++;\n      //} else if (x[i] <= 20) {\n      //  local_bins[1]++;\n      //} else if (x[i] <= 30) {\n      //  local_bins[2]++;\n      //} else if (x[i] <= 40) {\n      //  local_bins[3]++;\n      //} else if (x[i] <= 50) {\n      //  local_bins[4]++;\n      //} else if (x[i] <= 60) {\n      //  local_bins[5]++;\n      //} else if (x[i] <= 70) {\n      //  local_bins[6]++;\n      //} else if (x[i] <= 80) {\n      //  local_bins[7]++;\n      //} else if (x[i] <= 90) {\n      //  local_bins[8]++;\n      //} else if (x[i] <= 100) {\n      //  local_bins[9]++;\n      //}\n      local_bins[floor(x[i]/10)]++;\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < 10; i++) {\n      bins[i] = 0;\n    }\n    for (int i = 0; i < size; i++) {\n      MPI_Send(&local_bins[0], 10, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n    }\n    for (int i = 0; i < 10; i++) {\n      for (int j = 0; j < size; j++) {\n        MPI_Recv(&bins[i], 1, MPI_DOUBLE, j, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n    }\n  } else {\n    MPI_Recv(&local_bins[0], 10, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    MPI_Send(&local_bins[0], 10, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "// TODO\n    for (int i = 0; i < bins.size(); i++) {\n        bins[i] = 0;\n    }\n    double b = 10;\n    int n = x.size();\n#pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        if (x[i] < 10) {\n            bins[0]++;\n        }\n        if (x[i] >= 10 && x[i] < 20) {\n            bins[1]++;\n        }\n        if (x[i] >= 20 && x[i] < 30) {\n            bins[2]++;\n        }\n        if (x[i] >= 30 && x[i] < 40) {\n            bins[3]++;\n        }\n        if (x[i] >= 40 && x[i] < 50) {\n            bins[4]++;\n        }\n        if (x[i] >= 50 && x[i] < 60) {\n            bins[5]++;\n        }\n        if (x[i] >= 60 && x[i] < 70) {\n            bins[6]++;\n        }\n        if (x[i] >= 70 && x[i] < 80) {\n            bins[7]++;\n        }\n        if (x[i] >= 80 && x[i] < 90) {\n            bins[8]++;\n        }\n        if (x[i] >= 90 && x[i] <= 100) {\n            bins[9]++;\n        }\n    }\n}",
            "#pragma omp parallel\n  {\n    int rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    double min = 0;\n    double max = 100;\n    if (rank == 0)\n      max = 10;\n    std::array<size_t, 10> myBins;\n    #pragma omp for schedule(static)\n    for(size_t i = 0; i < x.size(); ++i) {\n      auto bin = static_cast<int>(std::floor(x[i]/10.0));\n      ++myBins[bin];\n    }\n    MPI_Gather(myBins.data(), 10, MPI_UNSIGNED_LONG_LONG, bins.data(), 10, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  }\n}",
            "// Your code here\n}",
            "size_t n = x.size();\n    size_t n_mpi = n / omp_get_num_procs();\n    size_t n_omp = n / omp_get_max_threads();\n    size_t n_proc = omp_get_num_procs();\n    size_t n_omp_rank = omp_get_thread_num();\n    size_t n_proc_rank = omp_get_thread_num();\n\n    std::vector<double> x_local(n_omp);\n    MPI_Scatter(x.data(), n_mpi, MPI_DOUBLE,\n                x_local.data(), n_omp, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    std::array<size_t, 10> bins_local = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n    for (size_t i = 0; i < n_omp; i++) {\n        double a = 10 * floor(x_local[i] / 10);\n        if (a > 9.9) {\n            a = 9.9;\n        }\n        bins_local[static_cast<size_t>(a)]++;\n    }\n\n    std::array<size_t, 10> bins_global = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n    MPI_Reduce(bins_local.data(), bins_global.data(), 10, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (n_proc_rank == 0) {\n        for (size_t i = 0; i < 10; i++) {\n            bins[i] = bins_global[i];\n        }\n    }\n\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tsize_t N = x.size();\n\tint num_threads = omp_get_max_threads();\n\tstd::vector<double> my_bin_values(num_threads);\n\tsize_t bin_count = N / size;\n\tsize_t num_bins_to_process = bin_count;\n\tsize_t my_bin_start = rank * bin_count;\n\tsize_t my_bin_end = my_bin_start + bin_count;\n\n\tstd::vector<int> my_bins(num_threads);\n\n#pragma omp parallel for\n\tfor (size_t i = 0; i < num_threads; i++) {\n\t\tsize_t start_index = my_bin_start + (bin_count * i);\n\t\tsize_t end_index = start_index + bin_count;\n\t\tif (end_index > N) {\n\t\t\tend_index = N;\n\t\t}\n\t\tmy_bin_values[i] = 0;\n\t\tfor (size_t j = start_index; j < end_index; j++) {\n\t\t\tdouble val = x[j];\n\t\t\tif (val < 10) {\n\t\t\t\tmy_bin_values[i] += 1;\n\t\t\t}\n\t\t\telse if (val < 20) {\n\t\t\t\tmy_bin_values[i] += 2;\n\t\t\t}\n\t\t\telse if (val < 30) {\n\t\t\t\tmy_bin_values[i] += 3;\n\t\t\t}\n\t\t\telse if (val < 40) {\n\t\t\t\tmy_bin_values[i] += 4;\n\t\t\t}\n\t\t\telse if (val < 50) {\n\t\t\t\tmy_bin_values[i] += 5;\n\t\t\t}\n\t\t\telse if (val < 60) {\n\t\t\t\tmy_bin_values[i] += 6;\n\t\t\t}\n\t\t\telse if (val < 70) {\n\t\t\t\tmy_bin_values[i] += 7;\n\t\t\t}\n\t\t\telse if (val < 80) {\n\t\t\t\tmy_bin_values[i] += 8;\n\t\t\t}\n\t\t\telse if (val < 90) {\n\t\t\t\tmy_bin_values[i] += 9;\n\t\t\t}\n\t\t\telse if (val < 100) {\n\t\t\t\tmy_bin_values[i] += 10;\n\t\t\t}\n\t\t}\n\t}\n\n\tstd::vector<double> bin_values;\n\tstd::vector<int> bins_sum(num_threads);\n\tif (rank == 0) {\n\t\tfor (size_t i = 0; i < num_threads; i++) {\n\t\t\tbin_values.push_back(my_bin_values[i]);\n\t\t\tbins_sum[i] = 0;\n\t\t}\n\t}\n\tMPI_Allreduce(MPI_IN_PLACE, bin_values.data(), num_threads, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\tMPI_Allreduce(MPI_IN_PLACE, bins_sum.data(), num_threads, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tfor (size_t i = 0; i < num_threads; i++) {\n\t\t\tfor (size_t j = 0; j < num_bins_to_process; j++) {\n\t\t\t\tbins[j] += (bin_values[i] / bins_sum[i]);\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO\n\n}",
            "size_t myBin[10];\n    // TODO: Count how many numbers from x are in each 10-segment of the vector,\n    //       and store the counts in myBin.\n    //       Use MPI and OpenMP to compute the counts in parallel.\n    // Hint: you can use MPI_Reduce to collect the counts from every rank\n    //       and then divide by the number of ranks.\n    // Hint: you can use OpenMP to parallelize over the 10 segments of x\n\n    // TODO: Copy the counts from myBin into bins.\n\n}",
            "// TODO\n\n}",
            "if (bins.size()!= 10) {\n        throw std::invalid_argument(\"bins should have 10 elements.\");\n    }\n    if (x.empty()) {\n        return;\n    }\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int chunkSize = x.size() / size;\n    int rest = x.size() % size;\n\n    int startIdx = chunkSize * rank + std::min(rank, rest);\n    int endIdx = startIdx + chunkSize;\n\n    if (rank == size - 1) {\n        endIdx = x.size();\n    }\n\n#pragma omp parallel for\n    for (int i = startIdx; i < endIdx; i++) {\n        int bucket = std::floor(x[i] / 10.0);\n        bins[bucket]++;\n    }\n\n    if (rank == 0) {\n        for (size_t i = 0; i < size; i++) {\n            MPI_Barrier(MPI_COMM_WORLD);\n            MPI_Recv(bins.data(), 10, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Barrier(MPI_COMM_WORLD);\n        MPI_Send(bins.data(), 10, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int mpi_rank;\n  int mpi_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n  int nthreads = omp_get_max_threads();\n\n  #pragma omp parallel\n  {\n    int tid = omp_get_thread_num();\n    size_t b = 10*tid;\n\n    int nbins = 10;\n    int num_bins = (int)ceil(x.size()/nbins);\n\n    int start = b * num_bins;\n    int end = start + num_bins;\n\n    if(end > x.size())\n      end = x.size();\n\n    // #pragma omp parallel for\n    // for (int i = 0; i < num_bins; i++) {\n      // int idx = start + i;\n\n      // if (idx >= x.size())\n        // break;\n\n      // int bin = (int)floor(x[idx]/10);\n      // bins[bin]++;\n    // }\n\n    for (int i = start; i < end; i++) {\n      int bin = (int)floor(x[i]/10);\n      bins[bin]++;\n    }\n  }\n\n  // if (mpi_rank == 0) {\n  //   for (int i = 0; i < bins.size(); i++) {\n  //     printf(\"%d: %d\\n\", i, bins[i]);\n  //   }\n  // }\n\n  MPI_Reduce(bins.data(), bins.data(), (int)bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // if (mpi_rank == 0) {\n  //   for (int i = 0; i < bins.size(); i++) {\n  //     printf(\"%d: %d\\n\", i, bins[i]);\n  //   }\n  // }\n}",
            "size_t n = x.size();\n  MPI_Comm comm = MPI_COMM_WORLD;\n  int rank, size;\n  MPI_Comm_rank(comm, &rank);\n  MPI_Comm_size(comm, &size);\n  int maxRank = 1;\n  while (size % maxRank == 0) {\n    maxRank *= 10;\n  }\n  maxRank /= 10;\n  if (size % maxRank!= 0) {\n    std::cerr << \"This program cannot be run with \" << size << \" MPI processes.\" << std::endl;\n    return;\n  }\n  if (size > maxRank) {\n    std::cerr << \"This program cannot be run with \" << size << \" MPI processes.\" << std::endl;\n    return;\n  }\n  int mpiRank, mpiSize;\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpiRank);\n  MPI_Comm_size(MPI_COMM_WORLD, &mpiSize);\n  std::vector<size_t> binsLoc;\n  std::array<size_t, 10> binsSum;\n  size_t nbins = maxRank / size;\n  for (int i = 0; i < maxRank; i++) {\n    binsSum[i % 10]++;\n  }\n  if (rank == 0) {\n    binsLoc.resize(10);\n    binsLoc[0] = 0;\n    for (int i = 0; i < 10; i++) {\n      binsLoc[i+1] = binsLoc[i] + nbins;\n    }\n  }\n  MPI_Bcast(&binsSum, 10, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  size_t binsLocStart = 0;\n  size_t binsLocEnd = nbins;\n  if (rank > 0) {\n    MPI_Send(&binsLocStart, 1, MPI_UNSIGNED_LONG_LONG, 0, rank, MPI_COMM_WORLD);\n    MPI_Send(&binsLocEnd, 1, MPI_UNSIGNED_LONG_LONG, 0, rank, MPI_COMM_WORLD);\n  } else {\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&binsLocStart, 1, MPI_UNSIGNED_LONG_LONG, i, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      MPI_Recv(&binsLocEnd, 1, MPI_UNSIGNED_LONG_LONG, i, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  }\n  #pragma omp parallel\n  {\n    size_t nbins_thread = binsLocEnd - binsLocStart;\n    std::vector<std::array<size_t, 10>> binsThread;\n    binsThread.resize(nbins_thread);\n    for (int i = 0; i < 10; i++) {\n      #pragma omp for\n      for (int j = 0; j < nbins_thread; j++) {\n        binsThread[j][i] = 0;\n      }\n    }\n    #pragma omp for\n    for (int i = binsLocStart; i < binsLocEnd; i++) {\n      int bin = x[i] / 10;\n      binsThread[i - binsLocStart][bin]++;\n    }\n    #pragma omp critical\n    {\n      for (int i = 0; i < 10; i++) {\n        for (int j = 0; j < nbins_thread; j++) {\n          bins[i] += binsThread[j][i];\n        }\n      }\n    }\n  }\n  if (rank == 0) {\n    for (int i = 1; i < size; i++)",
            "// fill up bins for current thread\n    std::array<size_t, 10> local_bins = {};\n\n    // Parallel for loop with OpenMP\n#pragma omp parallel for\n    for(size_t i = 0; i < x.size(); i++) {\n        local_bins[static_cast<size_t>((x[i]/10.0))]++;\n    }\n\n    // Reduce local bins to global bins\n    // Reduce only on rank 0\n    if (omp_get_thread_num() == 0) {\n        for (size_t i = 0; i < 10; i++) {\n            MPI_Reduce(&local_bins[i], &bins[i], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        }\n    } else {\n        MPI_Reduce(nullptr, &bins[0], 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "//Your code here\n}",
            "int num_threads = 8;\n    int my_rank, num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    std::vector<std::vector<int>> rank_bins(num_ranks);\n    int rank_size = x.size() / num_ranks;\n    int rank_start = rank_size * my_rank;\n    int rank_end = rank_size * (my_rank + 1);\n    if (my_rank == num_ranks - 1)\n        rank_end = x.size();\n\n    #pragma omp parallel num_threads(num_threads)\n    {\n        #pragma omp for\n        for (int i = rank_start; i < rank_end; i++)\n            rank_bins[my_rank].push_back(int(x[i]/10));\n    }\n\n    //std::cout << rank_bins[0][0] << '\\n';\n\n    int thread_id = omp_get_thread_num();\n    int thread_num = omp_get_num_threads();\n    int thread_start = thread_num * rank_start;\n    int thread_end = thread_num * rank_end;\n\n    // std::vector<std::vector<int>> all_bins(num_ranks);\n\n    // #pragma omp parallel for num_threads(num_threads)\n    // for (int i = thread_start; i < thread_end; i++)\n    //     std::cout << rank_bins[i%num_ranks][i/num_ranks] << '\\n';\n\n    //std::cout << bins[0] << '\\n';\n\n    #pragma omp parallel for num_threads(num_threads)\n    for (int i = thread_start; i < thread_end; i++) {\n        bins[rank_bins[i%num_ranks][i/num_ranks]]++;\n    }\n\n    //std::cout << bins[0] << '\\n';\n\n    // #pragma omp parallel for num_threads(num_threads)\n    // for (int i = thread_start; i < thread_end; i++) {\n    //     std::cout << bins[i%num_ranks][i/num_ranks] << '\\n';\n    // }\n\n    if (my_rank == 0) {\n        std::vector<int> total_bins(10);\n        #pragma omp parallel for num_threads(num_threads)\n        for (int i = 0; i < 10; i++) {\n            total_bins[i] = 0;\n            for (int j = 0; j < num_ranks; j++)\n                total_bins[i] += bins[j][i];\n        }\n        bins = total_bins;\n    }\n\n    //for (int i = 0; i < 10; i++)\n    //    std::cout << bins[i] << '\\n';\n    //\n    //if (my_rank == 0) {\n    //    for (int i = 0; i < 10; i++)\n    //        std::cout << bins[i] << '\\n';\n    //}\n\n    //if (my_rank == 0) {\n    //    std::vector<int> bins(10);\n    //    #pragma omp parallel for num_threads(num_threads)\n    //    for (int i = 0; i < 10; i++) {\n    //        bins[i] = 0;\n    //        for (int j = 0; j < num_ranks; j++) {\n    //            std::cout << rank_bins[j][i] << '\\n';\n    //            bins[i] += rank_bins[j][i];\n    //        }\n    //    }\n    //    bins = total_bins;\n    //}\n\n    // #pragma omp parallel for num_threads(num_threads)\n    // for (int i = 0; i < 10; i++) {\n    //     std",
            "MPI_Barrier(MPI_COMM_WORLD);\n    int rank, nprocs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    size_t my_size = x.size();\n    size_t i = rank * (x.size() / nprocs);\n    size_t j = (rank + 1) * (x.size() / nprocs);\n\n    #pragma omp parallel for\n    for (int k = i; k < j; k++) {\n        int bin = 10 * (int) x[k];\n        bins[bin / 10]++;\n    }\n\n    if (rank == 0) {\n        for (int k = 1; k < nprocs; k++) {\n            MPI_Recv(bins.data(), 10, MPI_INT, k, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(bins.data(), 10, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n}",
            "// TODO: implement using MPI and OpenMP\n    return;\n}",
            "// TODO: your code here\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    std::vector<double> xCopy = x;\n    size_t blockSize = x.size() / size;\n    size_t remainder = x.size() % size;\n\n    // Divide up the work and store each block in a separate array\n    std::vector<std::vector<double>> blocks(size);\n    for (int i = 0; i < size; i++) {\n        if (i < remainder) {\n            blocks[i] = std::vector<double>(x.begin() + i*blockSize, x.begin() + (i+1)*blockSize);\n        } else {\n            blocks[i] = std::vector<double>(x.begin() + i*blockSize, x.begin() + (i+1)*blockSize + blockSize);\n        }\n    }\n\n    // Compute the number of items in each block that fall in the corresponding interval\n    std::vector<std::array<size_t, 10>> blockCounts(size);\n    for (int i = 0; i < size; i++) {\n        int blockIndex = i;\n        if (i >= remainder) {\n            blockIndex = i - remainder;\n        }\n        for (size_t j = 0; j < blocks[blockIndex].size(); j++) {\n            blockCounts[i][(blocks[blockIndex][j] / 10)]++;\n        }\n    }\n\n    // Add the counts from each rank to the bins on rank 0\n    if (rank == 0) {\n        for (int i = 0; i < size; i++) {\n            for (size_t j = 0; j < 10; j++) {\n                bins[j] += blockCounts[i][j];\n            }\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "// your code goes here\n\n}",
            "std::vector<double> x_send;\n  std::vector<double> x_recv;\n  x_send.reserve(x.size());\n  x_recv.reserve(x.size());\n  auto MPI_COMM_WORLD = MPI_COMM_WORLD;\n\n  // TODO: split the work and perform a reduction\n  int num_of_ranks = 0, rank = 0, size = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_of_ranks);\n\n  std::vector<double> bins_local(10,0.);\n\n#pragma omp parallel num_threads(size)\n  {\n    int thread_num = omp_get_thread_num();\n    int threads_per_rank = size/num_of_ranks;\n\n    int x_start = 0, x_end = 0;\n    if(thread_num < threads_per_rank - 1){\n      x_start = thread_num * (x.size()/threads_per_rank);\n      x_end = (thread_num + 1) * (x.size()/threads_per_rank);\n    }else{\n      x_start = thread_num * (x.size()/threads_per_rank) + (x.size()/threads_per_rank) * threads_per_rank;\n      x_end = x.size();\n    }\n\n    for(auto i = x_start; i < x_end; i++){\n      x_send.push_back(x[i]);\n      x_recv.push_back(x[i]);\n    }\n    MPI_Gather(x_send.data(), x_send.size(), MPI_DOUBLE, x_recv.data(), x_send.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  }\n\n  if(rank == 0){\n    std::sort(x_recv.begin(), x_recv.end());\n    for(int i = 0; i < 10; i++){\n      double bin_start = i * 10;\n      double bin_end = bin_start + 10;\n      for(auto & val : x_recv){\n        if(val >= bin_start && val < bin_end)\n          bins_local[i] += 1;\n      }\n    }\n  }\n\n  MPI_Reduce(MPI_IN_PLACE, bins_local.data(), bins_local.size(), MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if(rank == 0){\n    for(int i = 0; i < 10; i++){\n      bins[i] = static_cast<size_t>(bins_local[i]);\n    }\n  }\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  //TODO:\n  std::vector<double> x_local;\n  x_local = x;\n\n  int p;\n  MPI_Allreduce(&rank, &p, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n  if (rank == 0)\n    std::cout << \"Total number of processors:\" << size << std::endl;\n\n  std::vector<double> bins_vec(10);\n  for (size_t i = 0; i < 10; i++)\n    bins_vec[i] = 0;\n\n  //TODO:\n  // MPI_Allreduce\n\n  if (rank == 0) {\n    for (size_t i = 0; i < 10; i++)\n      std::cout << bins_vec[i] << \" \";\n  }\n\n  //TODO:\n  // OpenMP\n\n  bins = std::array<size_t, 10>();\n  for (size_t i = 0; i < 10; i++)\n    bins[i] = bins_vec[i];\n\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int rank;\n    MPI_Comm_rank(comm, &rank);\n\n    double const * x_begin = x.data();\n    double const * x_end = x.data() + x.size();\n    std::sort(x_begin, x_end);\n\n    int nb_ranks;\n    MPI_Comm_size(comm, &nb_ranks);\n\n    size_t const chunk_size = x.size() / nb_ranks;\n    int nb_procs_per_group = nb_ranks / 10;\n    int nb_groups = nb_ranks / nb_procs_per_group;\n\n    int const rank_in_group = rank % nb_procs_per_group;\n\n    if(rank_in_group == 0) {\n        int const rank_in_group_plus_1 = rank_in_group + 1;\n        double *bins_begin = bins.data();\n        double *bins_end = bins.data() + bins.size();\n        std::fill(bins_begin, bins_end, 0);\n    }\n\n    if(nb_groups > 1) {\n        if(rank < 10) {\n            MPI_Send(&x[rank*chunk_size], chunk_size, MPI_DOUBLE, rank_in_group_plus_1, 0, comm);\n        } else if(rank < 20) {\n            MPI_Send(&x[rank*chunk_size], chunk_size, MPI_DOUBLE, rank_in_group_plus_1, 1, comm);\n        } else if(rank < 30) {\n            MPI_Send(&x[rank*chunk_size], chunk_size, MPI_DOUBLE, rank_in_group_plus_1, 2, comm);\n        } else if(rank < 40) {\n            MPI_Send(&x[rank*chunk_size], chunk_size, MPI_DOUBLE, rank_in_group_plus_1, 3, comm);\n        } else if(rank < 50) {\n            MPI_Send(&x[rank*chunk_size], chunk_size, MPI_DOUBLE, rank_in_group_plus_1, 4, comm);\n        } else if(rank < 60) {\n            MPI_Send(&x[rank*chunk_size], chunk_size, MPI_DOUBLE, rank_in_group_plus_1, 5, comm);\n        } else if(rank < 70) {\n            MPI_Send(&x[rank*chunk_size], chunk_size, MPI_DOUBLE, rank_in_group_plus_1, 6, comm);\n        } else if(rank < 80) {\n            MPI_Send(&x[rank*chunk_size], chunk_size, MPI_DOUBLE, rank_in_group_plus_1, 7, comm);\n        } else if(rank < 90) {\n            MPI_Send(&x[rank*chunk_size], chunk_size, MPI_DOUBLE, rank_in_group_plus_1, 8, comm);\n        } else if(rank < 100) {\n            MPI_Send(&x[rank*chunk_size], chunk_size, MPI_DOUBLE, rank_in_group_plus_1, 9, comm);\n        }\n    }\n\n    std::vector<double> vec;\n    std::vector<double> vec_bins;\n    vec.reserve(chunk_size);\n    vec_bins.reserve(chunk_size);\n    if(rank < 10) {\n        MPI_Status status;\n        MPI_Recv(&vec[0], chunk_size, MPI_DOUBLE, 0, 0, comm, &status);\n        for(size_t i = 0; i < vec.size(); i++) {\n            if(vec[i] >= 0 && vec[i] < 10) {\n                vec_bins[i] = vec[i];\n            }\n        }\n    } else if",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    auto const num_bins = bins.size();\n    size_t offset = size * rank;\n\n    for (auto i = 0; i < x.size(); ++i) {\n        int bin = x[i] / 10;\n        if (bin >= num_bins) bin = num_bins - 1;\n        bins[bin] += 1;\n    }\n\n    std::vector<size_t> all_bins(num_bins);\n#pragma omp parallel for\n    for (auto i = 0; i < num_bins; ++i) {\n        all_bins[i] = bins[i];\n    }\n    std::vector<size_t> final_bins(num_bins);\n    MPI_Allreduce(all_bins.data(), final_bins.data(), num_bins, MPI_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n    for (auto i = 0; i < num_bins; ++i) {\n        bins[i] = final_bins[i];\n    }\n\n}",
            "#pragma omp parallel\n    {\n        size_t thread_id = omp_get_thread_num();\n        size_t rank = omp_get_num_threads();\n        MPI_Status status;\n        MPI_Request request;\n        MPI_Recv(&bins, sizeof(size_t) * 10, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &status);\n        for (size_t i = 0; i < x.size(); i += rank) {\n            int bin_number = (x[i] / 10);\n            if (bin_number > 9) {\n                bin_number = 9;\n            }\n            if (thread_id == bin_number) {\n                bins[bin_number] += 1;\n            }\n        }\n        MPI_Send(&bins, sizeof(size_t) * 10, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &request);\n    }\n    MPI_Status status;\n    MPI_Recv(&bins, sizeof(size_t) * 10, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &status);\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "size_t n = x.size();\n  size_t m = (n + omp_get_max_threads() - 1)/omp_get_max_threads();\n  size_t nbins = 10;\n\n  // allocate and initialize the bins\n  #pragma omp parallel for\n  for(size_t i=0; i<nbins; ++i) {\n    bins[i] = 0;\n  }\n\n  #pragma omp parallel for\n  for(size_t i=0; i<n; i+=m) {\n    int rank = omp_get_thread_num();\n    size_t start = i + rank * m;\n    size_t end = i + (rank + 1) * m;\n    if(start >= n) start = n - 1;\n    if(end > n) end = n;\n    for(size_t j=start; j<end; ++j) {\n      int digit = floor(x[j]/10);\n      int bin = digit;\n      bins[bin]++;\n    }\n  }\n}",
            "const size_t nRanks = MPI_COMM_WORLD.Get_size();\n  const size_t myRank = MPI_COMM_WORLD.Get_rank();\n\n  std::vector<double> myBins(10);\n  for (size_t i = 0; i < 10; i++) {\n    myBins[i] = 0.0;\n  }\n  size_t N = x.size();\n  size_t start = myRank*N/nRanks;\n  size_t end = start + N/nRanks;\n\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for (size_t i = start; i < end; i++) {\n      double val = x[i];\n      if (val < 10.0) {\n        myBins[0]++;\n      }\n      else if (val >= 10.0 && val < 20.0) {\n        myBins[1]++;\n      }\n      else if (val >= 20.0 && val < 30.0) {\n        myBins[2]++;\n      }\n      else if (val >= 30.0 && val < 40.0) {\n        myBins[3]++;\n      }\n      else if (val >= 40.0 && val < 50.0) {\n        myBins[4]++;\n      }\n      else if (val >= 50.0 && val < 60.0) {\n        myBins[5]++;\n      }\n      else if (val >= 60.0 && val < 70.0) {\n        myBins[6]++;\n      }\n      else if (val >= 70.0 && val < 80.0) {\n        myBins[7]++;\n      }\n      else if (val >= 80.0 && val < 90.0) {\n        myBins[8]++;\n      }\n      else {\n        myBins[9]++;\n      }\n    }\n  }\n\n  MPI_Reduce(&myBins[0], &bins[0], 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "// Your code here\n    size_t xsize = x.size();\n    int rank = 0;\n    int nRanks = 1;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n\n    std::array<size_t, 10> tmpBins;\n    int blockSize = (int) ceil((double)xsize/nRanks);\n    int startIndex = rank*blockSize;\n    int endIndex = startIndex + blockSize;\n    if(endIndex > xsize) {\n        endIndex = xsize;\n    }\n    for (int i = startIndex; i < endIndex; i++) {\n        tmpBins[(int) (floor(x[i]/10.0))]++;\n    }\n\n    #pragma omp parallel\n    {\n        int threadNum = omp_get_thread_num();\n        int numThreads = omp_get_num_threads();\n        int chunk = (nRanks - 1) / numThreads;\n        int startRank = chunk * threadNum;\n        int endRank = startRank + chunk;\n        if (threadNum == numThreads - 1) {\n            endRank = nRanks;\n        }\n        for (int i = startRank; i < endRank; i++) {\n            MPI_Send(&(tmpBins[i*10]), 10, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD);\n        }\n    }\n    if (rank == 0) {\n        for (int i = 1; i < nRanks; i++) {\n            MPI_Recv(&bins[i * 10], 10, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        MPI_Reduce(&tmpBins, &bins, 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Reduce(&tmpBins, &bins, 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "int numtasks, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size_t N = x.size();\n    // Compute the size of each partition, for each task\n    int part_size = N / numtasks;\n    int extra = N % numtasks;\n    int local_start = part_size * rank;\n    if(rank < extra)\n        local_start += rank;\n    else\n        local_start += extra;\n    int local_stop = local_start + part_size;\n    if(rank < extra)\n        local_stop += 1;\n    else\n        local_stop += extra;\n\n    // Vector containing the bins for the current rank\n    std::vector<size_t> bins_local(10);\n    for(int i = local_start; i < local_stop; i++) {\n        if(i >= 100) {\n            std::cerr << \"ERROR: input x contains a value larger than 100\" << std::endl;\n            exit(1);\n        }\n        bins_local[x[i] / 10] += 1;\n    }\n\n    // Aggregate the local bins to obtain the global ones\n    //...\n    MPI_Gather(bins_local.data(), 10, MPI_UNSIGNED_LONG, bins.data(), 10, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n}",
            "bins = std::array<size_t, 10>{{0}};\n    if (x.size() <= 0) {\n        return;\n    }\n    if (x.size() <= 10) {\n        for (auto i = 0; i < x.size(); ++i) {\n            bins[i] = 1;\n        }\n        return;\n    }\n    size_t count = x.size();\n    size_t binsize = count / 10;\n    std::vector<double> x_mpi(x);\n\n    // split x vector\n    std::vector<double> x_mpi_r(binsize);\n    std::vector<double> x_mpi_l(x_mpi.begin(), x_mpi.begin() + binsize);\n    x_mpi.erase(x_mpi.begin(), x_mpi.begin() + binsize);\n\n    // split x vector\n    std::vector<double> x_mpi_r2(binsize);\n    std::vector<double> x_mpi_l2(x_mpi_r);\n    x_mpi_r.erase(x_mpi_r.begin(), x_mpi_r.begin() + binsize);\n\n    // split x vector\n    std::vector<double> x_mpi_r3(binsize);\n    std::vector<double> x_mpi_l3(x_mpi_r2);\n    x_mpi_r2.erase(x_mpi_r2.begin(), x_mpi_r2.begin() + binsize);\n\n    // split x vector\n    std::vector<double> x_mpi_r4(binsize);\n    std::vector<double> x_mpi_l4(x_mpi_r3);\n    x_mpi_r3.erase(x_mpi_r3.begin(), x_mpi_r3.begin() + binsize);\n\n    // split x vector\n    std::vector<double> x_mpi_r5(binsize);\n    std::vector<double> x_mpi_l5(x_mpi_r4);\n    x_mpi_r4.erase(x_mpi_r4.begin(), x_mpi_r4.begin() + binsize);\n\n    // split x vector\n    std::vector<double> x_mpi_r6(binsize);\n    std::vector<double> x_mpi_l6(x_mpi_r5);\n    x_mpi_r5.erase(x_mpi_r5.begin(), x_mpi_r5.begin() + binsize);\n\n    // split x vector\n    std::vector<double> x_mpi_r7(binsize);\n    std::vector<double> x_mpi_l7(x_mpi_r6);\n    x_mpi_r6.erase(x_mpi_r6.begin(), x_mpi_r6.begin() + binsize);\n\n    // split x vector\n    std::vector<double> x_mpi_r8(binsize);\n    std::vector<double> x_mpi_l8(x_mpi_r7);\n    x_mpi_r7.erase(x_mpi_r7.begin(), x_mpi_r7.begin() + binsize);\n\n    // split x vector\n    std::vector<double> x_mpi_r9(binsize);\n    std::vector<double> x_mpi_l9(x_mpi_r8);\n    x_mpi_r8.erase(x_mpi_r8.begin(), x_mpi_r8.begin() + binsize);\n\n    std::vector<double> x_mpi_r10(binsize);\n    std::vector<double> x_mpi_l10(x_mpi_r9);\n    x_mpi_r9.erase(x_mpi_r9.begin(), x_mpi_r9.begin() + binsize);\n\n    std::",
            "int rank = 0;\n    int nproc = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    size_t global_x_size = x.size();\n    size_t local_x_size = global_x_size / nproc;\n\n    bins.fill(0);\n\n    // compute local bins\n    #pragma omp parallel\n    {\n        int thread_id = omp_get_thread_num();\n        int num_threads = omp_get_num_threads();\n        int i0 = thread_id * local_x_size / num_threads;\n        int i1 = (thread_id+1) * local_x_size / num_threads;\n        std::vector<double> local_x(x.begin() + i0, x.begin() + i1);\n        for(auto xi : local_x) {\n            int bin = (int) (xi/10);\n            if(bin >= 0 && bin < 10) {\n                bins[bin]++;\n            }\n        }\n    }\n\n    // reduce bins from each rank\n    MPI_Allreduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  size_t localSize = x.size() / size;\n\n  std::array<size_t, 10> localBins;\n  localBins.fill(0);\n\n  double b0, b1;\n  if(rank == 0) {\n    b0 = 0;\n    b1 = 10;\n  } else if (rank == size-1) {\n    b0 = (rank * localSize) + 10 * rank;\n    b1 = b0 + 10;\n  } else {\n    b0 = (rank * localSize) + 10 * rank;\n    b1 = b0 + 10;\n  }\n\n  #pragma omp parallel for\n  for(size_t i = 0; i < localSize; i++) {\n    if(x[i] < b1) {\n      localBins[int((x[i] - b0) / 10)]++;\n    }\n  }\n\n  MPI_Gather(&localBins, 10, MPI_UNSIGNED_LONG_LONG, &bins, 10, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "// MPI part\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int bin_count = 10;\n  int bin_per_proc = bin_count/size;\n\n  // OpenMP part\n  size_t offset = rank*bin_per_proc;\n  //std::array<size_t, 10> bins(0);\n  #pragma omp parallel for\n  for(int i = offset; i < offset + bin_per_proc; i++) {\n    //std::cout << i << \" \" << x[i] << std::endl;\n    int bin = x[i]/10;\n    //std::cout << \"bin = \" << bin << std::endl;\n    #pragma omp atomic\n    bins[bin]++;\n  }\n\n  // merge part\n  //MPI_Allreduce(MPI_IN_PLACE, &bins[0], bin_count, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n  std::array<size_t, 10> total_bins{0};\n  MPI_Allreduce(&bins[0], &total_bins[0], bin_count, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n  //print part\n  if(rank == 0) {\n    for(int i = 0; i < bin_count; i++) {\n      std::cout << total_bins[i] << std::endl;\n    }\n  }\n}",
            "//... your code here...\n}",
            "// Your code goes here\n}",
            "// Your code goes here\n    return;\n}",
            "// TODO\n}",
            "int nrank, nr;\n    MPI_Comm_rank(MPI_COMM_WORLD, &nrank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nr);\n\n    size_t n = x.size();\n    int nperrank = n / nr;\n    size_t nleft = n % nr;\n    if (nleft > nrank) {\n        ++nperrank;\n    }\n    int nbin = 10;\n    size_t bin_size = n / nbin;\n    size_t remain_size = n % nbin;\n    int i = 0;\n    size_t *hist_tmp = new size_t[10];\n    size_t *hist_disp = new size_t[nr];\n\n    #pragma omp parallel for num_threads(10) schedule(static)\n    for (i = 0; i < 10; i++) {\n        hist_tmp[i] = 0;\n    }\n\n    #pragma omp parallel for schedule(static)\n    for (i = 0; i < nperrank; i++) {\n        size_t bin = (size_t)(x[i]/10.0);\n        if (bin >= 10) {\n            bin = 9;\n        }\n        ++(hist_tmp[bin]);\n    }\n\n    hist_disp[0] = 0;\n    for (i = 1; i < nr; i++) {\n        hist_disp[i] = hist_disp[i-1] + hist_tmp[i-1];\n    }\n    if (nleft > nrank) {\n        ++(hist_tmp[nrank]);\n    }\n\n    MPI_Gatherv(hist_tmp, 10, MPI_UNSIGNED_LONG_LONG, bins.data(), hist_tmp, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n    MPI_Gatherv(hist_disp, 10, MPI_UNSIGNED_LONG_LONG, bins.data(), hist_disp, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n    if (nrank == 0) {\n        for (i = 0; i < nbin; i++) {\n            if (i < remain_size) {\n                bins[i] += bins[i+10];\n            }\n            bins[i] -= bin_size;\n        }\n    }\n\n    delete[] hist_tmp;\n    delete[] hist_disp;\n}",
            "//TODO\n}",
            "std::vector<double> x_sorted = x;\n  std::sort(x_sorted.begin(), x_sorted.end());\n\n  size_t nranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n  size_t nvals = x_sorted.size();\n  size_t nvals_per_rank = nvals / nranks;\n  size_t start_index = nvals_per_rank * omp_get_thread_num();\n  size_t end_index = start_index + nvals_per_rank;\n\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  for (size_t i = start_index; i < end_index; ++i) {\n    auto val = x_sorted[i];\n    if (val < 10) {\n      bins[0]++;\n    } else if (val < 20) {\n      bins[1]++;\n    } else if (val < 30) {\n      bins[2]++;\n    } else if (val < 40) {\n      bins[3]++;\n    } else if (val < 50) {\n      bins[4]++;\n    } else if (val < 60) {\n      bins[5]++;\n    } else if (val < 70) {\n      bins[6]++;\n    } else if (val < 80) {\n      bins[7]++;\n    } else if (val < 90) {\n      bins[8]++;\n    } else if (val < 100) {\n      bins[9]++;\n    }\n  }\n\n  if (rank == 0) {\n    size_t sum = 0;\n    for (size_t i = 0; i < bins.size(); ++i) {\n      sum += bins[i];\n    }\n\n    for (size_t i = 0; i < bins.size(); ++i) {\n      bins[i] = bins[i] / sum;\n    }\n  }\n}",
            "// TODO\n\n}",
            "int nranks, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size_t n = x.size();\n    std::vector<double> x_my;\n    x_my.reserve(n);\n    std::vector<size_t> bins_my;\n    bins_my.reserve(10);\n\n    size_t nperrank = n/nranks;\n    size_t nlast = n % nranks;\n    size_t offset = rank * nperrank;\n    if(rank == nranks-1)\n        offset += nlast;\n    for(size_t i = offset; i < offset + nperrank; i++)\n        x_my.push_back(x[i]);\n\n    std::vector<double> x_g;\n    std::vector<size_t> bins_g;\n    MPI_Allgather(&x_my[0], x_my.size(), MPI_DOUBLE, &x_g[0], x_my.size(), MPI_DOUBLE, MPI_COMM_WORLD);\n    MPI_Allgather(&bins_my[0], bins_my.size(), MPI_UNSIGNED_LONG_LONG, &bins_g[0], bins_my.size(), MPI_UNSIGNED_LONG_LONG, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for(size_t i = 0; i < 10; i++) {\n        size_t count = 0;\n        for(size_t j = 0; j < n; j++) {\n            if((x_g[j] >= i * 10) && (x_g[j] < (i+1) * 10))\n                count++;\n        }\n        bins[i] = count;\n    }\n\n    if(rank == 0)\n        for(size_t i = 0; i < 10; i++)\n            std::cout << bins[i] << \" \";\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if(rank == 0) {\n        std::cout << \"Assignment 1: binsBy10Count\" << std::endl;\n    }\n\n    double start = MPI_Wtime();\n    size_t max_rank_data = size * x.size() / size;\n    std::vector<double> data(x.begin(), x.begin() + max_rank_data);\n\n    std::vector<double> min_max_data;\n    std::vector<size_t> data_sizes;\n    std::vector<size_t> offsets;\n    size_t bin_size = x.size() / size;\n\n    for(int i = 0; i < size; i++) {\n        min_max_data.push_back(x.at(i * bin_size));\n        min_max_data.push_back(x.at(i * bin_size + bin_size - 1));\n        data_sizes.push_back(bin_size);\n        offsets.push_back(i * bin_size);\n    }\n\n    std::vector<double> min_max_data_all;\n    std::vector<size_t> data_sizes_all;\n    std::vector<size_t> offsets_all;\n    MPI_Gather(&min_max_data[0], min_max_data.size(), MPI_DOUBLE, &min_max_data_all[0], min_max_data.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(&data_sizes[0], data_sizes.size(), MPI_INT, &data_sizes_all[0], data_sizes.size(), MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Gather(&offsets[0], offsets.size(), MPI_INT, &offsets_all[0], offsets.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n    if(rank == 0) {\n        std::vector<double> bin_limits;\n        bin_limits.push_back(0);\n        for(int i = 1; i < 10; i++) {\n            bin_limits.push_back(i * 10);\n        }\n        bin_limits.push_back(100);\n        std::vector<int> bins_all(bin_limits.size() - 1);\n\n        for(int i = 0; i < size; i++) {\n            int start = i * bin_size;\n            int end = start + bin_size;\n\n            std::vector<double> data_slice(data.begin() + start, data.begin() + end);\n            std::sort(data_slice.begin(), data_slice.end());\n            std::vector<size_t> histogram_bins(bin_limits.size() - 1);\n            for(int i = 0; i < data_slice.size(); i++) {\n                for(int j = 1; j < bin_limits.size(); j++) {\n                    if(data_slice.at(i) >= bin_limits.at(j) && data_slice.at(i) < bin_limits.at(j + 1)) {\n                        histogram_bins.at(j - 1) += 1;\n                    }\n                }\n            }\n            for(int j = 0; j < bin_limits.size() - 1; j++) {\n                bins_all.at(j) += histogram_bins.at(j);\n            }\n        }\n        for(int i = 0; i < bins_all.size(); i++) {\n            bins.at(i) = bins_all.at(i);\n        }\n    }\n\n    double end = MPI_Wtime();\n    if(rank == 0) {\n        std::cout << \"Time: \" << end - start << std::endl;\n    }\n}",
            "// TODO\n    return;\n}",
            "// your code here\n\n}",
            "int rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tint nranks;\n\tMPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n\t// compute the total number of values to be counted\n\tint total_count = x.size();\n\tint nblocks = 10;\n\n\t// the number of values to be counted by each process\n\tint my_count = total_count / nranks;\n\n\t// the number of values that each process will receive\n\tint receive_count = my_count + (total_count % nranks);\n\n\t// the number of values that each process will send\n\tint send_count = my_count;\n\n\t// we know that the first block will be assigned to rank 0\n\t// but we still need to assign the remaining blocks\n\tif (rank == 0) {\n\t\tfor (int i = 1; i < nranks; i++) {\n\t\t\tint block_num = (rank + i) % nranks;\n\t\t\tint dest_rank = block_num;\n\t\t\tMPI_Send(&x[i * my_count], send_count, MPI_DOUBLE, dest_rank, 0, MPI_COMM_WORLD);\n\t\t}\n\t}\n\n\t// receive the values that rank 0 sent us\n\tif (rank == 0) {\n\t\tstd::vector<double> receive_buffer(receive_count);\n\t\tMPI_Status status;\n\t\tMPI_Recv(&receive_buffer[0], receive_count, MPI_DOUBLE, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &status);\n\t\tMPI_Get_count(&status, MPI_DOUBLE, &receive_count);\n\t}\n\n\t// process the values that rank 0 sent us\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < receive_count; i++) {\n\t\t\tint block = receive_buffer[i] / 10;\n\t\t\tbins[block]++;\n\t\t}\n\t}\n\n\t// count the values that each rank will receive\n\tstd::vector<double> receive_buffer(receive_count);\n\n\tif (rank!= 0) {\n\t\tMPI_Recv(&receive_buffer[0], receive_count, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t}\n\n\tfor (int i = 0; i < receive_count; i++) {\n\t\tint block = receive_buffer[i] / 10;\n\t\tbins[block]++;\n\t}\n\n\t// count the values that each rank will send\n\tstd::vector<double> send_buffer(send_count);\n\tif (rank!= 0) {\n\t\tfor (int i = 0; i < send_count; i++) {\n\t\t\tsend_buffer[i] = x[(rank - 1) * my_count + i];\n\t\t}\n\n\t\tMPI_Send(&send_buffer[0], send_count, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n\t}\n\n\t// count the values that each rank will send\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < send_count; i++) {\n\t\t\tint block = x[i] / 10;\n\t\t\tbins[block]++;\n\t\t}\n\t}\n\n\t// reduce to rank 0\n\tstd::vector<size_t> block_count(10);\n\n\tif (rank!= 0) {\n\t\tMPI_Status status;\n\t\tMPI_Recv(&block_count[0], 10, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < 10; i++) {\n\t\t\tMPI_Send(&block_count[i], 1, MPI_INT, i,",
            "// YOUR CODE HERE\n}",
            "// Your code goes here\n}",
            "// write your code here\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    bins[4] = 0;\n    bins[5] = 0;\n    bins[6] = 0;\n    bins[7] = 0;\n    bins[8] = 0;\n    bins[9] = 0;\n    //printf(\"%lu\\n\", x.size());\n    //printf(\"%lu\\n\", x.capacity());\n    //printf(\"%lu\\n\", x.max_size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] >= 0 && x[i] <= 10) {\n            //printf(\"%f\\n\", x[i]);\n            bins[0]++;\n        } else if (x[i] >= 10 && x[i] <= 20) {\n            bins[1]++;\n        } else if (x[i] >= 20 && x[i] <= 30) {\n            bins[2]++;\n        } else if (x[i] >= 30 && x[i] <= 40) {\n            bins[3]++;\n        } else if (x[i] >= 40 && x[i] <= 50) {\n            bins[4]++;\n        } else if (x[i] >= 50 && x[i] <= 60) {\n            bins[5]++;\n        } else if (x[i] >= 60 && x[i] <= 70) {\n            bins[6]++;\n        } else if (x[i] >= 70 && x[i] <= 80) {\n            bins[7]++;\n        } else if (x[i] >= 80 && x[i] <= 90) {\n            bins[8]++;\n        } else if (x[i] >= 90 && x[i] <= 100) {\n            bins[9]++;\n        }\n    }\n\n}",
            "const size_t num_bins = 10;\n    bins = std::array<size_t, num_bins>();\n    for (auto& bin : bins)\n        bin = 0;\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int chunkSize = x.size() / size;\n    int remainder = x.size() % size;\n    int start, end;\n\n    if (rank == 0) {\n        start = 0;\n        end = chunkSize + remainder;\n    } else {\n        start = rank * chunkSize;\n        end = start + chunkSize;\n    }\n\n    #pragma omp parallel for schedule(static, chunkSize)\n    for (int i = start; i < end; i++)\n    {\n        int bin = floor(x[i] / 10.0);\n        if (bin == 10) bin = 0;\n        bins[bin] += 1;\n    }\n\n    // Sum bins on rank 0\n    if (rank == 0)\n    {\n        for (int i = 1; i < size; i++)\n        {\n            MPI_Status status;\n            MPI_Recv(&bins[0], 10, MPI_UNSIGNED_LONG_LONG, i, 10, MPI_COMM_WORLD, &status);\n        }\n    }\n    else\n    {\n        MPI_Send(&bins[0], 10, MPI_UNSIGNED_LONG_LONG, 0, 10, MPI_COMM_WORLD);\n    }\n}",
            "}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    size_t bin_count = x.size();\n    size_t bin_size = bin_count / size;\n\n    size_t start_index = rank * bin_size;\n    size_t end_index = start_index + bin_size;\n\n    std::array<size_t, 10> local_bins{};\n\n    for(size_t i = start_index; i < end_index; i++) {\n\n        int index = static_cast<int>(x[i] / 10);\n        local_bins[index]++;\n    }\n\n    std::array<size_t, 10> global_bins{};\n\n#pragma omp parallel\n    {\n        int rank;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Allreduce(local_bins.data(), global_bins.data(), 10, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    }\n    bins = global_bins;\n}",
            "size_t num_values = x.size();\n    size_t size = num_values / 10;\n    size_t remainder = num_values % 10;\n    size_t offset = 0;\n    int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int block = size;\n    if (size > remainder) {\n        block = size - remainder;\n    }\n    if (rank < remainder) {\n        offset = rank * (size + 1);\n    } else {\n        offset = rank * size;\n    }\n    //printf(\"rank %d offset %ld size %ld\\n\", rank, offset, size);\n\n    //omp_set_num_threads(8);\n    omp_set_num_threads(size);\n#pragma omp parallel for\n    for (size_t i = 0; i < size; i++) {\n        size_t offset1 = offset + i;\n        if (offset1 >= num_values) {\n            break;\n        }\n        size_t val = size_t(x[offset1]);\n        val /= 10;\n        if (val >= 10) {\n            val = 9;\n        }\n        #pragma omp atomic\n        bins[val]++;\n        if (val < 10 && i == (block - 1)) {\n            size_t val = size_t(x[offset1 + block]);\n            val /= 10;\n            if (val >= 10) {\n                val = 9;\n            }\n            #pragma omp atomic\n            bins[val]++;\n        }\n    }\n}",
            "// TODO: your code here\n\n    return;\n}",
            "assert(x.size() >= 10);\n    bins.fill(0);\n\n    // FIXME: Fill in the code\n    MPI_Datatype MPI_double_2D = MPI_DOUBLE;\n    size_t n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t chunkSize = n / size;\n    std::vector<double> x_rank(chunkSize);\n\n    for (int i = 0; i < size; ++i) {\n        int start = i * chunkSize;\n        int end = start + chunkSize;\n\n        for (int j = start; j < end; ++j) {\n            x_rank[j - start] = x[j];\n        }\n\n#pragma omp parallel\n        {\n            int thread_num = omp_get_thread_num();\n            int thread_count = omp_get_num_threads();\n            int tid = thread_num / 10;\n            int id = thread_num % 10;\n\n            for (int i = tid; i < chunkSize; i += thread_count) {\n                if (x_rank[i] >= id * 10 && x_rank[i] < (id + 1) * 10) {\n                    bins[id] += 1;\n                }\n            }\n        }\n\n        MPI_Allreduce(&bins[0], &bins[0], 10, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    }\n}",
            "// Fill in code here\n\n}",
            "size_t start = x.size() / omp_get_num_threads() * omp_get_thread_num();\n  size_t end = x.size() / omp_get_num_threads() * (omp_get_thread_num() + 1);\n  for (auto i = start; i < end; ++i) {\n    bins[static_cast<int>(x[i]/10)]++;\n  }\n}",
            "}",
            "size_t num = x.size();\n    size_t rank = MPI_Comm_rank(MPI_COMM_WORLD);\n\n    size_t n = (num + numprocs - 1)/numprocs;\n    size_t start = rank*n;\n\n    int low = 0, high = 9;\n\n    int b = (start + low)/10;\n    int e = (start + high)/10;\n\n    int i = b;\n\n    for (int k = start + low; k < start + high; k++) {\n        if (x[k] >= i*10 && x[k] < (i+1)*10) {\n            bins[i]++;\n        }\n    }\n\n    std::array<size_t, 10> result;\n\n    MPI_Reduce(&bins, &result, 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::cout << \"Result: \";\n        for (auto b : result) {\n            std::cout << b <<'';\n        }\n    }\n\n}",
            "#pragma omp parallel\n  {\n    // TODO: Fill bins on each rank\n  }\n\n  // TODO: Gather bins from each rank into a single vector\n\n  // TODO: Write the result to bins on rank 0\n}",
            "// TODO\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    size_t size = x.size();\n    // each rank has a copy of the vector x.\n    std::vector<double> local_x(size);\n    std::copy(x.begin(), x.end(), local_x.begin());\n\n    // create 10 bins with sizes 0, 10, 20, 30,... 90, 100\n    std::vector<size_t> bins(10);\n    std::fill(bins.begin(), bins.end(), 0);\n\n    // count the elements in each bin\n    #pragma omp parallel\n    {\n        size_t thread_id = omp_get_thread_num();\n        size_t thread_count = omp_get_num_threads();\n        size_t start_index = (size + thread_count - 1)/ thread_count*thread_id;\n        size_t end_index = (size + thread_count - 1)/ thread_count*(thread_id + 1);\n        for(size_t i = start_index; i < end_index; i++) {\n            size_t bin = (size_t) (local_x[i]/10);\n            if(bin < 10) {\n                bins[bin]++;\n            }\n        }\n    }\n\n    // gather the result from each rank\n    MPI_Gather(bins.data(), 10, MPI_LONG_LONG, bins.data(), 10, MPI_LONG_LONG, 0, MPI_COMM_WORLD);\n\n    // if rank is 0, print out the result\n    if (rank == 0) {\n        std::copy(bins.begin(), bins.end(), std::ostream_iterator<size_t>(std::cout, \", \"));\n        std::cout << std::endl;\n    }\n}",
            "int mpi_size;\n    int mpi_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    if (mpi_rank == 0) {\n        std::array<size_t, 10> bins_per_proc{};\n        // each rank has its own private copy of bins_per_proc\n\n        std::vector<double> x_rank;\n        x_rank = x;\n\n        #pragma omp parallel\n        {\n            int tid = omp_get_thread_num();\n            int num_threads = omp_get_num_threads();\n            int chunk_size = x.size()/num_threads;\n\n            for (auto& num : x_rank) {\n\n                int bin = (int)(num / 10);\n                bins_per_proc[bin]++;\n            }\n        }\n\n        // sum the counts from each rank\n        for (int rank = 1; rank < mpi_size; rank++) {\n            MPI_Send(&bins_per_proc[0], 10, MPI_INT, rank, 0, MPI_COMM_WORLD);\n        }\n\n        for (int rank = 1; rank < mpi_size; rank++) {\n            MPI_Status status;\n            MPI_Recv(&bins_per_proc[0], 10, MPI_INT, rank, 0, MPI_COMM_WORLD, &status);\n\n            for (int i = 0; i < 10; i++) {\n                bins[i] += bins_per_proc[i];\n            }\n        }\n    }\n    else {\n        std::array<size_t, 10> bins_per_proc{};\n        // each rank has its own private copy of bins_per_proc\n\n        std::vector<double> x_rank;\n        x_rank = x;\n\n        #pragma omp parallel\n        {\n            int tid = omp_get_thread_num();\n            int num_threads = omp_get_num_threads();\n            int chunk_size = x.size()/num_threads;\n\n            for (auto& num : x_rank) {\n\n                int bin = (int)(num / 10);\n                bins_per_proc[bin]++;\n            }\n        }\n\n        MPI_Send(&bins_per_proc[0], 10, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "std::vector<double> x_local(x.size());\n    bins.fill(0);\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int rank;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Status status;\n    int size;\n    MPI_Comm_size(comm, &size);\n    size_t total_size = x.size();\n    int chunk = total_size / size;\n    int left = total_size % size;\n    int first = 0;\n    if (rank < left) {\n        first = rank * (chunk + 1);\n    } else {\n        first = left + rank * chunk;\n    }\n\n    int last = first + chunk;\n    if (rank < left) {\n        last++;\n    }\n    #pragma omp parallel for\n    for (int i = first; i < last; i++) {\n        double val = x[i];\n        int bin = val / 10;\n        int bin_index = (bin % 10);\n        #pragma omp critical\n        bins[bin_index]++;\n    }\n    int bin_count = 10;\n    size_t total_count = bins[bin_count - 1] + bins[bin_count - 2] + bins[bin_count - 3] + bins[bin_count - 4] + bins[bin_count - 5] + bins[bin_count - 6] + bins[bin_count - 7] + bins[bin_count - 8] + bins[bin_count - 9];\n\n    if (rank == 0) {\n        for (int i = 0; i < bin_count; i++) {\n            MPI_Send(&bins[i], 1, MPI_UNSIGNED_LONG_LONG, rank, 0, comm);\n        }\n        for (int i = 1; i < size; i++) {\n            for (int j = 0; j < bin_count; j++) {\n                MPI_Recv(&bins[j], 1, MPI_UNSIGNED_LONG_LONG, i, 0, comm, &status);\n                total_count += bins[j];\n            }\n        }\n        printf(\"%lu\\n\", total_count);\n        for (int i = 0; i < bin_count; i++) {\n            double percent = ((double) bins[i]) / total_count;\n            printf(\"%lu: %lu %f\\n\", i, bins[i], percent);\n        }\n    } else {\n        MPI_Send(&bins[0], 1, MPI_UNSIGNED_LONG_LONG, 0, 0, comm);\n        MPI_Recv(&bins[0], 1, MPI_UNSIGNED_LONG_LONG, 0, 0, comm, &status);\n    }\n\n    return;\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    size_t localBins[10];\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i)\n        localBins[x[i] / 10]++;\n\n    // combine results\n    #pragma omp parallel\n    {\n        int thread_id = omp_get_thread_num();\n        #pragma omp for\n        for (size_t i = 0; i < 10; ++i)\n            bins[i] = localBins[i] + bins[i];\n    }\n}",
            "}",
            "int n = x.size();\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    size_t bin = (size_t)x[i]/10;\n    bins[bin]++;\n  }\n}",
            "size_t size = x.size();\n\n  bins = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n\n#pragma omp parallel\n  {\n    int rank, n_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n\n    int chunk_size = size / n_ranks;\n    size_t remainder = size % n_ranks;\n\n    // Determine which ranks will receive a chunk with a larger number of elements\n    // If a rank is not the last one, then it will get an extra chunk\n    int send_extra = (rank < remainder)? 1 : 0;\n\n    // Determine which rank will get a chunk with the largest number of elements\n    // If multiple ranks have this maximum number of elements, then they will all get that chunk\n    int recv_extra = (send_extra == 0)? 0 : (rank - remainder);\n\n    int send_offset = rank * chunk_size + send_extra;\n    int recv_offset = rank * chunk_size + recv_extra;\n\n    int send_size = chunk_size + send_extra;\n    int recv_size = chunk_size + recv_extra;\n\n    std::vector<double> send_buffer(send_size, 0);\n    std::vector<double> recv_buffer(recv_size, 0);\n\n    for (int i = 0; i < send_size; i++) {\n      send_buffer[i] = x[send_offset + i];\n    }\n\n    MPI_Status status;\n    MPI_Sendrecv(&send_buffer[0], send_size, MPI_DOUBLE, 0, 0,\n                 &recv_buffer[0], recv_size, MPI_DOUBLE, 0, 0,\n                 MPI_COMM_WORLD, &status);\n\n    for (int i = 0; i < recv_size; i++) {\n      // Round down the number to the nearest 10\n      double d = recv_buffer[i] / 10;\n      int index = static_cast<int>(d);\n      if (index < 10) {\n        bins[index]++;\n      }\n    }\n  }\n}",
            "// Initialize the bins on rank 0\n    bins = {};\n    int rank = 0, num_procs = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n    // The data size per rank\n    size_t data_size = x.size() / num_procs;\n\n    // The offset in x\n    size_t offset = data_size * rank;\n\n    // The number of valid data per rank\n    size_t valid_size = rank == num_procs - 1? x.size() - data_size * rank : data_size;\n\n    if (rank == 0) {\n        // The data in the whole world\n        std::vector<double> data(x.begin() + offset, x.begin() + offset + data_size);\n        auto sorted = data;\n        // Sort the data\n        std::sort(sorted.begin(), sorted.end());\n        // Iterate the data in x\n        for (size_t i = 0; i < valid_size; ++i) {\n            // The interval that contains the current value\n            auto const &[l, h] = std::equal_range(sorted.begin(), sorted.end(), x[offset + i]);\n            // The index in the sorted data\n            size_t index = l - sorted.begin();\n            // The number of values in this interval\n            size_t count = h - l;\n            if (count == 0) {\n                // Out of range, just ignore\n                continue;\n            }\n            // The interval is included in the valid range\n            if (index < 10) {\n                // Store the count in the bin\n                ++bins[index];\n            }\n        }\n    } else {\n        // Compute the bins\n        std::vector<double> data(x.begin() + offset, x.begin() + offset + data_size);\n        // Iterate the data in x\n#pragma omp parallel for\n        for (size_t i = 0; i < valid_size; ++i) {\n            // The interval that contains the current value\n            auto const &[l, h] = std::equal_range(data.begin(), data.end(), x[offset + i]);\n            // The index in the sorted data\n            size_t index = l - data.begin();\n            // The number of values in this interval\n            size_t count = h - l;\n            if (count == 0) {\n                // Out of range, just ignore\n                continue;\n            }\n            // The interval is included in the valid range\n            if (index < 10) {\n                // Add the count to the corresponding rank\n#pragma omp atomic\n                ++bins[index];\n            }\n        }\n    }\n}",
            "// TODO: implement\n\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n\tint rank, nprocs;\n\tMPI_Comm_rank(comm, &rank);\n\tMPI_Comm_size(comm, &nprocs);\n\n\tint size = x.size();\n\tint chunkSize = size / nprocs;\n\tint remainder = size % nprocs;\n\n\tdouble* local_x = new double[chunkSize];\n\tdouble* local_bins = new double[10];\n\n\tfor (int i = 0; i < chunkSize; i++) {\n\t\tlocal_x[i] = x[i + rank * chunkSize];\n\t}\n\n#pragma omp parallel\n\t{\n\t\tint threadId = omp_get_thread_num();\n\n\t\t// Parallelize using MPI_Bcast and OpenMP to fill up bins array\n\t\tfor (int i = 0; i < 10; i++) {\n\t\t\tif (threadId == 0) {\n\t\t\t\tlocal_bins[i] = 0;\n\t\t\t\tMPI_Bcast(local_bins, 10, MPI_DOUBLE, 0, comm);\n\t\t\t}\n\t\t\telse {\n\t\t\t\tMPI_Bcast(local_bins, 10, MPI_DOUBLE, 0, comm);\n\t\t\t}\n\t\t}\n\t}\n\n\t// Count the values in the first 10s, second 10s, etc. and store the counts in local_bins\n#pragma omp parallel\n\t{\n\t\tint threadId = omp_get_thread_num();\n\t\tint chunkStart = chunkSize * rank;\n\t\tint chunkEnd = chunkSize * (rank + 1);\n\t\tint chunkSize = chunkEnd - chunkStart;\n\n\t\tint i, j;\n\t\tfor (i = 0; i < chunkSize; i++) {\n\t\t\tfor (j = 0; j < 10; j++) {\n\t\t\t\tif (local_x[i] >= j * 10 && local_x[i] < (j + 1) * 10) {\n\t\t\t\t\tlocal_bins[j]++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Gather all the results from each processor to processor 0\n#pragma omp parallel\n\t{\n\t\tint threadId = omp_get_thread_num();\n\n\t\t// Parallelize using MPI_Gather\n\t\tdouble* global_bins = new double[10];\n\t\tif (threadId == 0) {\n\t\t\tglobal_bins[0] = 0;\n\t\t\tMPI_Gather(local_bins, 10, MPI_DOUBLE, global_bins, 10, MPI_DOUBLE, 0, comm);\n\t\t}\n\t\telse {\n\t\t\tMPI_Gather(local_bins, 10, MPI_DOUBLE, global_bins, 10, MPI_DOUBLE, 0, comm);\n\t\t}\n\n\t\tif (threadId == 0) {\n\t\t\tfor (int i = 0; i < 10; i++) {\n\t\t\t\tbins[i] = global_bins[i];\n\t\t\t}\n\t\t}\n\t}\n\n\t// Free dynamically allocated memory\n\tdelete[] local_x;\n\tdelete[] local_bins;\n}",
            "// TODO: implement\n    int rank = 0;\n    int world_size = 0;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double min = 0.0;\n    double max = 100.0;\n    double step = (max - min)/10.0;\n\n    int start = rank*x.size()/world_size;\n    int end = start + x.size()/world_size;\n\n    std::vector<size_t> bins_t(10);\n\n    int local_bins = 0;\n    for(int i = start; i < end; i++){\n        for(int j = 0; j < 10; j++){\n            if(x[i] >= j*step && x[i] < (j+1)*step){\n                bins_t[j]++;\n                local_bins++;\n                break;\n            }\n        }\n    }\n\n    //std::cout << rank << \": \";\n    //for(int i = 0; i < local_bins; i++)\n    //    std::cout << bins_t[i] << \", \";\n    //std::cout << std::endl;\n\n    //std::cout << rank << \": \";\n    //for(int i = 0; i < 10; i++)\n    //    std::cout << bins_t[i] << \", \";\n    //std::cout << std::endl;\n\n    //std::cout << \"rank \" << rank << \" has \" << x.size()/world_size << \" items\" << std::endl;\n    //std::cout << \"rank \" << rank << \" is \" << start << \" to \" << end << std::endl;\n\n    if(rank == 0){\n        MPI_Reduce(bins_t.data(), bins.data(), 10, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n        //std::cout << \"rank \" << rank << \" final bins: \";\n        //for(int i = 0; i < 10; i++)\n        //    std::cout << bins[i] << \", \";\n        //std::cout << std::endl;\n    }else{\n        MPI_Reduce(bins_t.data(), NULL, 10, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n\n\n}",
            "// TODO: Implement me!\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n  int nproc, rank;\n  MPI_Comm_size(comm, &nproc);\n  MPI_Comm_rank(comm, &rank);\n\n  if (nproc > 1) {\n    int const chunk_size = x.size() / nproc;\n    int const chunk_remainder = x.size() % nproc;\n\n    std::vector<double> chunk(chunk_size);\n    std::vector<double> chunk_send(chunk_size);\n    int send_count = 0;\n\n    if (rank == 0) {\n      std::vector<double> x_remainder(chunk_remainder);\n      x_remainder.reserve(chunk_remainder);\n      for (int i = 0; i < chunk_remainder; i++)\n        x_remainder[i] = x[i + chunk_size * nproc];\n      x.erase(x.begin() + chunk_size * nproc, x.end());\n      MPI_Bcast(x_remainder.data(), x_remainder.size(), MPI_DOUBLE, 0, comm);\n      x.insert(x.end(), x_remainder.begin(), x_remainder.end());\n    }\n\n    MPI_Scatter(x.data(), chunk_size, MPI_DOUBLE, chunk_send.data(), chunk_size, MPI_DOUBLE, 0, comm);\n\n    // fill chunk_send with values from x\n    for (int i = 0; i < chunk_send.size(); i++) {\n      int const int_val = static_cast<int>(chunk_send[i]);\n      int const bin_val = (int_val / 10) * 10;\n      chunk_send[i] = bin_val;\n    }\n\n    // compute how many elements in chunk_send are less than 10 (1), greater than 10 (2), etc\n    #pragma omp parallel for\n    for (int i = 0; i < chunk_send.size(); i++) {\n      if (chunk_send[i] < 10)\n        bins[0]++;\n      else if (chunk_send[i] >= 10 && chunk_send[i] < 20)\n        bins[1]++;\n      else if (chunk_send[i] >= 20 && chunk_send[i] < 30)\n        bins[2]++;\n      else if (chunk_send[i] >= 30 && chunk_send[i] < 40)\n        bins[3]++;\n      else if (chunk_send[i] >= 40 && chunk_send[i] < 50)\n        bins[4]++;\n      else if (chunk_send[i] >= 50 && chunk_send[i] < 60)\n        bins[5]++;\n      else if (chunk_send[i] >= 60 && chunk_send[i] < 70)\n        bins[6]++;\n      else if (chunk_send[i] >= 70 && chunk_send[i] < 80)\n        bins[7]++;\n      else if (chunk_send[i] >= 80 && chunk_send[i] < 90)\n        bins[8]++;\n      else if (chunk_send[i] >= 90 && chunk_send[i] < 100)\n        bins[9]++;\n    }\n\n    MPI_Gather(bins.data(), 10, MPI_UNSIGNED_LONG_LONG, bins.data(), 10, MPI_UNSIGNED_LONG_LONG, 0, comm);\n\n    if (rank == 0) {\n      for (int i = 0; i < bins.size(); i++) {\n        bins[i] += bins_sum[i];\n      }\n    }\n\n  } else {\n    int const bin_val = (static_cast<int>(x[0]) / 10) * 10;\n    bins[0]++;\n\n    for (int i = 1; i",
            "// compute the number of bins\n    size_t num_bins = x.size() / 10;\n    // initialize MPI statuses\n    MPI_Status statuses[num_bins];\n\n    // create a MPI request for each MPI_Recv\n    std::vector<MPI_Request> requests(num_bins);\n\n    // initialize bin counter to 0\n    std::fill(bins.begin(), bins.end(), 0);\n\n    // loop through all of the values in the vector\n    for (size_t i = 0; i < x.size(); ++i) {\n        // get the rank of the process which owns the value\n        size_t owner_rank = x[i] / 10;\n        // get the current bin index\n        size_t bin_index = x[i] % 10;\n\n        // if the rank of the owner is different from the current rank\n        if (owner_rank!= rank) {\n            // send the current value to the process which owns it\n            MPI_Send(&x[i], 1, MPI_DOUBLE, owner_rank, 0, MPI_COMM_WORLD);\n        } else {\n            // receive the value from the process which owns it\n            MPI_Recv(&x[i], 1, MPI_DOUBLE, owner_rank, 0, MPI_COMM_WORLD, &requests[i]);\n        }\n        // if the rank is the one that owns the value, increment its bin counter\n        if (owner_rank == rank)\n            bins[bin_index]++;\n    }\n\n    // complete all of the MPI requests\n    MPI_Waitall(num_bins, requests.data(), statuses);\n\n    // sum the counts across all of the ranks\n    // and store the results in bins\n    MPI_Reduce(&bins[0], &bins[0], bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "// TODO\n}",
            "int const numprocs = omp_get_num_procs();\n    int const rank = omp_get_thread_num();\n\n    //TODO\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    //TODO\n\n    if (rank == 0) {\n        bins[0] += bins_thread_0[0];\n        bins[1] += bins_thread_0[1];\n        bins[2] += bins_thread_0[2];\n        bins[3] += bins_thread_0[3];\n        bins[4] += bins_thread_0[4];\n        bins[5] += bins_thread_0[5];\n        bins[6] += bins_thread_0[6];\n        bins[7] += bins_thread_0[7];\n        bins[8] += bins_thread_0[8];\n        bins[9] += bins_thread_0[9];\n    }\n}",
            "// Your code here\n  // Assumption: All ranks have a complete copy of the vector x\n\n  // Get the rank and number of ranks\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // Get the number of elements in the vector\n  size_t N = x.size();\n\n  // Check if the vector has an even number of elements\n  if (N % 2 == 0) {\n    // Every rank with an even number of elements has one more element than the others.\n    // So, each rank must check if its rank has an even number of elements, and act accordingly.\n    if (rank % 2 == 0) {\n      // Rank has an even number of elements\n      N = N - 1;\n    } else {\n      // Rank has an odd number of elements\n      N = N;\n    }\n  }\n  // Calculate how many elements each rank should have\n  size_t N_per_rank = N / size;\n\n  // Initialize the bins array\n  std::fill(bins.begin(), bins.end(), 0);\n\n  // Make the local copy of the vector\n  std::vector<double> x_local = x;\n\n  // Get the starting index and end index of the vector in the current rank\n  size_t start_index = rank * N_per_rank;\n  size_t end_index = start_index + N_per_rank;\n\n  // Get the elements from the start to the end index in the current rank and count the values\n  for (size_t i = start_index; i < end_index; i++) {\n    bins[static_cast<int>((x_local[i] / 10)) % 10]++;\n  }\n\n  // Get the sum of the counts for each bin\n  std::vector<size_t> bins_sum(10);\n  for (size_t i = 0; i < 10; i++) {\n    // Get the sum of the counts for each bin\n    bins_sum[i] = std::accumulate(bins.begin(), bins.begin() + i + 1, 0);\n  }\n\n  // Use the sum to calculate the offset for each bin\n  for (size_t i = 0; i < 10; i++) {\n    if (i == 0) {\n      // Special case where offset is the sum of the counts\n      bins[i] = bins_sum[i];\n    } else {\n      bins[i] = bins_sum[i] - bins_sum[i - 1];\n    }\n  }\n\n  // Sum the local counts in the vector and subtract the offset\n  for (size_t i = start_index; i < end_index; i++) {\n    // Add the bin number of the current element to its value\n    x_local[i] = static_cast<int>((x_local[i] / 10)) % 10;\n    // Subtract the offset\n    x_local[i] = x_local[i] - bins[x_local[i]];\n  }\n\n  // Get the sum of all the counts in the current rank\n  size_t sum_count = std::accumulate(x_local.begin(), x_local.end(), 0);\n\n  // Sum the counts in the first rank\n  size_t sum = 0;\n  if (rank == 0) {\n    for (size_t i = 0; i < 10; i++) {\n      sum = sum + bins[i];\n    }\n  }\n\n  // Scatter the values to all the ranks\n  MPI_Scatter(&x_local, N_per_rank, MPI_DOUBLE, &x, N_per_rank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  // Get the sum of the counts in all the ranks\n  if (rank == 0) {\n    MPI_Reduce(&sum_count, &sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    //",
            "#pragma omp parallel\n  {\n    int rank = omp_get_thread_num();\n    int worldSize = omp_get_num_threads();\n\n    // Figure out how much of the x vector we have to handle\n    int x_begin = rank * x.size() / worldSize;\n    int x_end = (rank + 1) * x.size() / worldSize;\n\n    // Allocate local storage for the local bins\n    std::array<size_t, 10> local_bins = {};\n    for(size_t i = x_begin; i < x_end; i++) {\n      local_bins[(int)std::floor(x[i] / 10)]++;\n    }\n\n    // Gather the local bins to the first rank\n    MPI_Gather(local_bins.data(), 10, MPI_LONG, bins.data(), 10, MPI_LONG, 0, MPI_COMM_WORLD);\n  }\n}",
            "int nprocs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // divide x into 10 parts\n    size_t n = x.size();\n    size_t numperproc = n / nprocs;\n    size_t nremaining = n - numperproc * nprocs;\n\n    // allocate memory for local x\n    std::vector<double> xlocal(numperproc);\n\n    // get local part of x\n    if (rank < nremaining)\n        std::copy(x.begin() + (rank * numperproc + rank), x.begin() + ((rank + 1) * numperproc + rank + 1), xlocal.begin());\n    else\n        std::copy(x.begin() + (rank * numperproc + nremaining), x.begin() + ((rank + 1) * numperproc + nremaining), xlocal.begin());\n\n    // count values in each bin\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < xlocal.size(); ++i) {\n        size_t bin = (size_t)(xlocal[i] / 10.0);\n        if (bin > 9) bin = 9;\n        bins[bin]++;\n    }\n\n    // combine bins from different ranks\n    // initialize bins\n    std::array<size_t, 10> bins_local = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n\n    // get bins from different ranks\n    #pragma omp parallel for schedule(static)\n    for (int i = 1; i < nprocs; ++i) {\n        int otherrank = (rank + i) % nprocs;\n        MPI_Status status;\n        MPI_Recv(&bins_local, sizeof(bins_local), MPI_BYTE, otherrank, 0, MPI_COMM_WORLD, &status);\n\n        // sum bins from different ranks\n        for (size_t j = 0; j < 10; ++j) {\n            bins[j] += bins_local[j];\n        }\n    }\n}",
            "// TODO\n}",
            "// Your code here\n}",
            "size_t num_elements = x.size();\n\n    // Determine the number of elements to compute each rank\n    size_t block_size = num_elements / omp_get_num_threads();\n    size_t remainder = num_elements % omp_get_num_threads();\n\n    // Compute the starting index for each thread\n    size_t offset = 0;\n    for(size_t i = 0; i < omp_get_thread_num(); i++) {\n        offset += block_size;\n        if(i < remainder)\n            offset++;\n    }\n\n    // Compute the ending index for each thread\n    size_t max_offset = offset + block_size;\n    if(omp_get_thread_num() < remainder)\n        max_offset++;\n\n    // Count elements and place the result into bins\n    #pragma omp parallel for schedule(static)\n    for(size_t i = offset; i < max_offset; i++) {\n        size_t index = static_cast<size_t>(10.0 * x[i]);\n        bins[index]++;\n    }\n}",
            "//...\n}",
            "// TODO: YOUR CODE HERE\n\n}",
            "int rank, nranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n    double local_sum = 0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for(int i = 0; i < x.size(); ++i)\n        local_sum += x[i] < 10? 0 : x[i] < 20? 1 : x[i] < 30? 2 : 3;\n    int global_sum = 0;\n    MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if(rank == 0)\n    {\n        bins[0] = global_sum / 10;\n        for(int i = 1; i < 10; ++i)\n            bins[i] = bins[i - 1] + global_sum / 10;\n    }\n}",
            "// your code goes here\n}",
            "// set up mpi\n    int rank;\n    int size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    //set up openmp\n    int nthreads = omp_get_max_threads();\n\n    //set up the input vector for each rank\n    std::vector<double> local_x(x);\n\n    //initialize bins\n    std::fill(bins.begin(), bins.end(), 0);\n\n    //loop over the input vector\n    for (size_t i = 0; i < local_x.size(); i++) {\n\n        //use openmp to parallelize the counting\n        #pragma omp parallel for num_threads(nthreads)\n        for (int j = 0; j < 10; j++) {\n            if ((j * 10) <= local_x[i] && local_x[i] < ((j + 1) * 10)) {\n                bins[j]++;\n            }\n        }\n    }\n\n    //sum up bins for each rank\n    std::vector<size_t> bins_temp(10);\n    std::fill(bins_temp.begin(), bins_temp.end(), 0);\n    MPI_Allreduce(bins.data(), bins_temp.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n    bins = bins_temp;\n}",
            "size_t size = x.size();\n    size_t chunk_size = size/10;\n    size_t offset = 0;\n    if (size > 0) {\n        bins = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };\n        MPI_Request request[10];\n        MPI_Status status[10];\n        for (int i = 0; i < 10; i++) {\n            if (i < 9) {\n                MPI_Irecv(&bins[i], 1, MPI_UNSIGNED_LONG, MPI_ANY_SOURCE, i, MPI_COMM_WORLD, &request[i]);\n            }\n            MPI_Isend(&offset, 1, MPI_UNSIGNED_LONG, (i + 1) % 10, i, MPI_COMM_WORLD, &request[i + 10]);\n        }\n        for (int i = 0; i < 10; i++) {\n            if (i < 9) {\n                MPI_Wait(&request[i], &status[i]);\n            }\n            MPI_Wait(&request[i + 10], &status[i + 10]);\n        }\n        for (int i = 0; i < 10; i++) {\n            if (i < 9) {\n                offset += bins[i];\n            }\n        }\n        if (size - offset > 0) {\n            #pragma omp parallel for\n            for (int i = 0; i < size - offset; i++) {\n                if (i < chunk_size) {\n                    bins[i]++;\n                }\n                else if (i < 2 * chunk_size) {\n                    bins[1 + i - chunk_size]++;\n                }\n                else if (i < 3 * chunk_size) {\n                    bins[2 + i - 2 * chunk_size]++;\n                }\n                else if (i < 4 * chunk_size) {\n                    bins[3 + i - 3 * chunk_size]++;\n                }\n                else if (i < 5 * chunk_size) {\n                    bins[4 + i - 4 * chunk_size]++;\n                }\n                else if (i < 6 * chunk_size) {\n                    bins[5 + i - 5 * chunk_size]++;\n                }\n                else if (i < 7 * chunk_size) {\n                    bins[6 + i - 6 * chunk_size]++;\n                }\n                else if (i < 8 * chunk_size) {\n                    bins[7 + i - 7 * chunk_size]++;\n                }\n                else if (i < 9 * chunk_size) {\n                    bins[8 + i - 8 * chunk_size]++;\n                }\n            }\n        }\n        else {\n            offset = 0;\n        }\n        MPI_Request request[10];\n        MPI_Status status[10];\n        for (int i = 0; i < 10; i++) {\n            if (i < 9) {\n                MPI_Isend(&bins[i], 1, MPI_UNSIGNED_LONG, (i + 1) % 10, i, MPI_COMM_WORLD, &request[i]);\n            }\n            MPI_Irecv(&offset, 1, MPI_UNSIGNED_LONG, (i + 1) % 10, i, MPI_COMM_WORLD, &request[i + 10]);\n        }\n        for (int i = 0; i < 10; i++) {\n            if (i < 9) {\n                MPI_Wait(&request[i], &status[i]);\n            }\n            MPI_Wait(&request[i + 10], &status[i + 10]);\n        }\n        if (offset < size) {\n            #pragma omp parallel for\n            for (int i = offset; i < size; i++) {\n                if (i < chunk_size) {\n                    bins[",
            "size_t num_proc, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int bin_low = rank*10;\n  int bin_high = bin_low + 10;\n  bins[0] = 0;\n  for (int i=0; i<x.size(); i++) {\n    if (x[i] >= bin_low && x[i] < bin_high) {\n      bins[0]++;\n    }\n  }\n  bins[1] = 0;\n  for (int i=0; i<x.size(); i++) {\n    if (x[i] >= bin_low+1 && x[i] < bin_high+1) {\n      bins[1]++;\n    }\n  }\n  bins[2] = 0;\n  for (int i=0; i<x.size(); i++) {\n    if (x[i] >= bin_low+2 && x[i] < bin_high+2) {\n      bins[2]++;\n    }\n  }\n  bins[3] = 0;\n  for (int i=0; i<x.size(); i++) {\n    if (x[i] >= bin_low+3 && x[i] < bin_high+3) {\n      bins[3]++;\n    }\n  }\n  bins[4] = 0;\n  for (int i=0; i<x.size(); i++) {\n    if (x[i] >= bin_low+4 && x[i] < bin_high+4) {\n      bins[4]++;\n    }\n  }\n  bins[5] = 0;\n  for (int i=0; i<x.size(); i++) {\n    if (x[i] >= bin_low+5 && x[i] < bin_high+5) {\n      bins[5]++;\n    }\n  }\n  bins[6] = 0;\n  for (int i=0; i<x.size(); i++) {\n    if (x[i] >= bin_low+6 && x[i] < bin_high+6) {\n      bins[6]++;\n    }\n  }\n  bins[7] = 0;\n  for (int i=0; i<x.size(); i++) {\n    if (x[i] >= bin_low+7 && x[i] < bin_high+7) {\n      bins[7]++;\n    }\n  }\n  bins[8] = 0;\n  for (int i=0; i<x.size(); i++) {\n    if (x[i] >= bin_low+8 && x[i] < bin_high+8) {\n      bins[8]++;\n    }\n  }\n  bins[9] = 0;\n  for (int i=0; i<x.size(); i++) {\n    if (x[i] >= bin_low+9 && x[i] < bin_high+9) {\n      bins[9]++;\n    }\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  if (rank == 0) {\n    std::array<size_t, 10> bins_sum;\n    MPI_Reduce(MPI_IN_PLACE, bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Allreduce(MPI_IN_PLACE, bins_sum.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n    int sum_low = rank*10;\n    int sum_high = sum_low + 10;\n    int sum_total = 0;\n    for (int i=0; i<10; i++) {\n      if (sum_low+i < 0) {\n        sum_total += 0;\n      } else if (sum_low",
            "size_t n = x.size();\n  std::array<size_t, 10> local_bins;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n  MPI_Status status;\n  double* buffer = new double[n];\n\n  // send my part to rank 0\n  if (rank!= 0) {\n    MPI_Send(x.data() + n * rank, n / nranks, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n\n  // receive a part of x from rank 0\n  if (rank == 0) {\n    double* buffer = new double[n];\n    for (int i = 1; i < nranks; i++) {\n      MPI_Recv(buffer + i * (n / nranks), n / nranks, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &status);\n    }\n  }\n\n  // count local bins\n  for (int i = 0; i < n / nranks; i++) {\n    local_bins[std::floor(buffer[i] / 10)]++;\n  }\n\n  MPI_Reduce(MPI_IN_PLACE, local_bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    bins = local_bins;\n  }\n\n  // free buffer\n  delete[] buffer;\n}",
            "assert(x.size() > 1);\n    int n = x.size();\n    int r = omp_get_num_threads();\n    int l = omp_get_thread_num();\n\n    std::vector<size_t> bins_thread(10);\n    size_t min_val = 0;\n    size_t max_val = 0;\n\n    MPI_Allreduce(&x.at(0), &min_val, 1, MPI_UNSIGNED_LONG_LONG, MPI_MIN, MPI_COMM_WORLD);\n    MPI_Allreduce(&x.at(n-1), &max_val, 1, MPI_UNSIGNED_LONG_LONG, MPI_MAX, MPI_COMM_WORLD);\n\n    if(max_val - min_val >= 100){\n        std::cout << \"Vector size too big, aborting\" << std::endl;\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n    int total_elems = max_val - min_val + 1;\n    int elems_per_rank = total_elems / r;\n    int rest = total_elems % r;\n    int lower_limit = l * elems_per_rank;\n    int upper_limit = (lower_limit + elems_per_rank - 1);\n    if(l == r-1){\n        upper_limit = upper_limit + rest;\n    }\n\n    for(int i = lower_limit; i <= upper_limit; ++i){\n        if(i < 10){\n            bins_thread[0]++;\n        }\n        else if(i >= 10 && i < 20){\n            bins_thread[1]++;\n        }\n        else if(i >= 20 && i < 30){\n            bins_thread[2]++;\n        }\n        else if(i >= 30 && i < 40){\n            bins_thread[3]++;\n        }\n        else if(i >= 40 && i < 50){\n            bins_thread[4]++;\n        }\n        else if(i >= 50 && i < 60){\n            bins_thread[5]++;\n        }\n        else if(i >= 60 && i < 70){\n            bins_thread[6]++;\n        }\n        else if(i >= 70 && i < 80){\n            bins_thread[7]++;\n        }\n        else if(i >= 80 && i < 90){\n            bins_thread[8]++;\n        }\n        else if(i >= 90){\n            bins_thread[9]++;\n        }\n    }\n    MPI_Reduce(bins_thread.data(), bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if(l == 0){\n        for(int i = 0; i < 10; i++){\n            std::cout << bins[i] << std::endl;\n        }\n    }\n\n}",
            "int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    int num_per_proc = x.size() / world_size;\n    int remainder = x.size() % world_size;\n\n    std::vector<double> x_per_rank(num_per_proc);\n    int offset = world_rank * num_per_proc;\n\n    if (world_rank == 0) {\n        offset = 0;\n    }\n\n    for (int i = 0; i < num_per_proc; ++i) {\n        x_per_rank[i] = x[i + offset];\n    }\n\n    // Only one proc needs the remainder\n    if (world_rank == 0) {\n        for (int i = 0; i < remainder; ++i) {\n            x_per_rank[i + num_per_proc] = x[i + offset + num_per_proc];\n        }\n    }\n\n    // Initialize array to 0\n    for (size_t i = 0; i < 10; ++i) {\n        bins[i] = 0;\n    }\n\n    // OpenMP pragma to create threads\n    #pragma omp parallel for\n    for (int i = 0; i < num_per_proc; ++i) {\n        // Get the value to check from x_per_rank\n        double value = x_per_rank[i];\n\n        // Get the index of the bin to increment\n        int bin_index = (value / 10);\n\n        // Increment the bin\n        #pragma omp atomic\n        bins[bin_index] += 1;\n    }\n}",
            "int const num_procs = omp_get_num_threads();\n  int const rank = omp_get_thread_num();\n\n  if (x.size()!= num_procs) {\n    printf(\"ERROR: vectors are not the same size\\n\");\n    MPI_Abort(MPI_COMM_WORLD, 1);\n  }\n\n  int const num_bins = 10;\n  MPI_Status status;\n\n  for (int i = 0; i < num_bins; i++) {\n    bins[i] = 0;\n  }\n\n  int begin = rank*num_bins;\n  int end = begin + num_bins;\n\n  for (int i = begin; i < end; i++) {\n    double value = x[i];\n    bins[i] = bins[i] + 1;\n  }\n\n  MPI_Allreduce(MPI_IN_PLACE, bins.data(), num_bins, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "// get the number of ranks\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // compute which local range each rank is responsible for\n    int rank, comm_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n\n    double x_start = (double)rank * (x.size() / comm_size) * 10;\n    double x_end = (double)(rank+1) * (x.size() / comm_size) * 10;\n    double x_span = x_end - x_start;\n    std::vector<double> local_x(x.begin() + x_start, x.begin() + x_end);\n\n    // count the elements in each local range\n    std::array<size_t, 10> local_bins = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n    for (double x_val : local_x) {\n        size_t index = (x_val / 10);\n        if (index == 10) index = 9;\n        local_bins[index] += 1;\n    }\n\n    // reduce the local values to the rank 0 process\n    for (size_t i = 0; i < local_bins.size(); i++) {\n        MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n\n}",
            "// TODO: Your code here\n\n}",
            "}",
            "std::array<size_t, 10> localBins = {};\n\n    //... your code here...\n\n    MPI_Reduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::cout << \"bins: \" << bins << std::endl;\n    }\n}",
            "// TODO: Fill this in\n}",
            "// Use MPI to determine the size of the x vector\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  size_t nElems = x.size() / size;\n\n  // Copy the x vector to a shared vector\n  std::vector<double> sharedX(nElems);\n  #pragma omp parallel for\n  for (size_t i = 0; i < nElems; ++i) {\n    sharedX[i] = x[i];\n  }\n\n  // Each rank computes its own values\n  std::array<size_t, 10> localBins;\n  #pragma omp parallel for\n  for (size_t i = 0; i < nElems; ++i) {\n    // Compute the index in the bins\n    size_t idx = std::floor(sharedX[i] / 10.0) * 10;\n    localBins[idx/10]++;\n  }\n\n  // Combine the local bins\n  std::array<size_t, 10> allBins;\n  MPI_Allreduce(localBins.data(), allBins.data(), 10, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n  bins = allBins;\n}",
            "}",
            "// TODO: Your code here\n\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int rank;\n    MPI_Comm_rank(comm, &rank);\n\n    size_t chunk = x.size()/10;\n    int threadNum = omp_get_num_threads();\n    size_t bins_num = chunk * threadNum;\n    std::vector<double> bin;\n    bin.resize(bins_num);\n    std::fill(bin.begin(), bin.end(), 0);\n    double *bin_ptr = bin.data();\n\n    //parallel execution of the loop\n    #pragma omp parallel\n    {\n        size_t start = chunk * omp_get_thread_num();\n        size_t end = start + chunk;\n        if (end > x.size())\n            end = x.size();\n\n        //loop on local part of input array\n        for (size_t i = start; i < end; i++) {\n            if (x[i] >= 0 && x[i] < 10) {\n                bin_ptr[i]++;\n            } else if (x[i] >= 10 && x[i] < 20) {\n                bin_ptr[i]++;\n            } else if (x[i] >= 20 && x[i] < 30) {\n                bin_ptr[i]++;\n            } else if (x[i] >= 30 && x[i] < 40) {\n                bin_ptr[i]++;\n            } else if (x[i] >= 40 && x[i] < 50) {\n                bin_ptr[i]++;\n            } else if (x[i] >= 50 && x[i] < 60) {\n                bin_ptr[i]++;\n            } else if (x[i] >= 60 && x[i] < 70) {\n                bin_ptr[i]++;\n            } else if (x[i] >= 70 && x[i] < 80) {\n                bin_ptr[i]++;\n            } else if (x[i] >= 80 && x[i] < 90) {\n                bin_ptr[i]++;\n            } else if (x[i] >= 90 && x[i] <= 100) {\n                bin_ptr[i]++;\n            } else {\n                printf(\"Error in binning, value %d out of range\\n\", x[i]);\n            }\n        }\n    }\n\n    //get max in bins\n    double max = *std::max_element(bin.begin(), bin.end());\n\n    //get min in bins\n    double min = *std::min_element(bin.begin(), bin.end());\n\n    //sum bins\n    std::vector<double> sum_bins(bins_num, 0);\n    MPI_Allreduce(bin_ptr, sum_bins.data(), bins_num, MPI_DOUBLE, MPI_SUM, comm);\n\n    //get max in sum bins\n    double max_sum_bins = *std::max_element(sum_bins.begin(), sum_bins.end());\n\n    //get min in sum bins\n    double min_sum_bins = *std::min_element(sum_bins.begin(), sum_bins.end());\n\n    //get max in x\n    double max_x = *std::max_element(x.begin(), x.end());\n\n    //get min in x\n    double min_x = *std::min_element(x.begin(), x.end());\n\n    //get max in max_sum_bins\n    double max_max_sum_bins = *std::max_element(sum_bins.begin(), sum_bins.end());\n\n    //get min in min_sum_bins\n    double min_min_sum_bins = *std::min_element(sum_bins.begin(), sum_bins.end());\n\n    //printf(\"max_sum_bins %f\\n\", max_sum_bins);\n    //printf(\"min_sum_bins %f\\n\", min_sum_bins);\n    //printf(\"max_x %f\\n\", max_x);",
            "// TODO: YOUR CODE HERE\n  for(int i = 0; i < x.size(); i++){\n    int j = x[i]/10;\n    bins[j]++;\n  }\n  // MPI_Reduce(MPI_IN_PLACE, bins, 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // int ierr = MPI_Reduce(MPI_IN_PLACE, bins, 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  // if (ierr!= MPI_SUCCESS)\n  // {\n  //   std::cerr << \"Error in MPI_Reduce\" << std::endl;\n  //   std::abort();\n  // }\n\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  if (x.empty())\n    return;\n\n  // determine how many entries per rank we have\n  size_t num = x.size() / size;\n\n  // determine the starting index for this rank\n  size_t start_idx = rank * num;\n\n  // determine the ending index for this rank\n  size_t end_idx = (rank + 1) * num;\n\n  // copy the data for this rank into a local vector\n  std::vector<double> local_x(x.begin() + start_idx, x.begin() + end_idx);\n\n  // create a local histogram\n  std::array<size_t, 10> local_bins{};\n\n  // parallel for with OpenMP\n#pragma omp parallel for\n  for (size_t i = 0; i < local_x.size(); i++) {\n    local_bins[local_x[i]/10]++;\n  }\n\n  // sum local histograms\n  std::array<size_t, 10> global_bins = {0};\n\n  MPI_Allreduce(local_bins.data(), global_bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  // set output histogram\n  bins = global_bins;\n\n}",
            "// TODO: Implement this function.\n\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int size = 10;\n    int rank = 0;\n    int xsize = x.size();\n\n    int* buffer = new int[size];\n    for (int i = 0; i < size; i++) {\n        buffer[i] = 0;\n    }\n\n    int chunk = xsize / size;\n    int remainder = xsize % size;\n    int start = 0;\n    int end = 0;\n\n    MPI_Status status;\n\n    if (rank == 0) {\n        std::cout << \"rank: \" << rank << \" xsize: \" << xsize << \" chunk: \" << chunk << \" remainder: \" << remainder << \" start: \" << start << \" end: \" << end << std::endl;\n        for (int i = 0; i < size; i++) {\n            std::cout << \"rank: \" << rank << \" buffer: \" << i << \" \" << buffer[i] << std::endl;\n        }\n    }\n\n    for (int i = 0; i < size; i++) {\n        start = chunk * i;\n        end = chunk * (i + 1) + remainder;\n\n        if (i == size - 1)\n            end = xsize;\n\n        std::cout << \"rank: \" << rank << \" start: \" << start << \" end: \" << end << std::endl;\n\n        #pragma omp parallel for\n        for (int j = start; j < end; j++) {\n            int bin = x[j] / 10;\n            buffer[bin]++;\n            //std::cout << \"rank: \" << rank << \" bin: \" << bin << \" \" << buffer[bin] << std::endl;\n        }\n\n        for (int j = 0; j < size; j++) {\n            if (rank == 0) {\n                if (j == i)\n                    std::cout << \"rank: \" << rank << \" j: \" << j << \" buffer: \" << buffer[j] << std::endl;\n                else\n                    std::cout << \"rank: \" << rank << \" j: \" << j << \" buffer: \" << buffer[j] << \" \" << bins[j] << std::endl;\n            }\n        }\n\n        MPI_Reduce(buffer, bins.data(), size, MPI_INT, MPI_SUM, 0, comm);\n\n        if (rank == 0) {\n            for (int j = 0; j < size; j++) {\n                std::cout << \"rank: \" << rank << \" j: \" << j << \" buffer: \" << buffer[j] << std::endl;\n                std::cout << \"rank: \" << rank << \" j: \" << j << \" bins: \" << bins[j] << std::endl;\n            }\n        }\n    }\n\n    delete[] buffer;\n}",
            "int const nprocs = omp_get_num_threads();\n  int const rank = omp_get_thread_num();\n  double const mpi_size = nprocs;\n  size_t const N = x.size();\n\n  int const mpi_rank = MPI_Comm_rank(MPI_COMM_WORLD, nullptr);\n  int const mpi_size = MPI_Comm_size(MPI_COMM_WORLD, nullptr);\n  int const mpi_dims = mpi_size;\n  int const mpi_dims_max = mpi_size;\n  MPI_Dims_create(mpi_dims_max, 1, &mpi_dims);\n\n  // Initialize the MPI_Datatype\n  MPI_Datatype mpi_double;\n  MPI_Type_contiguous(1, MPI_DOUBLE, &mpi_double);\n  MPI_Type_commit(&mpi_double);\n\n  // Create Cartesian topology\n  int mpi_periods[2];\n  mpi_periods[0] = 0;\n  mpi_periods[1] = 0;\n  MPI_Cart_create(MPI_COMM_WORLD, 2, mpi_dims, mpi_periods, 1, &mpi_cart_comm);\n  int mpi_coords[2];\n  MPI_Cart_coords(mpi_cart_comm, mpi_rank, 2, mpi_coords);\n\n  // Initialize a 2D array, each row contains the count of [0,10), [10, 20), [20, 30), etc.\n  int mpi_doubles_per_proc = N / mpi_size;\n  int mpi_doubles_rem = N % mpi_size;\n  int mpi_doubles_per_proc_plus = mpi_doubles_per_proc + 1;\n\n  // Compute the size of each row\n  int mpi_rows_per_proc = mpi_doubles_per_proc / mpi_dims[1];\n  int mpi_rows_rem = mpi_doubles_per_proc % mpi_dims[1];\n  if (mpi_rows_rem > mpi_coords[1]) {\n    mpi_rows_per_proc++;\n  }\n\n  // Initialize the mpi_bins array\n  std::vector<std::array<size_t, 10>> mpi_bins(mpi_rows_per_proc);\n\n  // Compute the offset of each rank in the array\n  int mpi_row_offset = 0;\n  for (int i = 0; i < mpi_coords[1]; i++) {\n    mpi_row_offset += mpi_doubles_per_proc_plus;\n  }\n\n  // Count the number of values in [0,10), [10, 20), [20, 30), etc. and store the counts in `bins`\n  #pragma omp parallel for shared(mpi_bins)\n  for (size_t i = 0; i < N; i++) {\n    std::array<size_t, 10> bin;\n    for (size_t j = 0; j < 10; j++) {\n      bin[j] = 0;\n    }\n    int idx = (int)x[i];\n    int row = idx / 10;\n    int col = idx % 10;\n    int bin_idx = col;\n    int row_offset = mpi_row_offset;\n    if (mpi_coords[1] == mpi_dims[1] - 1) {\n      if (mpi_rows_rem > 0) {\n        row_offset -= mpi_rows_rem;\n      }\n    }\n    int mpi_bin_idx = row * mpi_dims[1] + col;\n    mpi_bins[mpi_bin_idx][bin_idx]++;\n\n    // Gather the results\n    MPI_Gatherv(mpi_bins[mpi_bin_idx].",
            "// Your code goes here.\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int total = x.size();\n  int binsize = total / size;\n\n  int start = rank * binsize;\n  int end = start + binsize;\n  if (rank == size - 1) end = total;\n\n  size_t cnt = 0;\n  int b = 0;\n\n  #pragma omp parallel for\n  for (int i = start; i < end; i++) {\n    int val = (int) x[i];\n    cnt += (val >= 10 * b && val < 10 * (b + 1));\n  }\n\n  bins[b] = cnt;\n\n  MPI_Reduce(&cnt, &bins[b], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "size_t my_bins[10] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        int bin = (int)(x[i]/10.);\n        my_bins[bin]++;\n    }\n\n    MPI_Gather(my_bins, 10, MPI_UNSIGNED_LONG, bins.data(), 10, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    if (0 == rank) {\n        for (size_t i = 0; i < 10; i++) {\n            std::cout << bins[i] << \" \";\n        }\n        std::cout << std::endl;\n    }\n}",
            "const double min = 0.0;\n  const double max = 100.0;\n  const double binSize = (max - min) / 10.0;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int chunk = x.size() / size;\n  int rem = x.size() % size;\n\n  // Create the bins\n  std::vector<std::array<size_t, 10>> local_bins(size);\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    int r = i / chunk;\n    int idx = static_cast<int>(x[i] / binSize);\n    ++local_bins[r][idx];\n  }\n\n  std::vector<std::array<size_t, 10>> bins_all(size, std::array<size_t, 10>{});\n  // Combine the bins\n#pragma omp parallel for\n  for (int i = 0; i < size; ++i) {\n    for (size_t j = 0; j < 10; ++j) {\n      bins_all[rank][j] += local_bins[i][j];\n    }\n  }\n  // Reduce the bins\n  if (rank == 0) {\n    for (size_t i = 0; i < 10; ++i) {\n      bins[i] = 0;\n    }\n    for (int i = 1; i < size; ++i) {\n      for (size_t j = 0; j < 10; ++j) {\n        bins[j] += bins_all[i][j];\n      }\n    }\n  }\n}",
            "// TODO: Your code here\n\n  // Get the number of ranks and the rank of this process\n  int num_procs = omp_get_num_threads();\n  int rank = omp_get_thread_num();\n\n  // Create an array to store the local bins\n  std::array<size_t, 10> local_bins{};\n\n  // Set the local values to 0\n  for (int i = 0; i < local_bins.size(); i++) {\n    local_bins[i] = 0;\n  }\n\n  // Loop over the input vector and calculate the bins for this rank\n  for (int i = rank; i < x.size(); i += num_procs) {\n    int bin = (int) (x[i]/10);\n    local_bins[bin]++;\n  }\n\n  // Sum the local bins to get the global bins\n  MPI_Allreduce(local_bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n}",
            "size_t N = x.size();\n  bins.fill(0);\n  // TODO: your code goes here\n}",
            "int n = x.size();\n  std::vector<size_t> bins(10);\n  std::vector<size_t> temp(10);\n  int binsize = n/10;\n  int rank, size, bsize;\n  int remainder;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  bsize = n/size;\n  remainder = n%size;\n\n  for (int i = 0; i < remainder; i++) {\n    temp[0]++;\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < size; i++) {\n      MPI_Recv(&temp[0], 10, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 10; j++) {\n        bins[j] = bins[j] + temp[j];\n      }\n    }\n  }\n  else {\n    int offset = 0;\n    for (int i = 0; i < rank; i++) {\n      offset = offset + bsize + (i < remainder? 1 : 0);\n    }\n    for (int i = offset; i < offset + bsize + (rank < remainder? 1 : 0); i++) {\n      int index = int(x[i] / 10);\n      temp[index]++;\n    }\n    MPI_Send(&temp[0], 10, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n\n  for (int i = 0; i < bsize; i++) {\n    int index = int(x[i+rank*bsize] / 10);\n    bins[index]++;\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < 10; i++) {\n      std::cout << bins[i] << \" \";\n    }\n  }\n\n}",
            "// TODO: Your code here\n\n  // Initialize bins to zero\n  for (int i = 0; i < 10; ++i) {\n    bins[i] = 0;\n  }\n\n  // Count the number of values in each bin\n  for (int i = 0; i < x.size(); ++i) {\n    bins[static_cast<int>(x[i] / 10)] += 1;\n  }\n}",
            "// TODO\n\n  bins = {0,0,0,0,0,0,0,0,0};\n\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n  if (rank == 0) {\n    int proc_num;\n    MPI_Comm_size(MPI_COMM_WORLD,&proc_num);\n\n    if (proc_num > 1) {\n\n      std::vector<size_t> partial_bins(10);\n\n      int total_length = x.size();\n      int proc_length = total_length/proc_num;\n      int remain_length = total_length%proc_num;\n      if (proc_length == 0) {\n        std::cout<<\"Error: total length can not be divided by proc number!\"<<std::endl;\n        return;\n      }\n      int start = proc_length*rank + remain_length;\n      int end = start + proc_length;\n      for (int i = start; i < end; i++) {\n        int id = x[i] / 10;\n        partial_bins[id]++;\n      }\n      int size;\n      MPI_Status status;\n      MPI_Get_count(&status,MPI_INT,&size);\n      std::cout<<\"proc \"<<rank<<\": \"<<size<<std::endl;\n      MPI_Reduce(partial_bins.data(),bins.data(),10,MPI_INT,MPI_SUM,0,MPI_COMM_WORLD);\n    }\n\n  } else {\n    int total_length = x.size();\n    int proc_length = total_length/proc_num;\n    int remain_length = total_length%proc_num;\n    if (proc_length == 0) {\n      std::cout<<\"Error: total length can not be divided by proc number!\"<<std::endl;\n      return;\n    }\n    int start = proc_length*rank + remain_length;\n    int end = start + proc_length;\n    for (int i = start; i < end; i++) {\n      int id = x[i] / 10;\n      bins[id]++;\n    }\n\n  }\n\n}",
            "size_t local_size = x.size();\n  size_t global_size;\n\n  //calculate global size\n  MPI_Allreduce(&local_size, &global_size, 1, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  size_t local_max = 0;\n  size_t global_max = 0;\n  //find max on each rank\n  for (int i = 0; i < local_size; ++i) {\n    if (x[i] > local_max)\n      local_max = x[i];\n  }\n\n  //find global max\n  MPI_Allreduce(&local_max, &global_max, 1, MPI_UNSIGNED_LONG, MPI_MAX, MPI_COMM_WORLD);\n\n  //set all bins to 0\n  bins.fill(0);\n  int thread_count = 0;\n  #pragma omp parallel\n  {\n    #pragma omp single\n    {\n      thread_count = omp_get_num_threads();\n    }\n    #pragma omp for\n    for (size_t i = 0; i < local_size; ++i) {\n      int bin = (int) (x[i] / (global_max / 10)) * 10;\n      int thread_id = omp_get_thread_num();\n      int rank = 0;\n      MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n      #pragma omp critical\n      {\n        bins[bin] += 1;\n      }\n    }\n  }\n\n  //sum up bins to get global bins\n  int rank = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  if (rank == 0) {\n    MPI_Reduce(MPI_IN_PLACE, bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n  else {\n    MPI_Reduce(bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n\n  for (int i = 0; i < 10; ++i) {\n    std::cout << bins[i] << \" \";\n  }\n  std::cout << std::endl;\n  std::cout << thread_count << std::endl;\n  std::cout << global_max << std::endl;\n}",
            "int nproc, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size_t const n = x.size();\n    size_t const nPerProc = n / nproc;\n    int const remainder = n % nproc;\n    if (rank < remainder) {\n        int start = rank * (nPerProc+1) + rank;\n        int end = start + nPerProc + 1;\n        std::vector<double> localX(x.begin() + start, x.begin() + end);\n        // TODO: Count number of values in each 10-bin range\n    } else {\n        int start = (rank-remainder) * nPerProc + remainder * (nPerProc+1);\n        int end = start + nPerProc;\n        std::vector<double> localX(x.begin() + start, x.begin() + end);\n        // TODO: Count number of values in each 10-bin range\n    }\n    // TODO: Use MPI to compute the histogram across all ranks\n    // TODO: Store the result in bins\n}",
            "bins.fill(0);\n    for (size_t i = 0; i < x.size(); ++i) {\n        int n = x[i] / 10;\n        ++bins[n];\n    }\n}",
            "size_t const total = x.size();\n    size_t my_start = (total * omp_get_thread_num()) / omp_get_num_threads();\n    size_t my_end = (total * (omp_get_thread_num() + 1)) / omp_get_num_threads();\n    int my_rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n    size_t local_bins[10];\n    for (int i = 0; i < 10; i++) {\n        local_bins[i] = 0;\n    }\n\n    for (size_t i = my_start; i < my_end; i++) {\n        int idx = x[i] / 10;\n        local_bins[idx] += 1;\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, local_bins, 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    for (int i = 0; i < 10; i++) {\n        bins[i] += local_bins[i];\n    }\n}",
            "int rank = 0;\n    int nprocs = 1;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    size_t N = x.size();\n    size_t num_bins = 10;\n    int num_threads = omp_get_max_threads();\n\n    size_t N_per_thread = N / num_threads;\n\n    size_t *counts = new size_t[num_threads];\n    for (int i = 0; i < num_threads; i++) {\n        counts[i] = 0;\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        int bin_idx = (int)(x[i] / 10.0);\n        int thread_idx = omp_get_thread_num();\n        counts[thread_idx] += 1;\n    }\n\n    size_t *bin_counts = new size_t[num_bins];\n    for (int i = 0; i < num_bins; i++) {\n        bin_counts[i] = 0;\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < num_threads; i++) {\n        bin_counts[i * num_bins / num_threads] += counts[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < num_bins; i++) {\n            bins[i] = bin_counts[i];\n        }\n    }\n\n    delete[] counts;\n    delete[] bin_counts;\n}",
            "int rank, num_ranks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n  // Calculate the number of values per rank\n  int num_values = x.size() / num_ranks;\n  // Add leftovers to the first rank\n  if(rank == 0)\n    num_values += x.size() % num_ranks;\n\n  std::vector<size_t> per_rank_counts(10, 0);\n  std::vector<size_t> per_rank_starts(num_ranks);\n\n  // Set the starts\n  int bin_count = 0;\n  for(int i=0; i < 10; i++) {\n    bin_count += i * num_values;\n    per_rank_starts[rank] = bin_count;\n    per_rank_counts[i] = bin_count;\n  }\n\n  // Scatter the starts\n  std::vector<int> mpi_starts(num_ranks);\n  MPI_Scatter(per_rank_starts.data(), 1, MPI_INT, mpi_starts.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  // Go through the values, keeping track of the start\n  int rank_start = mpi_starts[rank];\n  #pragma omp parallel for\n  for(int i = 0; i < num_values; i++) {\n    int bin = int(x[i + rank_start]) / 10;\n    int bin_index = bin * num_ranks + rank;\n    per_rank_counts[bin]++;\n  }\n\n  // Reduce the counts\n  std::vector<int> counts(10 * num_ranks);\n  std::copy(per_rank_counts.begin(), per_rank_counts.end(), counts.begin());\n  MPI_Allreduce(counts.data(), bins.data(), 10, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n}",
            "int num_procs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int chunk_size = 10;\n    int count = x.size() / num_procs;\n    int extra_elements = x.size() % num_procs;\n    int start = rank * count + std::min(extra_elements, rank);\n    int end = start + count - 1;\n    int start_index = start;\n    int end_index = end;\n    #pragma omp parallel for\n    for (int i = start_index; i <= end_index; i++) {\n        int bin = (int) floor(x[i]/10) % 10;\n        bins[bin]++;\n    }\n}",
            "//TODO: fill in your code here\n\n}",
            "/* Your code here */\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use Kokkos to compute in parallel. Assume that Kokkos has already been initialized.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(Kokkos::View<const double*> const& x, Kokkos::View<size_t[4]> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "}",
            "int size = x.size();\n    double quartile[4];\n    quartile[0] = x(0);\n    for(int i = 1; i < size; i++){\n        if(i%4 == 0){\n            if(quartile[0] > x(i)){\n                quartile[0] = x(i);\n            }\n        }\n        if(i%4 == 1){\n            if(quartile[1] > x(i)){\n                quartile[1] = x(i);\n            }\n        }\n        if(i%4 == 2){\n            if(quartile[2] > x(i)){\n                quartile[2] = x(i);\n            }\n        }\n        if(i%4 == 3){\n            if(quartile[3] > x(i)){\n                quartile[3] = x(i);\n            }\n        }\n    }\n    int count = 0;\n    for(int i = 0; i < size; i++){\n        if(x(i) >= quartile[0] && x(i) <= quartile[1]){\n            bins(0)++;\n            count++;\n        }\n        if(x(i) >= quartile[1] && x(i) <= quartile[2]){\n            bins(1)++;\n            count++;\n        }\n        if(x(i) >= quartile[2] && x(i) <= quartile[3]){\n            bins(2)++;\n            count++;\n        }\n        if(x(i) >= quartile[3] && x(i) <= quartile[4]){\n            bins(3)++;\n            count++;\n        }\n    }\n}",
            "// TODO: Your code here\n}",
            "Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int i) {\n\t\tif (x(i) >= 0 && x(i) < 0.25) {\n\t\t\tbins(0)++;\n\t\t} else if (x(i) >= 0.25 && x(i) < 0.5) {\n\t\t\tbins(1)++;\n\t\t} else if (x(i) >= 0.5 && x(i) < 0.75) {\n\t\t\tbins(2)++;\n\t\t} else if (x(i) >= 0.75 && x(i) < 1) {\n\t\t\tbins(3)++;\n\t\t}\n\t});\n\tKokkos::fence();\n}",
            "//TODO: Your code here\n}",
            "}",
            "bins = 0;\n}",
            "Kokkos::parallel_for(\"countQuartiles\", x.size(), KOKKOS_LAMBDA(const size_t &i) {\n    if (x(i) < 1.0) {\n      if (x(i) < 0.25) {\n        bins(0) += 1;\n      } else if (x(i) < 0.5) {\n        bins(1) += 1;\n      } else if (x(i) < 0.75) {\n        bins(2) += 1;\n      } else {\n        bins(3) += 1;\n      }\n    }\n  });\n\n}",
            "//TODO\n}",
            "// TODO: YOUR CODE HERE\n  Kokkos::parallel_for(\"countQuartiles\", Kokkos::RangePolicy<>(0,x.size()),\n                       KOKKOS_LAMBDA(int i){\n                         double n = x(i);\n                         if(n >= 0 && n <= 0.25){\n                           bins(0)++;\n                         }else if(n > 0.25 && n <= 0.5){\n                           bins(1)++;\n                         }else if(n > 0.5 && n <= 0.75){\n                           bins(2)++;\n                         }else if(n > 0.75 && n <= 1){\n                           bins(3)++;\n                         }\n                       });\n\n}",
            "// Implement this function\n  // Use the Kokkos view bins to store the counts of each bin.\n  // Hint: use Kokkos::deep_copy to copy the data from Kokkos::View to std::vector\n  // and use std::sort to sort the data.\n\n  double *d = new double[x.size()];\n  std::copy(x.data(), x.data() + x.size(), d);\n  std::sort(d, d + x.size());\n\n  for (size_t i = 0; i < bins.size(); i++) {\n    bins(i) = 0;\n  }\n\n  for (size_t i = 0; i < x.size(); i++) {\n    if (d[i] < 0.25)\n      bins(0)++;\n    if (d[i] < 0.5 && d[i] >= 0.25)\n      bins(1)++;\n    if (d[i] < 0.75 && d[i] >= 0.5)\n      bins(2)++;\n    if (d[i] >= 0.75)\n      bins(3)++;\n  }\n\n  delete[] d;\n}",
            "/* Your code here */\n  Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static> > policy(0, x.extent(0));\n  Kokkos::parallel_for(\"countQuartiles\", policy, [=] (int i) {\n    if (x(i) < 0.25)\n      bins(0) += 1;\n    else if (x(i) < 0.5)\n      bins(1) += 1;\n    else if (x(i) < 0.75)\n      bins(2) += 1;\n    else\n      bins(3) += 1;\n  });\n}",
            "Kokkos::parallel_for(\"countQuartiles\", x.size(), KOKKOS_LAMBDA (const int i) {\n        switch (int(x(i) * 4) % 4) {\n            case 0: bins(0)++; break;\n            case 1: bins(1)++; break;\n            case 2: bins(2)++; break;\n            case 3: bins(3)++; break;\n            default: break;\n        }\n    });\n}",
            "// Fill the vector with values 0, 1, 2, 3,..., 19\n  Kokkos::View<double*> vals(\"vals\", 20);\n  Kokkos::deep_copy(vals, Kokkos::View<double*>(vals.data(), 20));\n\n  // Initialize the bins with 0s\n  Kokkos::deep_copy(bins, Kokkos::View<size_t*>(bins.data(), 4));\n\n  // Map the vector vals to the array bins\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Dynamic>>(0, 20),\n    KOKKOS_LAMBDA(const int i) {\n    bins(vals(i)) += 1;\n  });\n}",
            "}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "Kokkos::parallel_for(\"Count quartiles\",\n                         x.size(),\n                         KOKKOS_LAMBDA(const int i) {\n        bins(0) += x(i) - std::floor(x(i)) < 0.25;\n        bins(1) += std::floor(x(i)) < x(i) && x(i) - std::floor(x(i)) < 0.25;\n        bins(2) += std::floor(x(i)) < x(i) && x(i) - std::floor(x(i)) < 0.5;\n        bins(3) += x(i) - std::floor(x(i)) >= 0.75;\n    });\n}",
            "Kokkos::parallel_for(x.size(), [=](int i) {\n    bins(0) += (x(i) >= 0 && x(i) < 0.25)? 1 : 0;\n    bins(1) += (x(i) >= 0.25 && x(i) < 0.5)? 1 : 0;\n    bins(2) += (x(i) >= 0.5 && x(i) < 0.75)? 1 : 0;\n    bins(3) += (x(i) >= 0.75 && x(i) < 1)? 1 : 0;\n  });\n}",
            "constexpr double fourth = 0.25;\n\n\tKokkos::parallel_for(\"count_quartiles\", x.size(), KOKKOS_LAMBDA(const int i) {\n\t\tdouble xi = x(i);\n\t\tif (xi >= 0.0 && xi <= fourth) {\n\t\t\tbins(0)++;\n\t\t} else if (xi > fourth && xi <= 0.5) {\n\t\t\tbins(1)++;\n\t\t} else if (xi > 0.5 && xi <= 0.75) {\n\t\t\tbins(2)++;\n\t\t} else if (xi > 0.75 && xi < 1.0) {\n\t\t\tbins(3)++;\n\t\t}\n\t});\n}",
            "/*\n    YOUR CODE GOES HERE\n    */\n\n    int count[4] = {0,0,0,0};\n    int index = 0;\n    size_t size = x.size();\n    Kokkos::parallel_for(size, [&](int i) {\n        if (x[i] >= 0.75) {\n            count[index]++;\n            index++;\n        }\n    });\n    Kokkos::deep_copy(bins, count);\n}",
            "// TODO: Your code here\n}",
            "// TODO: implement\n}",
            "int num_threads = 4;\n  int num_bins = 4;\n\n  // TODO: Implement your solution here.\n\n}",
            "// initialize the bins to 0\n    Kokkos::deep_copy(bins, 0);\n\n    // TODO: your code here\n}",
            "// TODO: Your code goes here\n\n}",
            "// TODO\n\n    // This code block can be used to test your solution.\n    // Kokkos::View<double*> x_test(\"x_test\", 7);\n    // double x_test_data[7] = {7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8};\n    // Kokkos::deep_copy(x_test, x_test_data);\n    // Kokkos::View<size_t[4]> bins_test(\"bins_test\");\n    // countQuartiles(x_test, bins_test);\n    // Kokkos::fence();\n    // std::cout << \"Test input\" << std::endl;\n    // Kokkos::deep_copy(x_test_data, x_test);\n    // for (int i = 0; i < 7; i++) {\n    //     std::cout << x_test_data[i] << \" \";\n    // }\n    // std::cout << std::endl;\n    // std::cout << \"Test output\" << std::endl;\n    // size_t bins_test_data[4];\n    // Kokkos::deep_copy(bins_test_data, bins_test);\n    // for (int i = 0; i < 4; i++) {\n    //     std::cout << bins_test_data[i] << \" \";\n    // }\n    // std::cout << std::endl;\n}",
            "// TODO\n  size_t count = x.size();\n  double *x_ptr = x.data();\n  Kokkos::parallel_for(\"countQuartiles\", count, KOKKOS_LAMBDA(const size_t& i) {\n    double value = x_ptr[i];\n    size_t index = 0;\n    if (value < 0.25) {\n      index = 0;\n    } else if (value >= 0.25 && value < 0.5) {\n      index = 1;\n    } else if (value >= 0.5 && value < 0.75) {\n      index = 2;\n    } else if (value >= 0.75 && value < 1) {\n      index = 3;\n    }\n    bins(index) += 1;\n  });\n}",
            "// TODO: replace this with Kokkos parallel_scan\n  // https://github.com/kokkos/kokkos-examples/tree/master/03_scans\n  size_t total = 0;\n  double x_local = 0;\n  Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA (const size_t& i, size_t& result) {\n    x_local = x(i);\n    int bin = 0;\n    if (x_local >= 0.25) {\n      bin = 1;\n    }\n    if (x_local >= 0.5) {\n      bin = 2;\n    }\n    if (x_local >= 0.75) {\n      bin = 3;\n    }\n    result += bin;\n  }, total);\n  bins(0) = total;\n  \n  total = 0;\n  Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA (const size_t& i, size_t& result) {\n    x_local = x(i);\n    int bin = 0;\n    if (x_local >= 0.25) {\n      bin = 1;\n    }\n    if (x_local >= 0.5) {\n      bin = 2;\n    }\n    if (x_local >= 0.75) {\n      bin = 3;\n    }\n    result += bin;\n  }, total);\n  bins(1) = total;\n  \n  total = 0;\n  Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA (const size_t& i, size_t& result) {\n    x_local = x(i);\n    int bin = 0;\n    if (x_local >= 0.25) {\n      bin = 1;\n    }\n    if (x_local >= 0.5) {\n      bin = 2;\n    }\n    if (x_local >= 0.75) {\n      bin = 3;\n    }\n    result += bin;\n  }, total);\n  bins(2) = total;\n  \n  total = 0;\n  Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA (const size_t& i, size_t& result) {\n    x_local = x(i);\n    int bin = 0;\n    if (x_local >= 0.25) {\n      bin = 1;\n    }\n    if (x_local >= 0.5) {\n      bin = 2;\n    }\n    if (x_local >= 0.75) {\n      bin = 3;\n    }\n    result += bin;\n  }, total);\n  bins(3) = total;\n}",
            "using Kokkos::View;\n  using Kokkos::RangePolicy;\n  using Kokkos::TeamPolicy;\n  using Kokkos::ThreadVectorRange;\n  using Kokkos::sum;\n  using Kokkos::subview;\n\n  // Count the number of doubles in each of the 4 bins\n  // 1 thread per bin\n  const int num_bins = 4;\n  TeamPolicy policy(num_bins, \"count\");\n  Kokkos::parallel_for(\"count\", policy, KOKKOS_LAMBDA(const TeamMember& member) {\n    int bin = member.league_rank();\n    const double* x_ptr = x.data();\n    const size_t num_values = x.size();\n    size_t& num_in_bin = bins(bin);\n    double bin_start = 0.25 * (bin + 1);\n    double bin_end = bin_start + 0.25;\n    for (size_t i = 0; i < num_values; ++i) {\n      if (x_ptr[i] >= bin_start && x_ptr[i] < bin_end) {\n        num_in_bin++;\n      }\n    }\n  });\n  // Sum the counts\n  Kokkos::deep_copy(bins, sum(bins));\n}",
            "// TODO\n}",
            "using namespace Kokkos;\n\n\t// Get the number of doubles in x\n\tint n = x.size();\n\n\t// Get the number of threads in each team\n\tint nt = TeamPolicy::team_size(n);\n\n\t// Get the number of teams\n\tint n_teams = n / nt;\n\t\n\t// Allocate a thread local variable\n\t// You should use a reduction to add up the local bins\n\tView<double[4]> local_bins(\"local_bins\", 4);\n\t\n\t// Use a parallel_for to count the quartiles on each team\n\tparallel_for(TeamPolicy(n_teams, nt), KOKKOS_LAMBDA(const TeamPolicy::member_type& team) {\n\t\tint i = team.league_rank() * team.team_size() + team.team_rank();\n\t\tif (i < n) {\n\t\t\tif (x(i) < 1.25) {\n\t\t\t\tteam.team_reduce(Kokkos::Max<int>(0), local_bins(0) += 1);\n\t\t\t} else if (x(i) < 2.5) {\n\t\t\t\tteam.team_reduce(Kokkos::Max<int>(1), local_bins(1) += 1);\n\t\t\t} else if (x(i) < 3.75) {\n\t\t\t\tteam.team_reduce(Kokkos::Max<int>(2), local_bins(2) += 1);\n\t\t\t} else {\n\t\t\t\tteam.team_reduce(Kokkos::Max<int>(3), local_bins(3) += 1);\n\t\t\t}\n\t\t}\n\t});\n\n\t// Use a parallel_reduce to add up the local bins\n\tparallel_reduce(TeamPolicy(n_teams, nt), KOKKOS_LAMBDA(const TeamPolicy::member_type& team, size_t& update) {\n\t\tupdate += team.team_reduce(Kokkos::Sum<size_t>(local_bins));\n\t}, Kokkos::Sum<size_t>(bins));\n}",
            "Kokkos::parallel_for(x.extent(0), [=](const int i) {\n    if (x(i) < 0.25) {\n      bins(0)++;\n    } else if (x(i) < 0.5) {\n      bins(1)++;\n    } else if (x(i) < 0.75) {\n      bins(2)++;\n    } else {\n      bins(3)++;\n    }\n  });\n}",
            "// TODO\n    int n = x.size();\n    Kokkos::View<double*> y(\"y\", n);\n    Kokkos::deep_copy(y, x);\n    Kokkos::parallel_for(n, KOKKOS_LAMBDA(const int &i) {\n        if (y(i) < 0.25) {\n            y(i) = 0.0;\n        }\n        else if (y(i) < 0.5) {\n            y(i) = 0.25;\n        }\n        else if (y(i) < 0.75) {\n            y(i) = 0.5;\n        }\n        else {\n            y(i) = 0.75;\n        }\n    });\n\n    Kokkos::parallel_for(4, KOKKOS_LAMBDA(const int &i) {\n        int count = 0;\n        for (int j = 0; j < n; j++) {\n            if (y(j) == i * 0.25) {\n                count++;\n            }\n        }\n        bins(i) = count;\n    });\n\n    Kokkos::deep_copy(x, y);\n\n}",
            "// TODO\n  // YOUR CODE HERE\n\n}",
            "}",
            "// TODO: Your code here\n\n\n}",
            "//TODO: Implement this!\n}",
            "}",
            "// TODO\n\n}",
            "// TODO: Your code here\n}",
            "//TODO\n}",
            "// TODO\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    const size_t N = x.size();\n    // TODO: Fill in the rest of the function\n    const size_t half = N/2;\n    Kokkos::parallel_reduce(N, 0, [&](const int i, size_t& t) {\n        double d = x[i];\n        if (d < 0.0) {\n            d = -d;\n        }\n        if (d <= 0.25) {\n            t += 1;\n        }\n        else if (d <= 0.5) {\n            t += 2;\n        }\n        else if (d <= 0.75) {\n            t += 3;\n        }\n        else {\n            t += 4;\n        }\n    });\n    bins[0] = t;\n    if (half >= 1) {\n        t = 0;\n        Kokkos::parallel_reduce(half, 0, [&](const int i, size_t& t) {\n            double d = x[i];\n            if (d < 0.0) {\n                d = -d;\n            }\n            if (d <= 0.25) {\n                t += 1;\n            }\n            else if (d <= 0.5) {\n                t += 2;\n            }\n            else if (d <= 0.75) {\n                t += 3;\n            }\n            else {\n                t += 4;\n            }\n        });\n        bins[1] = t;\n    }\n    if (half >= 2) {\n        t = 0;\n        Kokkos::parallel_reduce(half, 1, [&](const int i, size_t& t) {\n            double d = x[i];\n            if (d < 0.0) {\n                d = -d;\n            }\n            if (d <= 0.25) {\n                t += 1;\n            }\n            else if (d <= 0.5) {\n                t += 2;\n            }\n            else if (d <= 0.75) {\n                t += 3;\n            }\n            else {\n                t += 4;\n            }\n        });\n        bins[2] = t;\n    }\n    if (half >= 3) {\n        t = 0;\n        Kokkos::parallel_reduce(half, 2, [&](const int i, size_t& t) {\n            double d = x[i];\n            if (d < 0.0) {\n                d = -d;\n            }\n            if (d <= 0.25) {\n                t += 1;\n            }\n            else if (d <= 0.5) {\n                t += 2;\n            }\n            else if (d <= 0.75) {\n                t += 3;\n            }\n            else {\n                t += 4;\n            }\n        });\n        bins[3] = t;\n    }\n}",
            "}",
            "// TODO: Your code here\n  Kokkos::parallel_reduce(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, x.size()),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4),\n                          Kokkos::RangePolicy<Kokkos::DefaultHostExecution",
            "// Compute the number of elements in the vector `x`.\n    size_t N = x.size();\n    // Compute the sum of the vector `x`.\n    double sum = 0.0;\n    for(size_t i = 0; i < N; i++) sum += x(i);\n    // Compute the range of the vector `x`.\n    double min = x(0);\n    double max = x(0);\n    for(size_t i = 0; i < N; i++) {\n        if(min > x(i)) min = x(i);\n        if(max < x(i)) max = x(i);\n    }\n    // Compute the number of elements in the vector `x` that have a value between \n    // `min` and `min + 1/4 * range`, `min + 1/4 * range` and `min + 1/2 * range`, `min + 1/2 * range` and `min + 3/4 * range`.\n    bins(0) = 0;\n    bins(1) = 0;\n    bins(2) = 0;\n    bins(3) = 0;\n    for(size_t i = 0; i < N; i++) {\n        if(x(i) >= min && x(i) < min + 1.0 / 4 * (max - min)) bins(0)++;\n        else if(x(i) >= min + 1.0 / 4 * (max - min) && x(i) < min + 1.0 / 2 * (max - min)) bins(1)++;\n        else if(x(i) >= min + 1.0 / 2 * (max - min) && x(i) < min + 3.0 / 4 * (max - min)) bins(2)++;\n        else bins(3)++;\n    }\n}",
            "// TODO: write your code here\n}",
            "}",
            "}",
            "auto kokkos_execution_space = Kokkos::DefaultExecutionSpace();\n    Kokkos::parallel_for(\"KokkosCountQuartiles\", kokkos_execution_space, KOKKOS_LAMBDA(const int i) {\n        if (x(i) <= 0.25) {\n            bins(0) += 1;\n        } else if (x(i) <= 0.5) {\n            bins(1) += 1;\n        } else if (x(i) <= 0.75) {\n            bins(2) += 1;\n        } else if (x(i) <= 1.0) {\n            bins(3) += 1;\n        } else {\n            // Out of range\n        }\n    });\n}",
            "const size_t n = x.size();\n\n  // compute the quartiles using Kokkos\n  // TODO: compute the quartiles using Kokkos\n\n  // copy the results into the output array\n  // TODO: copy the results into the output array\n}",
            "size_t N = x.size();\n  size_t i = 0;\n  Kokkos::parallel_reduce(\"countQuartiles\", N, KOKKOS_LAMBDA(const int &i, size_t &local_sum) {\n    double frac = fmod(x(i), 1.0);\n    if (frac < 0.25) {\n      local_sum += 1;\n    } else if (frac < 0.5) {\n      local_sum += 2;\n    } else if (frac < 0.75) {\n      local_sum += 3;\n    } else {\n      local_sum += 4;\n    }\n  }, i);\n\n  for (size_t j = 0; j < 4; j++) {\n    bins(j) = i;\n  }\n}",
            "// TODO: Implement\n  // (1) Count the total number of doubles in x.\n  // (2) Loop over the doubles in x, and use std::fmod to check if each double has a fractional part in each of the four bins.\n  // (3) Increment the bins appropriately for each double in x.\n}",
            "//TODO: implement this function\n}",
            "//...\n\n}",
            "const size_t n = x.size();\n  bins = 0;\n  auto host_x = Kokkos::create_mirror_view_and_copy(Kokkos::HostSpace(), x);\n  for (size_t i = 0; i < n; i++) {\n    if (host_x(i) < 0.25) {\n      bins(0)++;\n    } else if (host_x(i) < 0.5) {\n      bins(1)++;\n    } else if (host_x(i) < 0.75) {\n      bins(2)++;\n    } else {\n      bins(3)++;\n    }\n  }\n}",
            "// YOUR CODE HERE\n}",
            "// Fill in the implementation here:\n\n\n\n\n\n\n}",
            "// Your code here\n    \n    return;\n}",
            "// TODO: Fill in the gaps to implement this function\n}",
            "// TODO\n    // Your code here.\n    // You will have to use Kokkos in order to compute this efficiently\n    // in parallel.\n    // Make sure to return the result in the `bins` argument.\n    // Make sure to call `bins.update_host()` in order to retrieve the final\n    // result from the device memory\n\n\n    auto policy = Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Dynamic> >(0, x.extent(0));\n    Kokkos::parallel_for(policy, [&](const int &i){\n        if (x(i) <= 0.25){\n            bins(0)++;\n        } else if (x(i) <= 0.5){\n            bins(1)++;\n        } else if (x(i) <= 0.75){\n            bins(2)++;\n        } else if (x(i) <= 1.0){\n            bins(3)++;\n        }\n    });\n\n    Kokkos::deep_copy(bins, bins);\n\n    bins.update_host();\n}",
            "int n = x.extent(0);\n  size_t* counts = bins.data();\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static>>(0, n),\n      KOKKOS_LAMBDA (int i) {\n        int idx = (int)((x(i) - floor(x(i))) * 4);\n        if (idx == 4) idx = 3;\n        counts[idx]++;\n      });\n  Kokkos::parallel_reduce(Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static>>(0, 4),\n      KOKKOS_LAMBDA (int, size_t& c) { c += counts[0]; },\n      Kokkos::Sum<size_t>(counts));\n  counts[1] += counts[0];\n  counts[2] += counts[1];\n  counts[3] += counts[2];\n}",
            "// Fill in this function with a Kokkos parallel_for\n  // You may need to define a lambda function\n  // You may need to use Kokkos::deep_copy\n}",
            "// TODO: Write your code here\n}",
            "// TODO: Implement your solution here\n}",
            "}",
            "// TODO: Your code goes here\n}",
            "// TODO\n}",
            "//TODO: implement the function\n  // HINT: you can use Kokkos::parallel_for(x.size(),...) {...} to iterate over x\n  // HINT: you can use Kokkos::min(x.size(),...) to get the number of elements in x\n  // HINT: you can use Kokkos::max(x.size(),...) to get the number of elements in x\n  // HINT: you can use Kokkos::subview(x,...) to get a subview of x\n  // HINT: you can use Kokkos::subview(bins,...) to get a subview of bins\n\n}",
            "/*\n        Fill in this function with the code you need to do the above\n        operation.\n    */\n}",
            "auto n = x.extent(0);\n    const double quarter = 1.0 / 4.0;\n    const double third = 1.0 / 3.0;\n    auto q1 = [=] (double x) { return x < quarter; };\n    auto q2 = [=] (double x) { return x < third; };\n    auto q3 = [=] (double x) { return x < 2.0 * third; };\n    auto q4 = [=] (double x) { return true; };\n\n    // Use the Kokkos Parallel Reduction template to count the number of elements in each interval\n    // of x for which the predicate returns true\n    bins() = 0;\n    Kokkos::parallel_reduce(n, [=] (int i, size_t& count) {\n        if (q1(x(i))) count++;\n        else if (q2(x(i))) count++;\n        else if (q3(x(i))) count++;\n        else if (q4(x(i))) count++;\n    }, bins);\n}",
            "// Your code goes here\n}",
            "auto host_bins = Kokkos::create_mirror_view_and_copy(Kokkos::HostSpace(), bins);\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const size_t i) {\n        int ix = (int)floor(x(i) * 4);\n        host_bins(ix)++;\n    });\n    Kokkos::deep_copy(bins, host_bins);\n}",
            "// TODO\n}",
            "}",
            "// TODO: implement a Kokkos solution\n\n    // a serial solution\n    double bins_[4] = {0};\n    for (int i=0; i<x.extent(0); ++i) {\n        int idx = floor(x(i)*4.);\n        if (idx<0 || idx>=4) continue;\n        bins_[idx]++;\n    }\n\n    Kokkos::deep_copy(bins, Kokkos::View<const double*>(bins_));\n}",
            "// TODO: implement this function\n    int N = x.size();\n    for (int i = 0; i < 4; i++)\n        bins[i] = 0;\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA (const int i) {\n        if (x[i] - std::floor(x[i]) < 0.25)\n            bins[0]++;\n        else if (x[i] - std::floor(x[i]) >= 0.25 && x[i] - std::floor(x[i]) < 0.5)\n            bins[1]++;\n        else if (x[i] - std::floor(x[i]) >= 0.5 && x[i] - std::floor(x[i]) < 0.75)\n            bins[2]++;\n        else if (x[i] - std::floor(x[i]) >= 0.75)\n            bins[3]++;\n    });\n}",
            "}",
            "// TO DO: Implement this function\n}",
            "}",
            "// TODO: your code here\n}",
            "// YOUR CODE GOES HERE\n    \n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.size()), KOKKOS_LAMBDA (const int& i) {\n        double num = x(i);\n        int bin = -1;\n        if(num >= 0.0 && num < 0.25){\n            bin = 0;\n        }else if(num >= 0.25 && num < 0.5){\n            bin = 1;\n        }else if(num >= 0.5 && num < 0.75){\n            bin = 2;\n        }else if(num >= 0.75 && num < 1.0){\n            bin = 3;\n        }else{\n            bin = -1;\n        }\n        if(bin >= 0){\n            Kokkos::atomic_fetch_add(&bins(bin), 1);\n        }\n    });\n}",
            "Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n\t\tdouble f = modf(x(i), &f);\n\t\tif (f > 0 && f < 0.25)\n\t\t\tbins(0)++;\n\t\telse if (f >= 0.25 && f < 0.5)\n\t\t\tbins(1)++;\n\t\telse if (f >= 0.5 && f < 0.75)\n\t\t\tbins(2)++;\n\t\telse if (f >= 0.75 && f < 1)\n\t\t\tbins(3)++;\n\t\telse \n\t\t\tbins(4)++;\n\t});\n}",
            "// Count the number of doubles in the vector `x` that are less than 0.25\n    // and store that count in `bins[0]`.\n\n\n    // Count the number of doubles in the vector `x` that are greater than 0.25\n    // and less than 0.5\n\n\n    // Count the number of doubles in the vector `x` that are greater than 0.5\n    // and less than 0.75\n\n\n    // Count the number of doubles in the vector `x` that are greater than 0.75\n    // and less than 1\n\n\n    // Copy the result to `bins`\n\n}",
            "using namespace Kokkos;\n    // Start your code here.\n    // Compute the minimum and maximum values of x, then divide x into\n    // four groups using an algorithm similar to this one.\n    // https://en.wikipedia.org/wiki/Quartile#Example_of_quartile_calculation\n    size_t size = x.extent(0);\n    auto x_data = x.data();\n    double min = x_data[0];\n    double max = x_data[0];\n    for (size_t i = 1; i < size; i++) {\n        if (x_data[i] < min) min = x_data[i];\n        if (x_data[i] > max) max = x_data[i];\n    }\n    double group_size = (max - min)/4;\n    bins(0) = 0;\n    bins(1) = 0;\n    bins(2) = 0;\n    bins(3) = 0;\n    for (size_t i = 0; i < size; i++) {\n        if (min + group_size*2 > x_data[i]) bins(0)++;\n        else if (min + group_size > x_data[i]) bins(1)++;\n        else if (min + group_size/2 > x_data[i]) bins(2)++;\n        else if (min + group_size/4 > x_data[i]) bins(3)++;\n    }\n    // End your code here.\n}",
            "auto policy = Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0));\n    Kokkos::parallel_for(policy, [&] (const int i) {\n        size_t bin;\n        if (x(i) < 0.25) {\n            bin = 0;\n        } else if (x(i) < 0.5) {\n            bin = 1;\n        } else if (x(i) < 0.75) {\n            bin = 2;\n        } else {\n            bin = 3;\n        }\n        Kokkos::atomic_fetch_add(&(bins[bin]), 1);\n    });\n}",
            "// TODO: Fill out this function\n    // Hint: You may use the `countIf` function in Kokkos to count the number of elements in a view\n    // that meet some condition.\n    // Hint: You may use the `createTransformedView` function in Kokkos to transform an existing view\n    // by applying a user-provided function to each element of the view.\n    // Hint: Use the `sumReduction` reduction view to accumulate counts for each bin.\n    // Hint: You may find the `Kokkos::Impl::ViewMapping<Kokkos::Experimental::Impl::view_traits<...>,...>::\n    //  value_type` to determine the type of the elements in the `bins` view.\n    // Hint: You may find the `Kokkos::Impl::ViewMapping<Kokkos::Experimental::Impl::view_traits<...>,...>::\n    //  is_array` macro to determine whether a view is an array view.\n\n    // TODO: You may need to #include <Kokkos_View.hpp>\n    // TODO: You may need to #include <Kokkos_ArithTraits.hpp>\n    // TODO: You may need to #include <Kokkos_Sort.hpp>\n    // TODO: You may need to #include <Kokkos_ScatterView.hpp>\n    // TODO: You may need to #include <Kokkos_Bitset.hpp>\n\n    // TODO: Start by defining the views here\n    Kokkos::View<int*> binned(bins.data(), 4);\n    Kokkos::View<double*> absx(\"absx\", x.size());\n    Kokkos::View<double*, Kokkos::MemoryTraits<Kokkos::Unmanaged> > xabs(\"xabs\", x.size());\n    Kokkos::View<double*, Kokkos::MemoryTraits<Kokkos::Unmanaged> > xcopy(\"xcopy\", x.size());\n    Kokkos::View<double*, Kokkos::MemoryTraits<Kokkos::Unmanaged> > sorted(\"sorted\", x.size());\n    Kokkos::View<Kokkos::Experimental::ScatterView<int, Kokkos::Experimental::ScatterSum, Kokkos::Experimental::ScatterTagNonAtomic, Kokkos::Experimental::ScatterRoot>*, Kokkos::MemoryTraits<Kokkos::Unmanaged> > scatter(\"scatter\", 4);\n    Kokkos::View<int*, Kokkos::MemoryTraits<Kokkos::Unmanaged> > is_neg(\"is_neg\", x.size());\n    Kokkos::View<int*, Kokkos::MemoryTraits<Kokkos::Unmanaged> > is_pos(\"is_pos\", x.size());\n    Kokkos::View<int*, Kokkos::MemoryTraits<Kokkos::Unmanaged> > is_zero(\"is_zero\", x.size());\n    Kokkos::View<int*, Kokkos::MemoryTraits<Kokkos::Unmanaged> > is_none(\"is_none\", x.size());\n    Kokkos::View<double*, Kokkos::MemoryTraits<Kokkos::Unmanaged> > abs_x_less_0_25(\"abs_x_less_0_25\", x.size());\n    Kokkos::View<double*, Kokkos::MemoryTraits<Kokkos::Unmanaged> > abs_x_less_0_5(\"abs_x_less_0_5\", x.size());\n    Kokkos::View<double*, Kokkos::MemoryTraits<Kokkos::Unmanaged> > abs_x_less_0_75(\"abs_x_less_0_75\", x.size());\n    Kokkos::View<double*, Kokkos::MemoryTraits<Kokkos::Unmanaged> > abs_x_less_1(\"abs_x_less_1\", x.size());\n    Kokkos::View<double*, Kokkos::MemoryTraits<Kokkos::Unmanaged> > sorted_bins(\"sorted_b",
            "const size_t n = x.size();\n\n  // TODO\n}",
            "// TODO: Your code here\n  bins = Kokkos::View<size_t[4]>(\"bins\");\n\n  auto count = Kokkos::create_reduction_type<size_t> (Kokkos::Sum<size_t>());\n\n  Kokkos::parallel_reduce(\"count\", x.size(), count, KOKKOS_LAMBDA (const int i, size_t &s) {\n      if (x[i] < 0.25) s++;\n      else if (x[i] < 0.5) s++;\n      else if (x[i] < 0.75) s++;\n      else s++;\n  });\n\n  Kokkos::deep_copy(bins, count.value);\n}",
            "// Initialize bins to 0\n    bins(0) = 0;\n    bins(1) = 0;\n    bins(2) = 0;\n    bins(3) = 0;\n\n    //TODO: Count the number of doubles in the vector x that have a fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n    // HINT: Use the built-in Kokkos functions Kokkos::min and Kokkos::max\n    // HINT: If you need to print a Kokkos::View, use Kokkos::deep_copy and print the array\n\n    // print x\n    //for (size_t i = 0; i < x.extent(0); ++i) {\n    //  std::cout << x(i) << \" \";\n    //}\n    //std::cout << \"\\n\";\n\n    // calculate min and max of the array\n    double max_val = Kokkos::max(x);\n    double min_val = Kokkos::min(x);\n    //std::cout << \"max_val: \" << max_val << \" min_val: \" << min_val << \"\\n\";\n\n    // calculate difference between max and min\n    double diff = max_val - min_val;\n    //std::cout << \"diff: \" << diff << \"\\n\";\n\n    // define the interval size\n    double interval = diff / 4;\n    //std::cout << \"interval: \" << interval << \"\\n\";\n\n    // calculate the number of elements of each interval\n    double one_interval = diff / 4;\n    double one_interval_minus_val = max_val - 1.0 / 4 * diff;\n    double one_interval_plus_val = min_val + 1.0 / 4 * diff;\n\n    bins(0) = count_if(x, [&one_interval_minus_val](double& x){return x > one_interval_minus_val;});\n    //std::cout << \"bins(0): \" << bins(0) << \"\\n\";\n\n    bins(1) = count_if(x, [&one_interval_plus_val, &one_interval](double& x){return x >= one_interval_plus_val && x <= one_interval_plus_val + one_interval;});\n    //std::cout << \"bins(1): \" << bins(1) << \"\\n\";\n\n    bins(2) = count_if(x, [&one_interval_plus_val, &one_interval](double& x){return x > one_interval_plus_val + one_interval && x <= one_interval_plus_val + one_interval * 2;});\n    //std::cout << \"bins(2): \" << bins(2) << \"\\n\";\n\n    bins(3) = count_if(x, [&one_interval_plus_val, &one_interval](double& x){return x > one_interval_plus_val + one_interval * 2 && x <= one_interval_plus_val + one_interval * 3;});\n    //std::cout << \"bins(3): \" << bins(3) << \"\\n\";\n}",
            "// Fill in your solution here\n}",
            "auto x2 = x;\n\tKokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int i) {\n\t\tif (x2(i) >= 0.0 && x2(i) < 0.25) {\n\t\t\tbins(0) += 1;\n\t\t} else if (x2(i) >= 0.25 && x2(i) < 0.5) {\n\t\t\tbins(1) += 1;\n\t\t} else if (x2(i) >= 0.5 && x2(i) < 0.75) {\n\t\t\tbins(2) += 1;\n\t\t} else if (x2(i) >= 0.75 && x2(i) <= 1.0) {\n\t\t\tbins(3) += 1;\n\t\t}\n\t});\n\tKokkos::finalize();\n}",
            "// TODO: Fill bins with the number of elements in x that are in each of the 4\n  // quartiles\n}",
            "Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Dynamic> > policy(0, x.size());\n    Kokkos::parallel_for(\"countQuartiles\", policy, KOKKOS_LAMBDA(const int i) {\n        double fractPart = std::modf(x(i), &fractPart);\n        if (fractPart < 0.25) {\n            bins(0)++;\n        }\n        else if (fractPart < 0.5) {\n            bins(1)++;\n        }\n        else if (fractPart < 0.75) {\n            bins(2)++;\n        }\n        else {\n            bins(3)++;\n        }\n    });\n}",
            "}",
            "}",
            "// TODO: fill in this function\n}",
            "// TODO\n}",
            "// TODO: implement me\n}",
            "// TODO\n}",
            "// Write your code here.\n}",
            "Kokkos::parallel_for(\"countQuartiles\", 0, x.extent_int(0), KOKKOS_LAMBDA(int i) {\n    const double val = x(i);\n    if (val < 0.25) bins(0)++;\n    else if (val < 0.5) bins(1)++;\n    else if (val < 0.75) bins(2)++;\n    else bins(3)++;\n  });\n}",
            "//TODO: Fill in the blanks\n    const double a = 0.25;\n    const double b = 0.5;\n    const double c = 0.75;\n    const double d = 1.0;\n    const double n = 4;\n\tfor (size_t i = 0; i < x.size(); ++i){\n\t\tif (x[i] < a) bins[0]++;\n\t\telse if (x[i] < b) bins[1]++;\n\t\telse if (x[i] < c) bins[2]++;\n\t\telse if (x[i] < d) bins[3]++;\n\t}\n}",
            "const size_t n = x.extent(0);\n  const double dx = 1.0 / n;\n  Kokkos::parallel_for(\"countQuartiles\", Kokkos::RangePolicy<Kokkos::Serial>(0, n),\n                       KOKKOS_LAMBDA(const int i) {\n                         const double r = 4.0 * (x(i) - std::floor(x(i) - dx));\n                         if (r < 0.25) {\n                           bins[0] += 1;\n                         } else if (r < 0.5) {\n                           bins[1] += 1;\n                         } else if (r < 0.75) {\n                           bins[2] += 1;\n                         } else {\n                           bins[3] += 1;\n                         }\n                       });\n}",
            "// Fill in your code here.\n}",
            "// TODO\n}",
            "#if defined(KOKKOS_ENABLE_SERIAL)\n    if (Kokkos::Serial::is_initialized()) {\n      const int n = x.extent(0);\n      int b = 0;\n      for (int i = 0; i < n; i++) {\n        if (x(i) - floor(x(i)) < 0.25) bins(0)++;\n        else if (x(i) - floor(x(i)) < 0.5) bins(1)++;\n        else if (x(i) - floor(x(i)) < 0.75) bins(2)++;\n        else bins(3)++;\n      }\n    }\n  #endif\n\n  #if defined(KOKKOS_ENABLE_OPENMP)\n    if (Kokkos::OpenMP::is_initialized()) {\n      const int n = x.extent(0);\n      #pragma omp parallel for\n      for (int i = 0; i < n; i++) {\n        if (x(i) - floor(x(i)) < 0.25) bins(0)++;\n        else if (x(i) - floor(x(i)) < 0.5) bins(1)++;\n        else if (x(i) - floor(x(i)) < 0.75) bins(2)++;\n        else bins(3)++;\n      }\n    }\n  #endif\n\n  #if defined(KOKKOS_ENABLE_THREADS)\n    if (Kokkos::Threads::is_initialized()) {\n      const int n = x.extent(0);\n      #pragma omp parallel for\n      for (int i = 0; i < n; i++) {\n        if (x(i) - floor(x(i)) < 0.25) bins(0)++;\n        else if (x(i) - floor(x(i)) < 0.5) bins(1)++;\n        else if (x(i) - floor(x(i)) < 0.75) bins(2)++;\n        else bins(3)++;\n      }\n    }\n  #endif\n\n  #if defined(KOKKOS_ENABLE_CUDA)\n    if (Kokkos::Cuda::is_initialized()) {\n      const int n = x.extent(0);\n      #pragma omp parallel for\n      for (int i = 0; i < n; i++) {\n        if (x(i) - floor(x(i)) < 0.25) bins(0)++;\n        else if (x(i) - floor(x(i)) < 0.5) bins(1)++;\n        else if (x(i) - floor(x(i)) < 0.75) bins(2)++;\n        else bins(3)++;\n      }\n    }\n  #endif\n}",
            "// TODO: YOUR CODE HERE\n    // You may want to use Kokkos::parallel_reduce.\n}",
            "// Fill in your solution here.\n}",
            "// TODO\n  auto num = x.extent(0);\n  for(size_t i = 0; i < num; i++) {\n    if(x(i) <= 0.25) {\n      bins(0)++;\n    } else if(x(i) <= 0.5) {\n      bins(1)++;\n    } else if(x(i) <= 0.75) {\n      bins(2)++;\n    } else {\n      bins(3)++;\n    }\n  }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "auto countQuartile = KOKKOS_LAMBDA(const int i) {\n        if (x(i) < 0.25) {\n            bins(0) += 1;\n        } else if (x(i) < 0.5) {\n            bins(1) += 1;\n        } else if (x(i) < 0.75) {\n            bins(2) += 1;\n        } else {\n            bins(3) += 1;\n        }\n    };\n\n    Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Dynamic> > policy(0, x.size());\n    Kokkos::parallel_for(policy, countQuartile);\n}",
            "// TODO 1: Implement this function\n\n    // TODO 2: Implement this function\n}",
            "// Compute the number of elements in each of the four bins.\n  // Use the Kokkos parallel for construct.\n\n  // Initialize the output bins to 0.\n  Kokkos::deep_copy(bins, 0);\n\n  // Count the elements.\n\n  // Compute the fractional part of each element.\n  // Use Kokkos to do this in parallel.\n\n  // Count the elements in each of the four bins.\n  // Use Kokkos to do this in parallel.\n\n  // Print the four bins,\n  // and check that the results are correct.\n}",
            "}",
            "size_t n = x.size();\n\n  // Your code here\n  //...\n\n  // Your code here\n  //...\n}",
            "auto n = x.extent(0);\n\t// fill in a temporary vector\n\tKokkos::View<double*, Kokkos::HostSpace> temp_x(\"temp_x\", n);\n\tauto host_x = Kokkos::create_mirror_view(x);\n\tauto host_temp_x = Kokkos::create_mirror_view(temp_x);\n\tfor (size_t i = 0; i < n; i++) {\n\t\thost_temp_x(i) = x(i);\n\t}\n\tKokkos::deep_copy(host_x, x);\n\tKokkos::deep_copy(host_temp_x, temp_x);\n\t// sort the temp vector\n\tKokkos::sort(host_temp_x);\n\t// assign the values of the temp vector to the output vector\n\tsize_t count = 0;\n\tfor (size_t i = 0; i < n; i++) {\n\t\tif (host_temp_x(i) < 0.25) {\n\t\t\tbins(0)++;\n\t\t}\n\t\telse if (host_temp_x(i) < 0.5) {\n\t\t\tbins(1)++;\n\t\t}\n\t\telse if (host_temp_x(i) < 0.75) {\n\t\t\tbins(2)++;\n\t\t}\n\t\telse if (host_temp_x(i) < 1) {\n\t\t\tbins(3)++;\n\t\t}\n\t\telse {\n\t\t\tbreak;\n\t\t}\n\t}\n\tKokkos::deep_copy(temp_x, host_temp_x);\n}",
            "// TODO: implement the function\n}",
            "// Your code here\n    \n}",
            "// TODO: Replace with your own code\n    Kokkos::parallel_for(\n            \"my_kernel\",\n            Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n            KOKKOS_LAMBDA(const int i) {\n                if (x(i) >= 0.0 && x(i) < 0.25) {\n                    bins(0)++;\n                } else if (x(i) >= 0.25 && x(i) < 0.5) {\n                    bins(1)++;\n                } else if (x(i) >= 0.5 && x(i) < 0.75) {\n                    bins(2)++;\n                } else if (x(i) >= 0.75 && x(i) < 1.0) {\n                    bins(3)++;\n                }\n            });\n\n    // TODO: Replace with your own code\n    Kokkos::deep_copy(Kokkos::HostSpace(), bins, bins);\n}",
            "Kokkos::parallel_reduce(\"CountQuartiles\", x.size(), KOKKOS_LAMBDA(const int i, size_t &sum) {\n\t\tdouble xval = x(i);\n\t\tsize_t bin = (size_t)floor(xval * 4);\n\t\tif (bin >= 4) { bin = 3; }\n\t\tsum += 1;\n\t}, Kokkos::Sum<size_t>(bins));\n}",
            "// TODO\n}",
            "const size_t num_elements = x.extent(0);\n  if (num_elements == 0) return;\n\n  Kokkos::parallel_reduce(\n    Kokkos::RangePolicy<>(0, num_elements),\n    KOKKOS_LAMBDA(const int i, size_t& n0) {\n      const double frac = std::fmod(x[i], 1.0);\n      if (frac < 0.25) n0++;\n      else if (frac < 0.5) n0++;\n      else if (frac < 0.75) n0++;\n      else n0++;\n    },\n    Kokkos::View<size_t, Kokkos::HostSpace, Kokkos::MemoryUnmanaged>(\"n0\")\n  );\n\n  Kokkos::deep_copy(bins, Kokkos::View<size_t[4], Kokkos::HostSpace, Kokkos::MemoryUnmanaged>(\"bins\"));\n  Kokkos::deep_copy(bins, Kokkos::View<size_t[4], Kokkos::HostSpace, Kokkos::MemoryUnmanaged>(\"n0\"));\n}",
            "Kokkos::parallel_for(x.size(), [&](int i) {\n        // TODO\n    });\n    Kokkos::finalize();\n}",
            "// Your code goes here\n}",
            "/*\n    TODO: Your code here\n  */\n\n  ////////////////////////////////////////////////////////////////////////////////\n\n\n\n  ////////////////////////////////////////////////////////////////////////////////\n  // \n  //  Count the number of doubles in the vector x that have a fractional part \n  //  in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n  //  Use Kokkos to compute in parallel. Assume that Kokkos has already been initialized.\n  //  Examples:\n\n  //  input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n  //  output: [2, 1, 2, 2]\n\n  //  input: [1.9, 0.2, 0.6, 10.1, 7.4]\n  //  output: [2, 1, 1, 1]\n\n  //  Hints:\n  //    - Use Kokkos::parallel_reduce to count the number of doubles in each of the four\n  //      bins.\n  //    - For the double `x` in the input vector, it may be useful to convert it into\n  //      an integer using `static_cast<int>(x)`.\n  //    - For the integer `i` representing the double `x`, the fractional part can be\n  //      found using `i - int(i)`.\n  //    - To check whether a double `x` is in the interval [0, 0.25), use \n  //      `x > 0.0 && x <= 0.25`.\n  //    - To check whether a double `x` is in the interval [0.25, 0.5), use \n  //      `x > 0.25 && x <= 0.5`.\n  //    - To check whether a double `x` is in the interval [0.5, 0.75), use \n  //      `x > 0.5 && x <= 0.75`.\n  //    - To check whether a double `x` is in the interval [0.75, 1), use \n  //      `x > 0.75 && x <= 1.0`.\n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  // \n  //",
            "Kokkos::parallel_for(\"countQuartiles\", Kokkos::RangePolicy<>(0, x.size()), [&](int i) {\n        bins[int(x(i) - floor(x(i))) * 2]++;\n    });\n}",
            "// TODO\n}",
            "using Kokkos::deep_copy;\n  // TODO: your code here\n  const auto n = x.size();\n  Kokkos::parallel_for(\"countingQuartiles\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0,n),\n    KOKKOS_LAMBDA(const int i) {\n      double fractionalPart = std::modf(x(i),&x(i));\n      if (fractionalPart <= 0.25 && fractionalPart >= 0.0) {\n        bins(0)++;\n      } else if (fractionalPart <= 0.5 && fractionalPart > 0.25) {\n        bins(1)++;\n      } else if (fractionalPart <= 0.75 && fractionalPart > 0.5) {\n        bins(2)++;\n      } else if (fractionalPart <= 1.0 && fractionalPart > 0.75) {\n        bins(3)++;\n      }\n    });\n  Kokkos::fence();\n  deep_copy(bins,bins);\n}",
            "// Fill this in\n}",
            "}",
            "using namespace Kokkos;\n\n    //TODO: Fill in this function\n    const int N = x.extent_int(0);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA (const int i) {\n        if (x(i) < 0.25 && x(i) > 0) {\n            bins(0) += 1;\n        } else if (x(i) >= 0.25 && x(i) < 0.5) {\n            bins(1) += 1;\n        } else if (x(i) >= 0.5 && x(i) < 0.75) {\n            bins(2) += 1;\n        } else {\n            bins(3) += 1;\n        }\n    });\n}",
            "bins = Kokkos::View<size_t[4]>(\"bins\", 0, 0, 0, 0);\n  const size_t N = x.size();\n  constexpr double min = 0.0;\n  constexpr double max = 1.0;\n\n  Kokkos::RangePolicy<> policy(0, N);\n  Kokkos::parallel_for(policy, [=] (const int &i) {\n    const double frac = (x(i) - std::floor(x(i))) * 4;\n    switch (frac) {\n      case 0:\n        bins(0)++;\n        break;\n      case 1:\n        bins(1)++;\n        break;\n      case 2:\n        bins(2)++;\n        break;\n      case 3:\n        bins(3)++;\n        break;\n    }\n  });\n}",
            "}",
            "// TODO: Your code here\n    // Hint: use Kokkos::parallel_for to execute a loop over the elements of x\n    // and update bins in parallel\n    return;\n}",
            "bins = 0;\n\n  // Fill the bins with the correct values.\n  // Hint: there are 4 ranges!\n  // Examples:\n  //\n  //   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n  //   output: [2, 1, 2, 2]\n  //\n  //   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n  //   output: [2, 1, 1, 1]\n\n\n  // TODO: fill the bins in parallel\n  Kokkos::parallel_for(Kokkos::RangePolicy<>(0, x.size()),\n                       KOKKOS_LAMBDA(int i) {\n    int temp = (int)(4 * x(i));\n    if (temp < 1) bins(0)++;\n    else if (temp < 2) bins(1)++;\n    else if (temp < 3) bins(2)++;\n    else bins(3)++;\n  });\n\n}",
            "// TODO: fill in the gaps here\n\n  Kokkos::deep_copy(bins, 0);\n\n}",
            "// TODO\n  Kokkos::parallel_reduce(\"count_quartiles\", x.size(), KOKKOS_LAMBDA(int i, int& r) {\n    if(x(i) >= 0 && x(i) <= 0.25)\n      r += 1;\n    else if(x(i) > 0.25 && x(i) <= 0.5)\n      r += 1;\n    else if(x(i) > 0.5 && x(i) <= 0.75)\n      r += 1;\n    else if(x(i) > 0.75 && x(i) <= 1)\n      r += 1;\n  }, Kokkos::Sum<int>(bins));\n}",
            "// TODO\n}",
            "// TODO\n\n  /*\n  // Count the number of elements in the range [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n  // Note that the size of the input vector is n\n  size_t n = x.size();\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  for(size_t i = 0; i < n; i++){\n    if(x[i] > 0 && x[i] < 0.25){\n      bins[0]++;\n    }\n    else if(x[i] >= 0.25 && x[i] < 0.5){\n      bins[1]++;\n    }\n    else if(x[i] >= 0.5 && x[i] < 0.75){\n      bins[2]++;\n    }\n    else{\n      bins[3]++;\n    }\n  }\n  */\n}",
            "// TODO: implement this function\n}",
            "}",
            "Kokkos::parallel_reduce(\"countQuartiles\", 0, 1, [=](int, size_t &sum) {\n        int i = 0;\n        double lo = x(i);\n        size_t b = 0;\n        double hi = lo + 0.25;\n        while(i<x.extent(0) && x(i)<hi) {\n            if (lo<=x(i) && x(i)<hi) b++;\n            lo = hi;\n            hi += 0.25;\n            i++;\n        }\n        sum += b;\n    });\n    Kokkos::parallel_reduce(\"countQuartiles\", 0, 1, [=](int, size_t &sum) {\n        int i = 0;\n        double lo = x(i);\n        size_t b = 0;\n        double hi = lo + 0.25;\n        while(i<x.extent(0) && x(i)<hi) {\n            if (lo<=x(i) && x(i)<hi) b++;\n            lo = hi;\n            hi += 0.25;\n            i++;\n        }\n        sum += b;\n    });\n    Kokkos::parallel_reduce(\"countQuartiles\", 0, 1, [=](int, size_t &sum) {\n        int i = 0;\n        double lo = x(i);\n        size_t b = 0;\n        double hi = lo + 0.25;\n        while(i<x.extent(0) && x(i)<hi) {\n            if (lo<=x(i) && x(i)<hi) b++;\n            lo = hi;\n            hi += 0.25;\n            i++;\n        }\n        sum += b;\n    });\n    Kokkos::parallel_reduce(\"countQuartiles\", 0, 1, [=](int, size_t &sum) {\n        int i = 0;\n        double lo = x(i);\n        size_t b = 0;\n        double hi = lo + 0.25;\n        while(i<x.extent(0) && x(i)<hi) {\n            if (lo<=x(i) && x(i)<hi) b++;\n            lo = hi;\n            hi += 0.25;\n            i++;\n        }\n        sum += b;\n    });\n    bins(0) = bins(0) + bins(1);\n    bins(0) = bins(0) + bins(2);\n    bins(0) = bins(0) + bins(3);\n}",
            "// Write your solution here\n\n}",
            "bins = 0;\n  double frac[4] = {0.25, 0.5, 0.75, 1.0};\n  int inds[4] = {1, 2, 3, 4};\n  size_t count = x.size();\n\n  for (size_t i = 0; i < count; i++) {\n    for (size_t j = 0; j < 4; j++) {\n      if ((x(i) >= frac[j]) && (x(i) < frac[j] + 0.25)) {\n        bins[j]++;\n      }\n    }\n  }\n}",
            "Kokkos::RangePolicy<Kokkos::Serial> range(0, x.size());\n    Kokkos::parallel_for(\"countQuartiles\", range, [&] (int i) {\n        double x_i = x(i);\n        if (x_i > 0.75) bins(3)++;\n        else if (x_i > 0.5) bins(2)++;\n        else if (x_i > 0.25) bins(1)++;\n        else bins(0)++;\n    });\n}",
            "auto count = KOKKOS_LAMBDA(size_t i) {\n    constexpr double epsilon = 0.25;\n    if (x(i) < 0.0)\n      return;\n    size_t bin = 0;\n    while (x(i) > epsilon * (bin + 1))\n      ++bin;\n    bins(bin)++;\n  };\n  Kokkos::parallel_for(\"countQuartiles\", x.extent_int(0), count);\n}",
            "// Your code here\n\n}",
            "constexpr double min = 0.0;\n    constexpr double max = 1.0;\n    constexpr double step = (max - min) / 4.0;\n\n    Kokkos::parallel_for(\"countQuartiles\", x.size(), KOKKOS_LAMBDA(const int i) {\n        const double a = x(i);\n        if (a >= min && a < min + step) {\n            bins(0)++;\n        } else if (a >= min + step && a < min + 2*step) {\n            bins(1)++;\n        } else if (a >= min + 2*step && a < min + 3*step) {\n            bins(2)++;\n        } else if (a >= min + 3*step && a <= max) {\n            bins(3)++;\n        }\n    });\n}",
            "// TODO: add your code here\n\n}",
            "Kokkos::parallel_for(x.size(), [&](size_t i) {\n        const size_t bin_idx = static_cast<size_t>(x(i) * 4);\n        Kokkos::atomic_fetch_add(&bins(bin_idx), 1);\n    });\n}",
            "// TODO: Your code here\n  auto x_host = Kokkos::create_mirror_view(x);\n  Kokkos::deep_copy(x_host, x);\n  auto x_device = Kokkos::create_mirror_view_and_copy(Kokkos::DefaultExecutionSpace(), x_host);\n  Kokkos::View<double*, Kokkos::DefaultExecutionSpace> x_device_view(x_device.data());\n  auto bins_host = Kokkos::create_mirror_view(bins);\n  Kokkos::deep_copy(bins_host, bins);\n  auto bins_device = Kokkos::create_mirror_view_and_copy(Kokkos::DefaultExecutionSpace(), bins_host);\n  Kokkos::View<size_t*, Kokkos::DefaultExecutionSpace> bins_device_view(bins_device.data());\n  Kokkos::parallel_for(\"counting\", x.extent(0), KOKKOS_LAMBDA(int i) {\n    if(x_device_view(i) <= 0.25) {\n      bins_device_view(0)++;\n    } else if (x_device_view(i) <= 0.5) {\n      bins_device_view(1)++;\n    } else if (x_device_view(i) <= 0.75) {\n      bins_device_view(2)++;\n    } else {\n      bins_device_view(3)++;\n    }\n  });\n  Kokkos::deep_copy(bins_host, bins_device);\n  Kokkos::deep_copy(bins, bins_host);\n}",
            "Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n        const double val = x(i);\n        if (val >= 0 && val < 0.25) {\n            bins(0)++;\n        } else if (val >= 0.25 && val < 0.5) {\n            bins(1)++;\n        } else if (val >= 0.5 && val < 0.75) {\n            bins(2)++;\n        } else if (val >= 0.75 && val <= 1) {\n            bins(3)++;\n        } else {\n            throw std::runtime_error(\"countQuartiles: value out of bounds\");\n        }\n    });\n}",
            "// Your code here\n}",
            "// TODO: fill in this function to implement countQuartiles\n  // You will need to use the Kokkos API to implement parallel reduction.\n  // See the Kokkos tutorial for details\n}",
            "// TODO\n  auto num = x.extent(0);\n  auto d = x.data();\n  Kokkos::parallel_for(\"count_quartiles\", Kokkos::RangePolicy<>(0, num),\n                        KOKKOS_LAMBDA(int i) {\n                          double xi = d[i];\n                          if (xi >= 0.0 && xi < 0.25) {\n                            bins(0) += 1;\n                          } else if (xi >= 0.25 && xi < 0.5) {\n                            bins(1) += 1;\n                          } else if (xi >= 0.5 && xi < 0.75) {\n                            bins(2) += 1;\n                          } else if (xi >= 0.75 && xi <= 1.0) {\n                            bins(3) += 1;\n                          }\n                        });\n}",
            "const double lo = 0.0;\n    const double mid = 0.25;\n    const double hi = 0.75;\n    const double two = 2.0;\n    const size_t size = x.extent_int(0);\n    Kokkos::parallel_for(\"countQuartiles\", Kokkos::RangePolicy<>(0, size), KOKKOS_LAMBDA(const int& idx) {\n        const double d = x(idx);\n        if (d <= lo) {\n            bins(0)++;\n        } else if (d <= mid) {\n            bins(1)++;\n        } else if (d <= hi) {\n            bins(2)++;\n        } else {\n            bins(3)++;\n        }\n    });\n}",
            "// Initialize bins with zeros\n  Kokkos::deep_copy(bins, 0);\n\n  // Get a host view of the vector x\n  Kokkos::View<double*> host_x = Kokkos::create_mirror_view(x);\n  Kokkos::deep_copy(host_x, x);\n\n  // Create a host view of the vector bins\n  Kokkos::View<double*> host_bins = Kokkos::create_mirror_view(bins);\n  Kokkos::deep_copy(host_bins, bins);\n\n  // Count the number of elements in each quarter\n  for (int i = 0; i < host_x.size(); i++) {\n    if (host_x(i) < 0.25) {\n      host_bins(0) += 1;\n    } else if (host_x(i) < 0.5) {\n      host_bins(1) += 1;\n    } else if (host_x(i) < 0.75) {\n      host_bins(2) += 1;\n    } else {\n      host_bins(3) += 1;\n    }\n  }\n\n  // Write the new values back to Kokkos\n  Kokkos::deep_copy(bins, host_bins);\n}",
            "// Your code here:\n}",
            "// TODO: Your code here\n  for (size_t i = 0; i < bins.size(); ++i)\n    bins[i] = 0;\n\n  Kokkos::parallel_reduce(\"countQuartiles\", Kokkos::RangePolicy<>(0, x.extent(0)),\n                          KOKKOS_LAMBDA(const int i, size_t& lsum) {\n                            double num = x(i);\n                            double low = num - num % 0.25;\n                            double high = low + 0.25;\n                            if (num < low) {\n                              lsum += 1;\n                            } else if (num >= low && num < high) {\n                              lsum += 2;\n                            } else if (num >= high) {\n                              lsum += 3;\n                            }\n                          },\n                          Kokkos::Sum<size_t>(bins));\n}",
            "/* TODO: YOUR CODE GOES HERE */\n}",
            "// Your code here.\n\n}",
            "constexpr double quartile_bounds[4] = {0.25, 0.5, 0.75, 1.0};\n    bins(0) = 0;\n    bins(1) = 0;\n    bins(2) = 0;\n    bins(3) = 0;\n    for (size_t i=0; i<x.size(); i++) {\n        for (size_t j=0; j<4; j++) {\n            if (x(i) < quartile_bounds[j]) {\n                bins(j)++;\n                break;\n            }\n        }\n    }\n}",
            "// TODO: YOUR CODE GOES HERE\n    size_t num_entries = x.extent(0);\n    // create an array with the first entry being the 0th quartile\n    Kokkos::View<double[4]> quartiles(\"quartiles\", 4);\n    quartiles(0) = x(0);\n    for (size_t i = 1; i < num_entries; ++i) {\n        if (x(i) >= quartiles(0)) {\n            quartiles(0) = x(i);\n        }\n        if (x(i) >= quartiles(1)) {\n            quartiles(1) = x(i);\n        }\n        if (x(i) >= quartiles(2)) {\n            quartiles(2) = x(i);\n        }\n        if (x(i) >= quartiles(3)) {\n            quartiles(3) = x(i);\n        }\n    }\n    // create an array with the first entry being the number of elements in the 0th quartile\n    Kokkos::View<size_t[4]> quartile_count(\"quartile_count\", 4);\n    for (size_t i = 0; i < num_entries; ++i) {\n        if (x(i) < quartiles(0)) {\n            ++quartile_count(0);\n        } else if (x(i) < quartiles(1)) {\n            ++quartile_count(1);\n        } else if (x(i) < quartiles(2)) {\n            ++quartile_count(2);\n        } else {\n            ++quartile_count(3);\n        }\n    }\n\n    // initialize the bins\n    Kokkos::deep_copy(bins, 0);\n    for (size_t i = 0; i < 4; ++i) {\n        bins(i) = quartile_count(i);\n    }\n    // TODO: YOUR CODE GOES HERE\n\n}",
            "// TODO: implement the function\n}",
            "// TODO: Compute bins in parallel\n  auto quartile_bins = bins;\n  auto size = x.size();\n  for(int i=0; i<4; i++){\n    quartile_bins(i) = 0;\n  }\n  for(int i=0; i<size; i++){\n    if(x(i) < 0.25){\n      quartile_bins(0)++;\n    } else if(x(i) >= 0.25 && x(i) < 0.5){\n      quartile_bins(1)++;\n    } else if(x(i) >= 0.5 && x(i) < 0.75){\n      quartile_bins(2)++;\n    } else if(x(i) >= 0.75 && x(i) < 1){\n      quartile_bins(3)++;\n    }\n  }\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO: Your code goes here\n    Kokkos::parallel_for(\"countQuartiles\", x.size(), KOKKOS_LAMBDA(const size_t i) {\n        if(x[i] >= 0 && x[i] < 0.25)\n            bins[0]++;\n        else if(x[i] >= 0.25 && x[i] < 0.5)\n            bins[1]++;\n        else if(x[i] >= 0.5 && x[i] < 0.75)\n            bins[2]++;\n        else if(x[i] >= 0.75 && x[i] <= 1.0)\n            bins[3]++;\n        else\n            printf(\"Error: input value %lf out of bounds.\\n\", x[i]);\n    });\n}",
            "// TODO: Your code here\n    constexpr int N = 4;\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Serial>(0,x.size()),[&](const int& i){\n        int j = 0;\n        if(x(i) >= 0 && x(i) < 0.25) j = 0;\n        else if(x(i) >= 0.25 && x(i) < 0.5) j = 1;\n        else if(x(i) >= 0.5 && x(i) < 0.75) j = 2;\n        else if(x(i) >= 0.75 && x(i) <= 1) j = 3;\n        bins(j)++;\n    });\n}",
            "// TODO\n}",
            "// TODO: Fill this in\n}",
            "Kokkos::fence();\n  Kokkos::parallel_for(\"count\", Kokkos::RangePolicy<>(0, x.size()), [&](const int idx) {\n    bins(0) += (x(idx) >= 0.0 && x(idx) < 0.25)? 1 : 0;\n    bins(1) += (x(idx) >= 0.25 && x(idx) < 0.5)? 1 : 0;\n    bins(2) += (x(idx) >= 0.5 && x(idx) < 0.75)? 1 : 0;\n    bins(3) += (x(idx) >= 0.75 && x(idx) <= 1.0)? 1 : 0;\n  });\n}",
            "Kokkos::View<double*, Kokkos::HostSpace> x_host(\"x_host\", x.extent(0));\n  Kokkos::deep_copy(x_host, x);\n  size_t size = x.extent(0);\n  double *bin_boundaries = new double[bins.extent(0)];\n  bin_boundaries[0] = 0.25;\n  for (int i = 1; i < bins.extent(0); i++) {\n    bin_boundaries[i] = bin_boundaries[i - 1] + 0.25;\n  }\n\n  for (size_t i = 0; i < size; i++) {\n    for (int j = 0; j < bins.extent(0); j++) {\n      if (x_host(i) <= bin_boundaries[j]) {\n        bins(j)++;\n        break;\n      }\n    }\n  }\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, x.size()),\n\t\t       KOKKOS_LAMBDA(const int &i) {\n    if (x[i] < 0.25) bins[0]++;\n    else if (x[i] < 0.5) bins[1]++;\n    else if (x[i] < 0.75) bins[2]++;\n    else bins[3]++;\n  });\n}",
            "// TODO: Implement this for each part of the assignment\n}",
            "// TODO: implement this function\n}",
            "constexpr double epsilon = 1e-5;\n  auto x_data = x.data();\n  auto x_size = x.size();\n  Kokkos::RangePolicy<Kokkos::Serial> policy(0, x_size);\n  Kokkos::parallel_for(\"countQuartiles\", policy, [&] (const int i) {\n    if (i == 0) {\n      if (x_data[i] <= 0.25 - epsilon) {\n        bins[0] += 1;\n      } else if (x_data[i] > 0.25 - epsilon && x_data[i] <= 0.5 - epsilon) {\n        bins[1] += 1;\n      } else if (x_data[i] > 0.5 - epsilon && x_data[i] <= 0.75 - epsilon) {\n        bins[2] += 1;\n      } else {\n        bins[3] += 1;\n      }\n    } else if (i == x_size - 1) {\n      if (x_data[i] <= 0.25 - epsilon) {\n        bins[0] += 1;\n      } else if (x_data[i] > 0.25 - epsilon && x_data[i] <= 0.5 - epsilon) {\n        bins[1] += 1;\n      } else if (x_data[i] > 0.5 - epsilon && x_data[i] <= 0.75 - epsilon) {\n        bins[2] += 1;\n      } else {\n        bins[3] += 1;\n      }\n    } else {\n      if (x_data[i] < x_data[i - 1]) {\n        if (x_data[i - 1] <= 0.25 - epsilon) {\n          bins[0] += 1;\n        } else if (x_data[i - 1] > 0.25 - epsilon && x_data[i - 1] <= 0.5 - epsilon) {\n          bins[1] += 1;\n        } else if (x_data[i - 1] > 0.5 - epsilon && x_data[i - 1] <= 0.75 - epsilon) {\n          bins[2] += 1;\n        } else {\n          bins[3] += 1;\n        }\n        if (x_data[i] <= 0.25 - epsilon) {\n          bins[0] += 1;\n        } else if (x_data[i] > 0.25 - epsilon && x_data[i] <= 0.5 - epsilon) {\n          bins[1] += 1;\n        } else if (x_data[i] > 0.5 - epsilon && x_data[i] <= 0.75 - epsilon) {\n          bins[2] += 1;\n        } else {\n          bins[3] += 1;\n        }\n      } else {\n        if (x_data[i] <= 0.25 - epsilon) {\n          bins[0] += 1;\n        } else if (x_data[i] > 0.25 - epsilon && x_data[i] <= 0.5 - epsilon) {\n          bins[1] += 1;\n        } else if (x_data[i] > 0.5 - epsilon && x_data[i] <= 0.75 - epsilon) {\n          bins[2] += 1;\n        } else {\n          bins[3] += 1;\n        }\n        if (x_data[i - 1] <= 0.25 - epsilon) {\n          bins[0] += 1;\n        } else if (x_data[i - 1] > 0.25 - epsilon && x_data[i - 1] <= 0.5 - epsilon) {\n          bins[1] += 1;\n        } else if (x_data[i - 1] > 0.5 - epsilon && x_data[i",
            "// TODO: Fill in this function. \n}",
            "// TODO: write your solution here\n}",
            "// Use the Kokkos parallel_reduce to count the number of values in x \n    // that have a fractional part in each of the four intervals.\n    // You can use the std::partial_sort() function to reorder the values \n    // in x so that they are ordered by their fractional part, and use\n    // the std::lower_bound() function to determine which interval each\n    // fractional part belongs to.\n    // You will need to use four partial_sorts and four lower_bounds.\n    // You will need to use a Kokkos view for temporary storage.\n\n    // TODO: Fill in your own code\n\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(\"count_quartiles\", x.size(), KOKKOS_LAMBDA(int i) {\n    const double fraction = x(i) - std::floor(x(i));\n    if (fraction < 0.25) {\n      bins(0)++;\n    } else if (fraction < 0.5) {\n      bins(1)++;\n    } else if (fraction < 0.75) {\n      bins(2)++;\n    } else {\n      bins(3)++;\n    }\n  });\n}",
            "bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  size_t n = x.size();\n  Kokkos::parallel_for(Kokkos::RangePolicy<>(0, n), KOKKOS_LAMBDA (int i) {\n    size_t bin = 0;\n    if (x(i) >= 0.0 && x(i) < 0.25) {\n      bin = 0;\n    } else if (x(i) >= 0.25 && x(i) < 0.5) {\n      bin = 1;\n    } else if (x(i) >= 0.5 && x(i) < 0.75) {\n      bin = 2;\n    } else if (x(i) >= 0.75 && x(i) < 1.0) {\n      bin = 3;\n    }\n    Kokkos::atomic_increment(&bins(bin));\n  });\n}",
            "//TODO: Implement\n    // YOUR CODE HERE\n\n    size_t N = x.size();\n    Kokkos::parallel_for(\"countQuartiles\", Kokkos::RangePolicy<>(0, N),\n    KOKKOS_LAMBDA(size_t i){\n\n        double x_i = x(i);\n        int i_lower;\n        int i_upper;\n\n        if (x_i < 0.25)\n        {\n            i_lower = 0;\n        }\n        else if (x_i >= 0.25 && x_i < 0.5)\n        {\n            i_lower = 1;\n        }\n        else if (x_i >= 0.5 && x_i < 0.75)\n        {\n            i_lower = 2;\n        }\n        else if (x_i >= 0.75 && x_i <= 1)\n        {\n            i_lower = 3;\n        }\n        else\n        {\n            i_lower = -1;\n        }\n\n        if (x_i <= 0.25)\n        {\n            i_upper = 0;\n        }\n        else if (x_i > 0.25 && x_i <= 0.5)\n        {\n            i_upper = 1;\n        }\n        else if (x_i > 0.5 && x_i <= 0.75)\n        {\n            i_upper = 2;\n        }\n        else if (x_i > 0.75 && x_i <= 1)\n        {\n            i_upper = 3;\n        }\n        else\n        {\n            i_upper = -1;\n        }\n\n        if (i_lower == -1)\n        {\n            bins(i_upper) += 1;\n        }\n        else if (i_upper == -1)\n        {\n            bins(i_lower) += 1;\n        }\n        else\n        {\n            bins(i_lower) += 1;\n            bins(i_upper) += 1;\n        }\n\n    });\n\n}",
            "bins(0) = 0;\n    bins(1) = 0;\n    bins(2) = 0;\n    bins(3) = 0;\n    for (int i = 0; i < x.size(); i++) {\n        double tmp = x(i);\n        int j = tmp - static_cast<int>(tmp);\n        if (j == 0) {\n            bins(0) += 1;\n        } else if (j == 1) {\n            bins(1) += 1;\n        } else if (j == 2) {\n            bins(2) += 1;\n        } else {\n            bins(3) += 1;\n        }\n    }\n}",
            "// You should not need to change this code.\n    auto policy = Kokkos::RangePolicy<>(0, x.size());\n\n    // Note that you need to use a 2D view.\n    auto x_2d = Kokkos::create_mirror_view_and_copy(Kokkos::HostSpace(), x);\n    auto bins_2d = Kokkos::create_mirror_view_and_copy(Kokkos::HostSpace(), bins);\n\n    Kokkos::parallel_for(policy,\n        KOKKOS_LAMBDA(const int& i) {\n            const auto quartile = ((int)x_2d(i) * 4) + 0.5;\n            if (quartile < 1) {\n                bins_2d(0)++;\n            } else if (quartile < 2) {\n                bins_2d(1)++;\n            } else if (quartile < 3) {\n                bins_2d(2)++;\n            } else {\n                bins_2d(3)++;\n            }\n        });\n\n    // Copy back to bins\n    Kokkos::deep_copy(bins, bins_2d);\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function.\n\n    // TODO: Use the Kokkos range policy to parallelize the for loop.\n    //       You can use the Kokkos::RangePolicy class (http://kokkos.github.io/kokkos/classKokkos_1_1RangePolicy.html).\n    //       You might want to use the _rebalance policy, which rebalances the workload and tries to even out the workload across threads.\n    //       A good starting point for using the range policy is in the 08-for-loops example (http://kokkos.github.io/kokkos/tutorial.html#_08-for-loops).\n    //       Note that the range policy works for single-threaded execution space, too.\n\n    // TODO: Set the range policy to include all of the elements in `x`.\n    //       Note that you should probably use the \"auto\" type for your range policy to avoid getting type errors.\n    //       Note also that the range policy is a class, not a function. You must use the \".\" operator to access the methods.\n\n    // TODO: Use the _rebalance policy with your range policy.\n    //       Note that the range policy you defined above is still in scope, so you can use it as an argument to _rebalance.\n\n    // TODO: Initialize bins to all zeroes.\n    //       This is a view, so you should use Kokkos::deep_copy to initialize the memory.\n    //       You can use the Kokkos::deep_copy function (http://kokkos.github.io/kokkos/classKokkos_1_1deep__copy.html).\n    //       Note that you should probably use the \"auto\" type for your deep_copy function, too.\n\n    // TODO: In parallel, loop over the elements in `x`.\n    //       Loop over the range of the range policy.\n    //       For each element in the range, use an if-statement to determine which of the four quartiles it belongs to.\n    //       Increment the appropriate element in `bins`.\n    //       (Note that you can use a single if statement, or several if statements.)\n\n    // TODO: Use Kokkos::deep_copy to copy bins from the device to the host and print it.\n    //       The output should look like:\n    //       \"Input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\"\n    //       \"Output: [2, 1, 2, 2]\"\n}",
            "using Kokkos::RangePolicy;\n    Kokkos::parallel_for(\n        \"countQuartiles\", RangePolicy<>(0, x.extent_int(0)),\n        KOKKOS_LAMBDA(int i) {\n            double xi = x(i);\n            if (xi >= 0.0 && xi < 0.25) {\n                bins(0)++;\n            } else if (xi >= 0.25 && xi < 0.5) {\n                bins(1)++;\n            } else if (xi >= 0.5 && xi < 0.75) {\n                bins(2)++;\n            } else if (xi >= 0.75 && xi <= 1.0) {\n                bins(3)++;\n            } else {\n                // should not reach this, but handle if it does\n                bins(3)++;\n            }\n        });\n}",
            "// TODO\n}",
            "// TO DO: your code here\n}",
            "// TODO\n}",
            "// TODO\n}",
            "}",
            "Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(const size_t i, size_t& count) {\n      if (x(i) >= 0 && x(i) < 0.25) count++;\n      else if (x(i) >= 0.25 && x(i) < 0.5) count++;\n      else if (x(i) >= 0.5 && x(i) < 0.75) count++;\n      else count++;\n    }, Kokkos::Sum<size_t>(bins));\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "#pragma omp parallel\n  {\n    int thread_num = omp_get_thread_num();\n    int num_threads = omp_get_num_threads();\n    int total_threads = omp_get_max_threads();\n\n    size_t i = thread_num;\n    while (i < x.size()) {\n      int j = 0;\n      if (x[i] < 0.25) {\n        j = 0;\n      } else if (x[i] < 0.5) {\n        j = 1;\n      } else if (x[i] < 0.75) {\n        j = 2;\n      } else {\n        j = 3;\n      }\n      #pragma omp critical\n      {\n        bins[j]++;\n      }\n      i += num_threads;\n    }\n  }\n}",
            "int num_threads = omp_get_max_threads();\n  int id = omp_get_thread_num();\n  int64_t size = x.size();\n  std::array<size_t, 4> local_bins;\n  for (int i=0; i<4; i++)\n    local_bins[i] = 0;\n\n  if (size == 0) {\n    for (int i=0; i<4; i++)\n      bins[i] = 0;\n    return;\n  }\n\n  #pragma omp for\n  for (int64_t i=id; i<size; i+=num_threads) {\n    double value = x[i];\n    if (value < 0.25)\n      local_bins[0]++;\n    else if (value < 0.5)\n      local_bins[1]++;\n    else if (value < 0.75)\n      local_bins[2]++;\n    else\n      local_bins[3]++;\n  }\n\n  #pragma omp critical\n  for (int i=0; i<4; i++)\n    bins[i] += local_bins[i];\n}",
            "int num_threads = omp_get_max_threads();\n  int num_bins = 4;\n  // int bin_size = x.size()/num_bins;\n  int bin_size = x.size()/num_threads;\n  // cout<<\"bin_size\"<<bin_size<<endl;\n  // int bin_start = 0;\n  int thread_num = omp_get_thread_num();\n  // int thread_num = 1;\n  // cout<<\"thread_num\"<<thread_num<<endl;\n  // int thread_num = 1;\n  // int thread_num = 3;\n  // int thread_num = 0;\n  // int thread_num = 2;\n  // int thread_num = 4;\n\n  for(int i=0; i<4; i++) {\n    bins[i] = 0;\n  }\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for(int i=0; i<bin_size; i++) {\n      // cout<<i<<endl;\n      if((x[i+thread_num*bin_size]-floor(x[i+thread_num*bin_size])) < 0.25) {\n        bins[0] += 1;\n        // cout<<\"0\"<<endl;\n      }\n      else if((x[i+thread_num*bin_size]-floor(x[i+thread_num*bin_size])) >= 0.25 && \n            (x[i+thread_num*bin_size]-floor(x[i+thread_num*bin_size])) < 0.5) {\n        bins[1] += 1;\n        // cout<<\"1\"<<endl;\n      }\n      else if((x[i+thread_num*bin_size]-floor(x[i+thread_num*bin_size])) >= 0.5 && \n            (x[i+thread_num*bin_size]-floor(x[i+thread_num*bin_size])) < 0.75) {\n        bins[2] += 1;\n        // cout<<\"2\"<<endl;\n      }\n      else if((x[i+thread_num*bin_size]-floor(x[i+thread_num*bin_size])) >= 0.75) {\n        bins[3] += 1;\n        // cout<<\"3\"<<endl;\n      }\n    }\n  }\n}",
            "// TODO: add your code here\n    omp_set_num_threads(4);\n    #pragma omp parallel\n    {\n        int id = omp_get_thread_num();\n        std::cout<<id<<\"\\n\";\n        size_t local_sum=0;\n        for (size_t i=0; i<x.size(); i++)\n        {\n            if (id==0 && x[i] >= 0 && x[i] < 0.25)\n                local_sum++;\n            if (id==1 && x[i] >= 0.25 && x[i] < 0.5)\n                local_sum++;\n            if (id==2 && x[i] >= 0.5 && x[i] < 0.75)\n                local_sum++;\n            if (id==3 && x[i] >= 0.75 && x[i] < 1)\n                local_sum++;\n        }\n        #pragma omp critical\n        bins[id]=local_sum;\n    }\n}",
            "bins.fill(0);\n  for (auto const& e : x) {\n    if (e < 0.25) bins[0]++;\n    else if (e < 0.5) bins[1]++;\n    else if (e < 0.75) bins[2]++;\n    else bins[3]++;\n  }\n}",
            "bins.fill(0);\n  const size_t size = x.size();\n  if(size <= 2) {\n    return;\n  }\n  #pragma omp parallel for\n  for(int i=0; i < size; ++i) {\n    int id = floor(x[i]*4);\n    if(id == 0) {\n      #pragma omp atomic\n      bins[0] += 1;\n    } else if(id == 1) {\n      #pragma omp atomic\n      bins[1] += 1;\n    } else if(id == 2) {\n      #pragma omp atomic\n      bins[2] += 1;\n    } else if(id == 3) {\n      #pragma omp atomic\n      bins[3] += 1;\n    }\n  }\n}",
            "double a = 0.25;\n    double b = 0.5;\n    double c = 0.75;\n\n    bins = std::array<size_t, 4>();\n    for (int i = 0; i < 4; i++)\n        bins[i] = 0;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] < a)\n            bins[0]++;\n        else if (x[i] < b)\n            bins[1]++;\n        else if (x[i] < c)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n\t\tif (x[i] >= 0.25 && x[i] < 0.5)\n\t\t\tbins[0] += 1;\n\t\telse if (x[i] >= 0.5 && x[i] < 0.75)\n\t\t\tbins[1] += 1;\n\t\telse if (x[i] >= 0.75 && x[i] < 1)\n\t\t\tbins[2] += 1;\n\t\telse if (x[i] >= 0 && x[i] < 0.25)\n\t\t\tbins[3] += 1;\n\t}\n}",
            "size_t x_size = x.size();\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < x_size; ++i) {\n\t\tsize_t bin = 0;\n\t\tdouble const val = x[i];\n\t\tif (val >= 0.75)\n\t\t\tbin = 3;\n\t\telse if (val >= 0.5)\n\t\t\tbin = 2;\n\t\telse if (val >= 0.25)\n\t\t\tbin = 1;\n\t\telse \n\t\t\tbin = 0;\n\n\t\tbins[bin] += 1;\n\t}\n}",
            "size_t N = x.size();\n  std::array<size_t, 4> counts = {0, 0, 0, 0};\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    size_t index;\n    if (x[i] < 0.25) {\n      index = 0;\n    }\n    else if (x[i] < 0.5) {\n      index = 1;\n    }\n    else if (x[i] < 0.75) {\n      index = 2;\n    }\n    else {\n      index = 3;\n    }\n    counts[index]++;\n  }\n  bins[0] = counts[0];\n  bins[1] = counts[1];\n  bins[2] = counts[2];\n  bins[3] = counts[3];\n}",
            "int num_threads = 4;\n    int n = x.size();\n    int i = 0;\n    int bin_size = n / num_threads;\n    int j = 0;\n    double min_val = x[i];\n    double max_val = x[i];\n    for (i = 1; i < n; i++) {\n        if (x[i] < min_val) min_val = x[i];\n        if (x[i] > max_val) max_val = x[i];\n    }\n    for (j = 0; j < num_threads; j++) {\n        bins[j] = 0;\n    }\n    for (j = 0; j < num_threads; j++) {\n        if (j == num_threads - 1) bin_size = n - bin_size * j;\n        for (i = bin_size * j; i < bin_size * (j + 1); i++) {\n            if ((x[i] - min_val) / (max_val - min_val) < 0.25) bins[0]++;\n            else if ((x[i] - min_val) / (max_val - min_val) < 0.5) bins[1]++;\n            else if ((x[i] - min_val) / (max_val - min_val) < 0.75) bins[2]++;\n            else bins[3]++;\n        }\n    }\n}",
            "// TODO: Implement\n}",
            "auto count_in_quartile = [&](double const& x, size_t const& quartile) {\n        if (x < quartile) {\n            ++bins[0];\n        } else if (x < quartile * 2) {\n            ++bins[1];\n        } else if (x < quartile * 3) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    };\n\n    const size_t n = x.size();\n\n    //omp_set_num_threads(1);\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        count_in_quartile(x[i], 0.25);\n    }\n    //#pragma omp parallel for\n    //for (int i = 0; i < n; ++i) {\n    //    count_in_quartile(x[i], 0.5);\n    //}\n    //#pragma omp parallel for\n    //for (int i = 0; i < n; ++i) {\n    //    count_in_quartile(x[i], 0.75);\n    //}\n    //#pragma omp parallel for\n    //for (int i = 0; i < n; ++i) {\n    //    count_in_quartile(x[i], 1);\n    //}\n\n    return;\n}",
            "// YOUR CODE HERE\n}",
            "double lo = 0;\n  double hi = 0.25;\n  size_t count = 0;\n  size_t bin = 0;\n\n  #pragma omp parallel for private(count, bin) shared(x, lo, hi, bins) reduction(+: count, bin)\n  for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] < lo) {\n      bin = 0;\n    }\n    if (lo < x[i] && x[i] < hi) {\n      bin = 1;\n    }\n    if (hi < x[i] && x[i] < 0.5) {\n      bin = 2;\n    }\n    if (0.5 < x[i] && x[i] < 0.75) {\n      bin = 3;\n    }\n    count += 1;\n  }\n\n  bins[0] = count / 4;\n  bins[1] = count / 4;\n  bins[2] = count / 4;\n  bins[3] = count / 4;\n\n}",
            "// Hint: use std::upper_bound to search for the element in a vector\n  // Hint: use `std::get<>` to retrieve the value of a tuple\n  // Hint: remember that a tuple is a structured array\n  // Hint: remember that you need to #include <tuple>\n  // Hint: remember that you need to #include <algorithm>\n  // Hint: remember that you need to #include <array>\n  // Hint: remember that you need to #include <functional>\n  // Hint: remember that you need to #include <cmath>\n\n  // Note: you do not need to sort the input vector\n\n  constexpr double eps = 0.25;\n  constexpr double mid = 0.5;\n  constexpr double third = 0.333;\n  constexpr double half = 0.5;\n  constexpr double fourth = 0.25;\n\n  bins.fill(0);\n\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    auto tmp = (std::upper_bound(x.begin(), x.end(), x[i] + eps, std::less<double>{}) - x.begin());\n    std::array<double, 4> quartiles = {third, mid, half, fourth};\n    std::tuple<double, double> bounds;\n    bounds = std::make_tuple(x.front(), x.front() + quartiles[tmp] * (x.back() - x.front()));\n\n    if (x[i] < std::get<1>(bounds)) {\n      ++bins[tmp - 1];\n    }\n    else {\n      ++bins[tmp];\n    }\n\n  }\n}",
            "}",
            "double q1 = 0.25;\n    double q2 = 0.50;\n    double q3 = 0.75;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] < q1) {\n            bins[0]++;\n        } else if (x[i] < q2) {\n            bins[1]++;\n        } else if (x[i] < q3) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "int n = x.size();\n    for (int i = 0; i < n; ++i) {\n        if (x[i] > 1.0 || x[i] < 0.0) {\n            std::cout << \"All values should be in [0, 1]\" << std::endl;\n            exit(0);\n        }\n    }\n\n    int chunk_size = x.size() / omp_get_max_threads();\n    int i;\n    #pragma omp parallel shared(x) private(i)\n    {\n        #pragma omp for\n        for (i = 0; i < n; i = i + chunk_size) {\n            if (x[i] < 0.25) {\n                bins[0]++;\n            } else if (x[i] >= 0.25 && x[i] < 0.5) {\n                bins[1]++;\n            } else if (x[i] >= 0.5 && x[i] < 0.75) {\n                bins[2]++;\n            } else {\n                bins[3]++;\n            }\n        }\n    }\n}",
            "bins = std::array<size_t, 4>{};\n    for(size_t i = 0; i < x.size(); ++i){\n        if(x[i] < 0.25){\n            ++bins[0];\n        }\n        else if(x[i] < 0.5){\n            ++bins[1];\n        }\n        else if(x[i] < 0.75){\n            ++bins[2];\n        }\n        else{\n            ++bins[3];\n        }\n    }\n}",
            "/* Your code here */\n  // sort the vector \n  std::sort(x.begin(), x.end());\n  int l = x.size();\n  // the number of doubles in x that have a fractional part in [0, 0.25) \n  bins[0] = (int)((l * 0.25)/100);\n  // the number of doubles in x that have a fractional part in [0.25, 0.5) \n  bins[1] = (int)((l * 0.5)/100);\n  // the number of doubles in x that have a fractional part in [0.5, 0.75) \n  bins[2] = (int)((l * 0.75)/100);\n  // the number of doubles in x that have a fractional part in [0.75, 1) \n  bins[3] = (int)((l * 1)/100);\n  \n  \n}",
            "size_t n = x.size();\n  bins.fill(0);\n\n  //#pragma omp parallel\n  //#pragma omp for schedule(static)\n  //for(size_t i = 0; i < n; ++i) {\n  //  if(x[i] < 0.25) ++bins[0];\n  //  else if(x[i] < 0.5) ++bins[1];\n  //  else if(x[i] < 0.75) ++bins[2];\n  //  else ++bins[3];\n  //}\n\n  #pragma omp parallel for schedule(static)\n  for(size_t i = 0; i < n; ++i) {\n    if(x[i] < 0.25) ++bins[0];\n    else if(x[i] < 0.5) ++bins[1];\n    else if(x[i] < 0.75) ++bins[2];\n    else ++bins[3];\n  }\n}",
            "const int N = x.size();\n  std::vector<std::pair<double, double>> sorted_x(N);\n  std::iota(sorted_x.begin(), sorted_x.end(), 0);\n  std::sort(sorted_x.begin(), sorted_x.end(), [&](auto i1, auto i2) {return x[i1] < x[i2]; });\n\n  double quartile_min, quartile_max;\n\n  #pragma omp parallel\n  {\n    #pragma omp for nowait\n    for (int i = 0; i < N; i++) {\n      double quartile = (i + 0.25) / N;\n      if (i == 0) {\n        quartile_min = quartile;\n      }\n      if (i == N - 1) {\n        quartile_max = quartile;\n      }\n\n      if (x[sorted_x[i]] <= quartile) {\n        if (omp_get_thread_num() == 0) {\n          #pragma omp critical\n          {\n            bins[0]++;\n          }\n        }\n      } else if (x[sorted_x[i]] <= quartile - 0.25) {\n        if (omp_get_thread_num() == 0) {\n          #pragma omp critical\n          {\n            bins[1]++;\n          }\n        }\n      } else if (x[sorted_x[i]] <= quartile - 0.5) {\n        if (omp_get_thread_num() == 0) {\n          #pragma omp critical\n          {\n            bins[2]++;\n          }\n        }\n      } else if (x[sorted_x[i]] <= quartile - 0.75) {\n        if (omp_get_thread_num() == 0) {\n          #pragma omp critical\n          {\n            bins[3]++;\n          }\n        }\n      } else if (x[sorted_x[i]] > quartile_max) {\n        if (omp_get_thread_num() == 0) {\n          #pragma omp critical\n          {\n            bins[4]++;\n          }\n        }\n      }\n    }\n  }\n\n  std::cout << \"Min: \" << quartile_min << \" Max: \" << quartile_max << std::endl;\n\n  #pragma omp parallel\n  {\n    #pragma omp for nowait\n    for (int i = 0; i < N; i++) {\n      double quartile = (i + 0.25) / N;\n      if (x[sorted_x[i]] > quartile_max) {\n        if (omp_get_thread_num() == 0) {\n          #pragma omp critical\n          {\n            bins[4]--;\n          }\n        }\n      }\n    }\n  }\n\n  #pragma omp parallel\n  {\n    #pragma omp for nowait\n    for (int i = 0; i < N; i++) {\n      double quartile = (i + 0.25) / N;\n      if (x[sorted_x[i]] > quartile_max) {\n        if (omp_get_thread_num() == 0) {\n          #pragma omp critical\n          {\n            bins[3]--;\n          }\n        }\n      }\n    }\n  }\n\n  #pragma omp parallel\n  {\n    #pragma omp for nowait\n    for (int i = 0; i < N; i++) {\n      double quartile = (i + 0.25) / N;\n      if (x[sorted_x[i]] > quartile_max) {\n        if (omp_get_thread_num() == 0) {\n          #pragma omp critical\n          {\n            bins[2]--;\n          }\n        }\n      }\n    }\n  }\n\n  #pragma omp parallel\n  {\n    #pragma omp for nowait\n    for (int i = 0; i < N; i++) {\n      double quartile = (i + 0.25) / N;\n      if (x[sorted_x[i]] > quartile_max) {\n        if (omp",
            "bins = {0,0,0,0};\n\n\tdouble l_bound = 0.0, u_bound = 0.25;\n\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); i++) {\n\n\t\tif (x[i] >= l_bound && x[i] < u_bound) {\n\t\t\tbins[0]++;\n\t\t}\n\t\telse if (x[i] >= u_bound && x[i] < l_bound + 0.5) {\n\t\t\tbins[1]++;\n\t\t}\n\t\telse if (x[i] >= l_bound + 0.5 && x[i] < u_bound + 0.5) {\n\t\t\tbins[2]++;\n\t\t}\n\t\telse if (x[i] >= u_bound + 0.5 && x[i] < 1.0) {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "}",
            "}",
            "bins = std::array<size_t, 4>{0,0,0,0};\n    #pragma omp parallel for\n    for(size_t i = 0; i < x.size(); i++) {\n        if(x[i] > 1.0) {\n            if(x[i] > 2.0) {\n                if(x[i] > 3.0) {\n                    bins[0] += 1;\n                } else {\n                    bins[1] += 1;\n                }\n            } else {\n                bins[2] += 1;\n            }\n        } else {\n            bins[3] += 1;\n        }\n    }\n}",
            "// your code goes here\n\n\n}",
            "bins = {0, 0, 0, 0};\n    for(int i = 0; i < x.size(); i++) {\n        if (0.0 <= x[i] && x[i] < 0.25) {\n            bins[0]++;\n        } else if (0.25 <= x[i] && x[i] < 0.5) {\n            bins[1]++;\n        } else if (0.5 <= x[i] && x[i] < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n\n}",
            "#pragma omp parallel\n    {\n        #pragma omp single\n        {\n            for (int i = 0; i < x.size(); ++i) {\n                if (x[i] < 0.25) {\n                    bins[0] += 1;\n                } else if (x[i] >= 0.25 && x[i] < 0.5) {\n                    bins[1] += 1;\n                } else if (x[i] >= 0.5 && x[i] < 0.75) {\n                    bins[2] += 1;\n                } else if (x[i] >= 0.75 && x[i] < 1) {\n                    bins[3] += 1;\n                }\n            }\n        }\n    }\n}",
            "// Initialize the quartiles to 0\n  bins[0] = bins[1] = bins[2] = bins[3] = 0;\n  // Check input vector is not empty\n  if (x.empty()) {\n    return;\n  }\n  // Create a temporary array to store the vector sorted by value\n  std::vector<double> temp_array;\n  temp_array.reserve(x.size());\n  temp_array = x;\n  // Sort the array using insertion sort\n  for (size_t i = 1; i < x.size(); i++) {\n    double key = temp_array[i];\n    size_t j = i - 1;\n    while (j >= 0 && temp_array[j] > key) {\n      temp_array[j + 1] = temp_array[j];\n      j--;\n    }\n    temp_array[j + 1] = key;\n  }\n  // Iterate through the vector to compute the number of values in each bin\n  for (size_t i = 0; i < x.size(); i++) {\n    if (temp_array[i] <= 0.25) {\n      bins[0]++;\n    } else if (temp_array[i] > 0.25 && temp_array[i] <= 0.5) {\n      bins[1]++;\n    } else if (temp_array[i] > 0.5 && temp_array[i] <= 0.75) {\n      bins[2]++;\n    } else if (temp_array[i] > 0.75 && temp_array[i] <= 1) {\n      bins[3]++;\n    }\n  }\n  // Iterate through the bins and compute the quartiles\n  double quartiles[4] = {0.0};\n  double bin_size = static_cast<double>(x.size()) / 4.0;\n  for (size_t i = 0; i < 4; i++) {\n    double lower_bound = static_cast<double>(i) * bin_size;\n    double upper_bound = lower_bound + bin_size;\n    double quartile = temp_array[lower_bound];\n    for (size_t j = 1; j < bins[i]; j++) {\n      quartile += temp_array[j + lower_bound];\n    }\n    if (bins[i]!= 0) {\n      quartile /= static_cast<double>(bins[i]);\n    }\n    quartiles[i] = quartile;\n  }\n  // Set the output quartiles\n  bins[0] = static_cast<size_t>(quartiles[0]);\n  bins[1] = static_cast<size_t>(quartiles[1]);\n  bins[2] = static_cast<size_t>(quartiles[2]);\n  bins[3] = static_cast<size_t>(quartiles[3]);\n}",
            "}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i)\n    {\n      int index = floor((x[i]%1)*4);\n      if(index >= 4)\n\tindex = 3;\n      bins[index]++;\n    }\n}",
            "// Make sure that the output vector is zero-initialized.\n  for (auto &i : bins) {\n    i = 0;\n  }\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for (size_t i = 0; i < x.size(); ++i) {\n      if (x[i] - floor(x[i]) < 0.25) {\n        bins[0]++;\n      } else if (x[i] - floor(x[i]) >= 0.25 && x[i] - floor(x[i]) < 0.5) {\n        bins[1]++;\n      } else if (x[i] - floor(x[i]) >= 0.5 && x[i] - floor(x[i]) < 0.75) {\n        bins[2]++;\n      } else {\n        bins[3]++;\n      }\n    }\n  }\n}",
            "// Fill the quartile bins with zeros.\n  bins.fill(0);\n  // Count the number of items in each quartile.\n  #pragma omp parallel for\n  for (size_t i=0; i<x.size(); i++) {\n    double number = x[i];\n    if (number < 0.25) {\n      bins[0]++;\n    } else if (number < 0.5) {\n      bins[1]++;\n    } else if (number < 0.75) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "// TODO: Your code goes here\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tsize_t j = 0;\n\t\tif (x[i] >= 0.25 && x[i] < 0.5) j = 1;\n\t\telse if (x[i] >= 0.5 && x[i] < 0.75) j = 2;\n\t\telse if (x[i] >= 0.75 && x[i] < 1.0) j = 3;\n\t\tbins[j]++;\n\t}\n}",
            "// fill bins with zeros\n    std::fill(bins.begin(), bins.end(), 0);\n\n    // compute the fractions\n    #pragma omp parallel for\n    for (auto const& e : x) {\n        if (e < 0.25) {\n            bins[0]++;\n        } else if (e >= 0.25 && e < 0.5) {\n            bins[1]++;\n        } else if (e >= 0.5 && e < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "//TODO:\n    // 1. Initialize bins to all zero\n    // 2. Create a loop that does this for each element:\n    //    if(0.0 <= x[i] < 0.25) {\n    //        bins[0]++;\n    //    } else if (0.25 <= x[i] < 0.5) {\n    //        bins[1]++;\n    //    } else if (0.5 <= x[i] < 0.75) {\n    //        bins[2]++;\n    //    } else if (0.75 <= x[i] < 1.0) {\n    //        bins[3]++;\n    //    }\n    // 3. Print the bins\n\n    // 1.\n    bins = {0, 0, 0, 0};\n    // 2.\n    for (int i = 0; i < x.size(); i++) {\n        if (0.0 <= x[i] < 0.25) {\n            bins[0]++;\n        } else if (0.25 <= x[i] < 0.5) {\n            bins[1]++;\n        } else if (0.5 <= x[i] < 0.75) {\n            bins[2]++;\n        } else if (0.75 <= x[i] < 1.0) {\n            bins[3]++;\n        }\n    }\n    // 3.\n    std::cout << \"[\" << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << \"]\" << std::endl;\n}",
            "bins.fill(0);\n    size_t N = x.size();\n    #pragma omp parallel for\n    for(size_t i = 0; i < N; i++)\n    {\n        double d = x[i];\n        if(d < 0.25) bins[0]++;\n        else if(d < 0.5) bins[1]++;\n        else if(d < 0.75) bins[2]++;\n        else bins[3]++;\n    }\n}",
            "// Fill this in!\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        double fraction = x[i] - std::floor(x[i]);\n        if (fraction <= 0.25 && fraction > 0) {\n            bins[0]++;\n        }\n        else if (fraction <= 0.5 && fraction > 0.25) {\n            bins[1]++;\n        }\n        else if (fraction <= 0.75 && fraction > 0.5) {\n            bins[2]++;\n        }\n        else {\n            bins[3]++;\n        }\n    }\n}",
            "size_t n = x.size();\n  double* x_arr = x.data();\n\n  #pragma omp parallel\n  {\n    // Create the number of bins to be used by the number of threads\n    size_t num_bins = omp_get_num_threads();\n    std::vector<size_t> local_bins(num_bins);\n\n    // Set the chunk size to be used by the number of threads\n    size_t chunk_size = (n+num_bins-1)/num_bins;\n\n    // Create the section of the vector that each thread will be responsible for\n    std::vector<double> thread_x(x_arr+omp_get_thread_num()*chunk_size, x_arr+(omp_get_thread_num()+1)*chunk_size);\n    std::vector<double> thread_quartiles(4);\n\n    // Count the fractional part for each thread\n    for(auto i=0; i<thread_x.size(); i++)\n    {\n      if(thread_x[i] >= 0 && thread_x[i] < 1)\n      {\n        thread_quartiles[0]++;\n      }\n      else if(thread_x[i] >= 1 && thread_x[i] < 2)\n      {\n        thread_quartiles[1]++;\n      }\n      else if(thread_x[i] >= 2 && thread_x[i] < 3)\n      {\n        thread_quartiles[2]++;\n      }\n      else if(thread_x[i] >= 3 && thread_x[i] < 4)\n      {\n        thread_quartiles[3]++;\n      }\n    }\n\n    // Add up the counts of each thread to the local_bins vector\n    #pragma omp critical\n    {\n      for(auto i=0; i<num_bins; i++)\n      {\n        local_bins[i] += thread_quartiles[i];\n      }\n    }\n\n    // Add the results to the global bins\n    #pragma omp critical\n    {\n      for(auto i=0; i<num_bins; i++)\n      {\n        bins[i] += local_bins[i];\n      }\n    }\n  }\n}",
            "std::array<double, 4> quartiles = { 0, 0.25, 0.5, 0.75 };\n    std::vector<size_t> counts(4, 0);\n\n    for (double i : x) {\n        if (i < quartiles[0])\n            counts[0]++;\n        else if (i >= quartiles[0] && i < quartiles[1])\n            counts[1]++;\n        else if (i >= quartiles[1] && i < quartiles[2])\n            counts[2]++;\n        else\n            counts[3]++;\n    }\n\n    bins = counts;\n}",
            "std::array<size_t, 4> counts{0,0,0,0};\n    #pragma omp parallel for shared(x, counts)\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] - std::trunc(x[i]) < 0.25)\n            counts[0]++;\n        if (x[i] - std::trunc(x[i]) >= 0.25 && x[i] - std::trunc(x[i]) < 0.5)\n            counts[1]++;\n        if (x[i] - std::trunc(x[i]) >= 0.5 && x[i] - std::trunc(x[i]) < 0.75)\n            counts[2]++;\n        if (x[i] - std::trunc(x[i]) >= 0.75 && x[i] - std::trunc(x[i]) <= 1)\n            counts[3]++;\n    }\n    bins = counts;\n}",
            "// TODO: implement\n  bins.fill(0);\n  size_t n = x.size();\n  if (n == 0) {\n    return;\n  }\n\n  for (size_t i = 0; i < n; i++) {\n    if (x[i] < 0.25) {\n      bins[0]++;\n    }\n    if (x[i] >= 0.25 && x[i] < 0.5) {\n      bins[1]++;\n    }\n    if (x[i] >= 0.5 && x[i] < 0.75) {\n      bins[2]++;\n    }\n    if (x[i] >= 0.75 && x[i] < 1) {\n      bins[3]++;\n    }\n  }\n\n  return;\n}",
            "// TODO\n  int nthreads = omp_get_max_threads();\n  int tid = omp_get_thread_num();\n\n  for (size_t i = 0; i < 4; i++){\n    bins[i] = 0;\n  }\n\n  // #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    double fractpart = modf(x[i], NULL);\n    int index = static_cast<int>(fractpart * 4);\n    bins[index] += 1;\n  }\n}",
            "int n = x.size();\n  omp_set_num_threads(8);\n#pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    if (x[i] < 0.25) {\n      bins[0]++;\n    } else if (x[i] < 0.5) {\n      bins[1]++;\n    } else if (x[i] < 0.75) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "// TODO: write code here\n\t// #pragma omp parallel for \n\tfor(size_t i=0; i<x.size(); i++){\n\t\tif(x[i]<0.25){\n\t\t\tbins[0]++;\n\t\t}\n\t\telse if(x[i]<0.5){\n\t\t\tbins[1]++;\n\t\t}\n\t\telse if(x[i]<0.75){\n\t\t\tbins[2]++;\n\t\t}\n\t\telse{\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "std::sort(x.begin(), x.end());\n    size_t n = x.size();\n    double quartile = 0;\n    size_t q = 0;\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n#pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        if (x[i] < 0.25) {\n            bins[0]++;\n        } else if (x[i] < 0.5) {\n            bins[1]++;\n        } else if (x[i] < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n    return;\n}",
            "}",
            "// Fill bins with 0 to begin with\n\tbins.fill(0);\n\n\t// Number of elements in x\n\tint n = x.size();\n\n\t// Upper and lower bounds for quartiles\n\tdouble lo = 0.0, hi = 0.25, inc = 0.25;\n\n\t// Find the element that is greater than the lower bound\n\tint i = 0;\n\twhile (i < n && x[i] < lo)\n\t\ti++;\n\t// If there is no element greater than the lower bound\n\t// or all the elements are less than the lower bound\n\tif (i == n) {\n\t\t// Fill bins with 0\n\t\tbins.fill(0);\n\t\treturn;\n\t}\n\t// Find the element that is smaller than the upper bound\n\tint j = n - 1;\n\twhile (j >= 0 && x[j] > hi)\n\t\tj--;\n\t// If there is no element smaller than the upper bound\n\t// or all the elements are greater than the upper bound\n\tif (j == -1) {\n\t\t// Fill bins with 0\n\t\tbins.fill(0);\n\t\treturn;\n\t}\n\n\t// Create vectors to store the elements for each quartile\n\tstd::vector<double> q1, q2, q3, q4;\n\t// Create iterators for each quartile\n\tstd::vector<double>::iterator it1 = q1.begin(), it2 = q2.begin(), it3 = q3.begin(), it4 = q4.begin();\n\n\t// Loop until the upper bound is greater than 1\n\twhile (hi < 1) {\n\t\t// Iterate until the element is greater than the lower bound\n\t\twhile (i < n && x[i] <= lo)\n\t\t\ti++;\n\t\t// Iterate until the element is smaller than the upper bound\n\t\twhile (j >= 0 && x[j] > hi)\n\t\t\tj--;\n\t\t// Check if the upper bound is greater than 1\n\t\tif (hi < 1) {\n\t\t\t// If the lower bound is greater than the upper bound\n\t\t\tif (lo > hi) {\n\t\t\t\t// Insert the element to the q4 vector\n\t\t\t\tif (j >= 0)\n\t\t\t\t\tq4.push_back(x[j]);\n\t\t\t\telse\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\telse {\n\t\t\t\t// If the lower bound is less than the upper bound\n\t\t\t\t// Insert the elements to the appropriate vector\n\t\t\t\tif (j >= 0) {\n\t\t\t\t\t// Insert the elements to the q2 vector\n\t\t\t\t\twhile (it2!= q2.end() && j >= 0) {\n\t\t\t\t\t\tit2 = q2.insert(it2, x[j]);\n\t\t\t\t\t\tj--;\n\t\t\t\t\t}\n\t\t\t\t\twhile (it3!= q3.end() && j >= 0) {\n\t\t\t\t\t\tit3 = q3.insert(it3, x[j]);\n\t\t\t\t\t\tj--;\n\t\t\t\t\t}\n\t\t\t\t\twhile (it1!= q1.end() && i < n) {\n\t\t\t\t\t\tit1 = q1.insert(it1, x[i]);\n\t\t\t\t\t\ti++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\t// If j is less than 0\n\t\t\t\t\t// Insert the elements to the appropriate vector\n\t\t\t\t\tif (i < n) {\n\t\t\t\t\t\t// Insert the elements to the q1 vector\n\t\t\t\t\t\twhile (it1!= q1.end() && i < n) {\n\t\t\t\t\t\t\tit1 = q1.insert(it1, x[i]);\n\t\t\t\t\t\t\ti++;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\t// If i is greater than n\n\t\t\t\t\t\t// Insert the elements to the q4 vector",
            "#pragma omp parallel\n  {\n    size_t n = x.size();\n    size_t p = omp_get_num_threads();\n\n    int l = 0;\n    int r = n - 1;\n    int q = (l + r) / 2;\n    double qd = 0;\n\n    while (l < r) {\n      if (x[q] < 0.25) {\n        l = q + 1;\n        q = (l + r) / 2;\n      } else if (x[q] >= 0.75) {\n        r = q - 1;\n        q = (l + r) / 2;\n      } else {\n        qd = x[q];\n        break;\n      }\n    }\n\n    qd = x[q];\n\n    //std::cout << \"qd \" << qd << std::endl;\n\n    if (qd < 0.25) {\n      bins[0] += 1;\n    } else if (qd < 0.5) {\n      bins[1] += 1;\n    } else if (qd < 0.75) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n\n    std::vector<double> lx;\n    std::vector<double> rx;\n    std::vector<double> qx;\n\n    if (omp_get_thread_num() == 0) {\n      lx = std::vector<double>(x.begin(), x.begin() + p - 1);\n      rx = std::vector<double>(x.begin() + p, x.end());\n    } else if (omp_get_thread_num() == p - 1) {\n      lx = std::vector<double>(x.begin(), x.begin() + p - 1);\n      rx = std::vector<double>(x.begin() + p, x.end());\n    } else {\n      lx = std::vector<double>(x.begin(), x.begin() + p);\n      rx = std::vector<double>(x.begin() + p, x.end());\n    }\n\n    //std::cout << \"lx \" << lx.size() << std::endl;\n    //std::cout << \"rx \" << rx.size() << std::endl;\n\n    if (omp_get_thread_num() == 0) {\n      countQuartiles(lx, bins);\n      countQuartiles(rx, bins);\n    } else if (omp_get_thread_num() == p - 1) {\n      countQuartiles(lx, bins);\n      countQuartiles(rx, bins);\n    } else {\n      countQuartiles(lx, bins);\n      countQuartiles(rx, bins);\n    }\n  }\n\n  return;\n}",
            "// TODO\n\n}",
            "size_t N = x.size();\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < 4; ++i)\n    {\n        for (size_t j = 0; j < N; ++j)\n        {\n            if (i == 0)\n            {\n                if (x[j] < 0.25)\n                    ++bins[0];\n                else if (x[j] < 0.50)\n                    ++bins[1];\n                else if (x[j] < 0.75)\n                    ++bins[2];\n                else\n                    ++bins[3];\n            }\n\n            else if (i == 1)\n            {\n                if (x[j] < 0.50)\n                    ++bins[1];\n                else if (x[j] < 0.75)\n                    ++bins[2];\n                else\n                    ++bins[3];\n            }\n\n            else if (i == 2)\n            {\n                if (x[j] < 0.75)\n                    ++bins[2];\n                else\n                    ++bins[3];\n            }\n\n            else\n            {\n                if (x[j] < 1)\n                    ++bins[3];\n            }\n        }\n    }\n}",
            "int const n = x.size();\n\n    if (n == 0) {\n        bins.fill(0);\n        return;\n    }\n\n    std::array<double, 4> quartiles = {0.25, 0.5, 0.75, 1};\n\n    std::vector<double> x_copy = x;\n    std::sort(x_copy.begin(), x_copy.end());\n\n#pragma omp parallel for\n    for (int i = 0; i < 4; i++) {\n        bins[i] = std::count_if(x_copy.begin(), x_copy.end(),\n                                [&quartiles, i](double x) {\n                                    return x >= quartiles[i];\n                                });\n    }\n}",
            "//TODO: implement\n}",
            "const size_t n = x.size();\n    std::sort(x.begin(), x.end());\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        if (x[i] >= 0.75) {\n            bins[3]++;\n        } else if (x[i] >= 0.5) {\n            bins[2]++;\n        } else if (x[i] >= 0.25) {\n            bins[1]++;\n        } else {\n            bins[0]++;\n        }\n    }\n}",
            "size_t n = x.size();\n\t#pragma omp parallel\n\t{\n\t\tstd::vector<double> vec;\n\t\t#pragma omp critical\n\t\t{\n\t\t\tvec = x;\n\t\t}\n\n\t\t#pragma omp for\n\t\tfor(int i = 0; i < n; i++) {\n\t\t\tif(vec[i] < 0.25) {\n\t\t\t\tbins[0] += 1;\n\t\t\t} else if(vec[i] >= 0.25 && vec[i] < 0.5) {\n\t\t\t\tbins[1] += 1;\n\t\t\t} else if(vec[i] >= 0.5 && vec[i] < 0.75) {\n\t\t\t\tbins[2] += 1;\n\t\t\t} else if(vec[i] >= 0.75 && vec[i] < 1.0) {\n\t\t\t\tbins[3] += 1;\n\t\t\t}\n\t\t}\n\t}\n}",
            "#pragma omp parallel\n    {\n        // Create local arrays\n        std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n        #pragma omp for\n        for (size_t i = 0; i < x.size(); ++i) {\n            if (x[i] < 0.25) {\n                ++local_bins[0];\n            } else if (x[i] < 0.5) {\n                ++local_bins[1];\n            } else if (x[i] < 0.75) {\n                ++local_bins[2];\n            } else {\n                ++local_bins[3];\n            }\n        }\n        // Reduce to one array\n        #pragma omp critical\n        for (size_t i = 0; i < 4; ++i) {\n            bins[i] += local_bins[i];\n        }\n    }\n}",
            "// Your code goes here\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for(int i = 0; i < x.size(); ++i)\n        {\n            if(x[i] < 0.25) bins[0]++;\n            else if(x[i] < 0.5) bins[1]++;\n            else if(x[i] < 0.75) bins[2]++;\n            else bins[3]++;\n        }\n    }\n}",
            "// TODO\n\n}",
            "auto low = 0.0, mid = 0.5, high = 1.0;\n\t#pragma omp parallel for num_threads(8)\n\tfor (int i=0; i<x.size(); i++) {\n\t\tif (x[i] < mid) {\n\t\t\tif (x[i] < low) {\n\t\t\t\tbins[0]++;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tbins[1]++;\n\t\t\t}\n\t\t}\n\t\telse if (x[i] < high) {\n\t\t\tbins[2]++;\n\t\t}\n\t\telse {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "// TODO: your code goes here\n\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for (size_t i = 0; i < x.size(); i++)\n    {\n      if (x[i] < 0.25) bins[0] += 1;\n      else if (x[i] < 0.5) bins[1] += 1;\n      else if (x[i] < 0.75) bins[2] += 1;\n      else bins[3] += 1;\n    }\n  }\n}",
            "// TODO: fill in the missing code\n    size_t N = x.size();\n#pragma omp parallel for\n    for(size_t i=0; i<N; i++){\n        if(x[i] >= 0.25 && x[i] < 0.50)\n            bins[0]++;\n        else if(x[i] >= 0.50 && x[i] < 0.75)\n            bins[1]++;\n        else if(x[i] >= 0.75 && x[i] <= 1.0)\n            bins[2]++;\n        else if(x[i] < 0.25)\n            bins[3]++;\n    }\n}",
            "// TODO\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for(int i = 0; i < x.size(); i++){\n            if(x[i] < 0.25){\n                #pragma omp atomic\n                bins[0]++;\n            }\n            else if(x[i] < 0.5){\n                #pragma omp atomic\n                bins[1]++;\n            }\n            else if(x[i] < 0.75){\n                #pragma omp atomic\n                bins[2]++;\n            }\n            else{\n                #pragma omp atomic\n                bins[3]++;\n            }\n        }\n    }\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for\n        for (int i = 0; i < x.size(); i++) {\n            if (x[i] >= 0.75)\n                bins[3]++;\n            else if (x[i] >= 0.5)\n                bins[2]++;\n            else if (x[i] >= 0.25)\n                bins[1]++;\n            else\n                bins[0]++;\n        }\n    }\n}",
            "bins = {0, 0, 0, 0};\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    if (x[i] >= 0 && x[i] < 0.25) {\n      ++bins[0];\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      ++bins[1];\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      ++bins[2];\n    } else if (x[i] >= 0.75 && x[i] < 1.0) {\n      ++bins[3];\n    }\n  }\n}",
            "int nthreads = omp_get_max_threads();\n    int thread_num = omp_get_thread_num();\n    size_t chunk_size = x.size() / nthreads;\n    size_t offset = chunk_size * thread_num;\n    size_t end_index = offset + chunk_size;\n    if(thread_num == nthreads - 1) end_index = x.size();\n    std::array<size_t, 4> t_bins{};\n    for(size_t i = offset; i < end_index; i++) {\n        if(x[i] >= 0.0 && x[i] <= 0.25) t_bins[0]++;\n        else if(x[i] > 0.25 && x[i] <= 0.5) t_bins[1]++;\n        else if(x[i] > 0.5 && x[i] <= 0.75) t_bins[2]++;\n        else t_bins[3]++;\n    }\n    #pragma omp critical\n    for(size_t i = 0; i < 4; i++) bins[i] += t_bins[i];\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++){\n        if (x[i] < 0.25)\n            bins[0]++;\n        if (x[i] >= 0.25 && x[i] < 0.5)\n            bins[1]++;\n        if (x[i] >= 0.5 && x[i] < 0.75)\n            bins[2]++;\n        if (x[i] >= 0.75)\n            bins[3]++;\n    }\n\n}",
            "}",
            "// TODO: Fill this in\n}",
            "// fill this in\n}",
            "#pragma omp parallel for \n    for (size_t i = 0; i < x.size(); i++){\n      bins[(x[i]-floor(x[i]))*4]++;\n    }\n}",
            "// TODO\n\n    int n = x.size();\n    int chunk_size = n / omp_get_max_threads();\n    int i = 0;\n    int j = chunk_size;\n\n    #pragma omp parallel \n    {\n        #pragma omp for\n        for (int k = 0; k < 4; k++) {\n            for (int l = i; l < j; l++) {\n                if (x[l] >= 0 && x[l] < 0.25) {\n                    bins[k]++;\n                } else if (x[l] >= 0.25 && x[l] < 0.5) {\n                    bins[k]++;\n                } else if (x[l] >= 0.5 && x[l] < 0.75) {\n                    bins[k]++;\n                } else if (x[l] >= 0.75 && x[l] < 1) {\n                    bins[k]++;\n                }\n            }\n            i += chunk_size;\n            j += chunk_size;\n        }\n    }\n    return;\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    for (int i = 0; i < x.size(); i++) {\n\n        if (x[i] < 0.25) {\n\n            bins[0] += 1;\n\n        }\n\n        else if (x[i] >= 0.25 && x[i] < 0.5) {\n\n            bins[1] += 1;\n\n        }\n\n        else if (x[i] >= 0.5 && x[i] < 0.75) {\n\n            bins[2] += 1;\n\n        }\n\n        else if (x[i] >= 0.75 && x[i] <= 1) {\n\n            bins[3] += 1;\n\n        }\n\n    }\n}",
            "//TODO: implement this\n\n}",
            "#pragma omp parallel\n{\n    size_t i;\n    #pragma omp for nowait\n    for (i = 0; i < x.size(); ++i){\n        if (x[i] >= 0.0 && x[i] < 0.25){\n            #pragma omp atomic\n            bins[0] += 1;\n        }\n        if (x[i] >= 0.25 && x[i] < 0.5){\n            #pragma omp atomic\n            bins[1] += 1;\n        }\n        if (x[i] >= 0.5 && x[i] < 0.75){\n            #pragma omp atomic\n            bins[2] += 1;\n        }\n        if (x[i] >= 0.75 && x[i] < 1.0){\n            #pragma omp atomic\n            bins[3] += 1;\n        }\n    }\n}\n}",
            "/*\n    #pragma omp parallel\n    {\n      #pragma omp single\n      {\n        //printf(\"Hello from %d\\n\", omp_get_thread_num());\n      }\n    }\n  */\n\n  int threads = 0;\n  #pragma omp parallel\n  {\n    threads = omp_get_num_threads();\n  }\n  size_t chunk_size = x.size() / threads;\n\n  #pragma omp parallel\n  {\n    size_t thread_id = omp_get_thread_num();\n    size_t start = chunk_size * thread_id;\n    size_t end = chunk_size * (thread_id+1);\n\n    // If it is the last thread\n    if (thread_id == threads-1) end = x.size();\n\n    for (size_t i = start; i < end; i++) {\n\n      double val = x[i];\n\n      if (val < 0.25) {\n        bins[0]++;\n      } else if (val < 0.5) {\n        bins[1]++;\n      } else if (val < 0.75) {\n        bins[2]++;\n      } else {\n        bins[3]++;\n      }\n    }\n  }\n}",
            "// YOUR CODE GOES HERE\n#pragma omp parallel\n    {\n        size_t i = 0;\n        double x1 = 0.25;\n        double x2 = 0.5;\n        double x3 = 0.75;\n        double x4 = 1;\n\n        #pragma omp for schedule(dynamic)\n        for (i = 0; i < x.size(); i++)\n        {\n            if (x[i] >= 0 && x[i] < x1)\n            {\n                bins[0]++;\n            }\n            else if (x[i] >= x1 && x[i] < x2)\n            {\n                bins[1]++;\n            }\n            else if (x[i] >= x2 && x[i] < x3)\n            {\n                bins[2]++;\n            }\n            else if (x[i] >= x3 && x[i] < x4)\n            {\n                bins[3]++;\n            }\n        }\n    }\n\n}",
            "// TODO\n}",
            "double lo = 0.0;\n  double hi = 0.25;\n  double mid = (lo + hi) / 2.0;\n  for (int i = 0; i < 4; i++) {\n    bins[i] = 0;\n  }\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] > lo && x[i] <= mid) {\n      bins[0] += 1;\n    } else if (x[i] > mid && x[i] <= hi) {\n      bins[1] += 1;\n    } else if (x[i] > hi && x[i] <= 1.0) {\n      bins[2] += 1;\n    } else if (x[i] > 1.0) {\n      bins[3] += 1;\n    }\n  }\n  // std::cout << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << std::endl;\n  // std::cout << lo << \", \" << mid << \", \" << hi << std::endl;\n}",
            "#pragma omp parallel for\n  for(int i = 0; i < 4; i++)\n  {\n    bins[i] = 0;\n  }\n  for(size_t i = 0; i < x.size(); i++)\n  {\n    if(x[i] >= 0.25)\n    {\n      if(x[i] < 0.5)\n        bins[0]++;\n      else if(x[i] < 0.75)\n        bins[1]++;\n      else if(x[i] < 1)\n        bins[2]++;\n      else\n        bins[3]++;\n    }\n    else\n    {\n      if(x[i] < 0.25)\n        bins[0]++;\n      else if(x[i] < 0.5)\n        bins[1]++;\n      else if(x[i] < 0.75)\n        bins[2]++;\n      else if(x[i] < 1)\n        bins[3]++;\n    }\n  }\n}",
            "int n_threads = omp_get_max_threads();\n\n    // for each thread, create a vector of size n_bins\n    std::vector<std::vector<double>> bin_counts(n_threads);\n    for(auto& vec: bin_counts)\n        vec.resize(bins.size(), 0.);\n\n    // assign a chunk of elements to each thread\n    size_t chunk_size = x.size() / n_threads;\n    size_t remainder = x.size() % n_threads;\n    size_t start = 0;\n    size_t end = chunk_size;\n    for(int i = 0; i < n_threads; i++) {\n        if(remainder > 0) {\n            end += 1;\n            remainder--;\n        }\n        #pragma omp parallel for\n        for(size_t j = start; j < end; j++) {\n            bin_counts[i][getBin(x[j])]++;\n        }\n        start = end;\n        end += chunk_size;\n    }\n\n    // combine counts\n    for(int i = 0; i < n_threads; i++) {\n        for(size_t j = 0; j < bin_counts[i].size(); j++)\n            bins[j] += bin_counts[i][j];\n    }\n}",
            "#pragma omp parallel\n  {\n    #pragma omp single\n    {\n      double frac, quart;\n      for (auto i=0; i<x.size(); i++) {\n        frac = modf(x[i], &quart);\n        if (frac < 0.25) {\n          #pragma omp atomic\n          bins[0] += 1;\n        }\n        else if (frac < 0.5) {\n          #pragma omp atomic\n          bins[1] += 1;\n        }\n        else if (frac < 0.75) {\n          #pragma omp atomic\n          bins[2] += 1;\n        }\n        else {\n          #pragma omp atomic\n          bins[3] += 1;\n        }\n      }\n    }\n  }\n}",
            "size_t i;\n    size_t n = x.size();\n    int id, nthrds;\n    #pragma omp parallel\n    {\n        #pragma omp single\n        {\n            nthrds = omp_get_num_threads();\n        }\n    }\n    std::vector<double> b(nthrds);\n    std::vector<double> p(nthrds+1);\n    std::vector<double> m(nthrds+1);\n    std::vector<double> q(nthrds);\n    std::vector<double> w(nthrds);\n    double sum = 0;\n    for (i = 0; i < n; i++) {\n        sum += x[i];\n    }\n    for (i = 0; i < nthrds; i++) {\n        b[i] = (double)i/(nthrds-1);\n        p[i] = (double)(i+1)/(nthrds-1);\n        m[i] = ((double)(i+1)/nthrds)*sum;\n    }\n    m[nthrds] = sum;\n    q[0] = 0;\n    for (i = 0; i < nthrds; i++) {\n        w[i] = (m[i+1]-m[i])/(p[i+1]-b[i]);\n    }\n    #pragma omp parallel\n    {\n        id = omp_get_thread_num();\n        double k = 0;\n        for (i = 0; i < n; i++) {\n            if (x[i] < w[id]) {\n                k += 1;\n            }\n        }\n        bins[id] = k;\n    }\n    for (i = 0; i < nthrds; i++) {\n        bins[i] += bins[nthrds-i-1];\n        bins[nthrds-i-1] = 0;\n    }\n}",
            "std::array<double, 4> cutoffs = {0.25, 0.5, 0.75, 1.0};\n    std::array<size_t, 4> result{0};\n#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::upper_bound(cutoffs.begin(), cutoffs.end(), x[i]);\n        size_t idx = it - cutoffs.begin();\n        result[idx - 1]++;\n    }\n    bins = result;\n}",
            "bins = {0, 0, 0, 0};\n  double minVal = x[0], maxVal = x[0];\n  for(auto val : x) {\n    minVal = minVal < val? minVal : val;\n    maxVal = maxVal > val? maxVal : val;\n  }\n  double step = (maxVal - minVal) / 4;\n  for(auto val : x) {\n    int i = floor((val - minVal)/step);\n    if (i == 4) {\n      bins[3]++;\n    } else if (i == 3) {\n      bins[2]++;\n    } else if (i == 2) {\n      bins[1]++;\n    } else {\n      bins[0]++;\n    }\n  }\n}",
            "for (int i = 0; i < 4; ++i) {\n    bins[i] = 0;\n  }\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    int index = 0;\n    if (x[i] >= 0.75) {\n      index = 3;\n    } else if (x[i] >= 0.5) {\n      index = 2;\n    } else if (x[i] >= 0.25) {\n      index = 1;\n    }\n    #pragma omp atomic\n    bins[index] += 1;\n  }\n}",
            "// TODO\n}",
            "size_t n = x.size();\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; i++) {\n        if (0.25 >= x[i] && x[i] > 0.0) {\n            bins[0]++;\n        }\n        else if (0.5 >= x[i] && x[i] > 0.25) {\n            bins[1]++;\n        }\n        else if (0.75 >= x[i] && x[i] > 0.5) {\n            bins[2]++;\n        }\n        else if (1.0 >= x[i] && x[i] > 0.75) {\n            bins[3]++;\n        }\n    }\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for\n        for(int i = 0; i < x.size(); ++i)\n        {\n            if(x[i] >= 0 && x[i] < 0.25)\n            {\n                bins[0]++;\n            }\n            else if(x[i] >= 0.25 && x[i] < 0.5)\n            {\n                bins[1]++;\n            }\n            else if(x[i] >= 0.5 && x[i] < 0.75)\n            {\n                bins[2]++;\n            }\n            else if(x[i] >= 0.75 && x[i] < 1)\n            {\n                bins[3]++;\n            }\n            else\n            {\n                std::cout << \"invalid input\" << std::endl;\n            }\n        }\n    }\n\n}",
            "auto n = x.size();\n  double nn = (double) n;\n  std::array<double, 4> sums;\n  sums[0] = 0.0;\n  sums[1] = nn * 0.25;\n  sums[2] = nn * 0.5;\n  sums[3] = nn;\n\n  std::array<double, 4> partialSums;\n  partialSums[0] = 0.0;\n  partialSums[1] = 0.0;\n  partialSums[2] = 0.0;\n  partialSums[3] = 0.0;\n\n  std::array<double, 4> cutoffs;\n  cutoffs[0] = 0.0;\n  cutoffs[1] = 0.25;\n  cutoffs[2] = 0.5;\n  cutoffs[3] = 0.75;\n\n  #pragma omp parallel\n  {\n    auto tid = omp_get_thread_num();\n    auto num_threads = omp_get_num_threads();\n    auto start_idx = (double) tid * nn / num_threads;\n    auto end_idx = (double) (tid + 1) * nn / num_threads;\n    for (auto i = start_idx; i < end_idx; i++) {\n      partialSums[tid] += x[i];\n    }\n    for (int i = 0; i < 4; i++) {\n      partialSums[i] /= nn;\n    }\n    for (int i = 0; i < 4; i++) {\n      partialSums[i] = (partialSums[i] - sums[i]) / (sums[i + 1] - sums[i]);\n    }\n  }\n  for (int i = 0; i < 4; i++) {\n    for (int j = 0; j < num_threads; j++) {\n      partialSums[i] *= sums[j];\n      partialSums[i] += sums[i];\n    }\n  }\n  for (int i = 0; i < 4; i++) {\n    partialSums[i] /= nn;\n  }\n  for (int i = 0; i < 4; i++) {\n    bins[i] = (size_t) std::floor(partialSums[i]);\n  }\n}",
            "bins = {0, 0, 0, 0};\n    if (x.empty()) {\n        return;\n    }\n    const int threads = omp_get_max_threads();\n    const int chunk = x.size() / threads;\n\n#pragma omp parallel for\n    for (int i = 0; i < threads; ++i) {\n        int low = i * chunk;\n        int high = (i + 1) * chunk;\n        if (i == threads - 1) {\n            high = x.size();\n        }\n        std::vector<double> chunkX(x.begin() + low, x.begin() + high);\n        std::sort(chunkX.begin(), chunkX.end());\n        for (double i : chunkX) {\n            if (i < 0.25) {\n                bins[0] += 1;\n            } else if (i >= 0.25 && i < 0.5) {\n                bins[1] += 1;\n            } else if (i >= 0.5 && i < 0.75) {\n                bins[2] += 1;\n            } else {\n                bins[3] += 1;\n            }\n        }\n    }\n}",
            "// Fill in your code here.\n\n\n}",
            "// This is a simple (albeit not necessarily efficient) way to do this\n    // algorithm.\n    // Make sure that all of the elements in x are actually in [0, 1)\n    double minVal = std::min_element(x.begin(), x.end())[0];\n    double maxVal = std::max_element(x.begin(), x.end())[0];\n    double range = maxVal - minVal;\n    double numBins = 4.0;\n    double binSize = range / numBins;\n    double binStarts[] = {minVal + 0.25 * binSize, minVal + 0.5 * binSize, minVal + 0.75 * binSize};\n    double binEnds[] = {minVal + 0.25 * binSize, minVal + 0.75 * binSize, minVal + 1.0 * binSize};\n    \n    // Parallel for loop to compute the count for each bin\n#pragma omp parallel for schedule(dynamic)\n    for (size_t binIdx = 0; binIdx < 4; binIdx++) {\n        size_t binCount = 0;\n        for (auto const& elem : x) {\n            if (elem >= binStarts[binIdx] && elem < binEnds[binIdx]) {\n                binCount++;\n            }\n        }\n        bins[binIdx] = binCount;\n    }\n}",
            "#pragma omp parallel for num_threads(4)\n    for (int i = 0; i < 4; i++) {\n        for (auto it = x.begin(); it!= x.end(); it++) {\n            if (std::abs(((double)i * 0.25 - *it)) < 0.25) {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "// fill bins with 0.0s\n  for(int i = 0; i < 4; i++) {\n    bins[i] = 0;\n  }\n\n  // sort x\n  std::vector<double> x_copy = x;\n  std::sort(x_copy.begin(), x_copy.end());\n\n  // find size of vector\n  int size = x_copy.size();\n\n  // if size is odd, add x[size/2] to x[size/2+1] to get the middle double\n  if (size % 2 == 1) {\n    size++;\n  }\n\n  // get the middle double\n  double middle_double = x_copy[size/2];\n\n  // count the number of doubles in the vector x that have a fractional part \n  // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n  #pragma omp parallel for\n  for (int i = 0; i < size; i++) {\n    if (x_copy[i] >= 0.75) {\n      bins[3]++;\n    } else if (x_copy[i] >= 0.5 && x_copy[i] < 0.75) {\n      bins[2]++;\n    } else if (x_copy[i] >= 0.25 && x_copy[i] < 0.5) {\n      bins[1]++;\n    } else if (x_copy[i] >= 0.0 && x_copy[i] < 0.25) {\n      bins[0]++;\n    }\n  }\n}",
            "//omp_set_num_threads(2);\n\t#pragma omp parallel for schedule(dynamic, 1)\n\tfor(size_t i = 0; i < x.size(); i++){\n\t\tif(x[i] < 0.25) {\n\t\t\tbins[0] += 1;\n\t\t} else if(x[i] < 0.5) {\n\t\t\tbins[1] += 1;\n\t\t} else if(x[i] < 0.75) {\n\t\t\tbins[2] += 1;\n\t\t} else {\n\t\t\tbins[3] += 1;\n\t\t}\n\t}\n}",
            "// TODO: implement me!\n    \n    auto start = omp_get_wtime();\n    #pragma omp parallel for\n    for (int i = 0; i < 4; i++)\n    {\n        bins[i] = 0;\n        for (int j = 0; j < x.size(); j++)\n        {\n            if (i == 0)\n                if (x[j] < 0.25)\n                    bins[i]++;\n            if (i == 1)\n                if (x[j] >= 0.25 && x[j] < 0.5)\n                    bins[i]++;\n            if (i == 2)\n                if (x[j] >= 0.5 && x[j] < 0.75)\n                    bins[i]++;\n            if (i == 3)\n                if (x[j] >= 0.75 && x[j] < 1.0)\n                    bins[i]++;\n        }\n    }\n    auto end = omp_get_wtime();\n    std::cout << \"Time: \" << end - start << std::endl;\n}",
            "bins = {0,0,0,0};\n  int n = x.size();\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    if (x[i] < 0.25) {\n      bins[0] += 1;\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      bins[1] += 1;\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      bins[2] += 1;\n    } else if (x[i] >= 0.75) {\n      bins[3] += 1;\n    }\n  }\n}",
            "#pragma omp parallel\n{\n\tdouble x0 = 0;\n\tdouble x1 = 0.25;\n\tdouble x2 = 0.5;\n\tdouble x3 = 0.75;\n\tdouble x4 = 1;\n\tsize_t index = 0;\n\t#pragma omp for\n\tfor (int i = 0; i < x.size(); i++){\n\t\tindex = 0;\n\t\tif ((x.at(i) >= x0) && (x.at(i) < x1)){\n\t\t\tindex = 0;\n\t\t} else if ((x.at(i) >= x1) && (x.at(i) < x2)){\n\t\t\tindex = 1;\n\t\t} else if ((x.at(i) >= x2) && (x.at(i) < x3)){\n\t\t\tindex = 2;\n\t\t} else if ((x.at(i) >= x3) && (x.at(i) < x4)){\n\t\t\tindex = 3;\n\t\t}\n\t\tbins[index] += 1;\n\t}\n}\n}",
            "size_t const N = x.size();\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    int idx = 0;\n    if (x[i] < 0.25) {\n      idx = 0;\n    }\n    else if (x[i] < 0.5) {\n      idx = 1;\n    }\n    else if (x[i] < 0.75) {\n      idx = 2;\n    }\n    else {\n      idx = 3;\n    }\n    bins[idx]++;\n  }\n}",
            "double q1 = 0.25;\n    double q2 = 0.5;\n    double q3 = 0.75;\n    double q4 = 1;\n    int b1 = 0;\n    int b2 = 0;\n    int b3 = 0;\n    int b4 = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < q1) {\n            b1++;\n        } else if (x[i] < q2) {\n            b2++;\n        } else if (x[i] < q3) {\n            b3++;\n        } else if (x[i] < q4) {\n            b4++;\n        }\n    }\n    bins[0] = b1;\n    bins[1] = b2;\n    bins[2] = b3;\n    bins[3] = b4;\n}",
            "size_t N = x.size();\n\tif (N == 0) return;\n\tstd::sort(x.begin(), x.end());\n\t// parallel for\n\t//omp_set_num_threads(2);\n\t//omp_set_dynamic(1);\n\n\tdouble size = x.size();\n\tdouble q1 = (double) (size / 4);\n\tdouble q2 = (double) (size / 2);\n\tdouble q3 = (double) ((3.0 / 4.0) * size);\n\tdouble q4 = (double) ((7.0 / 4.0) * size);\n\n\tdouble x1 = 0.25;\n\tdouble x2 = 0.5;\n\tdouble x3 = 0.75;\n\tdouble x4 = 1.0;\n\n\tbins[0] = 0;\n\tbins[1] = 0;\n\tbins[2] = 0;\n\tbins[3] = 0;\n\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i] >= 0.0 && x[i] < x1)\n\t\t\tbins[0]++;\n\t\telse if (x[i] >= x1 && x[i] < x2)\n\t\t\tbins[1]++;\n\t\telse if (x[i] >= x2 && x[i] < x3)\n\t\t\tbins[2]++;\n\t\telse if (x[i] >= x3 && x[i] < x4)\n\t\t\tbins[3]++;\n\t}\n\n\t//omp_set_num_threads(4);\n\t//omp_set_dynamic(0);\n}",
            "int num_threads = omp_get_max_threads();\n    for (int i = 0; i < 4; i++) {\n        bins[i] = 0;\n    }\n    #pragma omp parallel\n    {\n        int thread_id = omp_get_thread_num();\n        int num_thread = omp_get_num_threads();\n        double bin_size = 4.0 / (num_thread);\n        int start_bin = thread_id * bin_size;\n        int end_bin = start_bin + bin_size;\n        int bin_id = 0;\n        for (int i = 0; i < x.size(); i++) {\n            double val = x[i];\n            if (val > start_bin && val <= end_bin) {\n                bin_id = val / bin_size;\n                bins[bin_id]++;\n            }\n        }\n    }\n}",
            "}",
            "double numElements = static_cast<double>(x.size());\n  double quarterElements = numElements/4.0;\n\n  for (size_t i = 0; i < 4; i++) {\n    bins[i] = 0;\n  }\n\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    int bucket = 0;\n    if (x[i] < 0.25) {\n      bucket = 0;\n    }\n    else if (x[i] >= 0.25 && x[i] < 0.5) {\n      bucket = 1;\n    }\n    else if (x[i] >= 0.5 && x[i] < 0.75) {\n      bucket = 2;\n    }\n    else if (x[i] >= 0.75 && x[i] <= 1) {\n      bucket = 3;\n    }\n\n    #pragma omp atomic\n    bins[bucket]++;\n  }\n}",
            "std::array<size_t, 4> tmp;\n    tmp.fill(0);\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++){\n        if (x[i] < 0.25){\n            tmp[0]++;\n        } else if (x[i] >= 0.25 && x[i] < 0.5){\n            tmp[1]++;\n        } else if (x[i] >= 0.5 && x[i] < 0.75){\n            tmp[2]++;\n        } else {\n            tmp[3]++;\n        }\n    }\n    bins = tmp;\n}",
            "int n = x.size();\n    omp_set_num_threads(4);\n    #pragma omp parallel\n    {\n        int id = omp_get_thread_num();\n        size_t count = 0;\n        for (int i = 0; i < n; i++) {\n            if (x[i] < 0.25) {\n                count++;\n            } else if (x[i] < 0.5) {\n                count++;\n            } else if (x[i] < 0.75) {\n                count++;\n            } else {\n                count++;\n            }\n        }\n        bins[id] = count;\n    }\n}",
            "//TODO\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for (size_t i = 0; i < x.size(); ++i) {\n      if (x[i] >= 0.0 && x[i] < 0.25)\n        bins[0] += 1;\n      else if (x[i] >= 0.25 && x[i] < 0.5)\n        bins[1] += 1;\n      else if (x[i] >= 0.5 && x[i] < 0.75)\n        bins[2] += 1;\n      else\n        bins[3] += 1;\n    }\n  }\n}",
            "int n = x.size();\n  for (int i = 0; i < n; i++) {\n    double value = x[i];\n    int bin = 0;\n    if (value < 0.25) {\n      bin = 0;\n    } else if (value < 0.5) {\n      bin = 1;\n    } else if (value < 0.75) {\n      bin = 2;\n    } else {\n      bin = 3;\n    }\n    bins[bin]++;\n  }\n}",
            "std::array<double, 4> lower_boundary = {0.0, 0.25, 0.5, 0.75};\n  size_t n = x.size();\n  std::array<size_t, 4> counts;\n  for(int i=0; i<4; ++i) {\n    #pragma omp parallel for\n    for (size_t j=0; j<n; j++) {\n      if(x[j] < lower_boundary[i+1]) {\n        counts[i]++;\n      }\n    }\n  }\n  for(int i=0; i<4; ++i) {\n    bins[i] = counts[i];\n  }\n}",
            "int N = x.size();\n    int nthreads = omp_get_max_threads();\n    #pragma omp parallel for num_threads(nthreads)\n    for (int i=0; i<4; i++) {\n        bins[i] = 0;\n        for (int j=0; j<N; j++) {\n            if (x[j] >= 0.25*i && x[j] < 0.25*(i+1)) {\n                bins[i] += 1;\n            }\n        }\n    }\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for schedule(static)\n        for(int i = 0; i < x.size(); i++) {\n            if(x[i] <= 0.25) {\n                bins[0]++;\n            }\n            else if(x[i] <= 0.5) {\n                bins[1]++;\n            }\n            else if(x[i] <= 0.75) {\n                bins[2]++;\n            }\n            else {\n                bins[3]++;\n            }\n        }\n    }\n}",
            "size_t const n = x.size();\n\n  std::vector<double> x_sorted(x);\n  std::sort(x_sorted.begin(), x_sorted.end());\n\n  bins[0] = x_sorted.begin()[0] <= x_sorted.begin()[1]? 0 : 1;\n  bins[1] = x_sorted.begin()[n - 1] <= x_sorted.begin()[n - 2]? 0 : 1;\n  bins[2] = x_sorted.begin()[n / 2] <= x_sorted.begin()[n / 2 + 1]? 0 : 1;\n  bins[3] = x_sorted.begin()[n - 1] <= x_sorted.begin()[n - 2]? 0 : 1;\n\n  #pragma omp parallel for schedule(static)\n  for (size_t i = 0; i < n; i++) {\n    if (0.25 < x_sorted.begin()[i] && x_sorted.begin()[i] < 0.5) {\n      #pragma omp atomic\n      bins[0]++;\n    }\n    if (0.5 < x_sorted.begin()[i] && x_sorted.begin()[i] < 0.75) {\n      #pragma omp atomic\n      bins[1]++;\n    }\n    if (0.75 < x_sorted.begin()[i] && x_sorted.begin()[i] < 1.0) {\n      #pragma omp atomic\n      bins[2]++;\n    }\n    if (1.0 < x_sorted.begin()[i]) {\n      #pragma omp atomic\n      bins[3]++;\n    }\n  }\n}",
            "int num_threads = 4;\n#pragma omp parallel num_threads(num_threads)\n    {\n        size_t thread_num = omp_get_thread_num();\n        size_t thread_count = omp_get_num_threads();\n        std::vector<double> local_x(x.begin()+thread_num*x.size()/thread_count, x.begin()+(thread_num+1)*x.size()/thread_count);\n        std::vector<size_t> local_bins;\n\n#pragma omp barrier\n\n        std::vector<double> sorted_x = local_x;\n        std::sort(sorted_x.begin(), sorted_x.end());\n        size_t start = 0;\n        size_t end = sorted_x.size();\n        if (thread_num == 0) {\n            start = 0;\n            end = sorted_x.size()/2;\n            local_bins.push_back(end);\n        } else if (thread_num == (num_threads-1)) {\n            start = sorted_x.size()/2;\n            end = sorted_x.size();\n            local_bins.push_back(end);\n        } else {\n            start = sorted_x.size()/2;\n            end = (thread_num+1)*sorted_x.size()/thread_count;\n            local_bins.push_back(start);\n            local_bins.push_back(end);\n        }\n\n        for (size_t i=start; i<end; i++) {\n            if (sorted_x[i] >= 0 && sorted_x[i] < 0.25) {\n                local_bins.push_back(0);\n            } else if (sorted_x[i] >= 0.25 && sorted_x[i] < 0.5) {\n                local_bins.push_back(1);\n            } else if (sorted_x[i] >= 0.5 && sorted_x[i] < 0.75) {\n                local_bins.push_back(2);\n            } else if (sorted_x[i] >= 0.75 && sorted_x[i] <= 1) {\n                local_bins.push_back(3);\n            }\n        }\n\n#pragma omp barrier\n\n#pragma omp critical\n        {\n            bins[0] += local_bins[0];\n            bins[1] += local_bins[1];\n            bins[2] += local_bins[2];\n            bins[3] += local_bins[3];\n        }\n\n#pragma omp barrier\n\n    }\n}",
            "int nthreads = omp_get_max_threads();\n\t#pragma omp parallel for num_threads(nthreads) schedule(static)\n\tfor (int i = 0; i < x.size(); ++i) {\n\t\tbins[0] += (x[i] >= 0 && x[i] < 0.25);\n\t\tbins[1] += (x[i] >= 0.25 && x[i] < 0.5);\n\t\tbins[2] += (x[i] >= 0.5 && x[i] < 0.75);\n\t\tbins[3] += (x[i] >= 0.75 && x[i] <= 1);\n\t}\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if ((x[i] - (int) x[i]) >= 0.75)\n      bins[3]++;\n    if ((x[i] - (int) x[i]) >= 0.5 && (x[i] - (int) x[i]) < 0.75)\n      bins[2]++;\n    if ((x[i] - (int) x[i]) >= 0.25 && (x[i] - (int) x[i]) < 0.5)\n      bins[1]++;\n    if ((x[i] - (int) x[i]) < 0.25)\n      bins[0]++;\n  }\n}",
            "#pragma omp parallel\n    {\n#pragma omp for nowait\n        for (size_t i = 0; i < x.size(); i++) {\n            switch (floor(x[i] * 4) / 4) {\n                case 0:\n                    bins[0]++;\n                    break;\n                case 0.25:\n                    bins[1]++;\n                    break;\n                case 0.5:\n                    bins[2]++;\n                    break;\n                case 0.75:\n                    bins[3]++;\n                    break;\n            }\n        }\n    }\n}",
            "omp_set_num_threads(16);\n\t#pragma omp parallel\n\t{\n\t\tsize_t thread_num = omp_get_thread_num();\n\t\t#pragma omp for\n\t\tfor (size_t i = 0; i < x.size(); i++)\n\t\t{\n\t\t\tif (x[i] >= 0 && x[i] <= 0.25)\n\t\t\t\tbins[0] += 1;\n\t\t\telse if (x[i] > 0.25 && x[i] <= 0.5)\n\t\t\t\tbins[1] += 1;\n\t\t\telse if (x[i] > 0.5 && x[i] <= 0.75)\n\t\t\t\tbins[2] += 1;\n\t\t\telse if (x[i] > 0.75 && x[i] <= 1)\n\t\t\t\tbins[3] += 1;\n\t\t}\n\t}\n\n}",
            "int n_threads = omp_get_max_threads();\n    size_t chunk_size = x.size()/n_threads;\n    size_t x_index = 0;\n    size_t bins_index = 0;\n    double current_value = 0.0;\n\n    //#pragma omp parallel num_threads(n_threads)\n    // {\n    //     int thread_id = omp_get_thread_num();\n    //     size_t chunk_start = thread_id * chunk_size;\n    //     size_t chunk_end = (thread_id + 1) * chunk_size;\n    //     //printf(\"Thread %i: (%i, %i)\\n\", thread_id, chunk_start, chunk_end);\n    //     for (size_t i=chunk_start; i<chunk_end; i++) {\n    //         if (x[i] < 0.25) {\n    //             bins[bins_index] += 1;\n    //             bins_index += 1;\n    //         }\n    //         else if (x[i] < 0.5) {\n    //             bins[bins_index] += 1;\n    //             bins_index += 1;\n    //         }\n    //         else if (x[i] < 0.75) {\n    //             bins[bins_index] += 1;\n    //             bins_index += 1;\n    //         }\n    //         else {\n    //             bins[bins_index] += 1;\n    //             bins_index += 1;\n    //         }\n    //     }\n    // }\n\n    //#pragma omp parallel num_threads(n_threads)\n    // {\n    //     int thread_id = omp_get_thread_num();\n    //     size_t chunk_start = thread_id * chunk_size;\n    //     size_t chunk_end = (thread_id + 1) * chunk_size;\n    //     //printf(\"Thread %i: (%i, %i)\\n\", thread_id, chunk_start, chunk_end);\n    //     for (size_t i=chunk_start; i<chunk_end; i++) {\n    //         if (x[i] < 0.25) {\n    //             bins[bins_index] += 1;\n    //             bins_index += 1;\n    //         }\n    //         else if (x[i] < 0.5) {\n    //             bins[bins_index] += 1;\n    //             bins_index += 1;\n    //         }\n    //         else if (x[i] < 0.75) {\n    //             bins[bins_index] += 1;\n    //             bins_index += 1;\n    //         }\n    //         else {\n    //             bins[bins_index] += 1;\n    //             bins_index += 1;\n    //         }\n    //     }\n    // }\n\n    //#pragma omp parallel num_threads(n_threads)\n    // {\n    //     int thread_id = omp_get_thread_num();\n    //     size_t chunk_start = thread_id * chunk_size;\n    //     size_t chunk_end = (thread_id + 1) * chunk_size;\n    //     //printf(\"Thread %i: (%i, %i)\\n\", thread_id, chunk_start, chunk_end);\n    //     for (size_t i=chunk_start; i<chunk_end; i++) {\n    //         if (x[i] < 0.25) {\n    //             bins[bins_index] += 1;\n    //             bins_index += 1;\n    //         }\n    //         else if (x[i] < 0.5) {\n    //             bins[bins_index] += 1;\n    //             bins_index += 1;\n    //         }\n    //         else if (x[i] < 0.75) {\n    //             bins[bins_index] +=",
            "omp_set_num_threads(8);\n\tint N = x.size();\n\t#pragma omp parallel for schedule(static, 1)\n\tfor (int i = 0; i < N; i++) {\n\t\tdouble fraction = std::fmod(x[i], 1);\n\t\tif (fraction < 0.25) {\n\t\t\tbins[0] += 1;\n\t\t}\n\t\telse if (fraction >= 0.25 && fraction < 0.5) {\n\t\t\tbins[1] += 1;\n\t\t}\n\t\telse if (fraction >= 0.5 && fraction < 0.75) {\n\t\t\tbins[2] += 1;\n\t\t}\n\t\telse {\n\t\t\tbins[3] += 1;\n\t\t}\n\t}\n}",
            "// TODO\n#pragma omp parallel\n    {\n#pragma omp for\n        for(int i = 0; i < x.size(); i++){\n            if(x[i] >= 0. && x[i] < 0.25){\n                bins[0] += 1;\n            }\n            else if(x[i] >= 0.25 && x[i] < 0.5){\n                bins[1] += 1;\n            }\n            else if(x[i] >= 0.5 && x[i] < 0.75){\n                bins[2] += 1;\n            }\n            else if(x[i] >= 0.75 && x[i] <= 1){\n                bins[3] += 1;\n            }\n            else{\n                std::cout << \"This is not possible\" << std::endl;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for schedule(dynamic)\n  for (int i = 0; i < 4; i++) {\n\n    int low = i * x.size() / 4;\n    int high = (i + 1) * x.size() / 4;\n\n    for (int j = low; j < high; j++) {\n      if (x[j] >= 0 && x[j] < (0.25 + i) && x[j] >= 0.25) {\n        bins[i]++;\n      }\n    }\n  }\n}",
            "#pragma omp parallel for\n\tfor(int i = 0; i < bins.size(); i++) {\n\t\tbins[i] = 0;\n\t}\n\tfor(int i = 0; i < x.size(); i++) {\n\t\tif(x[i] >= 0.0 && x[i] < 0.25) {\n\t\t\tbins[0] += 1;\n\t\t} else if (x[i] >= 0.25 && x[i] < 0.5) {\n\t\t\tbins[1] += 1;\n\t\t} else if (x[i] >= 0.5 && x[i] < 0.75) {\n\t\t\tbins[2] += 1;\n\t\t} else if (x[i] >= 0.75 && x[i] < 1.0) {\n\t\t\tbins[3] += 1;\n\t\t} else {\n\t\t\tstd::cout << \"Error - this shouldn't happen\" << std::endl;\n\t\t}\n\t}\n}",
            "int n = x.size();\n#pragma omp parallel for\n\tfor (int i = 0; i < n; i++) {\n\t\tint which = 0;\n\t\tif (x[i] < 0.25)\n\t\t\twhich = 0;\n\t\telse if (x[i] < 0.5)\n\t\t\twhich = 1;\n\t\telse if (x[i] < 0.75)\n\t\t\twhich = 2;\n\t\telse\n\t\t\twhich = 3;\n\t\tbins[which]++;\n\t}\n}",
            "// TODO: Your code here\n    size_t num_bins = 4;\n    size_t n = x.size();\n    double increment = (1.0 / num_bins);\n    for (size_t i = 0; i < num_bins; i++) {\n        double value = i * increment;\n        bins[i] = std::count_if(x.begin(), x.end(), [value](double x) {\n            return x >= value && x < value + increment;\n        });\n    }\n}",
            "}",
            "int nthreads = omp_get_max_threads();\n    int tid = omp_get_thread_num();\n    int num_bins = 4;\n    int bin_width = x.size() / nthreads;\n    int start = tid * bin_width;\n    int end = std::min((tid+1)*bin_width, (int)x.size());\n    for(int i=start;i<end;i++){\n        if(x[i] >= 0 && x[i] <= 0.25){\n            bins[0]++;\n        }else if(x[i] > 0.25 && x[i] <= 0.5){\n            bins[1]++;\n        }else if(x[i] > 0.5 && x[i] <= 0.75){\n            bins[2]++;\n        }else if(x[i] > 0.75 && x[i] <= 1){\n            bins[3]++;\n        }else{\n            std::cout << \"ERROR\" << std::endl;\n            exit(0);\n        }\n    }\n}",
            "// Your code here\n\tsize_t bins[4];\n\t#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i] <= 0.25) bins[0]++;\n\t\telse if (x[i] <= 0.5) bins[1]++;\n\t\telse if (x[i] <= 0.75) bins[2]++;\n\t\telse if (x[i] <= 1.0) bins[3]++;\n\t}\n}",
            "double low = 0;\n    double high = 1;\n    double step = (high-low)/4;\n    #pragma omp parallel for\n    for (int i = 0; i < 4; i++) {\n        double low = (i==0)? 0 : bins[i-1] + step;\n        double high = (i==3)? 1 : low + step;\n        std::cout<<\"low \"<<low<<\" high \"<<high<<std::endl;\n        bins[i] = std::count_if(x.begin(),x.end(),\n                                     [low,high](double const& d)\n                                     { return (d>=low) && (d<high); });\n    }\n}",
            "//TODO\n}",
            "}",
            "constexpr size_t NUM_THREADS = 4;\n    constexpr size_t N = 4;\n    std::array<std::vector<double>, N> thread_results;\n\n    for (size_t i = 0; i < N; i++) {\n        thread_results[i].reserve(x.size());\n    }\n\n    #pragma omp parallel for num_threads(NUM_THREADS)\n    for (size_t i = 0; i < x.size(); i++) {\n        size_t bin_index = 0;\n        if (x[i] >= 0.25 && x[i] < 0.5) {\n            bin_index = 1;\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            bin_index = 2;\n        } else if (x[i] >= 0.75 && x[i] < 1) {\n            bin_index = 3;\n        }\n        thread_results[bin_index % N].push_back(x[i]);\n    }\n\n    for (size_t i = 0; i < N; i++) {\n        bins[i] = thread_results[i].size();\n    }\n}",
            "// TODO: Replace with OpenMP code here\n\n}",
            "double num = x.size();\n  double den = num;\n\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0.25) {\n      bins[0] += 1;\n    }\n    else if (x[i] < 0.50) {\n      bins[1] += 1;\n    }\n    else if (x[i] < 0.75) {\n      bins[2] += 1;\n    }\n    else {\n      bins[3] += 1;\n    }\n  }\n\n  bins[0] /= den;\n  bins[1] /= den;\n  bins[2] /= den;\n  bins[3] /= den;\n\n  // for (size_t i = 0; i < 4; i++) {\n  //   std::cout << bins[i] << \" \";\n  // }\n  // std::cout << std::endl;\n}",
            "double low = 0.0;\n    double high = 1.0;\n    double binSize = (high-low) / 4.0;\n\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for(int i = 0; i < x.size(); ++i){\n            if(x[i] < low + binSize){\n                bins[0]++;\n            }\n            else if(x[i] < low + binSize*2){\n                bins[1]++;\n            }\n            else if(x[i] < low + binSize*3){\n                bins[2]++;\n            }\n            else{\n                bins[3]++;\n            }\n        }\n    }\n}",
            "// Compute number of threads to use\n  int nthreads = 0;\n  #pragma omp parallel reduction(+:nthreads)\n  nthreads += 1;\n\n  bins = {{0,0,0,0}};\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] < 0.25) {\n      bins[0]++;\n    }\n    else if (x[i] < 0.5) {\n      bins[1]++;\n    }\n    else if (x[i] < 0.75) {\n      bins[2]++;\n    }\n    else {\n      bins[3]++;\n    }\n  }\n}",
            "#pragma omp parallel\n\t{\n\t\t#pragma omp for\n\t\tfor (int i = 0; i < x.size(); i++) {\n\t\t\tsize_t idx = static_cast<size_t>(x[i] * 4);\n\t\t\tif (idx == 0)\n\t\t\t\tbins[0]++;\n\t\t\telse if (idx == 1)\n\t\t\t\tbins[1]++;\n\t\t\telse if (idx == 2)\n\t\t\t\tbins[2]++;\n\t\t\telse\n\t\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "double fractional_part;\n    std::array<size_t, 4> temp_bins = {0, 0, 0, 0};\n    int n = x.size();\n    for (int i = 0; i < n; i++) {\n        fractional_part = std::modf(x[i], &x[i]);\n        if (fractional_part < 0.25) {\n            temp_bins[0] += 1;\n        } else if (fractional_part >= 0.25 && fractional_part < 0.5) {\n            temp_bins[1] += 1;\n        } else if (fractional_part >= 0.5 && fractional_part < 0.75) {\n            temp_bins[2] += 1;\n        } else if (fractional_part >= 0.75 && fractional_part < 1) {\n            temp_bins[3] += 1;\n        }\n    }\n    bins = temp_bins;\n}",
            "// Initialize the bins to zero.\n  for (auto &i : bins) i = 0;\n  // Sort the vector.\n  std::vector<double> x_sorted = x;\n  std::sort(x_sorted.begin(), x_sorted.end());\n  // Find the quartile boundaries.\n  double p0 = 0.25 * x_sorted.size();\n  double p1 = 0.50 * x_sorted.size();\n  double p2 = 0.75 * x_sorted.size();\n  double p3 = x_sorted.size();\n  // Find the number of elements in the specified quartile.\n  for (size_t i = 0; i < x_sorted.size(); i++) {\n    if (i < p0) bins[0]++;\n    else if (i < p1) bins[1]++;\n    else if (i < p2) bins[2]++;\n    else if (i < p3) bins[3]++;\n  }\n}",
            "size_t n = x.size();\n  double minx = *std::min_element(x.begin(), x.end());\n  double maxx = *std::max_element(x.begin(), x.end());\n  double width = (maxx - minx) / n;\n  double lower_bound = minx;\n  for (size_t i = 0; i < 4; i++) {\n    while (x[lower_bound / width] < lower_bound)\n      lower_bound += width;\n    size_t upper_bound = lower_bound + width;\n    size_t count = 0;\n    for (size_t j = lower_bound / width; j < upper_bound / width; j++)\n      if (x[j] >= lower_bound && x[j] < upper_bound)\n        count++;\n    bins[i] = count;\n    lower_bound = upper_bound;\n  }\n}",
            "const size_t n = x.size();\n\n    std::array<size_t, 4> count{0, 0, 0, 0};\n\n    #pragma omp parallel for default(none) shared(x) private(count)\n    for(size_t i = 0; i < n; i++){\n\n        if(x[i] >= 0.0 && x[i] < 0.25){\n            count[0] += 1;\n        }\n        else if(x[i] >= 0.25 && x[i] < 0.5){\n            count[1] += 1;\n        }\n        else if(x[i] >= 0.5 && x[i] < 0.75){\n            count[2] += 1;\n        }\n        else if(x[i] >= 0.75 && x[i] < 1.0){\n            count[3] += 1;\n        }\n        else {\n            std::cout << \"Input element \" << i << \" is not in the range [0.0, 1.0]\" << std::endl;\n            throw std::domain_error(\"Input element is not in the range [0.0, 1.0]\");\n        }\n\n    }\n\n    bins = count;\n\n}",
            "std::array<size_t, 4> counts = {0, 0, 0, 0};\n    for (auto i : x) {\n        if (i < 0.25) {\n            counts[0] += 1;\n        } else if (i < 0.5) {\n            counts[1] += 1;\n        } else if (i < 0.75) {\n            counts[2] += 1;\n        } else {\n            counts[3] += 1;\n        }\n    }\n    bins = counts;\n}",
            "// your code here\n\t\n\tfor (int i = 0; i < bins.size(); i++) {\n\t\tbins[i] = 0;\n\t}\n\t// sort\n\tstd::sort(x.begin(), x.end());\n\n\t// count\n\tint start = 0;\n\tint end = x.size() - 1;\n\tint step = 0;\n\twhile (start < end) {\n\t\tif (x[start] < 0.25)\n\t\t\tbins[step]++;\n\t\telse if (x[start] < 0.5)\n\t\t\tbins[step + 1]++;\n\t\telse if (x[start] < 0.75)\n\t\t\tbins[step + 2]++;\n\t\telse\n\t\t\tbins[step + 3]++;\n\t\tstart++;\n\t}\n\t\n\treturn;\n}",
            "#pragma omp parallel \n    {\n        #pragma omp for\n        for(int i = 0; i < x.size(); i++) {\n            bins[floor(4*x[i])]++;\n        }\n    }\n}",
            "#pragma omp parallel\n  {\n    #pragma omp single\n    {\n      // OpenMP 4.0 Single Construct (https://www.openmp.org/spec-html/5.0/openmpsu40.html#x121-14400007.9)\n      bins.at(0) = 0; // initialize count bins\n      bins.at(1) = 0;\n      bins.at(2) = 0;\n      bins.at(3) = 0;\n\n      // OpenMP 3.0 Reduction Clause (https://www.openmp.org/spec-html/5.0/openmpsu30.html#x215-2180002.10.2)\n      #pragma omp for schedule(static) reduction(+:bins)\n      for (int i=0; i<x.size(); i++) {\n        // OpenMP 4.0 Ordered Construct (https://www.openmp.org/spec-html/5.0/openmpsu40.html#x232-23500023.16)\n        #pragma omp ordered\n        if (x.at(i) >= 0.75) {\n          bins.at(3)++;\n        } else if (x.at(i) >= 0.5) {\n          bins.at(2)++;\n        } else if (x.at(i) >= 0.25) {\n          bins.at(1)++;\n        } else {\n          bins.at(0)++;\n        }\n      }\n    }\n  }\n\n}",
            "omp_set_num_threads(4);\n\n\t#pragma omp parallel\n\t{\n\t\t#pragma omp single\n\t\t{\n\t\t\t#pragma omp task\n\t\t\t{\n\t\t\t\tbins[0] = 0;\n\t\t\t\tfor (int i = 0; i < x.size(); ++i)\n\t\t\t\t{\n\t\t\t\t\tif (x[i] > 0 && x[i] <= 0.25)\n\t\t\t\t\t{\n\t\t\t\t\t\tbins[0]++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t#pragma omp task\n\t\t\t{\n\t\t\t\tbins[1] = 0;\n\t\t\t\tfor (int i = 0; i < x.size(); ++i)\n\t\t\t\t{\n\t\t\t\t\tif (x[i] > 0.25 && x[i] <= 0.5)\n\t\t\t\t\t{\n\t\t\t\t\t\tbins[1]++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t#pragma omp task\n\t\t\t{\n\t\t\t\tbins[2] = 0;\n\t\t\t\tfor (int i = 0; i < x.size(); ++i)\n\t\t\t\t{\n\t\t\t\t\tif (x[i] > 0.5 && x[i] <= 0.75)\n\t\t\t\t\t{\n\t\t\t\t\t\tbins[2]++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t#pragma omp task\n\t\t\t{\n\t\t\t\tbins[3] = 0;\n\t\t\t\tfor (int i = 0; i < x.size(); ++i)\n\t\t\t\t{\n\t\t\t\t\tif (x[i] > 0.75 && x[i] <= 1)\n\t\t\t\t\t{\n\t\t\t\t\t\tbins[3]++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "size_t n = x.size();\n\tbins[0] = bins[1] = bins[2] = bins[3] = 0;\n\t#pragma omp parallel for reduction(+:bins[0]) reduction(+:bins[1]) reduction(+:bins[2]) reduction(+:bins[3])\n\tfor(size_t i=0; i<n; ++i) {\n\t\tif (x[i] >= 0.25 && x[i] < 0.5) bins[0] += 1;\n\t\telse if (x[i] >= 0.5 && x[i] < 0.75) bins[1] += 1;\n\t\telse if (x[i] >= 0.75 && x[i] < 1) bins[2] += 1;\n\t\telse if (x[i] >= 0 && x[i] < 0.25) bins[3] += 1;\n\t}\n\treturn;\n}",
            "int t = omp_get_max_threads();\n    bins = {0, 0, 0, 0};\n\n#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] >= 0 && x[i] < 0.25)\n            bins[0] += 1;\n        else if (x[i] >= 0.25 && x[i] < 0.5)\n            bins[1] += 1;\n        else if (x[i] >= 0.5 && x[i] < 0.75)\n            bins[2] += 1;\n        else if (x[i] >= 0.75 && x[i] < 1)\n            bins[3] += 1;\n    }\n\n    for (size_t i = 0; i < bins.size(); i++) {\n        bins[i] = bins[i] / t;\n    }\n}",
            "//omp_set_num_threads(4);\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        bins[0] += (x[i] < 0.25)? 1 : 0;\n        bins[1] += (x[i] >= 0.25 && x[i] < 0.5)? 1 : 0;\n        bins[2] += (x[i] >= 0.5 && x[i] < 0.75)? 1 : 0;\n        bins[3] += (x[i] >= 0.75 && x[i] < 1)? 1 : 0;\n    }\n}",
            "#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); ++i) {\n\t\tif (x[i] < 0.25) {\n\t\t\tbins[0]++;\n\t\t}\n\t\telse if (x[i] < 0.5) {\n\t\t\tbins[1]++;\n\t\t}\n\t\telse if (x[i] < 0.75) {\n\t\t\tbins[2]++;\n\t\t}\n\t\telse {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "int i, j;\n    int n = x.size();\n    size_t a[4] = { 0,0,0,0 };\n    double x1, x2;\n\n    for (i = 0; i < n; i++) {\n        x1 = 4 * x[i];\n        x2 = x1 - 1;\n        if (x2 >= 0 && x1 < 1) {\n            a[0]++;\n        }\n        else if (x2 >= 1 && x1 < 2) {\n            a[1]++;\n        }\n        else if (x2 >= 2 && x1 < 3) {\n            a[2]++;\n        }\n        else if (x2 >= 3 && x1 < 4) {\n            a[3]++;\n        }\n    }\n\n#pragma omp parallel for private(j)\n    for (i = 0; i < 4; i++) {\n        for (j = 0; j < n; j++) {\n            if (x[j] <= i) {\n                a[i]++;\n                break;\n            }\n        }\n    }\n    bins[0] = a[0];\n    bins[1] = a[1];\n    bins[2] = a[2];\n    bins[3] = a[3];\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        double xi = x[i];\n        if (xi >= 0.0 && xi < 0.25) {\n            bins[0]++;\n        }\n        else if (xi >= 0.25 && xi < 0.5) {\n            bins[1]++;\n        }\n        else if (xi >= 0.5 && xi < 0.75) {\n            bins[2]++;\n        }\n        else if (xi >= 0.75 && xi <= 1.0) {\n            bins[3]++;\n        }\n        else if (xi < 0.0) {\n            throw std::runtime_error(\"x must be nonnegative\");\n        }\n        else {\n            throw std::runtime_error(\"x must be less than or equal to 1.0\");\n        }\n    }\n}",
            "// TODO: YOUR CODE HERE\n\n    for (auto& el: bins)\n        el = 0;\n\n    double quartiles[4];\n    double quartile_size = x.size() / 4.0;\n    double i = 0.0;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        if (i < quartile_size) {\n            if (x[i] < 0.25)\n                bins[0]++;\n            else if (x[i] < 0.5)\n                bins[1]++;\n            else if (x[i] < 0.75)\n                bins[2]++;\n            else\n                bins[3]++;\n        }\n    }\n}",
            "int i,j,k,l;\n\tfor (i = 0; i < 4; i++)\n\t{\n\t\tbins[i] = 0;\n\t}\n\n\t#pragma omp parallel for reduction(+:bins[0])\n\tfor (i = 0; i < x.size(); i++)\n\t{\n\t\tif (x[i] >= 0 && x[i] < 0.25)\n\t\t{\n\t\t\tbins[0]++;\n\t\t}\n\t\telse if (x[i] >= 0.25 && x[i] < 0.5)\n\t\t{\n\t\t\tbins[1]++;\n\t\t}\n\t\telse if (x[i] >= 0.5 && x[i] < 0.75)\n\t\t{\n\t\t\tbins[2]++;\n\t\t}\n\t\telse if (x[i] >= 0.75 && x[i] <= 1)\n\t\t{\n\t\t\tbins[3]++;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tstd::cout << \"error\" << std::endl;\n\t\t}\n\t}\n\n\treturn;\n}",
            "int num_threads = 1;\n\n    // TODO: Set the number of threads to the value returned by\n    //       omp_get_max_threads()\n\n    int n = x.size();\n    std::vector<std::vector<double> > q(num_threads);\n    for (int i = 0; i < num_threads; i++){\n        q[i].resize(4);\n    }\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++){\n        // TODO: Fill in the quartile boundaries\n        int q1 = 0;\n        int q2 = n/4;\n        int q3 = n/2;\n        int q4 = 3*n/4;\n        // TODO: Put the correct values in q[omp_get_thread_num()]\n        q[omp_get_thread_num()][q1] = x[i];\n        q[omp_get_thread_num()][q2] = x[i];\n        q[omp_get_thread_num()][q3] = x[i];\n        q[omp_get_thread_num()][q4] = x[i];\n    }\n    for (int i = 0; i < num_threads; i++){\n        for (int j = 0; j < 4; j++){\n            bins[j] += q[i][j];\n        }\n    }\n}",
            "const size_t size = x.size();\n  std::array<double, 4> p = {0.25, 0.5, 0.75, 1.0};\n  #pragma omp parallel for\n  for (size_t i = 0; i < 4; ++i) {\n    size_t c = 0;\n    for (size_t j = 0; j < size; ++j)\n      if (x[j] >= p[i] && x[j] < p[i+1])\n        ++c;\n    bins[i] = c;\n  }\n\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < bins.size(); i++) {\n    for (size_t j = 0; j < x.size(); j++) {\n      if (x[j] > (i * 0.25) && x[j] <= ((i + 1) * 0.25)) {\n        bins[i] += 1;\n      }\n    }\n  }\n}",
            "// TODO\n   #pragma omp parallel\n   {\n   #pragma omp for \n   for (size_t i=0; i<x.size(); i++)\n   {\n      double temp = x[i];\n      size_t index = static_cast<size_t>(temp);\n      if (index==temp) \n      {\n         switch (index)\n         {\n         case 0:\n            bins[0]++;\n            break;\n         case 1:\n            bins[1]++;\n            break;\n         case 2:\n            bins[2]++;\n            break;\n         case 3:\n            bins[3]++;\n            break;\n         default:\n            break;\n         }\n      }\n      else \n      {\n         double frac = temp - index;\n         if (frac >= 0.0 && frac < 0.25)\n            bins[0]++;\n         else if (frac >= 0.25 && frac < 0.5)\n            bins[1]++;\n         else if (frac >= 0.5 && frac < 0.75)\n            bins[2]++;\n         else\n            bins[3]++;\n      }\n   }\n   }\n   #pragma omp barrier\n   #pragma omp critical\n   {\n   if (bins[0] + bins[1] + bins[2] + bins[3]!= x.size()) \n   {\n      std::cout << \"Wrong bins!\\n\";\n      exit(0);\n   }\n   }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < 4; i++) {\n    bins[i] = 0;\n  }\n\n  #pragma omp parallel for reduction(+:bins[0], bins[1], bins[2], bins[3])\n  for (int i = 0; i < (int)x.size(); i++) {\n    double bin = floor(x[i]);\n    if (bin >= 1 && bin <= 1.25)\n      bins[0]++;\n    else if (bin >= 1.25 && bin <= 1.5)\n      bins[1]++;\n    else if (bin >= 1.5 && bin <= 1.75)\n      bins[2]++;\n    else if (bin >= 1.75 && bin <= 2)\n      bins[3]++;\n  }\n}",
            "// TODO: Implement this function in 3-2-2.\n  int n = x.size();\n  int j=0;\n  bins[0]=0;\n  bins[1]=0;\n  bins[2]=0;\n  bins[3]=0;\n  for (int i=0; i<n; ++i) {\n    if (x[i]<0.25) bins[0]++;\n    else if (x[i]>=0.25 && x[i]<0.5) bins[1]++;\n    else if (x[i]>=0.5 && x[i]<0.75) bins[2]++;\n    else if (x[i]>=0.75) bins[3]++;\n  }\n\n  return;\n}",
            "// TODO\n}",
            "#pragma omp parallel for\n\tfor (size_t i=0; i<x.size(); i++){\n\t\tint k=0;\n\t\tif (x[i]<0.25){\n\t\t\tk=0;\n\t\t}\n\t\telse if(x[i]<0.5){\n\t\t\tk=1;\n\t\t}\n\t\telse if (x[i]<0.75){\n\t\t\tk=2;\n\t\t}\n\t\telse{\n\t\t\tk=3;\n\t\t}\n\t\tbins[k]++;\n\t}\n}",
            "const size_t size = x.size();\n    const double step = 0.25;\n    const double min = 0.0;\n    const double max = 1.0;\n    if (size == 0) {\n        for (size_t i = 0; i < bins.size(); ++i)\n            bins[i] = 0;\n    }\n    else {\n        std::vector<double> copy(x.begin(), x.end());\n        #pragma omp parallel for\n        for (size_t i = 0; i < size; ++i)\n            for (double k = min; k <= max; k += step)\n                if (x[i] >= k)\n                    ++copy[i];\n        bins[0] = copy[0];\n        for (size_t i = 1; i < copy.size(); ++i)\n            bins[i] += copy[i - 1];\n    }\n}",
            "std::sort(x.begin(), x.end());\n    auto last = x.end();\n    auto first = x.begin();\n    const double epsilon = 0.25;\n    const size_t n = x.size();\n\n    bins = {0, 0, 0, 0};\n\n    #pragma omp parallel\n    {\n        // count the elements of x in each bin\n        #pragma omp for\n        for (size_t i = 0; i < n; ++i) {\n            if (x[i] >= first + (last - first) * epsilon)\n                ++bins[0];\n            else if (x[i] >= first + (last - first) * 0.25)\n                ++bins[1];\n            else if (x[i] >= first + (last - first) * 0.5)\n                ++bins[2];\n            else\n                ++bins[3];\n        }\n    }\n\n    return;\n}",
            "// This is the size of x.\n  size_t n = x.size();\n  // Allocate memory for bins.\n  bins = std::array<size_t, 4>();\n  // Compute quartiles.\n  std::array<double, 4> quartiles = std::array<double, 4>();\n  quartiles[0] = x[int(n * 0.25)];\n  quartiles[1] = x[int(n * 0.5)];\n  quartiles[2] = x[int(n * 0.75)];\n  quartiles[3] = x[int(n * 1)];\n  // Start parallel region.\n  #pragma omp parallel\n  {\n    // Start threads.\n    #pragma omp for\n    for (int i = 0; i < n; i++)\n    {\n      // If x[i] is in the first quartile.\n      if (x[i] <= quartiles[0])\n        {\n          // Increment bin 1.\n          bins[0]++;\n        }\n      // If x[i] is in the second quartile.\n      if (x[i] > quartiles[0] && x[i] <= quartiles[1])\n        {\n          // Increment bin 2.\n          bins[1]++;\n        }\n      // If x[i] is in the third quartile.\n      if (x[i] > quartiles[1] && x[i] <= quartiles[2])\n        {\n          // Increment bin 3.\n          bins[2]++;\n        }\n      // If x[i] is in the fourth quartile.\n      if (x[i] > quartiles[2] && x[i] <= quartiles[3])\n        {\n          // Increment bin 4.\n          bins[3]++;\n        }\n    }\n  }\n  // End parallel region.\n}",
            "//TODO: Your code here\n\tbins.fill(0);\n\tomp_set_num_threads(4);\n#pragma omp parallel \n\t{\n\t\tomp_set_num_threads(4);\n\t\t#pragma omp for\n\t\tfor (auto i = 0; i < x.size(); i++) {\n\t\t\tswitch (floor(x[i] * 4))\n\t\t\t{\n\t\t\tcase 0:\n\t\t\t\tbins[0]++;\n\t\t\t\tbreak;\n\t\t\tcase 1:\n\t\t\t\tbins[1]++;\n\t\t\t\tbreak;\n\t\t\tcase 2:\n\t\t\t\tbins[2]++;\n\t\t\t\tbreak;\n\t\t\tcase 3:\n\t\t\t\tbins[3]++;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n}",
            "size_t N = x.size();\n\n    if (N == 0) {\n        return;\n    }\n\n    // Create 4 threads to count the number of doubles with fractions\n    // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n    #pragma omp parallel num_threads(4)\n    {\n        int thread_id = omp_get_thread_num();\n\n        // Calculate the index that will be used to start counting\n        size_t start = (N / 4) * thread_id;\n\n        // Calculate the index that will be used to end counting\n        size_t end = (N / 4) * (thread_id + 1);\n\n        for (size_t i = start; i < end; ++i) {\n            if (x[i] < 0.25) {\n                bins[0]++;\n            } else if (x[i] < 0.5) {\n                bins[1]++;\n            } else if (x[i] < 0.75) {\n                bins[2]++;\n            } else {\n                bins[3]++;\n            }\n        }\n    }\n}",
            "for (size_t i = 0; i < 4; ++i) bins[i] = 0;\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = std::modf(x[i], &x[i]);\n        if (fractionalPart < 0.25) {\n            bins[0]++;\n        }\n        else if (fractionalPart < 0.5) {\n            bins[1]++;\n        }\n        else if (fractionalPart < 0.75) {\n            bins[2]++;\n        }\n        else {\n            bins[3]++;\n        }\n    }\n}",
            "bins = {0, 0, 0, 0};\n\t#pragma omp parallel for reduction(+:bins[0], bins[1], bins[2], bins[3])\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tif (x[i] < 0.25) bins[0]++;\n\t\telse if (x[i] < 0.5) bins[1]++;\n\t\telse if (x[i] < 0.75) bins[2]++;\n\t\telse bins[3]++;\n\t}\n}",
            "#pragma omp parallel for num_threads(2)\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0.25) bins[0]++;\n    else if (x[i] < 0.50) bins[1]++;\n    else if (x[i] < 0.75) bins[2]++;\n    else bins[3]++;\n  }\n}",
            "#pragma omp parallel\n  {\n#pragma omp for\n    for (int i = 0; i < x.size(); i++) {\n      double n = x[i];\n      if (n < 0.25)\n        bins[0]++;\n      else if (n < 0.5)\n        bins[1]++;\n      else if (n < 0.75)\n        bins[2]++;\n      else\n        bins[3]++;\n    }\n  }\n}",
            "double a,b,c,d;\n  a = b = c = d = 0;\n  int n = x.size();\n  std::vector<double> xSorted = x;\n  std::sort(xSorted.begin(), xSorted.end());\n  // #pragma omp parallel for\n  // for (int i = 0; i < n; i++) {\n  //   if (xSorted[i] < 0.25) {\n  //     a++;\n  //   }\n  //   else if (xSorted[i] >= 0.25 && xSorted[i] < 0.5) {\n  //     b++;\n  //   }\n  //   else if (xSorted[i] >= 0.5 && xSorted[i] < 0.75) {\n  //     c++;\n  //   }\n  //   else if (xSorted[i] >= 0.75 && xSorted[i] <= 1.0) {\n  //     d++;\n  //   }\n  // }\n\n  // std::cout << \"a: \" << a << std::endl;\n  // std::cout << \"b: \" << b << std::endl;\n  // std::cout << \"c: \" << c << std::endl;\n  // std::cout << \"d: \" << d << std::endl;\n\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    if (xSorted[i] < 0.25) {\n      a++;\n    }\n    else if (xSorted[i] >= 0.25 && xSorted[i] < 0.5) {\n      b++;\n    }\n    else if (xSorted[i] >= 0.5 && xSorted[i] < 0.75) {\n      c++;\n    }\n    else if (xSorted[i] >= 0.75 && xSorted[i] <= 1.0) {\n      d++;\n    }\n  }\n  std::cout << \"a: \" << a << std::endl;\n  std::cout << \"b: \" << b << std::endl;\n  std::cout << \"c: \" << c << std::endl;\n  std::cout << \"d: \" << d << std::endl;\n\n  bins[0] = a;\n  bins[1] = b;\n  bins[2] = c;\n  bins[3] = d;\n}",
            "bins.fill(0);\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tif (x[i] < 0.25)\n\t\t\t++bins[0];\n\t\telse if (x[i] < 0.5)\n\t\t\t++bins[1];\n\t\telse if (x[i] < 0.75)\n\t\t\t++bins[2];\n\t\telse\n\t\t\t++bins[3];\n\t}\n}",
            "for(auto& i : bins)\n        i = 0;\n\n    #pragma omp parallel\n    {\n        double x_min, x_max;\n        #pragma omp for\n        for(int i = 0; i < x.size(); i++) {\n            if(i == 0)\n                x_min = x[i];\n            if(i == x.size() - 1)\n                x_max = x[i];\n        }\n        double interval = (x_max - x_min) / 4;\n        #pragma omp for\n        for(int i = 0; i < x.size(); i++) {\n            if(x[i] >= x_min && x[i] < x_min + interval)\n                bins[0]++;\n            else if(x[i] >= x_min + interval && x[i] < x_min + interval * 2)\n                bins[1]++;\n            else if(x[i] >= x_min + interval * 2 && x[i] < x_min + interval * 3)\n                bins[2]++;\n            else if(x[i] >= x_min + interval * 3 && x[i] <= x_max)\n                bins[3]++;\n        }\n    }\n\n}",
            "//TODO: Your code goes here\n#pragma omp parallel\n\t{\n\t\t//omp_set_num_threads(4);\n\t\tint i, n = x.size();\n#pragma omp for\n\t\tfor (i = 0; i < n; i++) {\n\t\t\tif (0.0 <= x[i] && x[i] < 0.25)\n\t\t\t\tbins[0]++;\n\t\t\telse if (0.25 <= x[i] && x[i] < 0.5)\n\t\t\t\tbins[1]++;\n\t\t\telse if (0.5 <= x[i] && x[i] < 0.75)\n\t\t\t\tbins[2]++;\n\t\t\telse if (0.75 <= x[i] && x[i] < 1.0)\n\t\t\t\tbins[3]++;\n\t\t}\n\n\t}\n\n\treturn;\n}",
            "// FIXME: Implement this function\n\n\n    return;\n}",
            "// TODO: Implement this!\n}",
            "size_t size = x.size();\n    #pragma omp parallel\n    {\n        size_t chunkSize = size/omp_get_num_threads();\n        size_t threadNum = omp_get_thread_num();\n        size_t startIndex = threadNum*chunkSize;\n        size_t endIndex = (threadNum+1)*chunkSize;\n        for (size_t i = startIndex; i < endIndex; i++) {\n            int index = 0;\n            if (x[i] < 0.25) index = 0;\n            else if (x[i] >= 0.25 && x[i] < 0.5) index = 1;\n            else if (x[i] >= 0.5 && x[i] < 0.75) index = 2;\n            else if (x[i] >= 0.75 && x[i] <= 1) index = 3;\n            #pragma omp critical\n            bins[index] += 1;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for(size_t i = 0; i < x.size(); ++i){\n        if (x[i] < 0.25){\n            bins[0]++;\n        }\n        else if (x[i] >= 0.25 && x[i] < 0.5){\n            bins[1]++;\n        }\n        else if (x[i] >= 0.5 && x[i] < 0.75){\n            bins[2]++;\n        }\n        else if (x[i] >= 0.75 && x[i] <= 1){\n            bins[3]++;\n        }\n        else{\n            throw std::runtime_error(\"Input value is not a valid double.\");\n        }\n    }\n}",
            "#pragma omp parallel\n\t{\n\t\t// omp for\n\t\tfor (size_t i = 0; i < x.size(); i++) {\n\t\t\tint index = floor(4 * x[i]);\n\t\t\tif (index == 0) {\n\t\t\t\t#pragma omp atomic\n\t\t\t\tbins[0] += 1;\n\t\t\t}\n\t\t\telse if (index == 1) {\n\t\t\t\t#pragma omp atomic\n\t\t\t\tbins[1] += 1;\n\t\t\t}\n\t\t\telse if (index == 2) {\n\t\t\t\t#pragma omp atomic\n\t\t\t\tbins[2] += 1;\n\t\t\t}\n\t\t\telse if (index == 3) {\n\t\t\t\t#pragma omp atomic\n\t\t\t\tbins[3] += 1;\n\t\t\t}\n\t\t}\n\t}\n}",
            "bins.fill(0);\n\n  const double x0 = x[0];\n\n  const double x_end = x.back();\n\n  for (int i = 0; i < x.size(); i++) {\n    const double xi = x[i];\n    if (xi > x0 && xi <= x0 + 0.25) bins[0]++;\n    else if (xi > x0 + 0.25 && xi <= x0 + 0.5) bins[1]++;\n    else if (xi > x0 + 0.5 && xi <= x0 + 0.75) bins[2]++;\n    else if (xi > x0 + 0.75 && xi <= x_end) bins[3]++;\n  }\n}",
            "// TODO: Your code goes here\n\n\tbins.fill(0);\n\tstd::vector<double> x_copy = x;\n\tstd::sort(x_copy.begin(), x_copy.end());\n\tdouble quartile1 = x_copy[0];\n\tdouble quartile2 = x_copy[int(x_copy.size() / 4)];\n\tdouble quartile3 = x_copy[int(2 * x_copy.size() / 4)];\n\tdouble quartile4 = x_copy[int(3 * x_copy.size() / 4)];\n\tfor (auto x_i : x_copy) {\n\t\tif (x_i >= quartile4) {\n\t\t\tbins[3]++;\n\t\t}\n\t\telse if (x_i >= quartile3) {\n\t\t\tbins[2]++;\n\t\t}\n\t\telse if (x_i >= quartile2) {\n\t\t\tbins[1]++;\n\t\t}\n\t\telse if (x_i >= quartile1) {\n\t\t\tbins[0]++;\n\t\t}\n\t}\n}",
            "double start_point = 0.0, end_point = 0.25, tmp;\n  size_t counter = 0;\n\n  #pragma omp parallel for reduction(+:counter)\n  for(int i = 0; i < x.size(); i++) {\n    tmp = x[i];\n    if(tmp >= start_point && tmp < end_point) {\n      counter += 1;\n    }\n  }\n  bins[0] = counter;\n\n  start_point = end_point;\n  end_point += 0.25;\n  counter = 0;\n  #pragma omp parallel for reduction(+:counter)\n  for(int i = 0; i < x.size(); i++) {\n    tmp = x[i];\n    if(tmp >= start_point && tmp < end_point) {\n      counter += 1;\n    }\n  }\n  bins[1] = counter;\n\n  start_point = end_point;\n  end_point += 0.25;\n  counter = 0;\n  #pragma omp parallel for reduction(+:counter)\n  for(int i = 0; i < x.size(); i++) {\n    tmp = x[i];\n    if(tmp >= start_point && tmp < end_point) {\n      counter += 1;\n    }\n  }\n  bins[2] = counter;\n\n  start_point = end_point;\n  end_point = 1.0;\n  counter = 0;\n  #pragma omp parallel for reduction(+:counter)\n  for(int i = 0; i < x.size(); i++) {\n    tmp = x[i];\n    if(tmp >= start_point && tmp < end_point) {\n      counter += 1;\n    }\n  }\n  bins[3] = counter;\n}",
            "// TODO: Your code here.\n\n    size_t n = x.size();\n\n    // initialize\n    bins = {0, 0, 0, 0};\n\n    // sort the input vector\n    std::vector<double> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n\n    // parallel for\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; ++i)\n    {\n        double frac = sortedX[i] - std::trunc(sortedX[i]);\n        if (frac >= 0.75)\n            bins[0] += 1;\n        else if (frac >= 0.5)\n            bins[1] += 1;\n        else if (frac >= 0.25)\n            bins[2] += 1;\n        else\n            bins[3] += 1;\n    }\n}",
            "size_t len = x.size();\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n\n  // openmp pragma\n  #pragma omp parallel for\n  for (size_t i = 0; i < len; i++) {\n    if (x[i] >= 0 && x[i] < 0.25) {\n      bins[0]++;\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      bins[1]++;\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      bins[2]++;\n    } else if (x[i] >= 0.75 && x[i] < 1) {\n      bins[3]++;\n    }\n  }\n}",
            "/* Fill in this function */\n\n\n\n\n\n\n\n\n\n}",
            "// Write your code here\n    int num_threads = 4;\n    #pragma omp parallel num_threads(num_threads) shared(x, bins)\n    {\n        // Create thread specific private arrays\n        size_t private_bins[4];\n        double const start = 0.25;\n        double const increment = 0.25;\n        double const end = 1.0;\n        double const step = end - start;\n        size_t i = 0;\n        #pragma omp for\n        for(double j = start; j < end; j += increment) {\n            private_bins[i++] = std::count_if(x.begin(), x.end(), [j](double d) {\n                return d < j;\n            });\n        }\n        #pragma omp critical\n        {\n            for (size_t k = 0; k < 4; k++) {\n                bins[k] += private_bins[k];\n            }\n        }\n    }\n    // end of countQuartiles\n}",
            "int n = x.size();\n    double q1, q2, q3, q4;\n    double epsilon = 0.25;\n    double sum = 0;\n\n    //omp_set_num_threads(4);\n\n#pragma omp parallel shared(x, epsilon) private(sum, n) reduction(+:sum)\n    {\n        double *local_x = (double *)malloc(sizeof(double) * n);\n        double qt = 0.25;\n        int my_id = omp_get_thread_num();\n\n#pragma omp for\n        for (int i = 0; i < n; i++) {\n            local_x[i] = x[i];\n        }\n\n        sort(local_x, local_x + n);\n\n        if (my_id == 0) {\n            q1 = local_x[0];\n            q2 = local_x[n / 4];\n            q3 = local_x[n / 2];\n            q4 = local_x[n * 3 / 4];\n            bins[0] = count_if(local_x, local_x + n, [q1, epsilon](double d) { return d >= q1 - epsilon; });\n            bins[1] = count_if(local_x, local_x + n, [q2, epsilon](double d) { return d >= q2 - epsilon; });\n            bins[2] = count_if(local_x, local_x + n, [q3, epsilon](double d) { return d >= q3 - epsilon; });\n            bins[3] = count_if(local_x, local_x + n, [q4, epsilon](double d) { return d >= q4 - epsilon; });\n        } else if (my_id == 1) {\n            q1 = local_x[n / 4];\n            q2 = local_x[n / 2];\n            q3 = local_x[n * 3 / 4];\n            q4 = local_x[n - 1];\n            bins[0] = count_if(local_x, local_x + n, [q1, epsilon](double d) { return d >= q1 - epsilon; });\n            bins[1] = count_if(local_x, local_x + n, [q2, epsilon](double d) { return d >= q2 - epsilon; });\n            bins[2] = count_if(local_x, local_x + n, [q3, epsilon](double d) { return d >= q3 - epsilon; });\n            bins[3] = count_if(local_x, local_x + n, [q4, epsilon](double d) { return d >= q4 - epsilon; });\n        } else if (my_id == 2) {\n            q1 = local_x[n / 2];\n            q2 = local_x[n * 3 / 4];\n            q3 = local_x[n - 1];\n            bins[0] = count_if(local_x, local_x + n, [q1, epsilon](double d) { return d >= q1 - epsilon; });\n            bins[1] = count_if(local_x, local_x + n, [q2, epsilon](double d) { return d >= q2 - epsilon; });\n            bins[2] = count_if(local_x, local_x + n, [q3, epsilon](double d) { return d >= q3 - epsilon; });\n        } else if (my_id == 3) {\n            q1 = local_x[n * 3 / 4];\n            q2 = local_x[n - 1];\n            bins[0] = count_if(local_x, local_x + n, [q1, epsilon](double d) { return d >= q1 - epsilon; });\n            bins[1] = count_if(local_x, local_x + n, [q2, epsilon](double d) { return d >= q2 - epsilon; });\n        }\n        free(local_x);\n    }\n}",
            "// TODO: Your code here\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] > 0 && x[i] < 0.25) bins[0]++;\n        if (x[i] > 0.25 && x[i] < 0.5) bins[1]++;\n        if (x[i] > 0.5 && x[i] < 0.75) bins[2]++;\n        if (x[i] > 0.75 && x[i] <= 1) bins[3]++;\n    }\n}",
            "// TODO: your code here\n\n    size_t len = x.size();\n    bins = std::array<size_t, 4>();\n\n    std::sort(x.begin(), x.end());\n    bins[0] = std::count_if(x.begin(), x.begin() + len / 4, [](double d) { return d < 0.25; });\n    bins[1] = std::count_if(x.begin() + len / 4, x.begin() + len / 2, [](double d) { return d < 0.5; });\n    bins[2] = std::count_if(x.begin() + len / 2, x.begin() + 3 * len / 4, [](double d) { return d < 0.75; });\n    bins[3] = std::count_if(x.begin() + 3 * len / 4, x.end(), [](double d) { return d <= 1.0; });\n}",
            "// TODO: your code here\n\t//std::cout << \"bins[0]:\" << bins[0] << std::endl;\n\t//std::cout << \"bins[1]:\" << bins[1] << std::endl;\n\t//std::cout << \"bins[2]:\" << bins[2] << std::endl;\n\t//std::cout << \"bins[3]:\" << bins[3] << std::endl;\n}",
            "// Your code here\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for (size_t i = 0; i < x.size(); ++i) {\n      if (x[i] < 0.25)\n        ++bins[0];\n      else if (x[i] < 0.5)\n        ++bins[1];\n      else if (x[i] < 0.75)\n        ++bins[2];\n      else\n        ++bins[3];\n    }\n  }\n}",
            "double *p = x.data();\n  int n = x.size();\n  int nthr = omp_get_num_threads();\n\n#pragma omp parallel num_threads(nthr) default(shared)\n  {\n#pragma omp for\n    for (int i = 0; i < n; i++) {\n      int tid = omp_get_thread_num();\n      if (p[i] >= 0.0 && p[i] < 0.25) {\n        bins[0]++;\n      } else if (p[i] >= 0.25 && p[i] < 0.5) {\n        bins[1]++;\n      } else if (p[i] >= 0.5 && p[i] < 0.75) {\n        bins[2]++;\n      } else if (p[i] >= 0.75 && p[i] < 1.0) {\n        bins[3]++;\n      } else {\n        printf(\"Warning, input out of range!\\n\");\n      }\n    }\n  }\n}",
            "// TODO: Your code here\n#pragma omp parallel for\n    for (int i = 0; i < 4; i++)\n        bins[i] = 0;\n    for (int i = 0; i < x.size(); i++)\n    {\n        switch (i / 4)\n        {\n        case 0:\n            bins[0]++;\n            break;\n        case 1:\n            bins[1]++;\n            break;\n        case 2:\n            bins[2]++;\n            break;\n        case 3:\n            bins[3]++;\n            break;\n        }\n    }\n    return;\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (x[i] < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (x[i] < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "auto n = x.size();\n    bins = {0, 0, 0, 0};\n    double q0 = 0.0, q1 = 0.25, q2 = 0.5, q3 = 0.75;\n    #pragma omp parallel\n    {\n        #pragma omp for reduction(+:bins[0],bins[1],bins[2],bins[3])\n        for (int i = 0; i < n; ++i) {\n            auto val = x[i];\n            if (val < q0) {\n                ++bins[0];\n            }\n            else if (q0 <= val && val < q1) {\n                ++bins[1];\n            }\n            else if (q1 <= val && val < q2) {\n                ++bins[2];\n            }\n            else if (q2 <= val && val < q3) {\n                ++bins[3];\n            }\n        }\n    }\n}",
            "// TODO: Your code goes here\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0.25) {\n      bins[0]++;\n    }\n    else if (x[i] < 0.5) {\n      bins[1]++;\n    }\n    else if (x[i] < 0.75) {\n      bins[2]++;\n    }\n    else {\n      bins[3]++;\n    }\n  }\n}",
            "size_t const N = x.size();\n    if (N == 0) {\n        bins[0] = 0;\n        bins[1] = 0;\n        bins[2] = 0;\n        bins[3] = 0;\n    }\n    else {\n        std::array<double, 4> quartiles = {0, 0, 0, 0};\n        for (size_t i = 0; i < N; ++i) {\n            if (x[i] < quartiles[0]) {\n                ++bins[0];\n            }\n            else if (x[i] < quartiles[1]) {\n                ++bins[1];\n            }\n            else if (x[i] < quartiles[2]) {\n                ++bins[2];\n            }\n            else if (x[i] < quartiles[3]) {\n                ++bins[3];\n            }\n        }\n    }\n}",
            "int nthreads = omp_get_max_threads();\n  std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n  double const quartile = 1.0 / 4;\n  double const fourth_quartile = 3 * quartile;\n\n#pragma omp parallel shared(x, local_bins)\n  {\n    // Fork a team of threads giving them their own copies of variables\n#pragma omp for\n    for (int i = 0; i < x.size(); ++i) {\n      if (x[i] >= 0 && x[i] <= quartile) {\n        ++local_bins[0];\n      } else if (x[i] > quartile && x[i] <= 2 * quartile) {\n        ++local_bins[1];\n      } else if (x[i] > 2 * quartile && x[i] <= 3 * quartile) {\n        ++local_bins[2];\n      } else if (x[i] > 3 * quartile && x[i] <= fourth_quartile) {\n        ++local_bins[3];\n      } else {\n        // throw invalid_argument(\"Value of x is not in [0, 1]\");\n      }\n    }\n\n#pragma omp critical //region of code where more than one thread may access the same data\n    for (int i = 0; i < 4; ++i) {\n      bins[i] += local_bins[i];\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "serial",
        "prompt": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// std::array<size_t, 4> bins = {0, 0, 0, 0};\n    // double low = 0.0;\n    // double high = 1.0;\n    // for (int i = 0; i < x.size(); i++) {\n    //     if (x[i] < 0.25) bins[0] += 1;\n    //     else if (x[i] < 0.50) bins[1] += 1;\n    //     else if (x[i] < 0.75) bins[2] += 1;\n    //     else bins[3] += 1;\n    // }\n    // return bins;\n    bins = {0, 0, 0, 0};\n    double low = 0.0;\n    double high = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0.25) bins[0] += 1;\n        else if (x[i] < 0.50) bins[1] += 1;\n        else if (x[i] < 0.75) bins[2] += 1;\n        else bins[3] += 1;\n    }\n    return;\n}",
            "double temp;\n    std::vector<double>::const_iterator iter;\n    std::sort(x.begin(), x.end());\n    for (size_t i = 0; i < bins.size(); i++) {\n        temp = x[i * x.size()/4];\n        bins[i] = 0;\n        for (iter = x.begin(); iter!= x.end(); iter++) {\n            if (*iter <= temp) {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "int num_bins = 4;\n    std::vector<double> sorted_x(x);\n    std::sort(sorted_x.begin(), sorted_x.end());\n    for (int i = 0; i < num_bins; ++i) {\n        double start_bound = i * 0.25;\n        double end_bound = (i + 1) * 0.25;\n        int count = 0;\n        for (double num : sorted_x) {\n            if (num >= start_bound && num < end_bound) {\n                ++count;\n            }\n        }\n        bins[i] = count;\n    }\n}",
            "// implement this function using std::count_if\n    bins[0] = std::count_if(x.begin(), x.end(), [](double val){return val < 0.25;});\n    bins[1] = std::count_if(x.begin(), x.end(), [](double val){return val >= 0.25 && val < 0.5;});\n    bins[2] = std::count_if(x.begin(), x.end(), [](double val){return val >= 0.5 && val < 0.75;});\n    bins[3] = std::count_if(x.begin(), x.end(), [](double val){return val >= 0.75;});\n\n    // std::vector<int> bins(4);\n    // for (auto val : x) {\n    //     if (val < 0.25) {\n    //         ++bins[0];\n    //     } else if (val >= 0.25 && val < 0.5) {\n    //         ++bins[1];\n    //     } else if (val >= 0.5 && val < 0.75) {\n    //         ++bins[2];\n    //     } else if (val >= 0.75) {\n    //         ++bins[3];\n    //     }\n    // }\n}",
            "// TODO: implement\n\n}",
            "}",
            "std::array<size_t, 4> counts{};\n    for (size_t i = 0; i < x.size(); ++i) {\n        int quartile;\n        if (x[i] < 0.25) {\n            quartile = 0;\n        } else if (x[i] < 0.5) {\n            quartile = 1;\n        } else if (x[i] < 0.75) {\n            quartile = 2;\n        } else {\n            quartile = 3;\n        }\n        counts[quartile]++;\n    }\n    bins = counts;\n}",
            "std::vector<double> vec;\n    vec.reserve(x.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (std::fmod(x[i], 1.0) == 0.0) {\n            vec.push_back(std::floor(x[i]));\n        }\n        else if (x[i] - std::floor(x[i]) >= 0.25 && x[i] - std::floor(x[i]) < 0.75) {\n            vec.push_back(std::ceil(x[i]));\n        }\n        else {\n            vec.push_back(x[i]);\n        }\n    }\n\n    size_t bin[4];\n    bin[0] = std::count(vec.begin(), vec.end(), 0.0);\n    bin[1] = std::count(vec.begin(), vec.end(), 1.0);\n    bin[2] = std::count(vec.begin(), vec.end(), 2.0);\n    bin[3] = std::count(vec.begin(), vec.end(), 3.0);\n\n    bins[0] = bin[0];\n    bins[1] = bin[1] - bin[0];\n    bins[2] = bin[2] - bin[1];\n    bins[3] = bin[3] - bin[2];\n}",
            "double const quartile1 = 0.25;\n    double const quartile2 = 0.50;\n    double const quartile3 = 0.75;\n    double const quartile4 = 1.00;\n\n    auto count = [&](double const value) {\n        if (value < quartile1) {\n            return 0;\n        } else if (value < quartile2) {\n            return 1;\n        } else if (value < quartile3) {\n            return 2;\n        } else {\n            return 3;\n        }\n    };\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto const bin = count(x[i]);\n        ++bins[bin];\n    }\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "double min = std::numeric_limits<double>::max();\n  double max = std::numeric_limits<double>::min();\n  for(auto n : x) {\n    if(n > max)\n      max = n;\n    if(n < min)\n      min = n;\n  }\n  double diff = max - min;\n\n  size_t i = 0;\n  for(auto n : x) {\n    double frac = (n - min) / diff;\n    if(frac <= 0.25)\n      bins[i]++;\n    else if(frac <= 0.5)\n      bins[i+1]++;\n    else if(frac <= 0.75)\n      bins[i+2]++;\n    else\n      bins[i+3]++;\n  }\n}",
            "size_t n = x.size();\n    for (size_t i = 0; i < n; i++) {\n        double frac = modf(x[i], &bins[0]);\n        if (frac <= 0.25) bins[0]++;\n        else if (frac <= 0.50) bins[1]++;\n        else if (frac <= 0.75) bins[2]++;\n        else bins[3]++;\n    }\n}",
            "// TODO: Your code goes here\n   bins.fill(0);\n\n   for (const auto& i : x) {\n       if(i < 0.25) {\n        bins[0] += 1;\n       }\n       else if (i >= 0.25 && i < 0.5) {\n        bins[1] += 1;\n       }\n       else if (i >= 0.5 && i < 0.75) {\n        bins[2] += 1;\n       }\n       else if (i >= 0.75 && i < 1) {\n        bins[3] += 1;\n       }\n\n       }\n       std::cout << \"The bins contain: \" << bins[0] << \" \" << bins[1] << \" \" << bins[2] << \" \" << bins[3] << std::endl;\n}",
            "// Your code here\n\n}",
            "double x_min = *std::min_element(x.begin(), x.end());\n  double x_max = *std::max_element(x.begin(), x.end());\n  size_t size = x.size();\n  if (size < 1) {\n    std::cout << \"ERROR: countQuartiles() requires a vector with at least 1 element\" << std::endl;\n    return;\n  }\n  if (x_min == x_max) {\n    std::cout << \"ERROR: countQuartiles() requires a vector with at least two different elements\" << std::endl;\n    return;\n  }\n\n  std::vector<size_t> x_bin;\n  x_bin.resize(x.size());\n  for (size_t i = 0; i < x.size(); i++) {\n    x_bin[i] = 0;\n  }\n\n  for (size_t i = 0; i < x.size(); i++) {\n    x_bin[i] = floor((x[i] - x_min) * 4 / (x_max - x_min));\n  }\n\n  std::vector<size_t> temp_x_bin;\n  temp_x_bin.resize(4);\n  for (size_t i = 0; i < 4; i++) {\n    temp_x_bin[i] = 0;\n  }\n\n  for (size_t i = 0; i < x_bin.size(); i++) {\n    temp_x_bin[x_bin[i]] += 1;\n  }\n\n  double min_freq = 1000000;\n  double max_freq = 0;\n  for (size_t i = 0; i < temp_x_bin.size(); i++) {\n    if (temp_x_bin[i] < min_freq) {\n      min_freq = temp_x_bin[i];\n    }\n    if (temp_x_bin[i] > max_freq) {\n      max_freq = temp_x_bin[i];\n    }\n  }\n  max_freq = std::max(max_freq, (double) 1);\n\n  double bin_width = max_freq / 4;\n  for (size_t i = 0; i < 4; i++) {\n    temp_x_bin[i] = floor(temp_x_bin[i] / bin_width);\n    if (temp_x_bin[i] > 0) {\n      bins[i] = temp_x_bin[i];\n    } else {\n      bins[i] = 0;\n    }\n  }\n}",
            "// TODO: YOUR CODE HERE\n}",
            "std::sort(x.begin(), x.end());\n\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    size_t n = x.size();\n    for (size_t i = 0; i < n; i++) {\n        if (x[i] >= 0.0 && x[i] < 0.25) {\n            bins[0]++;\n        } else if (x[i] >= 0.25 && x[i] < 0.5) {\n            bins[1]++;\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            bins[2]++;\n        } else if (x[i] >= 0.75 && x[i] <= 1.0) {\n            bins[3]++;\n        }\n    }\n}",
            "std::sort(x.begin(), x.end());\n    auto n = x.size();\n\n    bins[0] = 0;\n    for (size_t i = 1; i < n; i++) {\n        if (x[i - 1] > 0.75) {\n            bins[0]++;\n        } else if (x[i - 1] > 0.5) {\n            bins[1]++;\n        } else if (x[i - 1] > 0.25) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n    bins[0] = bins[0] + bins[1] + bins[2] + bins[3];\n}",
            "// TODO: Fill this in\n}",
            "//TODO: your code here\n\n}",
            "// sort input array\n    std::sort(x.begin(), x.end());\n\n    // bin counts\n    bins[0] = std::count_if(x.begin(), x.end(), [](double i) { return i < 0.25; });\n    bins[1] = std::count_if(x.begin(), x.end(), [](double i) { return i >= 0.25 && i < 0.5; });\n    bins[2] = std::count_if(x.begin(), x.end(), [](double i) { return i >= 0.5 && i < 0.75; });\n    bins[3] = std::count_if(x.begin(), x.end(), [](double i) { return i >= 0.75; });\n}",
            "}",
            "// TODO: implement\n    // std::sort(x.begin(), x.end());\n    // std::cout << x[0];\n    // bins[0] = 0;\n    // for (auto i = 1; i < x.size(); i++){\n    //     if (x[i] - x[i-1] < 0.5)\n    //         bins[0]++;\n    //     else\n    //         break;\n    // }\n    // for (auto i = 1; i < bins.size(); i++){\n    //     bins[i] = 0;\n    // }\n    // for (auto i = 0; i < x.size(); i++){\n    //     for (auto j = 0; j < bins.size(); j++){\n    //         if (x[i] - x[i-1] >= 0.25 * j + 0.25 * (j + 1) && x[i] - x[i-1] < 0.25 * j + 0.25 * (j + 1)){\n    //             bins[j]++;\n    //             break;\n    //         }\n    //     }\n    // }\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    for (auto i = 0; i < x.size(); i++){\n        if (x[i] >= 0.0 && x[i] < 0.25){\n            bins[0]++;\n        }\n        else if (x[i] >= 0.25 && x[i] < 0.5){\n            bins[1]++;\n        }\n        else if (x[i] >= 0.5 && x[i] < 0.75){\n            bins[2]++;\n        }\n        else if (x[i] >= 0.75){\n            bins[3]++;\n        }\n        else{\n            std::cout << \"ERROR\";\n        }\n    }\n}",
            "// Create an unordered map from the double value to the number of occurances\n  std::unordered_map<double, size_t> x_bin;\n\n  for (auto const& val: x) {\n    x_bin[val] = 0;\n  }\n\n  for (auto const& [val, count]: x_bin) {\n    if (val < 0.25) {\n      ++bins[0];\n    } else if (val >= 0.25 && val < 0.5) {\n      ++bins[1];\n    } else if (val >= 0.5 && val < 0.75) {\n      ++bins[2];\n    } else {\n      ++bins[3];\n    }\n  }\n}",
            "// You need to write this function\n    // \n    // There are many ways to do this, but one way is to use a pair of iterators to \n    // iterate over the input vector, and count the number of elements in the two\n    // halves of the vector that have fractional parts in the intervals [0, 0.25), \n    // [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Then, you can do a final step to \n    // check if there's a remainder.\n\n    // First, create iterators to the beginning and the end of the vector.\n    auto first = x.cbegin();\n    auto last = x.cend();\n\n    // Now use the `std::partition` function to partition the vector into four halves. \n    // You can use a lambda function that returns true for the first half and false \n    // for the other three.\n    // \n    // Note: `std::partition` returns a pair of iterators, so you'll need to assign \n    // them to separate variables, first and third.\n    auto firstHalfEnd = std::partition(first, last, [](auto const& d) { return true; });\n    auto secondHalfBegin = std::partition(firstHalfEnd, last, [](auto const& d) { return false; });\n    auto secondHalfEnd = std::partition(secondHalfBegin, last, [](auto const& d) { return true; });\n    auto thirdHalfBegin = std::partition(secondHalfEnd, last, [](auto const& d) { return false; });\n\n    // We now have four iterators pointing to the first element of each of the four \n    // halves. Use `std::distance` to count the number of elements in each of the \n    // four halves.\n    bins[0] = std::distance(first, firstHalfEnd);\n    bins[1] = std::distance(firstHalfEnd, secondHalfBegin);\n    bins[2] = std::distance(secondHalfBegin, secondHalfEnd);\n    bins[3] = std::distance(secondHalfEnd, thirdHalfBegin);\n\n    // If there's a remainder, add one to the appropriate bin. We can use a lambda\n    // function that returns true if the fractional part of the double is in the \n    // interval [0.25, 0.5).\n    bool fractionInFirstHalf = std::any_of(firstHalfEnd, secondHalfBegin, [](auto const& d) { return d - std::trunc(d) < 0.25; });\n    if (fractionInFirstHalf) {\n        ++bins[0];\n    }\n    bool fractionInSecondHalf = std::any_of(secondHalfBegin, secondHalfEnd, [](auto const& d) { return d - std::trunc(d) < 0.25; });\n    if (fractionInSecondHalf) {\n        ++bins[1];\n    }\n    bool fractionInThirdHalf = std::any_of(secondHalfEnd, thirdHalfBegin, [](auto const& d) { return d - std::trunc(d) < 0.25; });\n    if (fractionInThirdHalf) {\n        ++bins[2];\n    }\n    bool fractionInFourthHalf = std::any_of(thirdHalfBegin, last, [](auto const& d) { return d - std::trunc(d) < 0.25; });\n    if (fractionInFourthHalf) {\n        ++bins[3];\n    }\n}",
            "std::sort(x.begin(), x.end());\n\tfor(int i=0; i<4; ++i) {\n\t\tbins[i] = 0;\n\t}\n\tfor(double xi : x) {\n\t\tint i = std::floor(xi / 25);\n\t\tif(i >= 4) {\n\t\t\ti = 3;\n\t\t}\n\t\t++bins[i];\n\t}\n}",
            "bins.fill(0);\n  size_t count = x.size();\n  size_t bin_idx = 0;\n\n  if (count == 0) {\n    return;\n  }\n\n  for (double val : x) {\n    if (val < 0.25) {\n      ++bins[bin_idx];\n      bin_idx = 0;\n    }\n    else if (val < 0.5) {\n      ++bins[bin_idx];\n      bin_idx = 1;\n    }\n    else if (val < 0.75) {\n      ++bins[bin_idx];\n      bin_idx = 2;\n    }\n    else {\n      ++bins[bin_idx];\n      bin_idx = 3;\n    }\n  }\n}",
            "double min_val = std::numeric_limits<double>::max();\n    double max_val = -std::numeric_limits<double>::max();\n    for (auto it : x) {\n        min_val = (it < min_val)? it : min_val;\n        max_val = (it > max_val)? it : max_val;\n    }\n\n    double mid = (min_val + max_val) / 2;\n    double mid2 = (min_val + max_val) / 4;\n\n    bins.fill(0);\n    for (auto it : x) {\n        if (it < mid)\n            bins[0]++;\n        else if (it < mid2)\n            bins[1]++;\n        else if (it < mid + mid2)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n\n    // std::cout << \"Min: \" << min_val << \" Max: \" << max_val << std::endl;\n    // std::cout << \"Mid: \" << mid << \" Mid2: \" << mid2 << std::endl;\n\n    // for (auto it : x) {\n    //     std::cout << it << \" \";\n    // }\n    // std::cout << std::endl;\n\n    // for (auto it : bins) {\n    //     std::cout << it << \" \";\n    // }\n    // std::cout << std::endl;\n}",
            "std::sort(x.begin(), x.end());\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    for (auto d : x) {\n        if (d >= 0 && d < 0.25) {\n            bins[0] += 1;\n        } else if (d >= 0.25 && d < 0.5) {\n            bins[1] += 1;\n        } else if (d >= 0.5 && d < 0.75) {\n            bins[2] += 1;\n        } else if (d >= 0.75 && d < 1) {\n            bins[3] += 1;\n        } else {\n            std::cout << \"Error: \" << d << std::endl;\n        }\n    }\n}",
            "}",
            "std::vector<double> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n\n    std::vector<double> low;\n    std::vector<double> midLow;\n    std::vector<double> midHigh;\n    std::vector<double> high;\n\n    // split the sorted vector into 4 segments, then count the number of doubles in each segment\n    for (size_t i = 0; i < sortedX.size(); i++) {\n        if (sortedX[i] < 0.25) {\n            low.push_back(sortedX[i]);\n        }\n        else if (sortedX[i] >= 0.25 && sortedX[i] < 0.5) {\n            midLow.push_back(sortedX[i]);\n        }\n        else if (sortedX[i] >= 0.5 && sortedX[i] < 0.75) {\n            midHigh.push_back(sortedX[i]);\n        }\n        else {\n            high.push_back(sortedX[i]);\n        }\n    }\n\n    bins[0] = low.size();\n    bins[1] = midLow.size();\n    bins[2] = midHigh.size();\n    bins[3] = high.size();\n}",
            "bins.fill(0);\n\n   // We'll use a binary search to find the quartile boundaries.\n   // These are the limits of the sorted vector x.\n   std::vector<double> x_sorted = x;\n   std::sort(x_sorted.begin(), x_sorted.end());\n   auto start = x_sorted.begin();\n   auto end = x_sorted.end();\n\n   // We'll loop through each quartile, counting the number of values in\n   // each range.\n   double limit;\n   for (size_t i = 0; i < bins.size(); i++) {\n      // Find the lower bound of the range of values we want.\n      limit = *std::lower_bound(x_sorted.begin(), x_sorted.end(), 0.25 * (i + 1));\n      start = std::lower_bound(x_sorted.begin(), end, limit);\n      bins[i] = std::distance(start, end);\n\n      // Find the upper bound of the range of values we want.\n      limit = *std::lower_bound(x_sorted.begin(), x_sorted.end(), 0.75 * (i + 1));\n      end = std::lower_bound(start, x_sorted.end(), limit);\n   }\n}",
            "//std::vector<double> x(xin.begin(), xin.end());\n   size_t n = x.size();\n   std::sort(x.begin(), x.end());\n   std::vector<size_t> v;\n   for (size_t i = 0; i < n; ++i) {\n      if (x[i] >= 0.25*n && x[i] < 0.5*n)\n         ++bins[0];\n      else if (x[i] >= 0.5*n && x[i] < 0.75*n)\n         ++bins[1];\n      else if (x[i] >= 0.75*n && x[i] <= 1.0*n)\n         ++bins[2];\n      else if (x[i] < 0.25*n)\n         ++bins[3];\n   }\n}",
            "size_t n = x.size();\n   std::array<size_t, 4> counts = {0,0,0,0};\n   for (size_t i = 0; i < n; ++i) {\n      if (x[i] <= 0.25) ++counts[0];\n      else if (x[i] <= 0.5) ++counts[1];\n      else if (x[i] <= 0.75) ++counts[2];\n      else ++counts[3];\n   }\n   bins = counts;\n}",
            "// Fill in your answer here.\n    // You can use the helper functions in the file quad.h\n}",
            "// TODO: Your code here\n\n    // sort vector x\n    std::sort(x.begin(), x.end());\n\n    // count the number of doubles in the vector x that have a fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n    bins[0] = std::count_if(x.begin(), x.end(), [](double n) {return n - (int)n < 0.25;});\n    bins[1] = std::count_if(x.begin(), x.end(), [](double n) {return n - (int)n >= 0.25 && n - (int)n < 0.5;});\n    bins[2] = std::count_if(x.begin(), x.end(), [](double n) {return n - (int)n >= 0.5 && n - (int)n < 0.75;});\n    bins[3] = std::count_if(x.begin(), x.end(), [](double n) {return n - (int)n >= 0.75;});\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        double number = x[i];\n        if (number < 0) {\n            number = 0;\n        }\n        if (number >= 1) {\n            number = 1;\n        }\n        int intPart = floor(number);\n        int fracPart = floor(number * 4);\n        if (fracPart == 0) {\n            bins[0] += 1;\n        } else if (fracPart == 1) {\n            bins[1] += 1;\n        } else if (fracPart == 2) {\n            bins[2] += 1;\n        } else if (fracPart == 3) {\n            bins[3] += 1;\n        }\n    }\n}",
            "size_t count[4] = {0, 0, 0, 0};\n    for (auto& num : x) {\n        int index = std::floor(num * 4);\n        count[index] += 1;\n    }\n    bins = std::array<size_t, 4>{count[0], count[1], count[2], count[3]};\n}",
            "// The size of x must be a multiple of 4\n   assert(x.size() % 4 == 0);\n\n   // Make a vector of pointers to x\n   std::vector<double const*> pv;\n   for (auto it = x.begin(); it!= x.end(); ++it)\n      pv.push_back(&(*it));\n\n   // Now sort the pointers\n   std::sort(pv.begin(), pv.end());\n\n   // Now loop through the pointers and count the elements\n   for (int i = 0; i < 4; ++i) {\n      int count = 0;\n      auto p = pv[i];\n      while (p!= pv[i+3]) {\n         count++;\n         p++;\n      }\n      bins[i] = count;\n   }\n\n}",
            "std::sort(x.begin(), x.end());\n  size_t count_1 = 0;\n  size_t count_2 = 0;\n  size_t count_3 = 0;\n  size_t count_4 = 0;\n\n  size_t n = x.size();\n  for (size_t i = 0; i < n; i++) {\n    if (x[i] <= 0.25)\n      count_1++;\n    else if (x[i] <= 0.5)\n      count_2++;\n    else if (x[i] <= 0.75)\n      count_3++;\n    else\n      count_4++;\n  }\n  bins[0] = count_1;\n  bins[1] = count_2;\n  bins[2] = count_3;\n  bins[3] = count_4;\n}",
            "bins = {0,0,0,0};\n   size_t x_size = x.size();\n   if(x_size == 0) return;\n   std::sort(x.begin(), x.end());\n   size_t q1 = x_size/4;\n   size_t q2 = q1 * 2;\n   size_t q3 = q2 + q1;\n   size_t q4 = x_size;\n   for(size_t i = 0; i < x_size; i++) {\n      if(i < q1) bins[0]++;\n      else if(i < q2) bins[1]++;\n      else if(i < q3) bins[2]++;\n      else bins[3]++;\n   }\n}",
            "// Make a copy of the input vector because we'll sort it.\n    std::vector<double> x_copy(x);\n\n    // Sort the copy of the vector x.\n    std::sort(x_copy.begin(), x_copy.end());\n\n    // Count the number of elements in each quarter of the sorted vector.\n    size_t i;\n    size_t num = x_copy.size();\n    double min = x_copy[0];\n    double max = x_copy[num - 1];\n    double mid = (min + max)/2;\n    double third = (min + max)/3;\n    for(i = 0; i < num; i++) {\n        if(x_copy[i] < third) {\n            bins[0]++;\n        } else if(x_copy[i] < mid) {\n            bins[1]++;\n        } else if(x_copy[i] < (2 * mid)) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "bins.fill(0);\n    size_t index = 0;\n    for (auto val : x) {\n        double fractional = std::fmod(val, 1.0);\n        if (fractional < 0.25) {\n            ++bins[index];\n        }\n        else if (fractional < 0.5) {\n            ++bins[1 + index];\n        }\n        else if (fractional < 0.75) {\n            ++bins[2 + index];\n        }\n        else {\n            ++bins[3 + index];\n        }\n        ++index;\n    }\n}",
            "bins.fill(0);\n    for (auto const& v : x) {\n        if (v < 1.0)\n            bins[0]++;\n        else if (v < 2.0)\n            bins[1]++;\n        else if (v < 3.0)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "std::sort(x.begin(), x.end());\n    // std::cout << \"vector sorted: \";\n    // for (auto e : x) {\n    //     std::cout << e << \" \";\n    // }\n    // std::cout << \"\\n\";\n\n    double quarter = x.size() / 4;\n    bins[0] = 0;\n    for (auto i = 0; i < x.size(); ++i) {\n        if (i < quarter) {\n            if (x[i] > 0 && x[i] < 0.25) {\n                ++bins[0];\n            }\n        } else if (i < quarter * 2) {\n            if (x[i] > 0.25 && x[i] < 0.5) {\n                ++bins[1];\n            }\n        } else if (i < quarter * 3) {\n            if (x[i] > 0.5 && x[i] < 0.75) {\n                ++bins[2];\n            }\n        } else {\n            if (x[i] > 0.75 && x[i] < 1) {\n                ++bins[3];\n            }\n        }\n    }\n}",
            "double x_min = *std::min_element(x.begin(), x.end());\n    double x_max = *std::max_element(x.begin(), x.end());\n    std::vector<double> x_sorted = x;\n    std::sort(x_sorted.begin(), x_sorted.end());\n\n    size_t n = x.size();\n\n    // We start with the smallest value and count\n    // how many values are in each bin.\n    // We take the largest value, and count\n    // how many values are in each bin.\n    // Then we go through each value and add the counts.\n    double x_bin_min = x_min;\n    size_t bin_index = 0;\n    for (size_t i = 0; i < 4; i++) {\n        double bin_max = x_bin_min + n * 0.25;\n        while (x_sorted[bin_index] < bin_max) {\n            bins[i] += 1;\n            bin_index += 1;\n            if (bin_index >= n) {\n                break;\n            }\n        }\n        x_bin_min = bin_max;\n    }\n\n}",
            "assert(x.size() > 0);\n    std::sort(x.begin(), x.end());\n\n    // 1st quartile is the median of the lower half\n    double firstQ = x[(x.size() - 1) / 2];\n\n    // 2nd quartile is the median of the upper half\n    double secondQ = x[x.size() / 2];\n\n    // 3rd quartile is the median of the upper half\n    double thirdQ = x[x.size() / 2 + (x.size() - 1) / 2];\n\n    // 4th quartile is the median of the upper half\n    double fourthQ = x[x.size() - 1];\n\n    // lower 25% of the data is between 0 and 25%\n    bins[0] = count_if(x.begin(), x.begin() + (x.size() - 1) / 2, [&](double x) {\n        return x <= firstQ;\n    });\n\n    // upper 25% of the data is between 25% and 50%\n    bins[1] = count_if(x.begin() + (x.size() - 1) / 2, x.end(), [&](double x) {\n        return x <= secondQ;\n    });\n\n    // lower 50% of the data is between 50% and 75%\n    bins[2] = count_if(x.begin() + (x.size() - 1) / 2, x.begin() + (x.size() - 1) / 2 + (x.size() - 1) / 2, [&](double x) {\n        return x <= thirdQ;\n    });\n\n    // upper 50% of the data is between 75% and 100%\n    bins[3] = count_if(x.begin() + (x.size() - 1) / 2 + (x.size() - 1) / 2, x.end(), [&](double x) {\n        return x <= fourthQ;\n    });\n\n    std::cout << \"First quartile: \" << firstQ << \" Second quartile: \" << secondQ << \" Third quartile: \" << thirdQ << \" Fourth quartile: \" << fourthQ << std::endl;\n}",
            "std::sort(x.begin(), x.end());\n\n\tfor (int i = 0; i < 4; i++) {\n\t\tbins[i] = 0;\n\t}\n\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i] >= 0 && x[i] < 0.25) {\n\t\t\tbins[0]++;\n\t\t} else if (x[i] >= 0.25 && x[i] < 0.5) {\n\t\t\tbins[1]++;\n\t\t} else if (x[i] >= 0.5 && x[i] < 0.75) {\n\t\t\tbins[2]++;\n\t\t} else if (x[i] >= 0.75 && x[i] <= 1) {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "/*\n      YOUR CODE HERE\n    */\n    for (double i : x) {\n        if (i < 0.25) {\n            bins[0]++;\n        }\n        else if (i < 0.5) {\n            bins[1]++;\n        }\n        else if (i < 0.75) {\n            bins[2]++;\n        }\n        else {\n            bins[3]++;\n        }\n    }\n}",
            "assert(x.size() >= 1);\n    std::sort(x.begin(), x.end());\n\n    // Check if the input is empty or only one element.\n    if (x.size() == 1) {\n        bins[0] = 0;\n        bins[1] = 0;\n        bins[2] = 0;\n        bins[3] = 1;\n        return;\n    }\n\n    // Check if the input is less than 4 elements.\n    if (x.size() <= 4) {\n        size_t i = x.size() - 1;\n        bins[0] = 0;\n        bins[1] = 0;\n        bins[2] = 0;\n        while (i >= 2) {\n            bins[3] = bins[3] + 1;\n            i -= 2;\n        }\n        bins[2] = bins[3] + 1;\n        bins[1] = bins[2] + 1;\n        bins[0] = bins[1] + 1;\n        return;\n    }\n\n    // Compute the number of elements of the vector x that are less than\n    // 0.25, 0.5, 0.75 and 1.\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    for (auto val: x) {\n        if (val < 0.25) {\n            bins[0] += 1;\n        }\n        else if (val < 0.5) {\n            bins[1] += 1;\n        }\n        else if (val < 0.75) {\n            bins[2] += 1;\n        }\n        else {\n            bins[3] += 1;\n        }\n    }\n}",
            "assert(x.size() > 0);\n    std::array<double, 4> quartiles;\n    quartiles[0] = quartile(x, 0.25);\n    quartiles[1] = quartile(x, 0.5);\n    quartiles[2] = quartile(x, 0.75);\n    quartiles[3] = quartiles[1];\n    std::array<double, 4> fractions;\n    for (int i = 0; i < 4; ++i) {\n        if (i == 0) fractions[i] = 0.0;\n        else fractions[i] = quartiles[i] - quartiles[i - 1];\n    }\n    for (auto const& xi : x) {\n        auto f = std::min(std::max(0.0, (xi - quartiles[0]) / fractions[0]), 1.0);\n        if (f <= fractions[0] / 4.0) {\n            ++bins[0];\n        } else if (f <= fractions[0] / 2.0) {\n            ++bins[1];\n        } else if (f <= fractions[0] * 3.0 / 4.0) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "std::sort(x.begin(), x.end());\n  auto const first = x.begin();\n  auto const last = x.end();\n  std::array<double, 4> quartiles = { 0.25, 0.5, 0.75, 1.0 };\n  bins = std::array<size_t, 4>{};\n  for(auto& q : quartiles) {\n    bins[q - 0.25] = std::distance(first, std::upper_bound(first, last, q));\n  }\n}",
            "bins[0] = 0; // 0.00 - 0.25\n    bins[1] = 0; // 0.25 - 0.50\n    bins[2] = 0; // 0.50 - 0.75\n    bins[3] = 0; // 0.75 - 1.00\n\n    for (int i=0; i<x.size(); i++) {\n        if (x[i] < 0.25) {\n            bins[0]++;\n        } else if (x[i] >= 0.25 && x[i] < 0.50) {\n            bins[1]++;\n        } else if (x[i] >= 0.50 && x[i] < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "bins = {0, 0, 0, 0};\n    int counter = 0;\n    int num_bins = 4;\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] > 0 && x[i] <= 0.25) {\n            bins[0] += 1;\n            counter++;\n        } else if (x[i] > 0.25 && x[i] <= 0.5) {\n            bins[1] += 1;\n            counter++;\n        } else if (x[i] > 0.5 && x[i] <= 0.75) {\n            bins[2] += 1;\n            counter++;\n        } else if (x[i] > 0.75) {\n            bins[3] += 1;\n            counter++;\n        }\n    }\n    // std::cout << \"counter: \" << counter << \"\\n\";\n    // std::cout << \"bins[0]: \" << bins[0] << \"\\n\";\n    // std::cout << \"bins[1]: \" << bins[1] << \"\\n\";\n    // std::cout << \"bins[2]: \" << bins[2] << \"\\n\";\n    // std::cout << \"bins[3]: \" << bins[3] << \"\\n\";\n    if (counter!= x.size()) {\n        throw std::invalid_argument(\"Not all values fit in one of the four bins.\");\n    }\n}",
            "// The count of elements in each bin\n\tsize_t count_0_25 = 0;\n\tsize_t count_0_5 = 0;\n\tsize_t count_0_75 = 0;\n\tsize_t count_1 = 0;\n\n\t// Iterate over the elements in the vector and update the counts in the appropriate bin\n\tfor (double const& value : x) {\n\t\tif (value >= 0.0 && value < 0.25) {\n\t\t\t++count_0_25;\n\t\t}\n\t\telse if (value >= 0.25 && value < 0.5) {\n\t\t\t++count_0_5;\n\t\t}\n\t\telse if (value >= 0.5 && value < 0.75) {\n\t\t\t++count_0_75;\n\t\t}\n\t\telse if (value >= 0.75 && value <= 1.0) {\n\t\t\t++count_1;\n\t\t}\n\t}\n\n\t// Store the counts in an array\n\tbins[0] = count_0_25;\n\tbins[1] = count_0_5;\n\tbins[2] = count_0_75;\n\tbins[3] = count_1;\n}",
            "// Your code here\n   bins = std::array<size_t, 4>{};\n   std::sort(x.begin(), x.end());\n   bins[0] = std::distance(x.begin(), std::upper_bound(x.begin(), x.end(), 0.25));\n   bins[1] = std::distance(x.begin(), std::upper_bound(x.begin(), x.end(), 0.5));\n   bins[2] = std::distance(x.begin(), std::upper_bound(x.begin(), x.end(), 0.75));\n   bins[3] = x.size();\n   return;\n}",
            "size_t N = x.size();\n\n  std::vector<double> x_copy = x;\n  sort(x_copy.begin(), x_copy.end());\n\n  std::vector<double> quartiles;\n\n  quartiles.push_back(0);\n  quartiles.push_back(x_copy[N/4]);\n  quartiles.push_back(x_copy[N/2]);\n  quartiles.push_back(x_copy[3*N/4]);\n\n  for (size_t i = 0; i < 4; i++) {\n    bins[i] = 0;\n    for (size_t j = 0; j < N; j++) {\n      if (x_copy[j] >= quartiles[i] && x_copy[j] < quartiles[i+1]) {\n        bins[i] += 1;\n      }\n    }\n  }\n}",
            "std::vector<double> copy;\n    copy.assign(x.begin(), x.end());\n    std::sort(copy.begin(), copy.end());\n\n    double min = copy[0];\n    double max = copy[copy.size() - 1];\n    double mid = min + (max - min) / 2;\n    double quarter = min + (max - min) / 4;\n\n    // std::cout << \"min: \" << min << \", mid: \" << mid << \", quarter: \" << quarter << std::endl;\n\n    for (size_t i = 0; i < copy.size(); ++i) {\n        if (copy[i] < mid) {\n            bins[0]++;\n        }\n        else if (copy[i] < quarter) {\n            bins[1]++;\n        }\n        else if (copy[i] < 0.75 * mid) {\n            bins[2]++;\n        }\n        else {\n            bins[3]++;\n        }\n    }\n}",
            "int numQuartiles = (int) x.size() / 4;\n  bins.fill(0);\n\n  for (int i = 0; i < numQuartiles; ++i) {\n    auto x_i = x[i];\n    auto x_i_1 = x[i + numQuartiles];\n    auto x_i_2 = x[i + 2 * numQuartiles];\n    auto x_i_3 = x[i + 3 * numQuartiles];\n\n    if (x_i <= 0.25) {\n      ++bins[0];\n    }\n    else if (x_i > 0.25 && x_i <= 0.5) {\n      ++bins[1];\n    }\n    else if (x_i > 0.5 && x_i <= 0.75) {\n      ++bins[2];\n    }\n    else {\n      ++bins[3];\n    }\n\n    if (x_i_1 <= 0.25) {\n      ++bins[0];\n    }\n    else if (x_i_1 > 0.25 && x_i_1 <= 0.5) {\n      ++bins[1];\n    }\n    else if (x_i_1 > 0.5 && x_i_1 <= 0.75) {\n      ++bins[2];\n    }\n    else {\n      ++bins[3];\n    }\n\n    if (x_i_2 <= 0.25) {\n      ++bins[0];\n    }\n    else if (x_i_2 > 0.25 && x_i_2 <= 0.5) {\n      ++bins[1];\n    }\n    else if (x_i_2 > 0.5 && x_i_2 <= 0.75) {\n      ++bins[2];\n    }\n    else {\n      ++bins[3];\n    }\n\n    if (x_i_3 <= 0.25) {\n      ++bins[0];\n    }\n    else if (x_i_3 > 0.25 && x_i_3 <= 0.5) {\n      ++bins[1];\n    }\n    else if (x_i_3 > 0.5 && x_i_3 <= 0.75) {\n      ++bins[2];\n    }\n    else {\n      ++bins[3];\n    }\n  }\n\n  if (x.size() % 4!= 0) {\n    auto x_i = x[numQuartiles];\n    auto x_i_1 = x[2 * numQuartiles];\n    auto x_i_2 = x[3 * numQuartiles];\n    auto x_i_3 = x[4 * numQuartiles];\n\n    if (x_i <= 0.25) {\n      ++bins[0];\n    }\n    else if (x_i > 0.25 && x_i <= 0.5) {\n      ++bins[1];\n    }\n    else if (x_i > 0.5 && x_i <= 0.75) {\n      ++bins[2];\n    }\n    else {\n      ++bins[3];\n    }\n\n    if (x_i_1 <= 0.25) {\n      ++bins[0];\n    }\n    else if (x_i_1 > 0.25 && x_i_1 <= 0.5) {\n      ++bins[1];\n    }\n    else if (x_i_1 > 0.5 && x_i_1 <= 0.75) {\n      ++bins[2];\n    }\n    else {\n      ++bins[3];\n    }\n\n    if (x_i_2 <= 0.25) {\n      ++bins[0];\n    }\n    else if (x_i_2 > 0.25 && x_i_2 <= 0.5) {\n      ++bins[1];\n    }\n    else if (x_i_2 > 0.5 && x_i_2 <=",
            "size_t N = x.size();\n\n    // sort the vector x\n    std::vector<double> sorted(x);\n    std::sort(sorted.begin(), sorted.end());\n\n    // loop over bins, and count the number of elements in x whose fractional part\n    // is in [bin_start, bin_end)\n    for (size_t i = 0; i < 4; i++) {\n        size_t bin_start = i * N/4;\n        size_t bin_end = (i+1) * N/4;\n\n        // find the index of the first element whose fractional part is in [bin_start, bin_end)\n        size_t idx = 0;\n        while (sorted[idx] < bin_start)\n            ++idx;\n\n        // find the index of the last element whose fractional part is in [bin_start, bin_end)\n        size_t last = idx;\n        while (last < sorted.size() && sorted[last] < bin_end)\n            ++last;\n\n        // store the number of elements in x whose fractional part is in [bin_start, bin_end)\n        bins[i] = last - idx;\n    }\n}",
            "std::sort(x.begin(), x.end());\n\n    for (int i = 0; i < 4; i++) {\n        bins[i] = 0;\n    }\n\n    // get the fractional part of each element in x\n    for (size_t i = 0; i < x.size(); i++) {\n        int index = 0;\n        double frac = modf(x[i], &index);\n        frac *= 4;\n        if (frac >= 0.75) {\n            frac = 3;\n        } else if (frac >= 0.5) {\n            frac = 2;\n        } else if (frac >= 0.25) {\n            frac = 1;\n        }\n        bins[frac]++;\n    }\n}",
            "// Copy the input vector x into a new vector y.\n  std::vector<double> y(x);\n\n  // Sort the new vector y.\n  std::sort(y.begin(), y.end());\n\n  // Initialize the bins.\n  bins = {0, 0, 0, 0};\n\n  // Determine the upper and lower bound of the first bin.\n  double lower = 0.0;\n  double upper = 0.25;\n\n  // Loop over the new vector y and determine the bin that each element is in.\n  for (auto const& element : y) {\n    if (element < lower) {\n      ++bins[0];\n    }\n    else if (element < upper) {\n      ++bins[1];\n    }\n    else if (element < 0.5 + upper) {\n      ++bins[2];\n    }\n    else {\n      ++bins[3];\n    }\n  }\n}",
            "std::array<double, 4> binBoundaries{0.25, 0.5, 0.75, 1.0};\n    std::array<size_t, 4> counts{};\n    for (double const& xi : x) {\n        size_t i = 0;\n        while (xi >= binBoundaries[i] && i < binBoundaries.size() - 1) {\n            ++i;\n        }\n        ++counts[i];\n    }\n    bins = counts;\n}",
            "std::sort(x.begin(), x.end());\n\n    size_t n = x.size();\n    bins[0] = 0;\n    for (size_t i = 0; i < n; ++i) {\n        if (x[i] < 0.25) {\n            bins[0]++;\n        } else if (x[i] < 0.5) {\n            bins[1]++;\n        } else if (x[i] < 0.75) {\n            bins[2]++;\n        } else if (x[i] < 1.0) {\n            bins[3]++;\n        }\n    }\n\n}",
            "bins.fill(0);\n    size_t n = x.size();\n    if (n == 0) return;\n    std::sort(x.begin(), x.end());\n    std::vector<double> x2(x.size());\n    for (size_t i = 0; i < n; ++i) {\n        x2[i] = (x[i] - std::floor(x[i])) * 4;\n    }\n    std::sort(x2.begin(), x2.end());\n    size_t i = 0;\n    while (i < n && x2[i] < 1) ++i;\n    while (i < n && x2[i] < 2) ++bins[0], ++i;\n    while (i < n && x2[i] < 3) ++bins[1], ++i;\n    while (i < n && x2[i] < 4) ++bins[2], ++i;\n    while (i < n) ++bins[3], ++i;\n}",
            "// COMPLETE THIS\n  auto n = x.size();\n  std::sort(x.begin(), x.end());\n  for (auto i = 0; i < 4; ++i) {\n    bins[i] = 0;\n  }\n  for (auto i = 0; i < n; ++i) {\n    auto index = (i + 1) / 4;\n    if (i % 4 == 0) {\n      bins[index] += 1;\n    }\n  }\n}",
            "// Fill in the body of the function here.\n}",
            "std::array<double, 4> quartiles = {0.25, 0.5, 0.75, 1.0};\n    for (size_t i = 0; i < 4; i++) {\n        bins[i] = 0;\n        double lowerBound = quartiles[i] * 10.0;\n        double upperBound = (quartiles[i] + 0.25) * 10.0;\n        for (double number : x) {\n            if (number >= lowerBound && number < upperBound) {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "// YOUR CODE GOES HERE\n}",
            "if (x.empty()) {\n    bins = {0, 0, 0, 0};\n    return;\n  }\n  // Sort the elements in x.\n  std::vector<double> y(x);\n  std::sort(y.begin(), y.end());\n\n  size_t i = 0;\n  size_t n = y.size();\n  double firstQuartile, thirdQuartile;\n\n  // firstQuartile is defined as the smallest x in x such that [x, x+1) has\n  // 40% of the data.\n  // thirdQuartile is defined as the largest x in x such that [x-1, x) has\n  // 40% of the data.\n  firstQuartile = y[(4*n)/10];\n  thirdQuartile = y[((13*n)/10)-1];\n\n  while (y[i] <= firstQuartile) {\n    ++i;\n  }\n  bins[0] = i;\n  while (y[i] <= thirdQuartile) {\n    ++i;\n  }\n  bins[1] = i;\n  while (y[i] <= thirdQuartile + ((10*thirdQuartile-firstQuartile)/4)) {\n    ++i;\n  }\n  bins[2] = i;\n  while (i!= n) {\n    ++i;\n  }\n  bins[3] = i;\n}",
            "std::sort(x.begin(), x.end());\n    for (int i = 0; i < 4; i++) {\n        bins[i] = 0;\n    }\n    auto i = x.begin();\n    for (int i = 0; i < 4; i++) {\n        while (i < x.size() && *i <= 0.25) {\n            i++;\n        }\n        while (i < x.size() && *i <= 0.5) {\n            i++;\n        }\n        while (i < x.size() && *i <= 0.75) {\n            i++;\n        }\n        while (i < x.size() && *i <= 1.0) {\n            i++;\n        }\n    }\n}",
            "std::sort(x.begin(), x.end());\n    bins[0] = std::count_if(x.begin(), x.begin() + x.size() / 4, \n        [](double const& x) { return x % 1 < 0.25; });\n    bins[1] = std::count_if(x.begin() + x.size() / 4, x.begin() + x.size() / 2,\n        [](double const& x) { return x % 1 >= 0.25 && x % 1 < 0.5; });\n    bins[2] = std::count_if(x.begin() + x.size() / 2, x.begin() + 3 * x.size() / 4,\n        [](double const& x) { return x % 1 >= 0.5 && x % 1 < 0.75; });\n    bins[3] = std::count_if(x.begin() + 3 * x.size() / 4, x.end(),\n        [](double const& x) { return x % 1 >= 0.75; });\n}",
            "size_t count;\n  for (int i = 0; i < 4; i++) {\n    count = 0;\n    for (int j = 0; j < x.size(); j++) {\n      if (x[j] >= i * 0.25 && x[j] < (i + 1) * 0.25) {\n        count++;\n      }\n    }\n    bins[i] = count;\n  }\n}",
            "bins = std::array<size_t, 4>{};\n    size_t count = 0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] < 0.75) {\n            ++bins[0];\n        }\n        else if (x[i] < 0.5) {\n            ++bins[1];\n        }\n        else if (x[i] < 0.25) {\n            ++bins[2];\n        }\n        else {\n            ++bins[3];\n        }\n    }\n}",
            "// Count the numbers of double in the vector x with a fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n   // Store the counts in bins.\n   // \n   // Note: use the findIndex() function defined in the starter code.\n   // \n   // Examples:\n   //   x:  [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   //   output: [2, 1, 2, 2]\n   // \n   //   x:  [1.9, 0.2, 0.6, 10.1, 7.4]\n   //   output: [2, 1, 1, 1]\n\n   std::vector<double> copy{x};\n   std::sort(copy.begin(), copy.end());\n\n   bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n\n   for (size_t i = 0; i < x.size(); ++i) {\n      auto fract = std::fmod(copy[i], 1);\n      if (fract < 0.25) {\n         ++bins[0];\n      }\n      else if (fract < 0.5) {\n         ++bins[1];\n      }\n      else if (fract < 0.75) {\n         ++bins[2];\n      }\n      else {\n         ++bins[3];\n      }\n   }\n}",
            "// TODO\n}",
            "// TODO\n  // Compute quartiles of x\n  std::sort(x.begin(), x.end());\n  size_t quarter1 = x.size()/4;\n  size_t quarter2 = x.size()/2;\n  size_t quarter3 = x.size()*3/4;\n  bins[0] = 0;\n  for(size_t i = 0; i < quarter1; ++i) {\n    if(x[i] >= 0 && x[i] < 0.25) {\n      ++bins[0];\n    }\n  }\n  bins[1] = 0;\n  for(size_t i = quarter1; i < quarter2; ++i) {\n    if(x[i] >= 0.25 && x[i] < 0.5) {\n      ++bins[1];\n    }\n  }\n  bins[2] = 0;\n  for(size_t i = quarter2; i < quarter3; ++i) {\n    if(x[i] >= 0.5 && x[i] < 0.75) {\n      ++bins[2];\n    }\n  }\n  bins[3] = 0;\n  for(size_t i = quarter3; i < x.size(); ++i) {\n    if(x[i] >= 0.75) {\n      ++bins[3];\n    }\n  }\n}",
            "bins.fill(0);\n    for (auto d : x) {\n        int i = floor(d);\n        double f = d - i;\n        if (f < 0.25)\n            ++bins[0];\n        else if (f < 0.5)\n            ++bins[1];\n        else if (f < 0.75)\n            ++bins[2];\n        else\n            ++bins[3];\n    }\n}",
            "std::sort(x.begin(), x.end());\n    size_t const size = x.size();\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    for(int i = 0; i < size; i++){\n        if(0 <= x[i] && x[i] < 0.25){\n            bins[0] += 1;\n        }\n        else if(0.25 <= x[i] && x[i] < 0.5){\n            bins[1] += 1;\n        }\n        else if(0.5 <= x[i] && x[i] < 0.75){\n            bins[2] += 1;\n        }\n        else if(0.75 <= x[i] && x[i] < 1.0){\n            bins[3] += 1;\n        }\n        else{\n            std::cout << \"Wrong value \" << x[i] << std::endl;\n        }\n    }\n}",
            "double min = std::numeric_limits<double>::max();\n    double max = std::numeric_limits<double>::min();\n    for(auto v : x) {\n        min = std::min(min, v);\n        max = std::max(max, v);\n    }\n    double half = (max - min)/4;\n    std::vector<double> quarters{min, min + half, min + 2 * half, max};\n    for(auto v : x) {\n        auto it = std::lower_bound(quarters.begin(), quarters.end(), v);\n        if (it == quarters.end())\n            ++bins[3];\n        else\n            ++bins[it - quarters.begin()];\n    }\n}",
            "if (x.empty()) {\n        bins = { 0, 0, 0, 0 };\n        return;\n    }\n    size_t size = x.size();\n    size_t max = x[0];\n    size_t min = x[0];\n    for (size_t i = 0; i < size; i++) {\n        if (x[i] > max) max = x[i];\n        if (x[i] < min) min = x[i];\n    }\n    size_t total = 0;\n    size_t part = 0;\n    bins = { 0, 0, 0, 0 };\n    for (size_t i = 0; i < size; i++) {\n        if (x[i] <= max - min) {\n            size_t temp = x[i] - min;\n            bins[temp]++;\n            total++;\n            continue;\n        }\n        part++;\n        size_t temp = (size_t) ((x[i] - min) * 4);\n        temp = temp % 4;\n        if (temp == 0) {\n            bins[3]++;\n        } else if (temp == 1) {\n            bins[2]++;\n        } else if (temp == 2) {\n            bins[1]++;\n        } else {\n            bins[0]++;\n        }\n        total++;\n    }\n    double average = total / 4.;\n    bins[0] = bins[0] / average;\n    bins[1] = bins[1] / average;\n    bins[2] = bins[2] / average;\n    bins[3] = bins[3] / average;\n}",
            "assert(x.size() > 0);\n\n    std::sort(x.begin(), x.end());\n\n    bins = { 0, 0, 0, 0 };\n\n    for (auto i = 0; i < x.size(); i++) {\n        if (x[i] < 0.25) bins[0]++;\n        else if (x[i] < 0.5) bins[1]++;\n        else if (x[i] < 0.75) bins[2]++;\n        else bins[3]++;\n    }\n\n}",
            "// Your code here\n  return;\n}",
            "auto midPoint = [](double a, double b) {\n        return 0.5 * (a + b);\n    };\n\n    // Sort x\n    std::sort(x.begin(), x.end());\n\n    // Count the number of elements in the vector x that have a fractional part\n    // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n    auto lower = x.begin();\n    auto upper = x.begin();\n    for (size_t i = 0; i < 4; ++i) {\n\n        // Compute the lower and upper bounds\n        auto midPoint = *(lower + (x.size() - 1) / 4 * i);\n        auto midPointPlusHalf = midPoint + 0.5;\n\n        // Count the number of elements in [lower, midPoint]\n        auto lowerBound = lower;\n        auto upperBound = std::upper_bound(lower, x.end(), midPoint);\n        bins[i] = std::distance(lowerBound, upperBound);\n\n        // Set the lower bound to the upper bound\n        lower = upperBound;\n\n        // If x has an even number of elements, there is one more element in [lower, midPointPlusHalf]\n        if (x.size() % 2 == 0 && *lower <= midPointPlusHalf && midPointPlusHalf < *upper) {\n            auto lowerBound = lower;\n            auto upperBound = std::upper_bound(lower, x.end(), midPointPlusHalf);\n            bins[i] += std::distance(lowerBound, upperBound);\n        }\n\n        // Set the upper bound to the lower bound\n        upper = lower;\n    }\n}",
            "/* TODO: Your code here */\n}",
            "std::sort(x.begin(), x.end());\n    size_t n = x.size();\n    size_t n_below = n * 0.25;\n    size_t n_above = n * 0.75;\n    bins = {0, 0, 0, 0};\n\n    for (auto i = x.begin(); i < x.begin() + n_below; ++i) {\n        if (*i < 0.25) {\n            ++bins[0];\n        }\n        else if (*i < 0.5) {\n            ++bins[1];\n        }\n        else if (*i < 0.75) {\n            ++bins[2];\n        }\n        else {\n            ++bins[3];\n        }\n    }\n    for (auto i = x.begin() + n_below; i < x.begin() + n_above; ++i) {\n        if (*i < 0.5) {\n            ++bins[0];\n        }\n        else {\n            ++bins[1];\n        }\n    }\n    for (auto i = x.begin() + n_above; i < x.end(); ++i) {\n        if (*i < 0.25) {\n            ++bins[0];\n        }\n        else if (*i < 0.5) {\n            ++bins[1];\n        }\n        else if (*i < 0.75) {\n            ++bins[2];\n        }\n        else {\n            ++bins[3];\n        }\n    }\n}",
            "//TODO\n}",
            "int64_t size = x.size();\n    if (size <= 0) {\n        throw std::runtime_error(\"Empty vector passed to countQuartiles\");\n    }\n\n    std::array<int64_t, 4> counts = {0, 0, 0, 0};\n    double dmin = x.front(), dmax = x.back();\n\n    for (double xi : x) {\n        if (xi >= dmax || xi <= dmin) {\n            throw std::runtime_error(\"Fractional part of x out of bounds\");\n        }\n        int64_t i = (int64_t)((xi - dmin) / (dmax - dmin) * 4.0);\n        ++counts[i];\n    }\n\n    for (int64_t i = 0; i < 4; ++i) {\n        bins[i] = counts[i];\n    }\n\n}",
            "size_t i = 0;\n    while (i<4) {\n        bins[i] = 0;\n        ++i;\n    }\n    std::sort(x.begin(), x.end());\n    double value = 0.0;\n    size_t index = 0;\n    size_t k = 0;\n    size_t n = x.size();\n    for (k=0; k<n; ++k) {\n        value = x[k];\n        index = 3;\n        while (index >= 0 && value >= 0.25*k) {\n            value -= 0.25*k;\n            ++index;\n        }\n        ++bins[index];\n    }\n}",
            "int index = 0;\n  std::array<double, 4> temp;\n  temp[0] =.25;\n  temp[1] =.50;\n  temp[2] =.75;\n  temp[3] = 1.0;\n  for(double t : temp) {\n    bins[index++] = std::count_if(x.begin(), x.end(), [t](double n) { return n < t; });\n  }\n}",
            "std::array<size_t, 4> counts;\n    size_t N = x.size();\n    for (size_t i = 0; i < N; ++i) {\n        size_t bin;\n        if (x[i] >= 0.0 && x[i] < 0.25) {\n            bin = 0;\n        } else if (x[i] >= 0.25 && x[i] < 0.5) {\n            bin = 1;\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            bin = 2;\n        } else {\n            bin = 3;\n        }\n        counts[bin] += 1;\n    }\n    bins = counts;\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] < 0.25)\n            bins[0] += 1;\n        else if (x[i] < 0.5)\n            bins[1] += 1;\n        else if (x[i] < 0.75)\n            bins[2] += 1;\n        else\n            bins[3] += 1;\n    }\n}",
            "// TODO\n\n}",
            "std::array<double, 4> quartiles = quartiles(x);\n    for (int i=0; i < 4; ++i) {\n        bins[i] = 0;\n        for (auto const& n : x) {\n            if (n < quartiles[i] || n >= quartiles[i+1]) {\n                ++bins[i];\n            }\n        }\n    }\n}",
            "bins.fill(0);\n\n   // sort the vector\n   std::vector<double> x_sorted = x;\n   std::sort(x_sorted.begin(), x_sorted.end());\n\n   // store the number of doubles in each of the 4 bins\n   size_t n = x_sorted.size();\n   if (n > 0) {\n      double lo = 0.0;\n      double hi = 0.25;\n      for (size_t i = 0; i < n; i++) {\n         if (x_sorted[i] >= lo && x_sorted[i] < hi) {\n            bins[0]++;\n         }\n         if (x_sorted[i] >= hi && x_sorted[i] < 0.5) {\n            bins[1]++;\n         }\n         if (x_sorted[i] >= 0.5 && x_sorted[i] < 0.75) {\n            bins[2]++;\n         }\n         if (x_sorted[i] >= 0.75) {\n            bins[3]++;\n         }\n      }\n   }\n}",
            "auto const num_bins = 4;\n    size_t const max_bins = 100;\n    if (x.size() > max_bins) {\n        throw std::runtime_error(\"Too many elements in array\");\n    }\n\n    std::sort(x.begin(), x.end());\n    std::array<size_t, num_bins> bins_tmp;\n    for (auto i = 0; i < num_bins; i++) {\n        auto const& low = x[x.size() * i / num_bins];\n        auto const& high = x[x.size() * (i + 1) / num_bins];\n        auto const& low_index = std::lower_bound(x.begin(), x.end(), low) - x.begin();\n        auto const& high_index = std::lower_bound(x.begin(), x.end(), high) - x.begin();\n        bins_tmp[i] = high_index - low_index;\n    }\n    bins = bins_tmp;\n}",
            "// TODO\n    auto count = [](double x) {\n        size_t cnt = 0;\n        if (x <= 0.25) cnt++;\n        else if (x <= 0.50) cnt++;\n        else if (x <= 0.75) cnt++;\n        else if (x <= 1.0) cnt++;\n        return cnt;\n    };\n    bins[0] = std::count_if(x.begin(), x.end(), count);\n    bins[1] = std::count_if(x.begin(), x.end(), [](double x) {\n        if (x >= 0.25 && x <= 0.50) return true;\n        return false;\n    });\n    bins[2] = std::count_if(x.begin(), x.end(), [](double x) {\n        if (x >= 0.50 && x <= 0.75) return true;\n        return false;\n    });\n    bins[3] = std::count_if(x.begin(), x.end(), [](double x) {\n        if (x >= 0.75 && x <= 1.0) return true;\n        return false;\n    });\n}",
            "int n = x.size();\n  std::vector<double> xx(x.begin(), x.end());\n  std::sort(xx.begin(), xx.end());\n  size_t first = 0, last = 0, mid = 0;\n  for (auto i = 0; i < n; i++) {\n    if (xx[i] >= 0.75) {\n      last = i;\n    } else if (xx[i] >= 0.5) {\n      mid = i;\n    } else if (xx[i] >= 0.25) {\n      first = i;\n    }\n  }\n  bins[0] = first;\n  bins[1] = first + last - mid;\n  bins[2] = last - first;\n  bins[3] = n - last;\n}",
            "/* Your code here */\n\n  std::sort(x.begin(), x.end());\n  bins[0] = 0;\n  for (auto it = x.begin(); it!= x.end(); ++it) {\n    if (0 <= *it && *it <= 0.25) {\n      bins[0] += 1;\n    } else if (0.25 < *it && *it <= 0.5) {\n      bins[1] += 1;\n    } else if (0.5 < *it && *it <= 0.75) {\n      bins[2] += 1;\n    } else if (0.75 < *it && *it <= 1) {\n      bins[3] += 1;\n    }\n  }\n}",
            "std::array<size_t, 4> counts;\n\n\tfor (double i : x) {\n\t\tif (i < 0.25) {\n\t\t\tcounts[0]++;\n\t\t}\n\t\telse if (i < 0.5) {\n\t\t\tcounts[1]++;\n\t\t}\n\t\telse if (i < 0.75) {\n\t\t\tcounts[2]++;\n\t\t}\n\t\telse {\n\t\t\tcounts[3]++;\n\t\t}\n\t}\n\n\tbins = counts;\n}",
            "bins = { 0, 0, 0, 0 };\n\n    // The number of doubles in `x` that are < 0.25\n    size_t lo_count = 0;\n\n    // The number of doubles in `x` that are < 0.5\n    size_t mid_count = 0;\n\n    // The number of doubles in `x` that are < 0.75\n    size_t hi_count = 0;\n\n    // The number of doubles in `x` that are < 1\n    size_t high_count = 0;\n\n    for (double v : x) {\n        if (v >= 0 && v < 0.25)\n            ++lo_count;\n        else if (v >= 0.25 && v < 0.5)\n            ++mid_count;\n        else if (v >= 0.5 && v < 0.75)\n            ++hi_count;\n        else if (v >= 0.75 && v < 1)\n            ++high_count;\n    }\n\n    bins[0] = lo_count;\n    bins[1] = mid_count;\n    bins[2] = hi_count;\n    bins[3] = high_count;\n}",
            "// TODO: Your code here.\n    size_t i = 0;\n    for (auto const& val : x) {\n        if (val < 0.25) {\n            bins[i++] += 1;\n        }\n        else if (val >= 0.25 && val < 0.5) {\n            bins[i++] += 1;\n        }\n        else if (val >= 0.5 && val < 0.75) {\n            bins[i++] += 1;\n        }\n        else if (val >= 0.75 && val <= 1) {\n            bins[i++] += 1;\n        }\n    }\n    if (i!= 4) {\n        throw std::runtime_error(\"Error in quartiles function: input is not a double vector\");\n    }\n}",
            "// TODO: Your code here.\n\t//\n\t// You are given an array `x` of doubles. Write code that fills in the\n\t// elements of the `bins` array. The `bins` array should have four elements.\n\t// Each element should contain the number of elements in `x` with a fractional\n\t// part that is in a certain interval of values.\n\t// The four intervals are:\n\t// \t(0, 0.25), (0.25, 0.5), (0.5, 0.75), (0.75, 1).\n\t//\n\t// The array `x` should be sorted first, and then the fractional part of each\n\t// element should be counted. Note that the fractional part of a negative number\n\t// is the fractional part of its absolute value.\n\t//\n\t// In the example above, the first element of `x` has a fractional part of 0.27.\n\t// Therefore, the array `bins` should be updated to [2, 1, 1, 1].\n\t//\n\t// If there are no elements in `x` that fall into a particular interval, the\n\t// corresponding element of `bins` should be set to 0.\n\t//\n\t// You may assume that the first and last elements of `x` are not fractions.\n\t// You may assume that the elements of `x` are all non-negative.\n\n\tfor (size_t i = 0; i < bins.size(); i++) {\n\t\tbins[i] = 0;\n\t}\n\n\tfor (double xi : x) {\n\t\tsize_t idx = 0;\n\t\tif (xi < 0.25) {\n\t\t\tidx = 0;\n\t\t}\n\t\telse if (xi >= 0.25 && xi < 0.5) {\n\t\t\tidx = 1;\n\t\t}\n\t\telse if (xi >= 0.5 && xi < 0.75) {\n\t\t\tidx = 2;\n\t\t}\n\t\telse if (xi >= 0.75) {\n\t\t\tidx = 3;\n\t\t}\n\t\tbins[idx]++;\n\t}\n}",
            "bins.fill(0);\n  for (auto it = x.begin(); it!= x.end(); ++it) {\n    switch (int(4*(*it) + 0.5)) {\n      case 0: case 1: case 2: case 3: case 4:\n        ++bins[int(4*(*it)+0.5)]; break;\n      default:\n        ;\n    }\n  }\n}",
            "size_t num_elements = x.size();\n    bins = { 0, 0, 0, 0 };\n\n    for (size_t i = 0; i < num_elements; i++) {\n        double value = x[i];\n\n        if (value >= 0 && value < 0.25) {\n            bins[0]++;\n        }\n        else if (value >= 0.25 && value < 0.5) {\n            bins[1]++;\n        }\n        else if (value >= 0.5 && value < 0.75) {\n            bins[2]++;\n        }\n        else if (value >= 0.75 && value <= 1) {\n            bins[3]++;\n        }\n    }\n}",
            "std::sort(x.begin(), x.end());\n  size_t num_elements = x.size();\n\n  if (num_elements == 0) {\n    return;\n  }\n\n  bins[0] = 0;\n  for (size_t i = 1; i < num_elements; i++) {\n    if (x[i] > x[i - 1]) {\n      bins[0]++;\n    }\n  }\n  bins[1] = num_elements;\n  for (size_t i = 1; i < num_elements; i++) {\n    if (x[i] < x[i - 1]) {\n      bins[1]--;\n    }\n  }\n\n  if (bins[1] == num_elements) {\n    return;\n  }\n\n  size_t last = bins[1];\n  bins[2] = bins[0];\n  for (size_t i = 1; i < num_elements; i++) {\n    if (x[i] > x[i - 1]) {\n      bins[2]++;\n    }\n  }\n\n  bins[3] = last;\n  for (size_t i = 1; i < num_elements; i++) {\n    if (x[i] < x[i - 1]) {\n      bins[3]--;\n    }\n  }\n}",
            "std::sort(x.begin(), x.end());\n    std::vector<double> v = x;\n    for (double d : v) {\n        if (d == 0.0) {\n            continue;\n        }\n        if (d < 0.25) {\n            ++bins[0];\n        }\n        if (d >= 0.25 && d < 0.5) {\n            ++bins[1];\n        }\n        if (d >= 0.5 && d < 0.75) {\n            ++bins[2];\n        }\n        if (d >= 0.75 && d < 1.0) {\n            ++bins[3];\n        }\n    }\n    return;\n}",
            "bins.fill(0);\n    for (double val : x) {\n        if (val < 0.25) {\n            bins[0]++;\n        } else if (val < 0.5) {\n            bins[1]++;\n        } else if (val < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "double min = *std::min_element(x.begin(), x.end());\n    std::vector<double> x_n(x.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        x_n[i] = (x[i] - min) / 4.;\n    }\n\n    std::sort(x_n.begin(), x_n.end());\n\n    size_t i = 0;\n    bins[0] = 0;\n    while (i < x_n.size() && x_n[i] < 0.25) {\n        bins[0]++;\n        i++;\n    }\n\n    bins[1] = i;\n    while (i < x_n.size() && x_n[i] < 0.5) {\n        bins[1]++;\n        i++;\n    }\n\n    bins[2] = i;\n    while (i < x_n.size() && x_n[i] < 0.75) {\n        bins[2]++;\n        i++;\n    }\n\n    bins[3] = i;\n}",
            "/* Implementation here */\n}",
            "if (x.size() == 0) {\n        bins.fill(0);\n        return;\n    }\n    double step = 1.0 / x.size();\n    for (double i = 0; i < 1.0; i += step) {\n        if (x[x.size() * i] >= i && x[x.size() * i] < i + 0.25) {\n            bins[0]++;\n        } else if (x[x.size() * i] >= i + 0.25 && x[x.size() * i] < i + 0.5) {\n            bins[1]++;\n        } else if (x[x.size() * i] >= i + 0.5 && x[x.size() * i] < i + 0.75) {\n            bins[2]++;\n        } else if (x[x.size() * i] >= i + 0.75 && x[x.size() * i] <= 1) {\n            bins[3]++;\n        }\n    }\n}",
            "// TODO\n}",
            "// create vector of ints to store count of each quartile\n    std::array<size_t, 4> counts = {0, 0, 0, 0};\n\n    // TODO: implement\n}",
            "std::sort(x.begin(), x.end());\n\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] >= 0.25 && x[i] < 0.5) {\n            bins[1]++;\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            bins[2]++;\n        } else if (x[i] >= 0.75 && x[i] <= 1.0) {\n            bins[3]++;\n        } else if (x[i] >= 0.0 && x[i] < 0.25) {\n            bins[0]++;\n        }\n    }\n}",
            "// check if input is sorted\n    assert(std::is_sorted(x.begin(), x.end()));\n\n    size_t N = x.size();\n\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    for (size_t i = 0; i < N; i++) {\n\n        // compute the decimal part of the double\n        double fraction = std::modf(x[i], &x[i]);\n\n        // find which bin the number belongs to\n        if (fraction <= 0.25)\n            bins[0]++;\n        else if (fraction <= 0.50)\n            bins[1]++;\n        else if (fraction <= 0.75)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "bins[0] = bins[1] = bins[2] = bins[3] = 0;\n  double q = 0.25;\n  for (auto it = x.begin(); it!= x.end(); ++it) {\n    if (*it < q) {\n      bins[0]++;\n    }\n    else if (*it < q * 2) {\n      bins[1]++;\n    }\n    else if (*it < q * 3) {\n      bins[2]++;\n    }\n    else if (*it < q * 4) {\n      bins[3]++;\n    }\n  }\n}",
            "// Check if the vector is empty or not\n    if (x.empty()) {\n        // If it is empty, set bins to 0.\n        bins = {0, 0, 0, 0};\n    }\n    else {\n        // If it is not empty, sort the vector\n        std::sort(x.begin(), x.end());\n\n        // Initialize the bins\n        size_t start = 0;\n        size_t middle = 0;\n        size_t end = x.size();\n\n        // Set up the 4 bins, from smallest to largest\n        size_t bin1 = 0;\n        size_t bin2 = 0;\n        size_t bin3 = 0;\n        size_t bin4 = 0;\n\n        // If the vector is not empty, count the number of elements\n        // in each bin\n        while (start <= end) {\n            // If start is the first element in the array\n            if (start == 0) {\n                // If the element is in the first quartile\n                if (x[start] <= 0.25 * (x[start] + x[middle])) {\n                    bin1++;\n                    start++;\n                    middle++;\n                    end++;\n                }\n                // If the element is in the second quartile\n                else if (x[start] > 0.25 * (x[start] + x[middle]) && x[start] <= 0.5 * (x[start] + x[middle])) {\n                    bin2++;\n                    start++;\n                    middle++;\n                    end++;\n                }\n                // If the element is in the third quartile\n                else if (x[start] > 0.5 * (x[start] + x[middle]) && x[start] <= 0.75 * (x[start] + x[middle])) {\n                    bin3++;\n                    start++;\n                    middle++;\n                    end++;\n                }\n                // If the element is in the fourth quartile\n                else {\n                    bin4++;\n                    start++;\n                    middle++;\n                    end++;\n                }\n            }\n            // If start is not the first element in the array\n            else {\n                // If the element is in the first quartile\n                if (x[start] < 0.25 * (x[start - 1] + x[start]) && x[start] >= 0.25 * (x[start - 1] + x[start])) {\n                    bin1++;\n                    start++;\n                    middle++;\n                    end++;\n                }\n                // If the element is in the second quartile\n                else if (x[start] > 0.25 * (x[start - 1] + x[start]) && x[start] <= 0.5 * (x[start - 1] + x[start])) {\n                    bin2++;\n                    start++;\n                    middle++;\n                    end++;\n                }\n                // If the element is in the third quartile\n                else if (x[start] > 0.5 * (x[start - 1] + x[start]) && x[start] <= 0.75 * (x[start - 1] + x[start])) {\n                    bin3++;\n                    start++;\n                    middle++;\n                    end++;\n                }\n                // If the element is in the fourth quartile\n                else {\n                    bin4++;\n                    start++;\n                    middle++;\n                    end++;\n                }\n            }\n        }\n\n        // Store the bins\n        bins[0] = bin1;\n        bins[1] = bin2;\n        bins[2] = bin3;\n        bins[3] = bin4;\n    }\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    for(auto val : x) {\n        if (val < 0.25) bins[0]++;\n        else if (val < 0.5) bins[1]++;\n        else if (val < 0.75) bins[2]++;\n        else if (val <= 1.0) bins[3]++;\n        else {\n            std::cout << \"Error: input value \" << val << \" is not between 0 and 1.0\" << std::endl;\n            assert(false);\n        }\n    }\n}",
            "size_t i = 0;\n  size_t c = 0;\n  size_t n = x.size();\n  bins = {0, 0, 0, 0};\n  if (n == 0) {\n    return;\n  }\n  std::sort(x.begin(), x.end());\n  while (i < n) {\n    switch (c) {\n      case 0:\n        if (x[i] <= 0.25) {\n          c = 1;\n        }\n        break;\n      case 1:\n        if (x[i] <= 0.5) {\n          c = 2;\n        }\n        break;\n      case 2:\n        if (x[i] <= 0.75) {\n          c = 3;\n        }\n        break;\n      case 3:\n        bins[c]++;\n        c = 0;\n        break;\n    }\n    ++i;\n  }\n  bins[c]++;\n}",
            "// Fill with zeroes.\n    std::fill(bins.begin(), bins.end(), 0);\n\n    // The array of bins.\n    std::array<double, 4> quartiles = {0.0, 0.25, 0.5, 0.75};\n\n    // Loop through the vector x.\n    for (double x_val : x) {\n        // Loop through the bins.\n        for (size_t i = 0; i < 4; i++) {\n            if (x_val < quartiles[i]) {\n                bins[i]++;\n                break;\n            }\n        }\n    }\n}",
            "/* Implementation hint:\n\n     - sort the elements of `x` into ascending order;\n     - store the number of elements in `x` in a variable `N`;\n     - create a vector `y` of length `N`;\n     - copy the elements of `x` into `y` in order of increasing value;\n     - for each element `x[i]`, add `1` to `bins[i/N]` (where `i` is the index\n       of `x`);\n\n  */\n}",
            "double const LOW = 0.25;\n\tdouble const MEDIUM = 0.5;\n\tdouble const HIGH = 0.75;\n\tdouble const TOP = 1.0;\n\tbins[0] = 0;\n\tbins[1] = 0;\n\tbins[2] = 0;\n\tbins[3] = 0;\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tif (x[i] < LOW) {\n\t\t\tbins[0]++;\n\t\t} else if (LOW <= x[i] && x[i] < MEDIUM) {\n\t\t\tbins[1]++;\n\t\t} else if (MEDIUM <= x[i] && x[i] < HIGH) {\n\t\t\tbins[2]++;\n\t\t} else if (HIGH <= x[i] && x[i] < TOP) {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "// Initialize all four bins to zero\n   bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n\n   // Create an array of size x.size() containing 0.25, 0.5, 0.75\n   std::vector<double> breaks(x.size()-1);\n   breaks[0] = 0.25;\n   breaks[1] = 0.5;\n   breaks[2] = 0.75;\n\n   // Create a vector of the same size as the input vector containing the \n   // fractional parts of x.\n   std::vector<double> frac_parts(x.size());\n   for (int i = 0; i < x.size(); i++) {\n      frac_parts[i] = std::modf(x[i], &frac_parts[i]);\n   }\n\n   // Iterate over the vector of fractional parts. For each fractional part\n   // check if it is in the bins array. If it is increment the value in the\n   // bins array at the corresponding index.\n   for (auto frac_part : frac_parts) {\n      for (int i = 0; i < breaks.size(); i++) {\n         if (frac_part >= breaks[i]) {\n            bins[i]++;\n            break;\n         }\n      }\n   }\n}",
            "if (x.empty()) {\n        return;\n    }\n    std::sort(x.begin(), x.end());\n    size_t count = 0;\n    for (auto val : x) {\n        if (val > 0 && val < 0.25) {\n            bins[0]++;\n        } else if (val >= 0.25 && val < 0.5) {\n            bins[1]++;\n        } else if (val >= 0.5 && val < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "std::sort(x.begin(), x.end());\n\n    size_t N = x.size();\n    bins.fill(0);\n    for (size_t i = 0; i < N; i++) {\n        double xi = x[i];\n        if (xi < 0.25) {\n            bins[0]++;\n        } else if (xi < 0.5) {\n            bins[1]++;\n        } else if (xi < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: implement this\n    // HINT: use the std::partition function to partition the vector x into 4\n    //       subvectors.\n    // HINT: use the std::partition_point function to locate the midpoints of\n    //       the subvectors\n    // HINT: use the std::lower_bound function to locate the index of the\n    //       midpoints of the subvectors\n    // HINT: use the std::distance function to compute the number of elements\n    //       in each subvector\n\n    std::sort(x.begin(), x.end());\n\n    std::vector<double> quartiles{0.25, 0.5, 0.75, 1};\n    std::vector<double> bins_vec(4);\n    std::vector<double>::iterator low_itr;\n    std::vector<double>::iterator high_itr;\n    std::vector<double>::iterator middle_itr;\n    std::vector<double>::iterator middle_itr2;\n    std::vector<double>::iterator low_itr2;\n    std::vector<double>::iterator high_itr2;\n    size_t index{0};\n    for (auto const& quartile : quartiles) {\n        low_itr = std::lower_bound(x.begin(), x.end(), quartile);\n        high_itr = std::partition_point(x.begin(), x.end(), low_itr);\n        bins_vec[index] = std::distance(low_itr, high_itr);\n        index++;\n    }\n    bins = {bins_vec[0], bins_vec[1], bins_vec[2], bins_vec[3]};\n}",
            "//TODO: Implement this function.\n}",
            "// TODO: Your code here\n}",
            "// create an array of size 4 initialized to 0\n\tstd::array<size_t, 4> bins_of_x = {0, 0, 0, 0};\n\n\t// loop through the vector x to get the number of doubles with a fractional part \n\t// in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i] - static_cast<int>(x[i]) == 0.0) {\n\t\t\tif (x[i] >= 0.0 && x[i] < 0.25) {\n\t\t\t\tbins_of_x[0] += 1;\n\t\t\t} else if (x[i] >= 0.25 && x[i] < 0.5) {\n\t\t\t\tbins_of_x[1] += 1;\n\t\t\t} else if (x[i] >= 0.5 && x[i] < 0.75) {\n\t\t\t\tbins_of_x[2] += 1;\n\t\t\t} else if (x[i] >= 0.75 && x[i] < 1.0) {\n\t\t\t\tbins_of_x[3] += 1;\n\t\t\t}\n\t\t} else {\n\t\t\tstd::cout << \"ERROR: Input not double.\" << std::endl;\n\t\t}\n\t}\n\n\t// copy the content of bins_of_x into bins\n\tfor (int i = 0; i < 4; i++) {\n\t\tbins[i] = bins_of_x[i];\n\t}\n}",
            "bins[0] = std::count_if(x.begin(), x.end(), [](double x) { return x >= 0 && x < 0.25; });\n    bins[1] = std::count_if(x.begin(), x.end(), [](double x) { return x >= 0.25 && x < 0.5; });\n    bins[2] = std::count_if(x.begin(), x.end(), [](double x) { return x >= 0.5 && x < 0.75; });\n    bins[3] = std::count_if(x.begin(), x.end(), [](double x) { return x >= 0.75 && x < 1; });\n}",
            "// Compute the number of elements in each of the four buckets\n  size_t num_elements = x.size();\n  if (num_elements <= 1) {\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n    return;\n  }\n  std::vector<double> x_sorted = x;\n  std::sort(x_sorted.begin(), x_sorted.end());\n  size_t idx_0 = num_elements / 4;\n  size_t idx_1 = num_elements / 2;\n  size_t idx_2 = idx_1 + idx_0;\n  size_t idx_3 = num_elements - idx_0;\n  bins[0] = std::count_if(x_sorted.begin(), x_sorted.begin() + idx_0,\n      [](double x) { return x == std::floor(x); });\n  bins[1] = std::count_if(x_sorted.begin() + idx_0, x_sorted.begin() + idx_1,\n      [](double x) { return x == std::floor(x); });\n  bins[2] = std::count_if(x_sorted.begin() + idx_1, x_sorted.begin() + idx_2,\n      [](double x) { return x == std::floor(x); });\n  bins[3] = std::count_if(x_sorted.begin() + idx_2, x_sorted.end(),\n      [](double x) { return x == std::floor(x); });\n}",
            "size_t N = x.size();\n    if (N == 0) {\n        bins.fill(0);\n        return;\n    }\n    double dx = x[N-1] - x[0];\n    for (size_t i = 0; i < 4; i++) {\n        bins[i] = 0;\n    }\n    size_t i = 0;\n    while (x[i] <= 1.0/4.0 * dx) {\n        bins[0]++;\n        i++;\n    }\n    while (x[i] <= 2.0/4.0 * dx) {\n        bins[1]++;\n        i++;\n    }\n    while (x[i] <= 3.0/4.0 * dx) {\n        bins[2]++;\n        i++;\n    }\n    while (x[i] < 1.0) {\n        bins[3]++;\n        i++;\n    }\n\n}",
            "bins[0] = 0; bins[1] = 0; bins[2] = 0; bins[3] = 0;\n    for (int i = 0; i < x.size(); i++)\n    {\n        if (x[i] >= 0.25 && x[i] < 0.5)\n            bins[1]++;\n        else if (x[i] >= 0.5 && x[i] < 0.75)\n            bins[2]++;\n        else if (x[i] >= 0.75 && x[i] < 1)\n            bins[3]++;\n        else if (x[i] >= 0.0 && x[i] < 0.25)\n            bins[0]++;\n    }\n}",
            "// Your code here\n}",
            "size_t n = x.size();\n    size_t i;\n    size_t n_below_25 = 0;\n    size_t n_below_50 = 0;\n    size_t n_below_75 = 0;\n    size_t n_above_75 = 0;\n    for (i = 0; i < n; i++) {\n        if (x[i] < 0.25) n_below_25++;\n        else if (x[i] < 0.5) n_below_50++;\n        else if (x[i] < 0.75) n_below_75++;\n        else n_above_75++;\n    }\n    bins = {n_below_25, n_below_50, n_below_75, n_above_75};\n}",
            "double sum = 0.0;\n\tdouble quartiles[4];\n\tbins.fill(0);\n\tfor (auto v : x) {\n\t\tsum += v;\n\t}\n\n\tfor (int i = 0; i < 4; i++) {\n\t\tdouble n = (sum / x.size()) * (i * 0.25 + 0.25);\n\t\tquartiles[i] = n;\n\t}\n\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tfor (int j = 0; j < 4; j++) {\n\t\t\tif (x[i] <= quartiles[j]) {\n\t\t\t\tbins[j]++;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n}",
            "int len = x.size();\n    std::sort(x.begin(), x.end());\n    bins = {0, 0, 0, 0};\n    for (int i = 0; i < len; i++) {\n        if (x[i] >= 0.0 && x[i] <= 0.25) {\n            bins[0]++;\n        } else if (x[i] > 0.25 && x[i] <= 0.5) {\n            bins[1]++;\n        } else if (x[i] > 0.5 && x[i] <= 0.75) {\n            bins[2]++;\n        } else if (x[i] > 0.75 && x[i] <= 1.0) {\n            bins[3]++;\n        }\n    }\n    std::cout << \"Bins: \" << bins[0] << \" \" << bins[1] << \" \" << bins[2] << \" \" << bins[3] << std::endl;\n}",
            "std::sort(x.begin(), x.end());\n\n    size_t n = x.size();\n\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    for (size_t i = 0; i < n; i++) {\n        if (x[i] < 0.25)\n            bins[0]++;\n        else if (x[i] < 0.5)\n            bins[1]++;\n        else if (x[i] < 0.75)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "std::sort(x.begin(), x.end());\n    int N = x.size();\n    if (N == 0)\n        return;\n    int N1 = 1;\n    int N2 = N/4;\n    int N3 = N/2;\n    int N4 = N - N2;\n    bins[0] = std::lower_bound(x.begin(), x.end(), N1/N) - x.begin();\n    bins[1] = std::lower_bound(x.begin(), x.end(), N2/N) - x.begin();\n    bins[2] = std::lower_bound(x.begin(), x.end(), N3/N) - x.begin();\n    bins[3] = std::lower_bound(x.begin(), x.end(), N4/N) - x.begin();\n}",
            "size_t n = x.size();\n  if (n == 0) {\n    bins = {0, 0, 0, 0};\n    return;\n  }\n\n  std::vector<size_t> counts(4);\n  for (size_t i = 0; i < n; i++) {\n    size_t idx = static_cast<size_t>(4 * (x[i] - std::floor(x[i])));\n    counts[idx]++;\n  }\n\n  // If the number of elements in the last bin is less than 30% of the total\n  // number of elements, reassign the last bin to the first bin and remove one\n  // element from the first bin.\n  if (counts[3] < (n / 3.0)) {\n    counts[0] += counts[3];\n    counts[3] = 0;\n  }\n\n  size_t count = 0;\n  for (size_t i = 0; i < 4; i++) {\n    bins[i] = count;\n    count += counts[i];\n  }\n}",
            "// FIXME\n    size_t len = x.size();\n    std::sort(x.begin(), x.end());\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    for (size_t i = 0; i < len; i++) {\n        if (x[i] >= 0 && x[i] < 0.25)\n            bins[0]++;\n        else if (x[i] >= 0.25 && x[i] < 0.5)\n            bins[1]++;\n        else if (x[i] >= 0.5 && x[i] < 0.75)\n            bins[2]++;\n        else if (x[i] >= 0.75 && x[i] <= 1)\n            bins[3]++;\n    }\n}",
            "std::sort(x.begin(), x.end());\n    auto const minVal = x.front();\n    auto const maxVal = x.back();\n    auto const binWidth = (maxVal - minVal) / 4;\n    auto const N = x.size();\n    bins.fill(0);\n    for (auto val : x) {\n        auto const bin = static_cast<size_t>(std::floor((val - minVal) / binWidth));\n        bins[bin]++;\n    }\n}",
            "if (x.empty()) {\n        return;\n    }\n\n    std::vector<double> xs(x);\n    std::sort(xs.begin(), xs.end());\n\n    size_t n = xs.size();\n    double xmin = xs.front(), xmax = xs.back(),\n           xq25 = xmin + (xmax - xmin) * 0.25, xq50 = xmin + (xmax - xmin) * 0.5, xq75 = xmin + (xmax - xmin) * 0.75,\n           q25_count = 0, q50_count = 0, q75_count = 0;\n\n    for (auto x : xs) {\n        if (x < xq25) {\n            ++q25_count;\n        }\n        if (x < xq50) {\n            ++q50_count;\n        }\n        if (x < xq75) {\n            ++q75_count;\n        }\n    }\n\n    bins = {{q25_count, q50_count, q75_count, n - q25_count - q50_count - q75_count}};\n}",
            "// TODO: Your code goes here.\n    // Hint: Use countFractionalPart\n    bins[0] = 0;\n    for (auto &d : x) {\n        size_t n = countFractionalPart(d);\n        if (n < 4) {\n            bins[n]++;\n        }\n    }\n}",
            "if (x.size() < 4) {\n    std::cerr << \"Not enough data.\" << std::endl;\n    std::exit(1);\n  }\n\n  std::vector<double> a(x);\n\n  std::sort(a.begin(), a.end());\n  double min = a[0];\n  double max = a.back();\n\n  double bin_size = (max - min) / 4;\n  double next = min;\n  size_t i = 0;\n  for (size_t k = 0; k < 4; ++k) {\n    size_t j = i;\n    for (; i < a.size() && a[i] < next + bin_size; ++i) {\n      j++;\n    }\n    bins[k] = j - j / 4;\n    next += bin_size;\n  }\n}",
            "// Count how many numbers fall in each quartile\n    //\n    // 0.0 <= x[i] < 0.25\n    // 0.25 <= x[i] < 0.5\n    // 0.5 <= x[i] < 0.75\n    // 0.75 <= x[i] < 1.0\n    //\n    // To keep track of the counts, we use an array.\n\n    // Initialize the array.\n    for (size_t i = 0; i < 4; ++i) {\n        bins[i] = 0;\n    }\n\n    // Count the number of doubles in the vector x that have a fractional part \n    // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n    for (size_t i = 0; i < x.size(); ++i) {\n        double xi = x[i];\n        if (xi >= 0.0 && xi < 0.25) {\n            bins[0] += 1;\n        } else if (xi >= 0.25 && xi < 0.5) {\n            bins[1] += 1;\n        } else if (xi >= 0.5 && xi < 0.75) {\n            bins[2] += 1;\n        } else if (xi >= 0.75 && xi < 1.0) {\n            bins[3] += 1;\n        }\n    }\n}",
            "bins[0] = bins[1] = bins[2] = bins[3] = 0;\n    for (double val : x) {\n        if (val < 0.25)\n            bins[0]++;\n        else if (val < 0.5)\n            bins[1]++;\n        else if (val < 0.75)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "// TODO: Put your code here\n    for (int i = 0; i < 4; ++i) {\n        bins[i] = 0;\n    }\n    for (auto itr = x.begin(); itr!= x.end(); ++itr) {\n        if (*itr < 0.25) {\n            ++bins[0];\n        } else if (*itr < 0.5) {\n            ++bins[1];\n        } else if (*itr < 0.75) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "std::sort(x.begin(), x.end());\n\n    int n = x.size();\n    bins = std::array<size_t, 4>();\n\n    if (n > 0) {\n        bins[0] = static_cast<int>(n * (std::floor(x[0]) - x[0]));\n    }\n\n    if (n > 1) {\n        bins[1] = static_cast<int>(n * (std::floor(x[1]) - x[1]));\n    }\n\n    if (n > 2) {\n        bins[2] = static_cast<int>(n * (std::floor(x[n-2]) - x[n-2]));\n    }\n\n    if (n > 3) {\n        bins[3] = static_cast<int>(n * (std::floor(x[n-1]) - x[n-1]));\n    }\n}",
            "// TODO\n    std::array<double, 4> quartiles = computeQuartiles(x);\n    for(int i = 0; i < 4; ++i)\n    {\n        bins[i] = 0;\n        for(auto j = 0; j < x.size(); ++j)\n        {\n            if(x[j] < quartiles[i])\n            {\n                ++bins[i];\n            }\n        }\n    }\n}",
            "for(int i = 0; i < x.size(); i++) {\n        double frac = x[i] - floor(x[i]);\n        if(frac < 0.25) {\n            bins[0]++;\n        }\n        else if(frac < 0.5) {\n            bins[1]++;\n        }\n        else if(frac < 0.75) {\n            bins[2]++;\n        }\n        else {\n            bins[3]++;\n        }\n    }\n}",
            "// Your code here\n    // 1. sort\n    // 2. traverse and count the numbers\n    // 3. return\n\n    // sort\n    std::vector<double> x_sorted = x;\n    std::sort(x_sorted.begin(), x_sorted.end());\n    double max_val = x_sorted[x_sorted.size() - 1];\n    double min_val = x_sorted[0];\n\n    // count\n    int cnt = 0;\n    for (int i = 0; i < x_sorted.size(); ++i) {\n        double x_i = x_sorted[i];\n        if (x_i >= min_val && x_i <= max_val * 0.25) {\n            bins[cnt]++;\n        }\n        else if (x_i > max_val * 0.25 && x_i <= max_val * 0.5) {\n            bins[cnt + 1]++;\n        }\n        else if (x_i > max_val * 0.5 && x_i <= max_val * 0.75) {\n            bins[cnt + 2]++;\n        }\n        else if (x_i > max_val * 0.75 && x_i <= max_val) {\n            bins[cnt + 3]++;\n        }\n        else {\n            std::cout << \"error\" << std::endl;\n        }\n    }\n\n    // return\n    std::cout << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << std::endl;\n\n}",
            "size_t count[4] = {0, 0, 0, 0};\n    for(auto i: x) {\n        if(i > 0.75) {\n            count[3]++;\n        } else if(i > 0.5) {\n            count[2]++;\n        } else if(i > 0.25) {\n            count[1]++;\n        } else {\n            count[0]++;\n        }\n    }\n    bins[0] = count[0];\n    bins[1] = count[1];\n    bins[2] = count[2];\n    bins[3] = count[3];\n}",
            "bins = { 0, 0, 0, 0 };\n    for(auto &x_i: x) {\n        size_t i;\n        if(x_i < 0.25) {\n            i = 0;\n        } else if (x_i < 0.5) {\n            i = 1;\n        } else if (x_i < 0.75) {\n            i = 2;\n        } else {\n            i = 3;\n        }\n        ++bins[i];\n    }\n}",
            "// TODO: Your code here\n\n}",
            "bins.fill(0);\n    size_t n = x.size();\n    if (n == 0) return;\n    std::vector<double> data(x);\n    std::sort(data.begin(), data.end());\n\n    double x_min = data[0];\n    double x_max = data[n - 1];\n    double dx = (x_max - x_min) / 4;\n    double x_0 = x_min + dx;\n    double x_1 = x_min + dx * 2;\n    double x_2 = x_min + dx * 3;\n    double x_3 = x_min + dx * 4;\n\n    for (size_t i = 0; i < n; i++) {\n        double v = data[i];\n        if (v < x_0) bins[0]++;\n        else if (v < x_1) bins[1]++;\n        else if (v < x_2) bins[2]++;\n        else if (v < x_3) bins[3]++;\n    }\n}",
            "bins.fill(0);\n    for(auto const& val : x) {\n        double temp = val - floor(val);\n        if(temp > 0 && temp < 0.25) {\n            bins[0]++;\n        } else if(temp >= 0.25 && temp < 0.5) {\n            bins[1]++;\n        } else if(temp >= 0.5 && temp < 0.75) {\n            bins[2]++;\n        } else if(temp >= 0.75 && temp <= 1) {\n            bins[3]++;\n        }\n    }\n}",
            "// Compute the quartiles and fill in bins\n\tstd::vector<double> temp = x;\n\tstd::sort(temp.begin(), temp.end());\n\tdouble const len = temp.size();\n\tdouble const q1 = temp[0.25 * len];\n\tdouble const q2 = temp[0.5 * len];\n\tdouble const q3 = temp[0.75 * len];\n\tdouble const q4 = temp.back();\n\n\tbins[0] = 0;\n\tfor (double &value : temp)\n\t\tif (value < q1)\n\t\t\t++bins[0];\n\t\telse if (value < q2)\n\t\t\t++bins[1];\n\t\telse if (value < q3)\n\t\t\t++bins[2];\n\t\telse if (value < q4)\n\t\t\t++bins[3];\n\t\telse\n\t\t\t++bins[4];\n}",
            "assert(x.size()!= 0);\n  size_t quartile = (x.size() + 1)/4;\n  bins.at(0) = 0;\n  bins.at(1) = 0;\n  bins.at(2) = 0;\n  bins.at(3) = 0;\n\n  for(int i = 0; i < x.size(); i++) {\n    if(x.at(i) < 0.25)\n      bins.at(0)++;\n    else if(x.at(i) < 0.5)\n      bins.at(1)++;\n    else if(x.at(i) < 0.75)\n      bins.at(2)++;\n    else if(x.at(i) < 1)\n      bins.at(3)++;\n    else\n      std::cerr << \"ERROR: this point \" << x.at(i) << \" is not in [0,1]\" << std::endl;\n  }\n}",
            "// TODO: your code here\n\n  return;\n}",
            "std::array<size_t, 4> counts = {0, 0, 0, 0};\n    for (double value : x) {\n        if (value < 0.25) {\n            ++counts[0];\n        } else if (value < 0.5) {\n            ++counts[1];\n        } else if (value < 0.75) {\n            ++counts[2];\n        } else {\n            ++counts[3];\n        }\n    }\n    bins = counts;\n}",
            "// TODO\n    std::sort(x.begin(), x.end());\n    double step = 0.25;\n    size_t size = x.size();\n    size_t left = 0;\n    size_t right = size - 1;\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    while(left < right)\n    {\n        if(x[left] < step)\n        {\n            ++bins[0];\n        }\n        else if(x[left] >= step && x[left] < 2 * step)\n        {\n            ++bins[1];\n        }\n        else if(x[left] >= 2 * step && x[left] < 3 * step)\n        {\n            ++bins[2];\n        }\n        else\n        {\n            ++bins[3];\n        }\n        ++left;\n    }\n}",
            "// Count the number of doubles in the vector x that have a fractional part \n    // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n\n    // TODO: Compute the number of doubles in the vector `x` that have a \n    // fractional part in the interval [0, 0.25).\n    bins[0] = 0;\n    for (auto it = x.begin(); it!= x.end(); ++it){\n        if (0 < (*it) && (*it) <= 0.25){\n            bins[0]++;\n        }\n    }\n    // TODO: Compute the number of doubles in the vector `x` that have a \n    // fractional part in the interval [0.25, 0.5).\n    bins[1] = 0;\n    for (auto it = x.begin(); it!= x.end(); ++it){\n        if (0.25 < (*it) && (*it) <= 0.5){\n            bins[1]++;\n        }\n    }\n    // TODO: Compute the number of doubles in the vector `x` that have a \n    // fractional part in the interval [0.5, 0.75).\n    bins[2] = 0;\n    for (auto it = x.begin(); it!= x.end(); ++it){\n        if (0.5 < (*it) && (*it) <= 0.75){\n            bins[2]++;\n        }\n    }\n    // TODO: Compute the number of doubles in the vector `x` that have a \n    // fractional part in the interval [0.75, 1).\n    bins[3] = 0;\n    for (auto it = x.begin(); it!= x.end(); ++it){\n        if (0.75 < (*it) && (*it) <= 1){\n            bins[3]++;\n        }\n    }\n}",
            "std::array<double, 4> quartiles = {{0.25, 0.5, 0.75, 1.0}};\n    std::vector<double> temp;\n    double temp_element;\n    std::array<size_t, 4> counter;\n    counter = {0,0,0,0};\n\n    // sort\n    std::sort(x.begin(), x.end());\n\n    // make the temp vector\n    temp = x;\n\n    for(size_t i = 0; i < 4; i++){\n        for(size_t j = 0; j < x.size(); j++){\n            if(x[j] < quartiles[i]){\n                temp_element = temp[j];\n                temp.erase(temp.begin() + j);\n                temp.insert(temp.begin(), temp_element);\n                counter[i]++;\n                j=0;\n            }\n        }\n    }\n\n    bins = counter;\n}",
            "// Your code here\n    \n    // \u8ba1\u7b97\u51fa\u6bcf\u4e2a\u533a\u95f4\u7684\u4e0b\u754c\n    double l1 = 0.0, l2 = 0.25, l3 = 0.5, l4 = 0.75, l5 = 1.0;\n    double up1 = 1.0, up2 = 0.25, up3 = 0.5, up4 = 0.75, up5 = 1.0;\n    double epsilon = 0.0001;\n    for (int i = 0; i < x.size(); i++) {\n        if ((x[i] >= l1 && x[i] < l2) || (x[i] < l1 && x[i] > l2 - epsilon))\n            bins[0]++;\n        if ((x[i] >= l2 && x[i] < l3) || (x[i] < l2 && x[i] > l3 - epsilon))\n            bins[1]++;\n        if ((x[i] >= l3 && x[i] < l4) || (x[i] < l3 && x[i] > l4 - epsilon))\n            bins[2]++;\n        if ((x[i] >= l4 && x[i] < l5) || (x[i] < l4 && x[i] > l5 - epsilon))\n            bins[3]++;\n    }\n}",
            "// TODO: Your code here.\n}",
            "auto mid = x.size() / 2;\n    auto first = x.begin();\n    auto last = x.begin();\n    last += mid;\n    std::advance(first, 0);\n    std::advance(last, 3);\n    std::nth_element(x.begin(), first, last);\n    std::nth_element(x.begin(), last, x.end());\n    first = x.begin();\n    last = x.begin();\n    last += mid;\n    std::advance(first, 1);\n    std::advance(last, 2);\n    std::nth_element(x.begin(), first, last);\n    std::nth_element(x.begin(), last, x.end());\n    first = x.begin();\n    last = x.begin();\n    last += mid;\n    std::advance(first, 2);\n    std::advance(last, 3);\n    std::nth_element(x.begin(), first, last);\n    std::nth_element(x.begin(), last, x.end());\n    first = x.begin();\n    last = x.begin();\n    last += mid;\n    std::advance(first, 3);\n    std::nth_element(x.begin(), first, last);\n    std::nth_element(x.begin(), last, x.end());\n    bins[0] = std::count_if(x.begin(), x.begin()+mid, [](double x){ return (x % 1) <= 0.25; });\n    bins[1] = std::count_if(x.begin()+mid, x.begin()+2*mid, [](double x){ return (x % 1) <= 0.25; });\n    bins[2] = std::count_if(x.begin()+2*mid, x.begin()+3*mid, [](double x){ return (x % 1) <= 0.25; });\n    bins[3] = std::count_if(x.begin()+3*mid, x.end(), [](double x){ return (x % 1) <= 0.25; });\n\n}",
            "assert(x.size() > 0);\n  double low = 0.0;\n  double high = 0.25;\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n\n  for (int i = 0; i < x.size(); i++) {\n    double xi = x[i];\n    if (low <= xi && xi < high) {\n      bins[0]++;\n    } else if (high <= xi && xi < low + 0.5) {\n      bins[1]++;\n    } else if (low + 0.5 <= xi && xi < high + 0.5) {\n      bins[2]++;\n    } else if (high + 0.5 <= xi && xi < low + 1.0) {\n      bins[3]++;\n    }\n  }\n}",
            "std::array<size_t, 4> counts{};\n    for (auto const& item : x) {\n        auto index = static_cast<size_t>(item);\n        if (index < counts.size()) {\n            ++counts[index];\n        } else {\n            ++counts[counts.size() - 1];\n        }\n    }\n    bins = counts;\n}",
            "// TODO: Your code here\n}",
            "std::sort(x.begin(), x.end());\n   size_t n = x.size();\n   bins.fill(0);\n   size_t i = 0;\n   while (i < n && x[i] < 0.25) {\n      bins[0] += 1;\n      i += 1;\n   }\n   while (i < n && x[i] < 0.5) {\n      bins[1] += 1;\n      i += 1;\n   }\n   while (i < n && x[i] < 0.75) {\n      bins[2] += 1;\n      i += 1;\n   }\n   while (i < n) {\n      bins[3] += 1;\n      i += 1;\n   }\n}",
            "size_t n = x.size();\n   // Initialize the `bins` array to zero\n   for (int i = 0; i < 4; i++) {\n      bins[i] = 0;\n   }\n   // Sort the array `x`\n   std::sort(x.begin(), x.end());\n   // Iterate over the array `x`\n   for (int i = 0; i < n; i++) {\n      // If the `i`-th element of the array `x` has a fractional part in [0, 0.25),\n      // then add 1 to the 0-th element of the array `bins`.\n      if (x[i] - std::floor(x[i]) < 0.25) {\n         bins[0] += 1;\n      }\n      // If the `i`-th element of the array `x` has a fractional part in [0.25, 0.5),\n      // then add 1 to the 1-th element of the array `bins`.\n      else if (x[i] - std::floor(x[i]) >= 0.25 && x[i] - std::floor(x[i]) < 0.5) {\n         bins[1] += 1;\n      }\n      // If the `i`-th element of the array `x` has a fractional part in [0.5, 0.75),\n      // then add 1 to the 2-th element of the array `bins`.\n      else if (x[i] - std::floor(x[i]) >= 0.5 && x[i] - std::floor(x[i]) < 0.75) {\n         bins[2] += 1;\n      }\n      // If the `i`-th element of the array `x` has a fractional part in [0.75, 1),\n      // then add 1 to the 3-th element of the array `bins`.\n      else {\n         bins[3] += 1;\n      }\n   }\n   // Return the array `bins`\n   return;\n}",
            "std::sort(x.begin(), x.end());\n  std::array<double, 4> bins_ = {0.25, 0.5, 0.75, 1.0};\n  bins = {0};\n  for (double bin : bins_) {\n    bins[bins.size() - 1] += std::count_if(x.begin(), x.end(),\n      [&bin](double val) { return val < bin; });\n  }\n}",
            "//TODO: Your code here\n}",
            "std::vector<double>::const_iterator it;\n    std::vector<double>::const_iterator end = x.end();\n    double temp;\n    for(int i = 0; i < 4; i++){\n        it = x.begin();\n        while(it!= end) {\n            temp = fmod(*it, 1.0);\n            if(temp < 0.25 && temp >= 0) {\n                bins[i]++;\n            }\n            else if(temp < 0.5 && temp >= 0.25) {\n                bins[i]++;\n            }\n            else if(temp < 0.75 && temp >= 0.5) {\n                bins[i]++;\n            }\n            else if(temp < 1 && temp >= 0.75) {\n                bins[i]++;\n            }\n            it++;\n        }\n    }\n}",
            "std::vector<double> x_copy = x;\n   std::sort(x_copy.begin(), x_copy.end());\n\n   // get number of samples\n   size_t N = x_copy.size();\n   // bin boundaries\n   double x0 = x_copy[0];\n   double x1 = x_copy[N - 1];\n   double dx = (x1 - x0) / 4.;\n\n   bins = {0, 0, 0, 0};\n   for (size_t i = 0; i < N; ++i) {\n      if (x_copy[i] < x0 + dx) {\n         ++bins[0];\n      } else if (x_copy[i] < x0 + 2 * dx) {\n         ++bins[1];\n      } else if (x_copy[i] < x0 + 3 * dx) {\n         ++bins[2];\n      } else if (x_copy[i] < x1 - dx) {\n         ++bins[3];\n      }\n   }\n}",
            "// TO BE IMPLEMENTED\n}",
            "// TODO: Replace with a one-liner using a lambda.\n    //       Make sure to use std::sort\n    std::array<double, 4> quartiles;\n    quartiles[0] = getQuartile(x, 0);\n    quartiles[1] = getQuartile(x, 0.25);\n    quartiles[2] = getQuartile(x, 0.5);\n    quartiles[3] = getQuartile(x, 0.75);\n\n    bins[0] = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < quartiles[0]) {\n            bins[0]++;\n        }\n        else if (x[i] >= quartiles[0] && x[i] < quartiles[1]) {\n            bins[1]++;\n        }\n        else if (x[i] >= quartiles[1] && x[i] < quartiles[2]) {\n            bins[2]++;\n        }\n        else if (x[i] >= quartiles[2] && x[i] <= quartiles[3]) {\n            bins[3]++;\n        }\n    }\n}",
            "std::sort(x.begin(), x.end());\n    bins[0] = x.size() * 0.25;\n    bins[1] = x.size() * 0.5;\n    bins[2] = x.size() * 0.75;\n    bins[3] = x.size();\n}",
            "size_t count[4] = { 0, 0, 0, 0 };\n\n    for (auto v : x) {\n        double d = fmod(v, 1);\n        if (d <= 0.25)\n            count[0]++;\n        else if (d <= 0.5)\n            count[1]++;\n        else if (d <= 0.75)\n            count[2]++;\n        else\n            count[3]++;\n    }\n\n    bins = { count[0], count[1], count[2], count[3] };\n}",
            "std::vector<double> v;\n    v.resize(x.size());\n    v = x;\n    std::sort(v.begin(), v.end());\n    double q1 = v[v.size() / 4];\n    double q2 = v[v.size() / 2];\n    double q3 = v[3 * v.size() / 4];\n\n    bins[0] = 0;\n    for (auto iter = v.begin(); iter!= v.end(); ++iter) {\n        if (*iter >= q3)\n            break;\n        if (*iter >= q2)\n            bins[1] += 1;\n        if (*iter >= q1)\n            bins[2] += 1;\n        else\n            bins[3] += 1;\n    }\n}",
            "std::vector<double> x_copy = x;\n    std::sort(x_copy.begin(), x_copy.end());\n    // Count the number of doubles in the vector x that have a fractional part \n    // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n    size_t size = x_copy.size();\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    for (int i = 0; i < size; i++) {\n        if ((x_copy[i] - floor(x_copy[i])) < 0.25) {\n            bins[0]++;\n        }\n        else if ((x_copy[i] - floor(x_copy[i])) >= 0.25 && (x_copy[i] - floor(x_copy[i])) < 0.5) {\n            bins[1]++;\n        }\n        else if ((x_copy[i] - floor(x_copy[i])) >= 0.5 && (x_copy[i] - floor(x_copy[i])) < 0.75) {\n            bins[2]++;\n        }\n        else {\n            bins[3]++;\n        }\n    }\n}",
            "// sort the vector of doubles into ascending order\n  std::vector<double> x_sorted = x;\n  std::sort(x_sorted.begin(), x_sorted.end());\n  \n  // Count the number of doubles in the vector x that have a fractional part \n  // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  for(int i = 0; i < x_sorted.size(); i++) {\n    double val = x_sorted[i];\n    double fraction = std::modf(val, &val);\n    if (fraction <= 0.25) {\n      bins[0]++;\n    } else if (fraction <= 0.5) {\n      bins[1]++;\n    } else if (fraction <= 0.75) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n  \n}",
            "// Sort the vector x.\n    std::vector<double> x_copy = x;\n    std::sort(x_copy.begin(), x_copy.end());\n\n    // Count the elements in the vector x that have a fractional part in each bin.\n    size_t i = 0;\n    size_t j = 0;\n    size_t k = 0;\n    size_t l = 0;\n    while (i < x_copy.size()) {\n        if (x_copy[i] - (int) x_copy[i] < 0.25) bins[j]++;\n        else if (x_copy[i] - (int) x_copy[i] < 0.5) bins[k]++;\n        else if (x_copy[i] - (int) x_copy[i] < 0.75) bins[l]++;\n        else bins[0]++;\n        i++;\n    }\n}",
            "// TODO: Your code here\n  std::sort(x.begin(),x.end());\n  std::array<double,4> quartiles = getQuartiles(x);\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  for(double xval : x){\n    if (xval < quartiles[0]){\n      bins[0]++;\n    }\n    else if (quartiles[0] <= xval && xval < quartiles[1]){\n      bins[1]++;\n    }\n    else if (quartiles[1] <= xval && xval < quartiles[2]){\n      bins[2]++;\n    }\n    else if (quartiles[2] <= xval && xval <= quartiles[3]){\n      bins[3]++;\n    }\n    else{\n      std::cerr << \"Wrong quartile input\" << std::endl;\n    }\n  }\n}",
            "std::sort(x.begin(), x.end());\n\n    int i = 0;\n    int j = 0;\n    int k = 0;\n    int l = 0;\n    for (auto &d : x) {\n        if (d < 0.25) {\n            i++;\n        }\n        else if (d < 0.5) {\n            j++;\n        }\n        else if (d < 0.75) {\n            k++;\n        }\n        else if (d < 1.0) {\n            l++;\n        }\n    }\n\n    bins[0] = i;\n    bins[1] = j;\n    bins[2] = k;\n    bins[3] = l;\n}",
            "bins.fill(0);\n   size_t n = x.size();\n\n   for (size_t i = 0; i < n; i++) {\n      if (x[i] < 0.25) bins[0] += 1;\n      else if (x[i] < 0.5) bins[1] += 1;\n      else if (x[i] < 0.75) bins[2] += 1;\n      else if (x[i] < 1) bins[3] += 1;\n   }\n}",
            "std::array<double, 4> quartiles;\n  quartiles[0] = 0.0;\n  quartiles[1] = 0.25;\n  quartiles[2] = 0.5;\n  quartiles[3] = 0.75;\n\n  for (double quartile : quartiles) {\n\n    size_t bin = 0;\n    for (double xi : x) {\n      if (xi < quartile) {\n        ++bin;\n      }\n    }\n\n    bins[bin]++;\n  }\n}",
            "// std::sort(x.begin(), x.end());\n    for(double i : x) {\n        if (i < 0.25) {\n            bins[0]++;\n        } else if (i < 0.5) {\n            bins[1]++;\n        } else if (i < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n    return;\n}",
            "// initialize the bins to zero\n    std::fill(bins.begin(), bins.end(), 0);\n\n    for (auto i : x) {\n        if (i < 0) {\n            throw std::out_of_range(\"The value of the input array must be non-negative.\");\n        }\n\n        if (i >= 1) {\n            throw std::out_of_range(\"The value of the input array must be less than 1.\");\n        }\n\n        if (i >= 0.25 && i < 0.5) {\n            bins[0]++;\n        } else if (i >= 0.5 && i < 0.75) {\n            bins[1]++;\n        } else if (i >= 0.75 && i < 1) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "std::vector<double> xCopy(x.size());\n    std::copy(x.begin(), x.end(), xCopy.begin());\n    std::sort(xCopy.begin(), xCopy.end());\n    size_t xSize = xCopy.size();\n\n    double min = xCopy[0];\n    double max = xCopy[xSize - 1];\n\n    double epsilon = 0.25;\n    bins = {0, 0, 0, 0};\n\n    if(min == max) {\n        return;\n    }\n\n    double bin_size = (max - min) / 4;\n    for(size_t i = 0; i < xSize; i++) {\n        double num = xCopy[i] - min;\n        int binIndex = (int) (num / bin_size);\n        if(binIndex > 3) {\n            binIndex = 3;\n        }\n        if(binIndex < 0) {\n            binIndex = 0;\n        }\n        bins[binIndex]++;\n    }\n\n}",
            "size_t const n = x.size();\n  std::array<size_t, 4> counts = {0, 0, 0, 0};\n\n  for (size_t i = 0; i < n; i++) {\n    double x_i = x[i];\n    // TODO: Fill in the `counts` array, using `floor`.\n    // Do not forget to check that the values of x_i are in [0, 1).\n    counts[0] += floor(x_i);\n    counts[1] += floor(x_i) + 0.25;\n    counts[2] += floor(x_i) + 0.5;\n    counts[3] += floor(x_i) + 0.75;\n  }\n\n  bins = counts;\n}",
            "assert(x.size() > 1);\n    assert(bins.size() == 4);\n    double lo = x.front();\n    for (size_t i=0; i < x.size(); ++i) {\n        if (x[i] >= lo && x[i] < lo+0.25) bins[0]++;\n        else if (x[i] >= lo+0.25 && x[i] < lo+0.5) bins[1]++;\n        else if (x[i] >= lo+0.5 && x[i] < lo+0.75) bins[2]++;\n        else if (x[i] >= lo+0.75 && x[i] < lo+1.0) bins[3]++;\n        lo = x[i];\n    }\n}",
            "// Write your code here\n\n}",
            "size_t N = x.size();\n   if (N == 0)\n   {\n      bins = { 0, 0, 0, 0 };\n      return;\n   }\n\n   size_t floor_part = 0;\n   size_t frac_part = 0;\n\n   for (size_t i = 0; i < N; i++)\n   {\n      size_t i_floor = std::floor(x[i]);\n      if (i_floor!= x[i])\n      {\n         frac_part++;\n      }\n      else\n      {\n         floor_part++;\n      }\n   }\n\n   size_t part_size = floor_part / 4;\n   std::array<size_t, 4> res = {\n      part_size + (floor_part % 4),\n      part_size,\n      part_size,\n      part_size\n   };\n\n   bins = res;\n}",
            "int num_samples = x.size();\n    if (num_samples == 0) {\n        bins = {0, 0, 0, 0};\n        return;\n    }\n    std::sort(x.begin(), x.end());\n    double min = x.at(0);\n    double max = x.at(num_samples-1);\n    double bin_size = (max - min) / 4.0;\n\n    for (int i = 0; i < 4; ++i) {\n        bins[i] = 0;\n    }\n    double curr = min;\n    while (curr <= max) {\n        bins[(int)((curr - min) / bin_size)] += 1;\n        curr += bin_size;\n    }\n}",
            "auto it = x.begin();\n    double const max = x.back();\n    bins[0] = 0;\n    while(it < x.end() && *it < 0.25 * max) {\n        ++bins[0];\n        ++it;\n    }\n    bins[1] = 0;\n    while(it < x.end() && *it < 0.5 * max) {\n        ++bins[1];\n        ++it;\n    }\n    bins[2] = 0;\n    while(it < x.end() && *it < 0.75 * max) {\n        ++bins[2];\n        ++it;\n    }\n    bins[3] = 0;\n    while(it < x.end()) {\n        ++bins[3];\n        ++it;\n    }\n}",
            "bins = {0, 0, 0, 0};\n\tdouble fractional_part;\n\tint index;\n\n\tfor (auto val : x) {\n\t\tfractional_part = modf(val, &index);\n\n\t\tif (fractional_part >= 0 && fractional_part <= 0.25) {\n\t\t\tbins[0]++;\n\t\t}\n\t\telse if (fractional_part > 0.25 && fractional_part <= 0.5) {\n\t\t\tbins[1]++;\n\t\t}\n\t\telse if (fractional_part > 0.5 && fractional_part <= 0.75) {\n\t\t\tbins[2]++;\n\t\t}\n\t\telse if (fractional_part > 0.75 && fractional_part < 1) {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "// TODO: implement\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "hip",
        "prompt": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\n__global__ void countQuartiles(const double *x, size_t N, size_t[4] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n\n    if (i >= N) {\n        return;\n    }\n\n    // For the first four threads in a block, compute the quartile indices\n    size_t j = i;\n    size_t quartile_indices[4] = {};\n    if (i < 4) {\n        double quartile_vals[4] = {0.0, 0.25, 0.5, 0.75};\n        for (size_t k = 0; k < 4; k++) {\n            while (j < N && x[j] < quartile_vals[k]) {\n                j++;\n            }\n            quartile_indices[k] = j;\n        }\n    }\n\n    // Synchronize the threads in this block, so that they have all computed the quartile indices\n    __syncthreads();\n\n    // Every thread in the block counts the number of elements in its quartile\n    for (size_t k = 0; k < 4; k++) {\n        if (i < 4) {\n            for (size_t j = quartile_indices[k]; j < N && x[j] < quartile_vals[k + 1]; j++) {\n                bins[k]++;\n            }\n        }\n    }\n}",
            "//...\n}",
            "// TODO: YOUR CODE HERE\n    // Calculate the number of elements in each bin\n    int start = threadIdx.x;\n    int step = blockDim.x;\n    int i = start;\n    int j = 0;\n    while(i < N){\n        if(x[i] < 0.25) bins[j] += 1;\n        else if(x[i] < 0.5) bins[j+1] += 1;\n        else if(x[i] < 0.75) bins[j+2] += 1;\n        else bins[j+3] += 1;\n        i += step;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N)\n        return;\n\n    double x_val = x[i];\n    double frac = modf(x_val, &x_val);\n    size_t index = 4 * (size_t)frac;\n\n    atomicAdd(&bins[index], 1);\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n   int stride = blockDim.x * gridDim.x;\n   for (int j = i; j < N; j += stride) {\n      double value = x[j];\n      int bin = int(value*4);\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n    int stride = blockDim.x * gridDim.x;\n    for (size_t i = index; i < N; i += stride) {\n        // Fill this in\n    }\n}",
            "// Your code here\n    size_t tid = threadIdx.x;\n    __shared__ size_t s_x[THREADS];\n    s_x[tid] = x[tid];\n    __syncthreads();\n    // check if x is in [0, 0.25), [0.25, 0.5), [0.5, 0.75), [0.75, 1)\n    if (tid < N) {\n        if (s_x[tid] >= 0 && s_x[tid] < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (s_x[tid] >= 0.25 && s_x[tid] < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (s_x[tid] >= 0.5 && s_x[tid] < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else if (s_x[tid] >= 0.75 && s_x[tid] <= 1) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "// allocate local memory for each thread\n    __shared__ double quartiles[4];\n\n    // calculate quartile boundaries\n    int low = threadIdx.x * 4;\n    int high = low + 3;\n    double boundaries[4] = {\n        x[low], \n        x[low + 1] * 0.25 + x[low], \n        x[high - 1] * 0.75 + x[low], \n        x[high]\n    };\n\n    // calculate quartile values for each thread\n    double x_thread[4];\n    for (int i = 0; i < 4; i++) {\n        double x_thread_i = 0;\n        for (int j = 0; j < N; j++) {\n            if (x[j] >= boundaries[i] && x[j] < boundaries[i + 1])\n                x_thread_i += 1;\n        }\n        x_thread[i] = x_thread_i;\n    }\n\n    // add quartiles to the global memory\n    for (int i = 0; i < 4; i++)\n        quartiles[i] += x_thread[i];\n\n    // reduction\n    for (int i = 16; i > 0; i /= 2)\n        if (threadIdx.x % (2 * i) == 0 && threadIdx.x + i < 16)\n            quartiles[threadIdx.x / (2 * i)] += quartiles[threadIdx.x + i];\n\n    // store the results in global memory\n    if (threadIdx.x == 0) {\n        for (int i = 0; i < 4; i++)\n            bins[i] = (size_t) quartiles[i];\n    }\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n    int i = threadIdx.x + blockDim.x * blockIdx.x;\n    if (i < N) {\n        int index = floor(x[i] * 4);\n        atomicAdd(&bins[index], 1);\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        bins[int(x[i])] += 1;\n    }\n}",
            "__shared__ int sdata[BLOCK_SIZE];\n    // Compute the index of the thread in the vector x\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n    // Store the fractional part of each thread in a thread-local variable\n    double t = fmod(x[tid], 1.0);\n\n    // Store the fractional part of the thread in the appropriate bin\n    // in a thread-local variable\n    int bin = (t < 0.25)? 0 : (t < 0.5)? 1 : (t < 0.75)? 2 : 3;\n\n    // Accumulate the fractional part of the thread in the appropriate bin\n    sdata[threadIdx.x] = bin;\n\n    __syncthreads();\n\n    // Perform tree-based parallel reduction\n    for (int s = BLOCK_SIZE / 2; s > 0; s /= 2) {\n        if (threadIdx.x < s) {\n            sdata[threadIdx.x] += sdata[threadIdx.x + s];\n        }\n\n        __syncthreads();\n    }\n\n    // Write the result of the reduction to global memory\n    if (threadIdx.x == 0) {\n        bins[0] = sdata[0];\n        bins[1] = sdata[1];\n        bins[2] = sdata[2];\n        bins[3] = sdata[3];\n    }\n}",
            "// Your code here\n  // Examples:\n  // x[i] = 0.27 => bin = 2\n  // x[i] = 7.6 => bin = 2\n}",
            "// TODO: implement\n}",
            "__shared__ size_t s[32];\n\n  // Compute the thread index in a 1D grid, and the corresponding\n  // index in the vector x.\n  size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  size_t xi = i * N / blockDim.x;\n\n  // Initialize the thread index.\n  s[threadIdx.x] = 0;\n\n  // Each thread sums the number of doubles in x that are in a particular range.\n  // The kernel is launched with at least N threads.\n  for (size_t j = xi; j < N; j += blockDim.x * gridDim.x) {\n    if (j < N && (x[j] - j % 1) < 0.25) s[threadIdx.x]++;\n    if (j < N && (x[j] - j % 1) >= 0.25 && (x[j] - j % 1) < 0.5) s[threadIdx.x]++;\n    if (j < N && (x[j] - j % 1) >= 0.5 && (x[j] - j % 1) < 0.75) s[threadIdx.x]++;\n    if (j < N && (x[j] - j % 1) >= 0.75 && (x[j] - j % 1) < 1) s[threadIdx.x]++;\n  }\n\n  // Each thread now has the number of doubles in x that fall in the corresponding range.\n  // This is used to compute the quartiles.\n\n  __syncthreads();\n\n  // Compute the quartiles.\n  // The thread index corresponds to the quartile.\n  // The thread sum corresponds to the number of doubles in x that fall in the quartile.\n  // The result is stored in the corresponding bin.\n  if (threadIdx.x == 0) {\n    bins[0] = s[0];\n  }\n  if (threadIdx.x == 1) {\n    bins[1] = s[0] + s[1];\n  }\n  if (threadIdx.x == 2) {\n    bins[2] = s[0] + s[1] + s[2];\n  }\n  if (threadIdx.x == 3) {\n    bins[3] = s[0] + s[1] + s[2] + s[3];\n  }\n\n  return;\n}",
            "const size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n\n    if (tid >= N) {\n        return;\n    }\n\n    size_t index = (N / 4) * (tid / (N / 4));\n    size_t i = (tid % (N / 4)) + 1;\n    double value = x[tid];\n\n    if (value > 0.25 && value <= 0.5) {\n        atomicAdd(&bins[1], 1);\n    } else if (value > 0.5 && value <= 0.75) {\n        atomicAdd(&bins[2], 1);\n    } else if (value > 0.75 && value < 1.0) {\n        atomicAdd(&bins[3], 1);\n    } else if (value <= 0.25) {\n        atomicAdd(&bins[0], 1);\n    } else {\n        printf(\"ERROR: value %lf out of range\\n\", value);\n    }\n}",
            "__shared__ size_t s_bins[4];\n\n  /* TODO */\n  s_bins[threadIdx.x] = 0;\n\n  __syncthreads();\n\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    double value = x[i];\n    size_t bin = (value * 4) > 0.75;\n    atomicAdd(&s_bins[bin], 1);\n  }\n\n  __syncthreads();\n\n  for (int i = 0; i < 4; i++) {\n    atomicAdd(&bins[i], s_bins[i]);\n  }\n}",
            "size_t idx = threadIdx.x + blockIdx.x*blockDim.x;\n    if (idx < N) {\n        int bin = 0;\n        double xd = x[idx];\n        if (xd < 0.25) bin = 0;\n        else if (xd < 0.5) bin = 1;\n        else if (xd < 0.75) bin = 2;\n        else bin = 3;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "__shared__ size_t sharedBins[4];\n  sharedBins[threadIdx.x] = 0;\n  __syncthreads();\n  size_t i = threadIdx.x;\n  while (i < N) {\n    // TODO\n    size_t index = (size_t)((x[i]-floor(x[i]))*4);\n    atomicAdd(&sharedBins[index],1);\n    i+=blockDim.x*gridDim.x;\n  }\n  __syncthreads();\n  if(threadIdx.x<4)\n    atomicAdd(&bins[threadIdx.x],sharedBins[threadIdx.x]);\n}",
            "// implement\n}",
            "// TODO: Implement this\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n    size_t stride = blockDim.x * gridDim.x;\n    for (int i = idx; i < N; i += stride) {\n        int bin = 0;\n        if (x[i] >= 0.75) {\n            bin = 3;\n        } else if (x[i] >= 0.5) {\n            bin = 2;\n        } else if (x[i] >= 0.25) {\n            bin = 1;\n        }\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "int i = threadIdx.x;\n    size_t index = blockDim.x*blockIdx.x + i;\n\n    size_t N32 = N >> 5;\n\n    if (index < N32) {\n        size_t offset = index << 5;\n        size_t lo = 0, hi = 0, med = 0, high = 0;\n\n        for (size_t i = offset; i < offset + 32; i++) {\n            if (x[i] < 0.25) lo++;\n            else if (x[i] < 0.5) med++;\n            else if (x[i] < 0.75) hi++;\n            else high++;\n        }\n\n        bins[0] = lo;\n        bins[1] = med;\n        bins[2] = hi;\n        bins[3] = high;\n    }\n}",
            "// Declare shared memory for storing values in the current thread block\n    __shared__ double smem[TPB_x * 4];\n\n    // Compute the index of the first element in the current thread block\n    const size_t tbIdx = (blockIdx.x * blockDim.x + threadIdx.x);\n\n    // Compute the index of the last element in the current thread block\n    const size_t end = min(tbIdx + TPB_x * 4, N);\n\n    // Copy x[tbIdx] to smem[threadIdx.x]\n    smem[threadIdx.x] = x[tbIdx];\n\n    // Wait for all threads to copy to shared memory\n    __syncthreads();\n\n    for (size_t i = tbIdx; i < end; i += TPB_x) {\n        // Check if the element at index i has a fractional part in [0, 0.25)\n        if (fmod(smem[i], 1.0) < 0.25) {\n            bins[0] += 1;\n        }\n\n        // Check if the element at index i has a fractional part in [0.25, 0.5)\n        else if (fmod(smem[i], 1.0) >= 0.25 && fmod(smem[i], 1.0) < 0.5) {\n            bins[1] += 1;\n        }\n\n        // Check if the element at index i has a fractional part in [0.5, 0.75)\n        else if (fmod(smem[i], 1.0) >= 0.5 && fmod(smem[i], 1.0) < 0.75) {\n            bins[2] += 1;\n        }\n\n        // Check if the element at index i has a fractional part in [0.75, 1)\n        else if (fmod(smem[i], 1.0) >= 0.75) {\n            bins[3] += 1;\n        }\n    }\n}",
            "const size_t start_idx = threadIdx.x + blockDim.x * blockIdx.x;\n    const size_t num_threads = blockDim.x * gridDim.x;\n\n    // TODO: Implement the kernel\n    // The implementation of the kernel should follow the pseudocode provided\n    // in the assignment description.\n    //\n    // Note:\n    //   1. The block size and grid size should be used to implement the\n    //      kernel.\n    //   2. For the case when the `N` is not divisible by `num_threads`,\n    //      make sure to account for the extra threads.\n    //   3. You do not need to use atomic operations for this kernel.\n\n    for (size_t i = start_idx; i < N; i += num_threads) {\n        if (x[i] >= 0 && x[i] < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (x[i] >= 0.25 && x[i] < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else if (x[i] >= 0.75 && x[i] < 1) {\n            atomicAdd(&bins[3], 1);\n        } else {\n            printf(\"Invalid input: x[i] = %f\\n\", x[i]);\n        }\n    }\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n    size_t stride = blockDim.x * gridDim.x;\n    for (; idx < N; idx += stride) {\n        double val = x[idx];\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        }\n        else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        }\n        else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        }\n        else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "// TODO: Your code here\n\n}",
            "// This kernel does not do anything with the data in x, but uses HIP's\n    // atomic primitive to avoid concurrent writes to bins[].\n\n    size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx >= N) return;\n\n    double xi = x[idx];\n\n    int bin = 0;\n    if (xi >= 0.0 && xi <= 0.25) {\n        bin = 0;\n    } else if (xi > 0.25 && xi <= 0.5) {\n        bin = 1;\n    } else if (xi > 0.5 && xi <= 0.75) {\n        bin = 2;\n    } else if (xi > 0.75 && xi <= 1.0) {\n        bin = 3;\n    }\n\n    atomicAdd(&bins[bin], 1);\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (tid < N) {\n        double q = 4 * ((double)tid + 1) / (double)N;\n        int bin = 0;\n        if (q >= 0.75) {\n            bin = 3;\n        } else if (q >= 0.5) {\n            bin = 2;\n        } else if (q >= 0.25) {\n            bin = 1;\n        }\n        atomicAdd(&(bins[bin]), 1);\n    }\n}",
            "const size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n    const size_t stride = blockDim.x * gridDim.x;\n    const size_t block_offset = blockIdx.x * blockDim.x;\n    for (size_t i = tid; i < N; i += stride) {\n        size_t index = floor(x[i] * 4);\n        atomicAdd(&bins[index], 1);\n    }\n}",
            "// TODO\n}",
            "size_t tid = threadIdx.x + blockIdx.x*blockDim.x;\n    if (tid < N) {\n        size_t index = (size_t)(x[tid] * 4);\n        atomicAdd(bins+index, 1);\n    }\n}",
            "// Add code here\n}",
            "unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N) {\n        bins[0] += (x[i] >= 0 && x[i] < 0.25)? 1 : 0;\n        bins[1] += (x[i] >= 0.25 && x[i] < 0.5)? 1 : 0;\n        bins[2] += (x[i] >= 0.5 && x[i] < 0.75)? 1 : 0;\n        bins[3] += (x[i] >= 0.75 && x[i] <= 1)? 1 : 0;\n    }\n}",
            "// TODO: implement this\n}",
            "// Your code goes here\n}",
            "}",
            "// TODO\n}",
            "__shared__ double values[1024];\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  int nthreads = blockDim.x * gridDim.x;\n  int offset = 0;\n  while (offset < N) {\n    // Copy values into shared memory\n    for (int i = tid; i < N; i += nthreads) {\n      values[i] = x[i];\n    }\n    __syncthreads();\n\n    // Sort values in shared memory\n    sort(values, N);\n    __syncthreads();\n\n    // Count the number of doubles in the vector x that have a fractional part \n    // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n    // Store the counts in bins\n    for (int i = threadIdx.x; i < 4; i += blockDim.x) {\n      bins[i] = 0;\n    }\n    for (int i = tid; i < N; i += nthreads) {\n      if (i == 0) {\n        bins[0] = values[i];\n      }\n      else if (values[i] - values[i - 1] < 0.25) {\n        bins[0]++;\n      }\n      else if (values[i] - values[i - 1] < 0.5) {\n        bins[1]++;\n      }\n      else if (values[i] - values[i - 1] < 0.75) {\n        bins[2]++;\n      }\n      else {\n        bins[3]++;\n      }\n    }\n    __syncthreads();\n\n    // Update offset\n    int nb = N / nthreads;\n    offset += nb;\n    if (offset < N) {\n      if (tid >= N - offset) {\n        offset = N;\n      }\n      else {\n        offset += nb;\n      }\n    }\n    __syncthreads();\n  }\n}",
            "__shared__ double s_data[BLOCK_SIZE];\n    size_t tid = threadIdx.x;\n    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = blockDim.x * gridDim.x;\n    double *s_sums = s_data + BLOCK_SIZE/2;\n\n    // Copy data to shared memory\n    s_data[tid] = 0;\n    if (i < N) {\n        s_data[tid] = x[i];\n    }\n    __syncthreads();\n\n    // Reduce to compute the quartile counts\n    for (int s = BLOCK_SIZE / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            s_sums[tid] += s_sums[tid + s];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        // Sum the quartile counts in global memory\n        bins[0] = s_sums[0];\n        for (int i=1; i < 4; i++) {\n            bins[i] = bins[i-1] + s_sums[i];\n        }\n    }\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        double x_i = x[tid];\n        if (x_i >= 0.0 && x_i < 0.25) bins[0]++;\n        else if (x_i >= 0.25 && x_i < 0.5) bins[1]++;\n        else if (x_i >= 0.5 && x_i < 0.75) bins[2]++;\n        else if (x_i >= 0.75 && x_i < 1.0) bins[3]++;\n    }\n}",
            "const int tid = threadIdx.x;\n   const int Nthreads = blockDim.x;\n   const int q = blockIdx.x;\n\n   // Local memory buffer.\n   // Store all the data needed by this thread.\n   // The size of the buffer depends on Nthreads.\n   __shared__ double myData[Nthreads];\n\n   // Load our data into shared memory.\n   // This data is only needed for the computation in this thread.\n   // The data is loaded from global memory.\n   myData[tid] = x[q * Nthreads + tid];\n\n   // Wait for all threads to load their data.\n   __syncthreads();\n\n   // The following code is executed by only one thread.\n   // Use it to compute the fractional part of the numbers\n   // in the buffer.\n   double fraction = myData[tid] - (int) myData[tid];\n\n   // Store the counts in the output array.\n   if (tid == 0) {\n      bins[0] = (fraction < 0.25)? 1 : 0;\n      bins[1] = (fraction < 0.50)? 1 : 0;\n      bins[2] = (fraction < 0.75)? 1 : 0;\n      bins[3] = (fraction < 1.00)? 1 : 0;\n   }\n\n   // All threads must wait for the thread with the output.\n   __syncthreads();\n\n   // Write the computed fractional parts into global memory.\n   // This will be read by the next thread.\n   x[q * Nthreads + tid] = fraction;\n}",
            "for (size_t i = threadIdx.x + blockIdx.x * blockDim.x; i < N;\n       i += blockDim.x * gridDim.x) {\n    // Use a switch-case statement to classify the value of `x[i]` into 4\n    // quartiles (bins). The switch-case will use a logical AND operator to\n    // classify a value into multiple bins, and the value `i` will be used\n    // as the index of the bin.\n\n    // Modified by student.\n  }\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n   // Your code goes here\n\n   if(idx >= N)\n   {\n      return;\n   }\n\n   int fract = (int)(x[idx]*100);\n\n   switch(fract)\n   {\n      case 0:\n      case 1:\n      case 2:\n      case 3:\n      case 4:\n      case 5:\n      case 6:\n      case 7:\n      case 8:\n      case 9:\n      case 10:\n      case 11:\n      case 12:\n      case 13:\n      case 14:\n      case 15:\n         bins[0]++;\n         break;\n\n      case 16:\n      case 17:\n      case 18:\n      case 19:\n      case 20:\n      case 21:\n      case 22:\n      case 23:\n      case 24:\n      case 25:\n      case 26:\n      case 27:\n      case 28:\n      case 29:\n      case 30:\n      case 31:\n      case 32:\n      case 33:\n      case 34:\n      case 35:\n      case 36:\n      case 37:\n      case 38:\n      case 39:\n      case 40:\n      case 41:\n      case 42:\n      case 43:\n      case 44:\n      case 45:\n      case 46:\n      case 47:\n      case 48:\n      case 49:\n      case 50:\n      case 51:\n      case 52:\n      case 53:\n      case 54:\n      case 55:\n      case 56:\n      case 57:\n      case 58:\n      case 59:\n      case 60:\n      case 61:\n      case 62:\n      case 63:\n      case 64:\n      case 65:\n      case 66:\n      case 67:\n      case 68:\n      case 69:\n      case 70:\n      case 71:\n      case 72:\n      case 73:\n      case 74:\n      case 75:\n      case 76:\n      case 77:\n      case 78:\n      case 79:\n      case 80:\n      case 81:\n      case 82:\n      case 83:\n      case 84:\n      case 85:\n      case 86:\n      case 87:\n      case 88:\n      case 89:\n      case 90:\n      case 91:\n      case 92:\n      case 93:\n      case 94:\n      case 95:\n      case 96:\n      case 97:\n      case 98:\n      case 99:\n      case 100:\n         bins[1]++;\n         break;\n\n      case 101:\n      case 102:\n      case 103:\n      case 104:\n      case 105:\n      case 106:\n      case 107:\n      case 108:\n      case 109:\n      case 110:\n      case 111:\n      case 112:\n      case 113:\n      case 114:\n      case 115:\n      case 116:\n      case 117:\n      case 118:\n      case 119:\n      case 120:\n      case 121:\n      case 122:\n      case 123:\n      case 124:\n      case 125:\n      case 126:\n      case 127:\n      case 12",
            "}",
            "// Each thread should compute the number of values in `x` that fall into a certain bin.\n   // To do this, the thread should examine each value in `x` and compute\n   // whether it is less than the quartile.\n\n   // TODO\n\n}",
            "const int idx = threadIdx.x;\n    const int stride = blockDim.x;\n    const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // declare all the local memory needed by the thread\n    double* x_local;\n    x_local = (double*)malloc(N * sizeof(double));\n    // if the thread is not active, set its local memory to 0\n    if (tid >= N) {\n        x_local[idx] = 0.0;\n    } else {\n        x_local[idx] = x[tid];\n    }\n    __syncthreads();\n\n    // create a histogram\n    for (int i = 0; i < 4; i++) {\n        bins[i] = 0;\n    }\n\n    // the thread will now go through the vector counting the values\n    // in each quartile.\n    for (int i = idx; i < N; i += stride) {\n        if (x_local[i] >= 0 && x_local[i] < 0.25) {\n            bins[0]++;\n        } else if (x_local[i] >= 0.25 && x_local[i] < 0.5) {\n            bins[1]++;\n        } else if (x_local[i] >= 0.5 && x_local[i] < 0.75) {\n            bins[2]++;\n        } else if (x_local[i] >= 0.75 && x_local[i] <= 1) {\n            bins[3]++;\n        } else {\n            printf(\"There are some values outside the range [0, 1].\\n\");\n        }\n    }\n    __syncthreads();\n\n    // sum the quartiles and the total number of elements in the vector\n    // this allows the first threads to sum all the elements in the vector\n    // and the last threads to sum all the quartiles\n    // TODO\n    for (int i = 0; i < 4; i++) {\n        if (idx == 0) {\n            for (int j = 1; j < stride; j++) {\n                bins[i] += x_local[j * i];\n            }\n        } else if (idx == stride - 1) {\n            for (int j = 0; j < (N - stride); j++) {\n                bins[i] += x_local[j + idx];\n            }\n        }\n        __syncthreads();\n    }\n\n    free(x_local);\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx >= N) return;\n    double val = x[idx];\n    if (val >= 0.75) {\n        atomicAdd(&bins[0], 1);\n    } else if (val >= 0.5) {\n        atomicAdd(&bins[1], 1);\n    } else if (val >= 0.25) {\n        atomicAdd(&bins[2], 1);\n    } else {\n        atomicAdd(&bins[3], 1);\n    }\n}",
            "size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n   if (i >= N)\n      return;\n\n   size_t count_0_to_025 = 0;\n   size_t count_025_to_05 = 0;\n   size_t count_05_to_075 = 0;\n   size_t count_075_to_1 = 0;\n\n   // Fill the 4 bins with the count of the number of elements in the given range\n   for (int j = 0; j < 4; j++) {\n      if (x[i] >= (j * 0.25) && x[i] < ((j + 1) * 0.25)) {\n         switch (j) {\n         case 0:\n            count_0_to_025++;\n            break;\n         case 1:\n            count_025_to_05++;\n            break;\n         case 2:\n            count_05_to_075++;\n            break;\n         case 3:\n            count_075_to_1++;\n            break;\n         }\n      }\n   }\n\n   // Update the bins with the sum of all thread's counts\n   atomicAdd(&bins[0], count_0_to_025);\n   atomicAdd(&bins[1], count_025_to_05);\n   atomicAdd(&bins[2], count_05_to_075);\n   atomicAdd(&bins[3], count_075_to_1);\n}",
            "// TODO: YOUR CODE HERE\n}",
            "__shared__ size_t scratch[BLOCKSIZE];\n\n    size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n\n    // Clear the shared memory\n    if (threadIdx.x == 0) {\n        for (int i = 0; i < BLOCKSIZE; i++) scratch[i] = 0;\n    }\n    __syncthreads();\n\n    // Each thread gets 1 element from the input vector\n    if (tid < N) {\n        double fraction = modf(x[tid], &fraction);\n        fraction = 4.0 * fraction + 0.5;\n        if (fraction < 1.0)\n            fraction = 0;\n        else if (fraction < 2.0)\n            fraction = 1;\n        else if (fraction < 3.0)\n            fraction = 2;\n        else\n            fraction = 3;\n        // Increment the corresponding bin\n        size_t index = (size_t)fraction;\n        atomicAdd(&scratch[index], 1);\n    }\n    __syncthreads();\n\n    // Copy the values back to the global memory\n    if (threadIdx.x == 0) {\n        for (int i = 0; i < BLOCKSIZE; i++) {\n            atomicAdd(&bins[i], scratch[i]);\n        }\n    }\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= N) {\n        return;\n    }\n\n    double xi = x[tid];\n    if (xi < 0.25) {\n        atomicAdd(&(bins[0]), 1);\n    }\n    else if (xi < 0.5) {\n        atomicAdd(&(bins[1]), 1);\n    }\n    else if (xi < 0.75) {\n        atomicAdd(&(bins[2]), 1);\n    }\n    else {\n        atomicAdd(&(bins[3]), 1);\n    }\n}",
            "// TODO: Implement this function\n\n    // 1. Use a shared memory array to store the data of vector x\n    extern __shared__ double x_data[];\n    const int tid = threadIdx.x;\n    if (tid < N) {\n        x_data[tid] = x[tid];\n    }\n\n    // 2. Use atomic operations to add 1 to the bin where the data falls in\n    for (int i = 0; i < N; i++) {\n        if (x_data[i] < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (x_data[i] < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (x_data[i] < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n\n    // 3. Sync all threads and return\n    __syncthreads();\n}",
            "}",
            "}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n    int i = tid * 2;\n    if (i < N) {\n        if (x[i] < 1.0) {\n            if (x[i] < 0.75) {\n                atomicAdd(&bins[0], 1);\n            } else if (x[i] < 1.0) {\n                atomicAdd(&bins[1], 1);\n            } else if (x[i] < 1.25) {\n                atomicAdd(&bins[2], 1);\n            } else {\n                atomicAdd(&bins[3], 1);\n            }\n        }\n    }\n}",
            "/* YOUR CODE HERE */\n}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (index < N) {\n    int bin = 0;\n    if (x[index] >= 0.0 && x[index] <= 0.25) {\n      bin = 0;\n    } else if (x[index] > 0.25 && x[index] <= 0.5) {\n      bin = 1;\n    } else if (x[index] > 0.5 && x[index] <= 0.75) {\n      bin = 2;\n    } else if (x[index] > 0.75 && x[index] <= 1.0) {\n      bin = 3;\n    }\n\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "const size_t thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n  if (thread_id >= N) {\n    return;\n  }\n\n  // Compute the fractional part of x[thread_id].\n  // The fractional part is given by x[thread_id] - floor(x[thread_id]).\n  double frac = x[thread_id] - floor(x[thread_id]);\n\n  // Compute which bin the fractional part belongs to.\n  size_t bin;\n  if (frac < 0.25) {\n    bin = 0;\n  } else if (frac < 0.5) {\n    bin = 1;\n  } else if (frac < 0.75) {\n    bin = 2;\n  } else {\n    bin = 3;\n  }\n\n  // Atomic operations for updating the bin counts.\n  // Counter increment: atomicAdd(address, value_to_add)\n  atomicAdd(&bins[bin], 1);\n}",
            "/* YOUR CODE HERE */\n\n  const int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index < N) {\n    const int i = floor((x[index] - floor(x[index])) * 4);\n    atomicAdd(&bins[i], 1);\n  }\n}",
            "// TODO: YOUR CODE HERE\n    size_t idx = threadIdx.x;\n    if(idx >= N){\n        return;\n    }\n    if(idx < N / 4){\n        if(x[idx] >= 0 && x[idx] <= 0.25){\n            bins[0]++;\n        }else if(x[idx] > 0.25 && x[idx] <= 0.5){\n            bins[1]++;\n        }else if(x[idx] > 0.5 && x[idx] <= 0.75){\n            bins[2]++;\n        }else if(x[idx] > 0.75 && x[idx] <= 1){\n            bins[3]++;\n        }\n    }\n}",
            "// TODO\n}",
            "int id = blockIdx.x * blockDim.x + threadIdx.x;\n  if (id < N) {\n    double d = x[id];\n    if (d < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (d < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (d < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "/*\n  TODO: Your code here\n  */\n\n}",
            "...\n}",
            "size_t tid = threadIdx.x;\n    //TODO: Launch block of threads to compute histogram\n}",
            "// TODO\n    // Start by getting your thread index\n    unsigned int my_thread_index = threadIdx.x;\n    unsigned int my_block_index = blockIdx.x;\n    // Each thread will check 4 numbers at a time\n    unsigned int loop_index = my_thread_index * 4;\n\n    // If we have 100 numbers and each thread handles 4 at a time,\n    // we have 25 blocks, each with 4 threads\n    unsigned int total_blocks = N / 4;\n    // If we have 100 numbers and each thread handles 4 at a time,\n    // we have 25 threads, each processing 4 numbers\n    unsigned int total_threads = N / 4;\n    // If we have 100 numbers and each thread handles 4 at a time,\n    // we have 25 numbers left over\n    unsigned int numbers_left_over = N % 4;\n\n    for (unsigned int i = 0; i < total_blocks; i++) {\n        if (my_block_index == i) {\n            // If we're in this block, we need to check all 4 numbers in the block\n            // For each number in the block, we will check the first 4 numbers after it\n            // In the example below, x[i * 4] is the 1st number in the block, x[i * 4 + 1] is the 2nd number in the block, etc\n            // For example, in the first block (i == 0), the numbers we will check are [0, 1, 2, 3]\n            // We start at the first number in the block (i * 4) and check 4 numbers after it,\n            // so we check x[0], x[1], x[2], and x[3]\n            // In the 2nd block (i == 1), the numbers we will check are [4, 5, 6, 7]\n            // We start at the first number in the block (i * 4) and check 4 numbers after it,\n            // so we check x[4], x[5], x[6], and x[7]\n            // In the 3rd block (i == 2), the numbers we will check are [8, 9, 10, 11]\n            // We start at the first number in the block (i * 4) and check 4 numbers after it,\n            // so we check x[8], x[9], x[10], and x[11]\n            // And so on\n            for (unsigned int j = i * 4; j < i * 4 + 4; j++) {\n                if (x[j] >= 0.0 && x[j] < 0.25) {\n                    bins[0] += 1;\n                }\n                else if (x[j] >= 0.25 && x[j] < 0.5) {\n                    bins[1] += 1;\n                }\n                else if (x[j] >= 0.5 && x[j] < 0.75) {\n                    bins[2] += 1;\n                }\n                else if (x[j] >= 0.75 && x[j] <= 1.0) {\n                    bins[3] += 1;\n                }\n            }\n        }\n        __syncthreads();\n    }\n    if (numbers_left_over > 0) {\n        // If we have numbers left over, we need to check them too\n        for (unsigned int j = total_blocks * 4; j < N; j++) {\n            if (x[j] >= 0.0 && x[j] < 0.25) {\n                bins[0] += 1;\n            }\n            else if (x[j] >= 0.25 && x[j] < 0.5) {\n                bins[1] += 1;\n            }\n            else if (x[j] >= 0.5 && x[j] < 0.75) {\n                bins[2] += 1;\n            }\n            else if (x[j] >= 0.75 && x[j] <= 1.0) {\n                bins[3] +=",
            "// TODO: Replace me!\n}",
            "size_t threadId = threadIdx.x + blockIdx.x * blockDim.x;\n    int i, k;\n\n    for (k = 0; k < 4; k++) bins[k] = 0;\n\n    for (i = threadId; i < N; i += blockDim.x * gridDim.x) {\n        if (x[i] < 0) {\n            printf(\"Warning: value %f < 0 in vector x.\\n\", x[i]);\n            continue;\n        }\n        if (x[i] >= 1) {\n            printf(\"Warning: value %f >= 1 in vector x.\\n\", x[i]);\n            continue;\n        }\n        k = floor(4 * x[i]);\n        if (k < 0) k = 0;\n        if (k > 3) k = 3;\n        atomicAdd(&bins[k], 1);\n    }\n}",
            "// TODO\n}",
            "...\n}",
            "// allocate shared memory\n  __shared__ double local_x[BLOCKSIZE];\n\n  // read input data into shared memory\n  size_t t = threadIdx.x;\n  local_x[t] = x[t];\n\n  // synchronize threads to make sure input has been read in\n  __syncthreads();\n\n  // count number of values less than the quartile\n  size_t i = 0;\n  double quartile = 0.25 * N;\n  for (size_t j = 0; j < BLOCKSIZE; j++) {\n    if (t == j) {\n      if (i < quartile) {\n        i += 1;\n      }\n    }\n    __syncthreads();\n  }\n  bins[0] = i;\n\n  // count number of values less than the quartile\n  quartile = 0.5 * N;\n  i = 0;\n  for (size_t j = 0; j < BLOCKSIZE; j++) {\n    if (t == j) {\n      if (i < quartile) {\n        i += 1;\n      }\n    }\n    __syncthreads();\n  }\n  bins[1] = i;\n\n  // count number of values less than the quartile\n  quartile = 0.75 * N;\n  i = 0;\n  for (size_t j = 0; j < BLOCKSIZE; j++) {\n    if (t == j) {\n      if (i < quartile) {\n        i += 1;\n      }\n    }\n    __syncthreads();\n  }\n  bins[2] = i;\n\n  // count number of values less than the quartile\n  quartile = N;\n  i = 0;\n  for (size_t j = 0; j < BLOCKSIZE; j++) {\n    if (t == j) {\n      if (i < quartile) {\n        i += 1;\n      }\n    }\n    __syncthreads();\n  }\n  bins[3] = i;\n}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n    if (index >= N) return;\n    int bin = 0;\n    if (x[index] < 0.25) {\n        bin = 0;\n    } else if (x[index] < 0.5) {\n        bin = 1;\n    } else if (x[index] < 0.75) {\n        bin = 2;\n    } else {\n        bin = 3;\n    }\n    atomicAdd(&bins[bin], 1);\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    size_t index = (tid + 1) * 4 / N;\n    atomicAdd(&bins[index], 1);\n  }\n}",
            "size_t t = threadIdx.x + blockDim.x * blockIdx.x;\n  if (t < N) {\n    bins[(int)floor(4.0*x[t] + 0.5)]++;\n  }\n}",
            "double xmin = __dv_min(x, N);\n  double xmax = __dv_max(x, N);\n  for (int i = threadIdx.x; i < N; i += blockDim.x) {\n    double xi = x[i];\n    int bin = floor(((xi-xmin)/(xmax-xmin))*4);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int id = blockIdx.x * blockDim.x + threadIdx.x;\n  if (id >= N) {\n    return;\n  }\n\n  int i;\n  double x_i;\n  for (i = 0; i < N; ++i) {\n    x_i = x[i];\n    if (x_i < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (x_i < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (x_i < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "const int t = threadIdx.x;\n  const int b = blockIdx.x;\n  const int n = blockDim.x;\n  const int f = N / n;\n  const int s = N - f * n;\n  const int l = b * f + min(s, t);\n  const int r = l + f;\n\n  bins[t] = 0;\n  for (int i = l; i < r; i++) {\n    if (x[i] >= 0 && x[i] < 0.25)\n      bins[t] += 1;\n    else if (x[i] >= 0.25 && x[i] < 0.5)\n      bins[t] += 1;\n    else if (x[i] >= 0.5 && x[i] < 0.75)\n      bins[t] += 1;\n    else if (x[i] >= 0.75 && x[i] < 1.0)\n      bins[t] += 1;\n  }\n\n  __syncthreads();\n\n  if (t == 0) {\n    for (int i = 1; i < n; i++)\n      bins[0] += bins[i];\n  }\n}",
            "// TODO: replace this comment with your code\n}",
            "unsigned int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  unsigned int stride = blockDim.x * gridDim.x;\n  for (size_t i = tid; i < N; i += stride) {\n    if (x[i] >= 0 && x[i] < 0.25) {\n      bins[0]++;\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      bins[1]++;\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      bins[2]++;\n    } else if (x[i] >= 0.75 && x[i] < 1.0) {\n      bins[3]++;\n    }\n  }\n}",
            "//TODO: launch a kernel here\n    //TODO: add your code here\n    //TODO: return the output\n}",
            "// Insert your code here\n\n}",
            "int i = threadIdx.x;\n  double quarter_val = 0.25;\n  if (i < N) {\n    int bin = 0;\n    if (x[i] < quarter_val) {\n      bin = 0;\n    } else if (x[i] >= quarter_val && x[i] < 0.5 + quarter_val) {\n      bin = 1;\n    } else if (x[i] >= 0.5 + quarter_val && x[i] < 0.75 + quarter_val) {\n      bin = 2;\n    } else if (x[i] >= 0.75 + quarter_val && x[i] < 1.0) {\n      bin = 3;\n    }\n    atomicAdd(&(bins[bin]), 1);\n  }\n}",
            "for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n    size_t i0 = i / 4;\n    size_t i1 = i % 4;\n    if (x[i] >= 0 && x[i] < 0.25) {\n      atomicAdd(&bins[i0], 1);\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      atomicAdd(&bins[i0 + 1], 1);\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      atomicAdd(&bins[i0 + 2], 1);\n    } else if (x[i] >= 0.75 && x[i] <= 1) {\n      atomicAdd(&bins[i0 + 3], 1);\n    }\n  }\n}",
            "// TODO\n}",
            "size_t gid = threadIdx.x + blockIdx.x * blockDim.x;\n    size_t stride = blockDim.x * gridDim.x;\n    size_t i;\n    double frac;\n\n    for (i = gid; i < N; i += stride) {\n        frac = modf(x[i], &x[i]);\n        if (frac < 0.25)\n            atomicAdd(&bins[0], 1);\n        else if (frac < 0.5)\n            atomicAdd(&bins[1], 1);\n        else if (frac < 0.75)\n            atomicAdd(&bins[2], 1);\n        else\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "unsigned int tid = threadIdx.x;\n    __shared__ double temp[256];\n    double t;\n\n    // Read a chunk of 256 numbers into shared memory and count the\n    // numbers in the four quartiles.\n    for (size_t i = tid; i < N; i += 256) {\n        temp[i] = x[i];\n    }\n    __syncthreads();\n\n    if (tid < 256) {\n        // count the number of values that are in the four quartiles.\n        bins[0] = 0;\n        bins[1] = 0;\n        bins[2] = 0;\n        bins[3] = 0;\n        for (int i = tid; i < N; i += 256) {\n            t = temp[i];\n            if (t <= 0.25) {\n                atomicAdd(&bins[0], 1);\n            } else if (t <= 0.5) {\n                atomicAdd(&bins[1], 1);\n            } else if (t <= 0.75) {\n                atomicAdd(&bins[2], 1);\n            } else {\n                atomicAdd(&bins[3], 1);\n            }\n        }\n    }\n}",
            "/*\n    TODO: YOUR CODE HERE\n    HINT: You can use at most one shared memory thread block to count\n          the number of doubles in the vector x that have a fractional\n          part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n\n    PSEUDOCODE:\n      - Determine the thread block size. Each thread block processes\n        a section of the array.\n      - Determine the thread ID of the current thread.\n      - Determine the index of the first double in the current thread block.\n      - Iterate through the array and determine the number of elements\n        in the thread block that have a fractional part in [0, 0.25),\n        [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n    */\n}",
            "// Calculate the index of the element that we are currently processing\n  const size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Check that the current thread is not beyond the end of the vector x\n  if (i < N) {\n    if (x[i] >= 0.0 && x[i] < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else if (x[i] >= 0.75 && x[i] <= 1.0) {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "/*\n        Hint:\n            - Use an integer index to identify the fractional part of the number.\n            - Use a count per bin to store the number of values in each fractional part.\n            - The kernel will be launched with at least N threads.\n    */\n}",
            "// This is the starting thread in the block.\n  int myThread = threadIdx.x;\n\n  // Compute the stride of the block.\n  int blockStride = blockDim.x;\n\n  // Number of threads in the block.\n  int blockSize = blockStride * gridDim.x;\n\n  // Compute the starting index of the block.\n  int blockId = blockIdx.x;\n  int blockStart = blockId * blockSize;\n\n  // Compute the ending index of the block.\n  int blockEnd = blockStart + blockSize - 1;\n  if (blockEnd > N) blockEnd = N;\n\n  // Initialize a thread in the block as the first thread\n  // of the block.\n  __shared__ int first;\n  if (myThread == 0) first = 1;\n  __syncthreads();\n\n  // Iterate through the block.\n  for (int i = blockStart; i <= blockEnd; i += blockStride) {\n    if (first) {\n      bins[0] = 0;\n      bins[1] = 0;\n      bins[2] = 0;\n      bins[3] = 0;\n    }\n\n    // Get the thread that's computing i.\n    int threadId = threadIdx.x;\n    int index = i + threadId;\n\n    // Get the fractional part of the thread's index.\n    double fract = index - (int) index;\n\n    // Use AMD HIP atomics to add the index to the appropriate bins.\n    if (fract < 0.25) {\n      atomicAdd(&(bins[0]), 1);\n    } else if (fract < 0.5) {\n      atomicAdd(&(bins[1]), 1);\n    } else if (fract < 0.75) {\n      atomicAdd(&(bins[2]), 1);\n    } else {\n      atomicAdd(&(bins[3]), 1);\n    }\n\n    // This thread is the last thread of the block.\n    if (index == blockEnd) first = 0;\n    __syncthreads();\n  }\n\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n    size_t step = blockDim.x * gridDim.x;\n    size_t i;\n    for (i = tid; i < N; i += step) {\n        if (x[i] < 0.25)\n            atomicAdd(&bins[0], 1);\n        else if (x[i] < 0.5)\n            atomicAdd(&bins[1], 1);\n        else if (x[i] < 0.75)\n            atomicAdd(&bins[2], 1);\n        else\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n  size_t gridSize = blockDim.x*gridDim.x;\n  size_t lo = 0, hi = 0, mi = 0;\n  for (; i < N; i += gridSize) {\n    double xi = x[i];\n    if (xi < 0.25)\n      lo++;\n    else if (xi < 0.5)\n      mi++;\n    else if (xi < 0.75)\n      hi++;\n    else\n      bins[3]++;\n  }\n  __shared__ size_t tmp[4];\n  if (threadIdx.x == 0) {\n    tmp[0] = lo;\n    tmp[1] = mi;\n    tmp[2] = hi;\n    tmp[3] = N - (lo + mi + hi);\n  }\n  __syncthreads();\n  for (size_t j = 0; j < 4; ++j)\n    atomicAdd(&bins[j], tmp[j]);\n}",
            "// Create a thread-private array to store the numbers that are being binned.\n    const int QUARTILE_THREADS = 16;\n    double thread_bin[QUARTILE_THREADS];\n    int my_idx = threadIdx.x;\n    int my_qrt;\n\n    // Loop over all the elements of x in chunks of QUARTILE_THREADS.\n    for (int i=0; i<N; i+=QUARTILE_THREADS) {\n        // Each thread will take care of 16 elements in a chunk.\n        double next_element = (i+my_idx < N)? x[i+my_idx] : 0;\n\n        // Assign the thread-private bins to the 4 quartiles.\n        if (next_element < 0.25)\n            my_qrt = 0;\n        else if (next_element < 0.5)\n            my_qrt = 1;\n        else if (next_element < 0.75)\n            my_qrt = 2;\n        else\n            my_qrt = 3;\n\n        // Write the thread-private bin to the thread-private array.\n        thread_bin[my_idx] = my_qrt;\n    }\n\n    // Wait for all the threads in the warp to finish their work.\n    __syncwarp();\n\n    // Count the number of elements in each quartile.\n    int num_elems_per_qrt[4] = {0, 0, 0, 0};\n    for (int i=0; i<QUARTILE_THREADS; i++)\n        num_elems_per_qrt[thread_bin[i]]++;\n\n    // Reduction to count the number of elements per quartile.\n    for (int i=0; i<4; i++) {\n        int my_bin = num_elems_per_qrt[my_idx];\n        for (int j=0; j<16; j++)\n            my_bin += num_elems_per_qrt[j];\n        if (my_idx == 0)\n            bins[i] = my_bin;\n    }\n}",
            "size_t i = threadIdx.x;\n    if (i < N) {\n        size_t bin = static_cast<size_t>(floor(x[i] * 4));\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO: Add your code here\n}",
            "int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  size_t i = tid + bid * blockDim.x;\n\n  if (i < N) {\n    int q = 0;\n    if (x[i] >= 0.0 && x[i] < 0.25)\n      q = 0;\n    else if (x[i] >= 0.25 && x[i] < 0.5)\n      q = 1;\n    else if (x[i] >= 0.5 && x[i] < 0.75)\n      q = 2;\n    else if (x[i] >= 0.75 && x[i] <= 1.0)\n      q = 3;\n    atomicAdd(&bins[q], 1);\n  }\n}",
            "// YOUR CODE HERE\n}",
            "/*\n    HINT: use shared memory to minimize the amount of memory needed for the\n    bins.\n\n    HINT: use an if/else if/else if/else pattern to count the values in each\n    bin.\n  */\n  __shared__ size_t s_bins[4];\n  __shared__ size_t threadid;\n  threadid = threadIdx.x;\n  s_bins[0] = 0;\n  s_bins[1] = 0;\n  s_bins[2] = 0;\n  s_bins[3] = 0;\n\n  int stride = blockDim.x;\n  int start = threadid;\n  int end = N;\n  int increment = stride;\n\n  for (int i = start; i < end; i += increment) {\n    double val = x[i];\n    if (val >= 0.25 && val < 0.5)\n      s_bins[0] += 1;\n    else if (val >= 0.5 && val < 0.75)\n      s_bins[1] += 1;\n    else if (val >= 0.75 && val < 1)\n      s_bins[2] += 1;\n    else if (val < 0.25)\n      s_bins[3] += 1;\n    else\n      printf(\"something wrong\\n\");\n  }\n  __syncthreads();\n\n  for (int i = 1; i < 4; i++) {\n    s_bins[0] += s_bins[i];\n  }\n  __syncthreads();\n\n  bins[0] = s_bins[0];\n  bins[1] = s_bins[1];\n  bins[2] = s_bins[2];\n  bins[3] = s_bins[3];\n}",
            "size_t nThreads = blockDim.x * gridDim.x;\n  // size_t nBlocks = gridDim.x;\n\n  size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t index = threadIdx.x;\n  size_t stride = blockDim.x;\n\n  __shared__ size_t counter[4];\n\n  for (; i < N; i += nThreads) {\n    int q = (x[i] - floor(x[i])) * 4;\n    // int q = (x[i] - floor(x[i])) * 4;\n    // int q = (x[i] - floor(x[i]));\n    // if (q > 3)\n    //   q = 3;\n    counter[q] += 1;\n    // size_t bin = floor(x[i] * 4);\n    // if (bin > 3)\n    //   bin = 3;\n    // counter[bin] += 1;\n  }\n  __syncthreads();\n\n  bins[0] = counter[0];\n  bins[1] = counter[1];\n  bins[2] = counter[2];\n  bins[3] = counter[3];\n  __syncthreads();\n}",
            "// TODO\n}",
            "// Your code here\n}",
            "size_t idx = threadIdx.x + blockDim.x * blockIdx.x;\n    size_t stride = blockDim.x * gridDim.x;\n    for (size_t i = idx; i < N; i += stride) {\n        // get index of lower bound\n        size_t binIdx = floor(x[i] * 4);\n        // increment corresponding bin index\n        atomicAdd(&bins[binIdx], 1);\n    }\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    // Calculate the index of the appropriate bin\n    int bin_index = 0;\n    if (x[tid] >= 0 && x[tid] < 0.25) bin_index = 0;\n    else if (x[tid] >= 0.25 && x[tid] < 0.5) bin_index = 1;\n    else if (x[tid] >= 0.5 && x[tid] < 0.75) bin_index = 2;\n    else if (x[tid] >= 0.75 && x[tid] < 1) bin_index = 3;\n    else printf(\"Error: 0 <= x[tid] < 1\\n\");\n\n    // Increment the appropriate bin\n    atomicAdd(&bins[bin_index], 1);\n  }\n}",
            "for (int i = 0; i < N; ++i) {\n        if (x[i] < 0.25) {\n            bins[0]++;\n        } else if (x[i] < 0.5) {\n            bins[1]++;\n        } else if (x[i] < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: implement\n}",
            "size_t tid = threadIdx.x;\n    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = blockDim.x * gridDim.x;\n\n    for (; i < N; i += stride) {\n        if (x[i] < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (x[i] < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (x[i] < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "// Fill this in\n}",
            "size_t start = (blockIdx.x * blockDim.x + threadIdx.x) * 4;\n  size_t end = min((blockIdx.x + 1) * blockDim.x, N);\n\n  if (start < end) {\n    int binIdx = 0;\n    for (size_t i = start; i < end; i++) {\n      binIdx = (x[i] < 0.25)? 0 : (x[i] < 0.5)? 1 : (x[i] < 0.75)? 2 : 3;\n      atomicAdd(&bins[binIdx], 1);\n    }\n  }\n}",
            "int thread_idx = threadIdx.x + blockIdx.x * blockDim.x;\n\n    int block_dim_x = blockDim.x;\n    int grid_dim_x = gridDim.x;\n\n    // Iterate over the vector, 2 threads per iteration.\n    for (int idx = thread_idx; idx < N; idx += block_dim_x * grid_dim_x) {\n        const double val = x[idx];\n\n        // The 4 bins are: [0, 0.25), [0.25, 0.5), [0.5, 0.75), [0.75, 1)\n        if (val < 0.25) {\n            atomicAdd(bins, 0);\n        } else if (val < 0.5) {\n            atomicAdd(bins, 1);\n        } else if (val < 0.75) {\n            atomicAdd(bins, 2);\n        } else {\n            atomicAdd(bins, 3);\n        }\n    }\n}",
            "bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n   // TODO: implement using atomicAdd()\n}",
            "// Use HIP CUDA shared memory to store the input vector\n  __shared__ double *input;\n  input = shared_memory<double>(N);\n  // Load the input vector to shared memory\n  for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n    input[i] = x[i];\n  }\n  __syncthreads();\n  // Compute the quartiles\n  if (threadIdx.x < 2) {\n    bins[threadIdx.x] = 0;\n    double frac = threadIdx.x * 0.25;\n    for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n      if (input[i] < frac || input[i] > frac + 0.25) {\n        bins[threadIdx.x]++;\n      }\n    }\n  }\n  __syncthreads();\n}",
            "// Your code here\n}",
            "// Compute the id of the thread within the block\n    int threadId = threadIdx.x;\n    int blockId = blockIdx.x;\n    int n_threads_per_block = blockDim.x;\n    // Compute the number of blocks\n    int n_blocks = gridDim.x;\n    // Compute the total number of threads\n    int n_threads = n_blocks * n_threads_per_block;\n\n    // Number of threads to use for computing quartiles\n    int n_threads_per_quartile = 1024;\n    int n_blocks_per_quartile = (n_threads / n_threads_per_quartile);\n    int n_threads_per_quartile_r = n_threads_per_quartile - (n_blocks_per_quartile * n_threads_per_block);\n    // Number of threads to use for computing quartiles\n    int n_threads_per_quartile_c = ((n_threads % n_threads_per_quartile) == 0)? n_threads_per_quartile : n_threads_per_quartile_r;\n\n    // Compute the thread id within the quartile\n    int threadId_quartile = threadId % n_threads_per_quartile_c;\n    // Compute the quartile id\n    int quartile_id = threadId / n_threads_per_quartile_c;\n    // Compute the start of the quartile\n    int start_quartile = blockId * n_threads_per_quartile + quartile_id * n_threads_per_block;\n\n    // Initialize the counters\n    int counter_quartile_0 = 0;\n    int counter_quartile_1 = 0;\n    int counter_quartile_2 = 0;\n    int counter_quartile_3 = 0;\n\n    // Loop over all threads that belong to the quartile\n    for (int thread = start_quartile; thread < start_quartile + n_threads_per_quartile_c; thread++) {\n        // Compute the id of the thread within the quartile\n        int threadId_quartile_c = thread % n_threads_per_quartile_c;\n        // Compute the id of the thread within the block\n        int threadId_block = thread / n_threads_per_quartile_c;\n\n        // Compute the id of the thread within the block\n        int threadId = threadId_block * n_threads_per_block + threadId_quartile_c;\n        // Compute the id of the block\n        int blockId = threadId_block;\n\n        // Compute the start of the thread\n        int start_thread = blockId * n_threads_per_block + threadId;\n        // Compute the end of the thread\n        int end_thread = (blockId + 1) * n_threads_per_block;\n        // Compute the end of the thread\n        int end_quartile = (blockId + 1) * n_threads_per_quartile + quartile_id * n_threads_per_block + n_threads_per_quartile_c;\n\n        // Loop over all elements of the thread\n        for (int i = start_thread; i < end_thread && i < N; i++) {\n            if (x[i] >= 0.0 && x[i] <= 0.25) {\n                atomicAdd(&counter_quartile_0, 1);\n            }\n            else if (x[i] > 0.25 && x[i] <= 0.5) {\n                atomicAdd(&counter_quartile_1, 1);\n            }\n            else if (x[i] > 0.5 && x[i] <= 0.75) {\n                atomicAdd(&counter_quartile_2, 1);\n            }\n            else if (x[i] > 0.75 && x[i] <= 1.0) {\n                atomicAdd(&counter_quartile_3, 1);\n            }\n        }\n    }\n\n    // Store the results in the output array\n    int counter_quartile_0_g =",
            "// Your code here\n\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i >= N) return;\n\n    double val = x[i];\n    int bin_idx = val * 4;\n    atomicAdd(&bins[bin_idx], 1);\n}",
            "}",
            "}",
            "// Compute this thread's index within the array\n   size_t i = threadIdx.x + blockIdx.x*blockDim.x;\n   // Don't bother to process values in excess of N\n   if (i < N) {\n      // Compute the fractional part of the current value, \n      // which will be in [0,1)\n      double f = x[i] - floor(x[i]);\n      // Assign this thread to one of four different bins based on the\n      // fractional part of the current value\n      if (f < 0.25) bins[0]++;\n      else if (f < 0.50) bins[1]++;\n      else if (f < 0.75) bins[2]++;\n      else bins[3]++;\n   }\n}",
            "const size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n  const size_t stride = blockDim.x * gridDim.x;\n  const size_t half = N / 2;\n\n  for (size_t i = idx; i < N; i += stride) {\n    size_t bin = 0;\n\n    if (i < half) {\n      bin = (x[i] >= 0.25 * N)? 1 : 0;\n    } else {\n      bin = (x[i] >= 0.75 * N)? 3 : (x[i] >= 0.5 * N)? 2 : (x[i] >= 0.25 * N)? 1 : 0;\n    }\n\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO: Fill the bins array with the counts\n\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t stride = blockDim.x * gridDim.x;\n\n  for (size_t i = tid; i < N; i += stride) {\n    int bin = 0;\n    if (x[i] >= 0 && x[i] < 0.25) {\n      bin = 0;\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      bin = 1;\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      bin = 2;\n    } else if (x[i] >= 0.75 && x[i] < 1) {\n      bin = 3;\n    }\n\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "const size_t start = threadIdx.x + blockIdx.x * blockDim.x;\n  const size_t stride = blockDim.x * gridDim.x;\n  const size_t block = stride * blockIdx.y;\n  const size_t iblock = blockIdx.y;\n  if (start + block < N) {\n    // Compute the fractional part of each element in the vector x\n    double frac = modf(x[start + block], &frac);\n    // Compute the fractional part of the index of the element\n    double frac_index = modf(start + block, &frac_index);\n    // Compute the quartile of the fractional part of each element\n    size_t quartile = 0;\n    if (frac >= 0.25) {\n      if (frac >= 0.5) {\n        if (frac >= 0.75) {\n          quartile = 3;\n        } else {\n          quartile = 2;\n        }\n      } else {\n        quartile = 1;\n      }\n    }\n    atomicAdd(bins + quartile, 1);\n  }\n}",
            "// compute a global thread index in range [0, N)\n    size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n    double fractpart;\n    size_t bin;\n    // make sure we have at least N threads\n    // if not we need to exit the kernel\n    if (tid >= N) return;\n    fractpart = modf(x[tid], &bin);\n    // use the fractpart to determine which bin the number falls in\n    if (fractpart > 0.25 && fractpart < 0.5) {\n        atomicAdd(&bins[1], 1);\n    }\n    else if (fractpart > 0.5 && fractpart < 0.75) {\n        atomicAdd(&bins[2], 1);\n    }\n    else if (fractpart > 0.75 && fractpart < 1.0) {\n        atomicAdd(&bins[3], 1);\n    }\n    else {\n        atomicAdd(&bins[0], 1);\n    }\n}",
            "size_t tid = threadIdx.x;\n\n    // 4-level reduction of 64-bit unsigned integers (64 -> 32 -> 16 -> 8 -> 4)\n    // Use threadIdx.x as index\n    __shared__ unsigned int sdata[1024];\n    sdata[tid] = 0;\n    __syncthreads();\n\n    // Compute 4 levels\n    for (unsigned int s=1; s < 1024; s *= 2) {\n        if (tid % (2*s) == 0) {\n            // Reduce each s-group\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        // Final count\n        bins[0] = sdata[0];\n        for (unsigned int i=1; i<4; i++) {\n            bins[i] = sdata[sdata[i-1]];\n        }\n    }\n}",
            "// implement this function\n}",
            "size_t t = blockDim.x * blockIdx.x + threadIdx.x;\n    if (t < N) {\n        double xd = x[t];\n        if (xd >= 0 && xd < 0.25) bins[0] += 1;\n        else if (xd >= 0.25 && xd < 0.5) bins[1] += 1;\n        else if (xd >= 0.5 && xd < 0.75) bins[2] += 1;\n        else if (xd >= 0.75 && xd <= 1) bins[3] += 1;\n    }\n}",
            "//TODO: use a \"binning\" approach to count the number of elements in each bin.\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (tid >= N) return;\n\tbins[0] += x[tid] - floor(x[tid]) < 0.25;\n\tbins[1] += x[tid] - floor(x[tid]) >= 0.25 && x[tid] - floor(x[tid]) < 0.5;\n\tbins[2] += x[tid] - floor(x[tid]) >= 0.5 && x[tid] - floor(x[tid]) < 0.75;\n\tbins[3] += x[tid] - floor(x[tid]) >= 0.75;\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i >= N) return;\n    double fract = (double)i / (double)N;\n    double floor = floorf(fract);\n    size_t idx = (size_t)(floor * 4);\n    if (idx < 4) {\n        atomicAdd(&bins[idx], 1);\n    }\n}",
            "// TODO: implement the kernel\n}",
            "__shared__ double partialSum[BLOCK_SIZE];\n  __shared__ size_t partialSums[BLOCK_SIZE];\n  size_t tid = threadIdx.x;\n\n  // 1. Intra-warp reduction\n  // Store the result in the shared memory\n  // for each thread, compute the fractional part in each of the intervals\n  // and then update the corresponding count\n  // Use a warp synchronization barrier\n  partialSum[tid] = 0.0;\n  for (size_t i = 0; i < N; i++) {\n    partialSum[tid] += floor(x[i] * 4.0) / 4.0;\n  }\n  __syncthreads();\n\n  // Reduce the partial sums on each block\n  size_t blockSum = 0;\n  for (size_t i = 0; i < BLOCK_SIZE; i++) {\n    blockSum += partialSum[i];\n  }\n\n  partialSums[tid] = blockSum;\n  __syncthreads();\n\n  // 2. Inter-warp reduction\n  for (size_t s = BLOCK_SIZE / 2; s > 0; s >>= 1) {\n    if (tid < s) {\n      partialSums[tid] += partialSums[tid + s];\n    }\n    __syncthreads();\n  }\n\n  // 3. Write to memory\n  if (tid == 0) {\n    bins[0] = (partialSums[0] * 4) / N;\n    bins[1] = (partialSums[BLOCK_SIZE / 4] * 4) / N;\n    bins[2] = (partialSums[BLOCK_SIZE / 2] * 4) / N;\n    bins[3] = (partialSums[3 * BLOCK_SIZE / 4] * 4) / N;\n  }\n}",
            "__shared__ double s_x[BLOCK_SIZE];\n\n    // load values into shared memory\n    const size_t tId = threadIdx.x;\n    if (tId < N) {\n        s_x[tId] = x[tId];\n    }\n    __syncthreads();\n\n    // process all values in block\n    double sum = 0;\n    size_t i_beg = 0;\n    size_t i_end = blockDim.x;\n    while (i_beg < N) {\n        size_t i = i_beg + tId;\n\n        // process values that have been loaded into shared memory\n        if (i < i_end) {\n            if (i_beg == 0) {\n                bins[0] = 0;\n            }\n            if (i_end < N) {\n                sum += (s_x[i] < 0.25) + (s_x[i] >= 0.25 && s_x[i] < 0.5) +\n                       (s_x[i] >= 0.5 && s_x[i] < 0.75) +\n                       (s_x[i] >= 0.75 && s_x[i] <= 1);\n            } else {\n                sum += (s_x[i] < 0.25) + (s_x[i] >= 0.25 && s_x[i] < 1);\n            }\n            __syncthreads();\n\n            // accumulate sum in shared memory\n            for (size_t i_s = 1; i_s < blockDim.x; i_s *= 2) {\n                if (tId % (2 * i_s) == 0) {\n                    s_x[tId] += s_x[tId + i_s];\n                }\n                __syncthreads();\n            }\n            if (tId == 0) {\n                bins[0] += s_x[0];\n            }\n\n            // update loop start and end indices\n            i_beg += blockDim.x * gridDim.x;\n            i_end += blockDim.x * gridDim.x;\n        }\n        __syncthreads();\n    }\n}",
            "// TODO\n}",
            "// TODO: Implement using thread blocks and AMD HIP\n\n    // Allocate memory on the device for the input vector\n    double* d_input;\n    hipMalloc(&d_input, sizeof(double) * N);\n\n    // Copy the input vector to the device\n    hipMemcpy(d_input, x, sizeof(double) * N, hipMemcpyHostToDevice);\n\n    // Allocate memory on the device for the output vector\n    double* d_output;\n    hipMalloc(&d_output, sizeof(double) * N);\n\n    // Initialize the output vector to 0.0\n    hipMemset(d_output, 0.0, sizeof(double) * N);\n\n    // Launch the kernel\n    // TODO: Hint - you may need to create multiple kernels to get the best performance\n    // Hint: Use hipLaunchKernelGGL\n    // Hint: Use hipStreamCreateWithFlags\n    // Hint: Use hipStreamSynchronize\n    // Hint: Use hipStreamDestroy\n    // Hint: Use hipDeviceSynchronize\n    hipStream_t stream;\n    hipStreamCreateWithFlags(&stream, hipStreamNonBlocking);\n    hipLaunchKernelGGL(countQuartilesKernel, dim3(1, 1, 1), dim3(N, 1, 1), 0, stream, d_input, N, d_output);\n    hipStreamSynchronize(stream);\n    hipStreamDestroy(stream);\n    hipDeviceSynchronize();\n\n    // Copy the output vector to the host\n    hipMemcpy(bins, d_output, sizeof(double) * 4, hipMemcpyDeviceToHost);\n\n    // Clean up\n    hipFree(d_input);\n    hipFree(d_output);\n}",
            "// TODO\n}",
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int nthreads = blockDim.x;\n\n    // Allocate shared memory.\n    __shared__ double sx[128];\n\n    // Transfer part of x to shared memory.\n    if (tid < 128) {\n        sx[tid] = x[bid * nthreads + tid];\n    }\n\n    // Synchronize threads to make sure x has been copied to shared memory.\n    __syncthreads();\n\n    // Compute quartiles using shared memory.\n    for (int i = tid; i < N; i += nthreads) {\n        double val = sx[i];\n        double f = fmod(val, 1);\n        if (f < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (f < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (f < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "// TODO: compute the count for each quartile in parallel\n    // Use AMD HIP\n\n    // You must allocate the `bins` array with hipMalloc and copy the result to the host\n    // with hipMemcpy.\n    //\n    // When you copy the result from the GPU memory to the host memory, use the flag\n    // hipMemcpyDeviceToHost.\n}",
            "// TODO\n\n}",
            "// Compute 1/4, 2/4, 3/4, 4/4 of N threads\n  size_t Nt = (N + blockDim.x * blockDim.y * blockDim.z - 1) / (blockDim.x * blockDim.y * blockDim.z);\n  size_t quartileId = blockIdx.x * blockDim.x * blockDim.y * blockDim.z + threadIdx.z * blockDim.y * blockDim.x + threadIdx.y * blockDim.x + threadIdx.x;\n  if (quartileId >= 4) {\n    return;\n  }\n\n  size_t blockId = blockIdx.x;\n  size_t threadId = threadIdx.x + threadIdx.y * blockDim.x + threadIdx.z * blockDim.x * blockDim.y;\n  size_t start = Nt * blockId;\n  size_t end = Nt * (blockId + 1);\n\n  // Loop through all threads in the block\n  for (size_t i = start + threadId; i < end; i += blockDim.x * blockDim.y * blockDim.z) {\n    // If thread is in range, increment the bin\n    if (i < N) {\n      if (quartileId == 0 && x[i] >= 0 && x[i] < 0.25)\n        atomicAdd(&bins[0], 1);\n      else if (quartileId == 1 && x[i] >= 0.25 && x[i] < 0.5)\n        atomicAdd(&bins[1], 1);\n      else if (quartileId == 2 && x[i] >= 0.5 && x[i] < 0.75)\n        atomicAdd(&bins[2], 1);\n      else if (quartileId == 3 && x[i] >= 0.75)\n        atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "//TODO\n}",
            "__shared__ int partial[4];\n  if (threadIdx.x == 0) {\n    partial[0] = 0;\n    partial[1] = 0;\n    partial[2] = 0;\n    partial[3] = 0;\n  }\n  __syncthreads();\n\n  const int tid = threadIdx.x + blockDim.x * blockIdx.x;\n  if (tid < N) {\n    const double xi = x[tid];\n    if (xi < 0.25) {\n      atomicAdd(&partial[0], 1);\n    } else if (xi < 0.5) {\n      atomicAdd(&partial[1], 1);\n    } else if (xi < 0.75) {\n      atomicAdd(&partial[2], 1);\n    } else {\n      atomicAdd(&partial[3], 1);\n    }\n  }\n\n  __syncthreads();\n  if (threadIdx.x == 0) {\n    for (size_t i = 1; i < blockDim.x; ++i) {\n      partial[0] += partial[i];\n      partial[1] += partial[i + 1];\n      partial[2] += partial[i + 2];\n      partial[3] += partial[i + 3];\n    }\n  }\n\n  __syncthreads();\n  if (threadIdx.x == 0) {\n    for (size_t i = 0; i < 4; ++i) {\n      bins[i] = partial[i];\n    }\n  }\n}",
            "// TODO: implement the kernels\n    //\n    // Note that you may have to add more than one thread to this kernel.\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i >= N) return;\n  // TODO 1: Count the number of doubles with a fractional part in the given quartiles\n\n  // TODO 2: Store the count of doubles in each of the given bins\n\n}",
            "...\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n\n  // TODO: insert your code here\n  // Use AMD HIP to compute in parallel and load the data into registers\n  // from global memory in a loop. The kernel is launched with at least N threads.\n  // Count the number of doubles in the vector x that have a fractional part\n  // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n  // Examples:\n\n  // input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n  // output: [2, 1, 2, 2]\n\n  // input: [1.9, 0.2, 0.6, 10.1, 7.4]\n  // output: [2, 1, 1, 1]\n\n  // Tip: HIP defines an integer constant, `hipDoubleSizeBytes` that is equal to\n  // the size in bytes of a `double`.\n\n  // Tip: The `hip_ldg` function loads a value from global memory using `double`\n  // atomic loads. This function provides a means for the compiler to reorder\n  // memory accesses and generate more efficient code.\n\n  // Tip: The `hip_fma` function performs a multiply-add (fused multiply-add)\n  // operation that is equivalent to:\n  // `prod = a * b + c`\n\n  // Tip: Use integer modulo division to compute which bin a double is in.\n\n  // Tip: To avoid spurious results, add 0.5 to the result of the modulo division\n  // before casting the result to an integer.\n\n  // Tip: When casting the fractional part of a double to an integer, use the\n  // `hip_rint` function. This function rounds a double to the nearest integer\n  // using round-to-nearest mode.\n\n  // Tip: The `hip_atomicAdd` function performs an atomic add of a value to an\n  // integer location.\n\n  // HIP threads are launched with at least N threads.\n  size_t offset = N / (size_t)hipBlockDim_x;\n  size_t i = tid;\n  bins[0] = 0; bins[1] = 0; bins[2] = 0; bins[3] = 0;\n  while (i < N) {\n    // load x[i] into register\n    double xi = hip_ldg(x + i);\n    // bins = [0, 0.25), [0.25, 0.5), [0.5, 0.75), [0.75, 1)\n    size_t q = (size_t)hip_rint((double)(hi_fma(xi, hipDoubleSizeBytes, 1) - 1) / (double)hipDoubleSizeBytes);\n    // add one to the bin\n    hip_atomicAdd(&bins[q], (size_t)1);\n    // offset to next thread\n    i += offset;\n  }\n}",
            "const double fourth = 1.0 / 4.0;\n    const double sixth = 1.0 / 6.0;\n    const double fifth = 1.0 / 5.0;\n\n    size_t tid = threadIdx.x;\n    size_t stride = blockDim.x;\n\n    double x2 = x[tid];\n    for (size_t i = stride; i < N; i += stride) {\n        if (tid < i)\n            x2 = (x2 > x[i])? x[i] : x2;\n    }\n\n    __syncthreads();\n\n    if (tid == 0) {\n        double x1 = x2;\n        for (size_t i = stride; i < N; i += stride) {\n            if (tid < i)\n                x1 = (x1 < x[i])? x[i] : x1;\n        }\n        __syncthreads();\n\n        double x3 = x1;\n        for (size_t i = stride; i < N; i += stride) {\n            if (tid < i)\n                x3 = (x3 > x[i])? x[i] : x3;\n        }\n        __syncthreads();\n\n        double x4 = x3;\n        for (size_t i = stride; i < N; i += stride) {\n            if (tid < i)\n                x4 = (x4 < x[i])? x[i] : x4;\n        }\n\n        double delta = x4 - x1;\n        size_t num = N;\n        size_t den = 4.0 * num;\n\n        double left = delta * fourth;\n        double right = delta * fifth;\n        bins[0] = round(num * left / den);\n\n        left = delta * sixth;\n        right = delta * fourth;\n        bins[1] = round(num * left / den);\n\n        left = delta * fourth;\n        right = delta * third;\n        bins[2] = round(num * left / den);\n\n        left = delta * third;\n        right = delta * fourth;\n        bins[3] = round(num * left / den);\n\n    }\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n\n  if (tid < N) {\n    double v = x[tid];\n    int index = 0;\n    if (v > 0 && v < 2.5) index = 0;\n    else if (v >= 2.5 && v < 5) index = 1;\n    else if (v >= 5 && v < 7.5) index = 2;\n    else if (v >= 7.5 && v <= 10) index = 3;\n\n    atomicAdd(&bins[index], 1);\n  }\n}",
            "// TODO: fill in this function\n}",
            "const size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    const size_t stride = blockDim.x * gridDim.x;\n\n    for (size_t i = tid; i < N; i += stride) {\n        double val = x[i];\n        if (val < 0.25) {\n            atomicAdd(bins, 1);\n        } else if (val < 0.5) {\n            atomicAdd(bins + 1, 1);\n        } else if (val < 0.75) {\n            atomicAdd(bins + 2, 1);\n        } else {\n            atomicAdd(bins + 3, 1);\n        }\n    }\n}",
            "// Allocate memory to store thread's quartile count in a shared array\n    // Use threadIdx.x as index into the shared array\n    extern __shared__ int count_shared[];\n    int* count = count_shared + threadIdx.x;\n\n    // Initialize count to 0\n    count[0] = 0;\n    __syncthreads();\n\n    // Do reduction to get the count of double in each quartile\n    for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n\n        // Get the fractional part of x[i]\n        double temp = fmod(x[i], 1.0);\n\n        // Use if statements to count the number of doubles in each quartile\n        if (temp <= 0.25) count[0]++;\n        else if (temp <= 0.5) count[1]++;\n        else if (temp <= 0.75) count[2]++;\n        else if (temp <= 1) count[3]++;\n    }\n    __syncthreads();\n\n    // Reduce the counts in the shared array to get the total count for each quartile\n    // The last thread in each block will be responsible for the total count\n    // In this case, there will be at least as many threads in each block as the number of quartiles\n    if (threadIdx.x == blockDim.x-1) {\n        for (int i = 0; i < blockDim.x-1; i++) {\n            count[0] += count[i+1];\n        }\n    }\n    __syncthreads();\n\n    // Copy the count for each quartile to the output array\n    if (threadIdx.x == 0) {\n        bins[0] = count[0];\n        bins[1] = count[1];\n        bins[2] = count[2];\n        bins[3] = count[3];\n    }\n}",
            "// compute the local thread ID\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n    // initialize shared memory\n    extern __shared__ double xBin[];\n    double* xBinLocal = &xBin[0];\n\n    // allocate shared memory for the thread's x value\n    double xThread;\n    if (tid < N) {\n        xThread = x[tid];\n    } else {\n        xThread = 0.0;\n    }\n\n    // compute the quartile range\n    double q = 1.0 / 4.0;\n    double dq = 0.25;\n\n    // compute the starting value\n    double x0 = 0.0;\n    if (tid == 0) {\n        x0 = 0.0;\n    } else {\n        x0 = x[tid - 1];\n    }\n\n    // compute the ending value\n    double xn = 0.0;\n    if (tid == N - 1) {\n        xn = 1.0;\n    } else {\n        xn = x[tid + 1];\n    }\n\n    // compute the bin\n    double xBin = 0.0;\n    if (tid < N) {\n        xBin = xThread;\n    }\n\n    // fill in shared memory\n    xBinLocal[tid] = xBin;\n\n    // synchronize the threads\n    __syncthreads();\n\n    // compute the bin counts\n    size_t* bins = (size_t*)&xBinLocal[0];\n    if (tid < N) {\n        bins[0] = (size_t)((x0 + dq) <= xBin? 0 : 1);\n        bins[1] = (size_t)((x0 + q * dq) <= xBin? 0 : 1);\n        bins[2] = (size_t)((x0 + q * dq) <= xBin? 0 : 1);\n        bins[3] = (size_t)((x0 + q * dq) <= xBin? 0 : 1);\n    }\n\n    // synchronize the threads\n    __syncthreads();\n\n    // copy the final bin counts to the global memory\n    if (tid == 0) {\n        bins[0] = 0;\n        for (size_t i = 1; i < N; ++i) {\n            bins[0] += bins[i];\n        }\n    }\n}",
            "// TODO: implement\n}",
            "// This is your code\n\n}",
            "size_t id = blockIdx.x * blockDim.x + threadIdx.x;\n  if (id < N) {\n    // determine which quartile x[id] belongs to\n    if (x[id] < 0.25) {\n      bins[0]++;\n    } else if (x[id] < 0.5) {\n      bins[1]++;\n    } else if (x[id] < 0.75) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "int tid = threadIdx.x;\n    if (tid < N) {\n        double fraction = fmod(x[tid], 1.0);\n        if (fraction >= 0.25 && fraction < 0.5) {\n            bins[1]++;\n        }\n        else if (fraction >= 0.5 && fraction < 0.75) {\n            bins[2]++;\n        }\n        else if (fraction >= 0.75 && fraction <= 1) {\n            bins[3]++;\n        }\n        else {\n            bins[0]++;\n        }\n    }\n}",
            "// TODO\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    size_t tg_id = blockDim.x * blockIdx.x + threadIdx.x;\n    size_t tg_sz = blockDim.x * gridDim.x;\n    size_t idx = tg_id + 1;\n    for (; idx <= N; idx += tg_sz) {\n        int i = (int)((x[idx] - 1) * 4);\n        if (i == 0)\n            atomicAdd(&bins[0], 1);\n        else if (i == 1)\n            atomicAdd(&bins[1], 1);\n        else if (i == 2)\n            atomicAdd(&bins[2], 1);\n        else\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "// TODO: implement the function\n    \n\n}",
            "// Implementation\n}",
            "size_t i = threadIdx.x + blockIdx.x*blockDim.x;\n    if (i >= N)\n        return;\n    double frac = fmod(x[i], 1);\n    if (frac >= 0 && frac < 0.25) {\n        bins[0]++;\n    } else if (frac >= 0.25 && frac < 0.5) {\n        bins[1]++;\n    } else if (frac >= 0.5 && frac < 0.75) {\n        bins[2]++;\n    } else if (frac >= 0.75 && frac <= 1) {\n        bins[3]++;\n    }\n}",
            "// Calculate the start and end indices of the current block\n    size_t block_start = blockDim.x * blockIdx.x + threadIdx.x;\n    size_t block_end = blockDim.x * (blockIdx.x + 1);\n    block_end = block_end > N? N : block_end;\n\n    size_t bin_start = 0;\n    size_t bin_end = N / 4;\n    size_t i;\n\n    // Initialize bins[0] and bins[1]\n    for (i = block_start; i < bin_end; i += blockDim.x) {\n        if (x[i] < 0.25) {\n            bins[0] += 1;\n        } else if (x[i] < 0.5) {\n            bins[1] += 1;\n        } else if (x[i] < 0.75) {\n            bins[2] += 1;\n        } else {\n            bins[3] += 1;\n        }\n    }\n\n    __syncthreads();\n\n    // Initialize bins[2] and bins[3]\n    for (i = block_start; i < bin_end; i += blockDim.x) {\n        if (x[i] < 0.5) {\n            bins[2] += 1;\n        } else {\n            bins[3] += 1;\n        }\n    }\n\n    __syncthreads();\n\n    // Initialize bins[1] and bins[2]\n    for (i = block_start; i < bin_end; i += blockDim.x) {\n        if (x[i] < 0.25) {\n            bins[1] += 1;\n        } else {\n            bins[2] += 1;\n        }\n    }\n\n    __syncthreads();\n\n    // Initialize bins[0] and bins[1]\n    for (i = block_start; i < bin_end; i += blockDim.x) {\n        if (x[i] < 0.75) {\n            bins[0] += 1;\n        } else {\n            bins[1] += 1;\n        }\n    }\n}",
            "// TODO\n    // For each thread in the block:\n    // 1. Compute the thread's local index in the x array\n    // 2. Compute the thread's index in the 4-element array `bins`\n    // 3. If `i` is an index in `x`, increment `bins[j]` where:\n    //    j = 0 if x[i] is between 0 and 0.25\n    //    j = 1 if x[i] is between 0.25 and 0.5\n    //    j = 2 if x[i] is between 0.5 and 0.75\n    //    j = 3 if x[i] is between 0.75 and 1\n\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // 1. Compute the thread's local index in the x array\n        int idx = tid;\n        // 2. Compute the thread's index in the 4-element array `bins`\n        int i = 0;\n        if (x[idx] >= 0 && x[idx] <= 0.25) {\n            i = 0;\n        } else if (x[idx] >= 0.25 && x[idx] <= 0.5) {\n            i = 1;\n        } else if (x[idx] >= 0.5 && x[idx] <= 0.75) {\n            i = 2;\n        } else if (x[idx] >= 0.75 && x[idx] <= 1) {\n            i = 3;\n        }\n        // 3. If `i` is an index in `x`, increment `bins[j]` where:\n        //    j = 0 if x[i] is between 0 and 0.25\n        //    j = 1 if x[i] is between 0.25 and 0.5\n        //    j = 2 if x[i] is between 0.5 and 0.75\n        //    j = 3 if x[i] is between 0.75 and 1\n        atomicAdd(&bins[i], 1);\n    }\n}",
            "}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (i >= N) {\n    return;\n  }\n\n  size_t index = floor(x[i] * 4) / 4;\n  atomicAdd(&bins[index], 1);\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n  if(tid >= N)\n    return;\n\n  // determine quartile\n  size_t quartile = 0;\n  if(x[tid] < 0.25)\n    quartile = 0;\n  else if(x[tid] < 0.5)\n    quartile = 1;\n  else if(x[tid] < 0.75)\n    quartile = 2;\n  else\n    quartile = 3;\n\n  // update the appropriate bin\n  atomicAdd(&bins[quartile], 1);\n}",
            "}",
            "const int i = threadIdx.x;\n    const int j = blockIdx.x;\n    const int n = blockDim.x;\n    double frac = j + i * n / N;\n    if (frac >= 1.0) frac = 1.0;\n\n    if (frac < 0.25) {\n        atomicAdd(&bins[0], 1);\n    } else if (frac < 0.5) {\n        atomicAdd(&bins[1], 1);\n    } else if (frac < 0.75) {\n        atomicAdd(&bins[2], 1);\n    } else {\n        atomicAdd(&bins[3], 1);\n    }\n}",
            "// TODO\n  // Launch one thread per data point.\n  // In each thread, round the data point to the closest quarter.\n  // Store the rounded data point in a shared array.\n  // Use atomic operations to update the array elements with the rounded values.\n  // Launch a thread that loops over the shared array and updates the histogram bins.\n  // Hint: use the __syncthreads() function.\n\n  __shared__ double shared[256];\n  int tid = threadIdx.x;\n  shared[tid] = 0.0;\n\n  for(int i = tid; i < N; i += blockDim.x){\n    shared[tid] = round(x[i] / 0.25) * 0.25;\n  }\n  __syncthreads();\n\n  int block_offset = blockIdx.x * blockDim.x;\n  for(int i = tid; i < 4; i += blockDim.x){\n    for(int j = 0; j < 128; j++){\n      atomicAdd(&bins[i], shared[i*128 + j]);\n    }\n  }\n\n  __syncthreads();\n}",
            "// 1D thread block.\n    const size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    const size_t stride = blockDim.x * gridDim.x;\n    // The number of threads in the grid.\n    const size_t n_threads = blockDim.x * gridDim.x;\n\n    for (; i < N; i += stride) {\n        size_t index;\n        if (x[i] <= 0.25)\n            index = 0;\n        else if (x[i] <= 0.50)\n            index = 1;\n        else if (x[i] <= 0.75)\n            index = 2;\n        else\n            index = 3;\n        atomicAdd(&bins[index], 1);\n    }\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t i = tid;\n  size_t c = 0;\n  while (i < N) {\n    double d = x[i];\n    if (d >= 0 && d < 0.25) {\n      c++;\n    } else if (d >= 0.25 && d < 0.5) {\n      c++;\n    } else if (d >= 0.5 && d < 0.75) {\n      c++;\n    } else if (d >= 0.75 && d < 1) {\n      c++;\n    }\n    i += blockDim.x * gridDim.x;\n  }\n  bins[0] = atomicAdd(&bins[0], c);\n}",
            "// TODO: implement the kernel\n}",
            "// Declare a shared memory array of doubles, with a size of 1024 elements.\n    extern __shared__ double values[];\n\n    // Initialize the shared memory array to zero.\n    for (size_t i = threadIdx.x; i < 1024; i += blockDim.x)\n        values[i] = 0.0;\n\n    // Wait for the initialization to finish.\n    __syncthreads();\n\n    // TODO: Implement this function\n    for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n        int idx = static_cast<int>(x[i] * 4);\n        if (idx < 4)\n            atomicAdd(&values[idx], 1.0);\n    }\n\n    __syncthreads();\n    for (size_t i = threadIdx.x; i < 4; i += blockDim.x)\n        atomicAdd(&bins[i], values[i]);\n}",
            "// TODO\n\n}",
            "// TODO: Your code goes here\n\n    // Get global thread index\n    int idx = threadIdx.x + blockDim.x * blockIdx.x;\n\n    // Check if global index is in bounds\n    if (idx < N) {\n        // Copy the global index to local index\n        int local = idx;\n        // Declare local pointers\n        double *x_ptr = &(x[local]);\n\n        // Determine the number of integers and decimal places of x[idx]\n        // Determine the index of the bin the number falls in\n        int whole = floor(*x_ptr);\n        int frac = (*x_ptr - whole) * 10;\n        int bin;\n        switch (frac) {\n            case 0:\n                bin = 0;\n                break;\n            case 1:\n            case 2:\n            case 3:\n                bin = 1;\n                break;\n            case 4:\n            case 5:\n            case 6:\n                bin = 2;\n                break;\n            case 7:\n            case 8:\n            case 9:\n                bin = 3;\n                break;\n        }\n\n        // Increment the bin count\n        atomicAdd(&(bins[bin]), 1);\n    }\n\n    // Return\n    return;\n}",
            "// TODO\n}",
            "int tid = threadIdx.x;\n\n    if (tid >= N) {\n        return;\n    }\n\n    const double PI = 3.14159265358979323846;\n    double q0 = floor(x[tid]/4.0)*4.0;\n    double q1 = floor((x[tid]+1.0)/4.0)*4.0;\n    double q2 = floor((x[tid]+2.0)/4.0)*4.0;\n    double q3 = floor((x[tid]+3.0)/4.0)*4.0;\n\n    if (q0 < x[tid] && x[tid] < q1) {\n        bins[0]++;\n    }\n    if (q1 < x[tid] && x[tid] < q2) {\n        bins[1]++;\n    }\n    if (q2 < x[tid] && x[tid] < q3) {\n        bins[2]++;\n    }\n    if (q3 < x[tid] && x[tid] < q3+PI) {\n        bins[3]++;\n    }\n}",
            "int i = threadIdx.x + blockDim.x * blockIdx.x;\n  if (i < N) {\n    double x_i = x[i];\n    if (x_i >= 0.0 && x_i < 0.25) {\n      atomicAdd(&bins[0], 1);\n    }\n    else if (x_i >= 0.25 && x_i < 0.5) {\n      atomicAdd(&bins[1], 1);\n    }\n    else if (x_i >= 0.5 && x_i < 0.75) {\n      atomicAdd(&bins[2], 1);\n    }\n    else if (x_i >= 0.75 && x_i <= 1.0) {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (tid < N) {\n    double x_i = x[tid];\n    size_t i = (size_t)floor(4.0 * x_i);\n    atomicAdd(&bins[i], 1);\n  }\n}",
            "/* YOUR CODE HERE */\n}",
            "int thread_id = threadIdx.x + blockIdx.x * blockDim.x;\n  if (thread_id >= N)\n    return;\n  double xi = x[thread_id];\n  // int bin = 0;\n  // if (0.0 <= xi && xi < 0.25) bin = 0;\n  // else if (0.25 <= xi && xi < 0.5) bin = 1;\n  // else if (0.5 <= xi && xi < 0.75) bin = 2;\n  // else if (0.75 <= xi && xi < 1.0) bin = 3;\n  // atomicAdd(&bins[bin], 1);\n\n  int b = 0;\n  if (0.0 <= xi && xi < 0.25)\n    b = 0;\n  else if (0.25 <= xi && xi < 0.5)\n    b = 1;\n  else if (0.5 <= xi && xi < 0.75)\n    b = 2;\n  else if (0.75 <= xi && xi < 1.0)\n    b = 3;\n  atomicAdd(&bins[b], 1);\n}",
            "// TODO\n}",
            "size_t tid = threadIdx.x;\n    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = blockDim.x * gridDim.x;\n\n    for (; i < N; i += stride) {\n        double y = x[i];\n        if (y < 0) {\n            y = 0.0;\n        }\n        if (y > 1) {\n            y = 1.0;\n        }\n        size_t idx = floor(y * 4);\n        atomicAdd(&bins[idx], 1);\n    }\n}",
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int i;\n    double fractionalPart;\n\n    size_t start = bid * N / blockDim.x;\n    size_t end = start + (N / blockDim.x) - 1;\n    if (end > N) end = N;\n\n    // printf(\"bid: %d, start: %d, end: %d\\n\", bid, start, end);\n\n    int bins_offset = 0;\n    if (end >= start) {\n        if (x[start] < 0.25)\n            bins[bins_offset++] = 0;\n        else if (x[start] < 0.5)\n            bins[bins_offset++] = 1;\n        else if (x[start] < 0.75)\n            bins[bins_offset++] = 2;\n        else\n            bins[bins_offset++] = 3;\n    }\n\n    for (i = start + 1; i <= end; i++) {\n        fractionalPart = modf(x[i], &i);\n        // printf(\"x[%d]: %f, fractionalPart: %f\\n\", i, x[i], fractionalPart);\n        if (fractionalPart < 0.25)\n            bins[bins_offset++]++;\n        else if (fractionalPart < 0.5)\n            bins[bins_offset++]++;\n        else if (fractionalPart < 0.75)\n            bins[bins_offset++]++;\n        else\n            bins[bins_offset++]++;\n    }\n    // printf(\"bins: %d %d %d %d\\n\", bins[0], bins[1], bins[2], bins[3]);\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx < N) {\n        const int i = static_cast<int>(std::floor(x[idx]));\n        const double fraction = x[idx] - i;\n\n        if (fraction <= 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (fraction <= 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (fraction <= 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else if (fraction <= 1.0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "int tid = threadIdx.x;\n    int numThreads = blockDim.x;\n    int start = tid * N / numThreads;\n    int end = (tid + 1) * N / numThreads;\n    double frac[4] = {0, 0.25, 0.5, 0.75};\n    for (int i = start; i < end; i++) {\n        double x_i = x[i];\n        for (int j = 0; j < 4; j++) {\n            if (x_i < frac[j] + 0.25) {\n                bins[j]++;\n                break;\n            }\n        }\n    }\n}",
            "//TODO\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  // Compute the fractional part of the number in x[tid]\n  double frac = fracpart(x[tid]);\n  // Compute the index in bins for this fractional part\n  size_t bin = (frac < 0.25)? 0 : (frac < 0.5)? 1 : (frac < 0.75)? 2 : 3;\n  // Add one to the corresponding bin\n  atomicAdd(&bins[bin], 1);\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        bins[int(x[i] * 4 + 0.5)] += 1;\n    }\n}",
            "// TODO: Your code here\n}",
            "// TODO\n    //...\n}",
            "bins[0] = 0; bins[1] = 0; bins[2] = 0; bins[3] = 0;\n    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N;\n         i += blockDim.x * gridDim.x) {\n        double d = x[i];\n        if (d < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (d < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (d < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else if (d <= 1.0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i >= N) return;\n    bins[(int)((i + 0.25) / (N / 4))]++;\n}",
            "//TODO: implement this function\n}",
            "// start = threadIdx.x * (N / blockDim.x)\n  // size = N / blockDim.x\n\n  size_t start = threadIdx.x * N / blockDim.x;\n  size_t size = N / blockDim.x;\n  size_t count[4] = {0};\n\n  for (int i = 0; i < size; i++) {\n    double val = x[start + i];\n    if (val >= 0 && val < 0.25)\n      count[0]++;\n    else if (val >= 0.25 && val < 0.5)\n      count[1]++;\n    else if (val >= 0.5 && val < 0.75)\n      count[2]++;\n    else if (val >= 0.75 && val < 1)\n      count[3]++;\n  }\n\n  for (int i = 0; i < 4; i++) {\n    bins[i] += count[i];\n  }\n}",
            "//TODO\n}",
            "// Compute the number of threads in a block\n    const size_t block_size = blockDim.x * blockDim.y * blockDim.z;\n    // Compute the number of threads in a grid\n    const size_t grid_size = gridDim.x * gridDim.y * gridDim.z;\n    // Compute the number of threads in the grid\n    const size_t total_threads = block_size * grid_size;\n    // Get the thread id\n    const size_t thread_id = blockIdx.z * blockDim.z * gridDim.x * gridDim.y +\n                             blockIdx.y * blockDim.y * gridDim.x +\n                             blockIdx.x * blockDim.x + threadIdx.z * blockDim.x * gridDim.x +\n                             threadIdx.y * blockDim.x + threadIdx.x;\n    // Compute the number of doubles per thread\n    const size_t doubles_per_thread = N / total_threads;\n    // Compute the thread id for the last thread in the grid\n    const size_t last_thread_id = grid_size * block_size - 1;\n    // Compute the index for the start of the last thread's work\n    const size_t last_thread_start = last_thread_id * doubles_per_thread;\n    // Compute the index for the start of the current thread's work\n    const size_t start = thread_id * doubles_per_thread;\n    // Compute the index for the end of the current thread's work\n    const size_t end = start + doubles_per_thread - 1;\n    // Compute the id of the last double\n    const size_t N_1 = N - 1;\n    // Compute the index of the last double\n    const size_t N_1_idx = N_1 / 8;\n    // Count the number of doubles in each bin for this thread\n    size_t bin[4];\n    // Initialize bins to 0\n    for (int i = 0; i < 4; i++) {\n        bin[i] = 0;\n    }\n    for (size_t i = start; i <= end; i++) {\n        const double xi = x[i];\n        const size_t idx = i / 8;\n        if (idx == N_1_idx) {\n            if (xi >= 0 && xi < 0.25) {\n                bin[0] += 1;\n            }\n            else if (xi >= 0.25 && xi < 0.5) {\n                bin[1] += 1;\n            }\n            else if (xi >= 0.5 && xi < 0.75) {\n                bin[2] += 1;\n            }\n            else {\n                bin[3] += 1;\n            }\n        }\n        else {\n            if (xi >= 0 && xi < 0.25) {\n                bin[0] += 1;\n            }\n            else if (xi >= 0.25 && xi < 0.5) {\n                bin[1] += 1;\n            }\n            else if (xi >= 0.5 && xi < 0.75) {\n                bin[2] += 1;\n            }\n            else {\n                bin[3] += 1;\n            }\n        }\n    }\n    // Save the bin counts in global memory\n    bins[0] = bin[0];\n    bins[1] = bin[1];\n    bins[2] = bin[2];\n    bins[3] = bin[3];\n}",
            "// TODO: fill in\n  int lane = threadIdx.x % 32;\n  int warp = threadIdx.x / 32;\n  int block = blockIdx.x + threadIdx.y * gridDim.x;\n  __shared__ double temp[32];\n  int pos = threadIdx.x + block * blockDim.x;\n  int base = threadIdx.x + warp * blockDim.x * gridDim.x;\n  int num_blocks = blockDim.x * gridDim.x;\n  int total_blocks = blockDim.x * gridDim.x * gridDim.y;\n\n  int offset = 0;\n  if (threadIdx.x < N) {\n    temp[threadIdx.x] = x[threadIdx.x];\n  }\n  __syncthreads();\n  if (threadIdx.x >= N) {\n    offset = (N % num_blocks);\n  }\n\n  if (threadIdx.x < N) {\n    int pos = threadIdx.x + base + offset;\n    double val = temp[pos];\n    if (val < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (val < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (val < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n\n  __syncthreads();\n  if (lane == 0) {\n    for (int i = 0; i < total_blocks; i += num_blocks) {\n      for (int j = 0; j < 4; j++) {\n        atomicAdd(&bins[j], bins[j + 4 * i]);\n      }\n    }\n  }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    int stride = blockDim.x * gridDim.x;\n\n    for (; i < N; i += stride) {\n        if (x[i] < 0) x[i] = 0;\n        else if (x[i] >= 1) x[i] = 0.999;\n\n        double dx = x[i] * 4;\n        int bin = (int)(dx + 0.5);\n\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use MPI to compute in parallel. Assume that MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "size_t n = x.size();\n  double sum = 0.0;\n  for (size_t i = 0; i < n; i++) {\n    sum += x[i];\n  }\n\n  std::vector<double> x_copy;\n  x_copy.resize(n);\n  for (size_t i = 0; i < n; i++) {\n    x_copy[i] = x[i];\n  }\n\n  // sort x and make a copy of it\n  std::sort(x_copy.begin(), x_copy.end());\n\n  // rank 0 get the average of x\n  double average = sum / n;\n\n  // compute the quartiles\n  double q1 = 0.25 * average;\n  double q2 = 0.50 * average;\n  double q3 = 0.75 * average;\n\n  // count the number of numbers in the vector that belong to each quartile\n  size_t i = 0;\n  size_t n_q1 = 0;\n  size_t n_q2 = 0;\n  size_t n_q3 = 0;\n  size_t n_q4 = 0;\n  for (double number : x_copy) {\n    if (number <= q1) {\n      n_q1++;\n    }\n    if (number <= q2) {\n      n_q2++;\n    }\n    if (number <= q3) {\n      n_q3++;\n    }\n    if (number <= q4) {\n      n_q4++;\n    }\n  }\n  bins = {n_q1, n_q2, n_q3, n_q4};\n\n}",
            "//TODO\n}",
            "assert(x.size() % MPI::COMM_WORLD.Get_size() == 0);\n\n    size_t const numDoublesPerProc = x.size() / MPI::COMM_WORLD.Get_size();\n    std::vector<double> xProc(x.begin() + numDoublesPerProc * MPI::COMM_WORLD.Get_rank(),\n                              x.begin() + numDoublesPerProc * (MPI::COMM_WORLD.Get_rank() + 1));\n\n    auto const numElementsInXProc = xProc.size();\n\n    std::vector<int> counts(4, 0);\n    for (size_t i = 0; i < numElementsInXProc; i++) {\n        if (xProc[i] < 0.25)\n            counts[0]++;\n        else if (xProc[i] < 0.5)\n            counts[1]++;\n        else if (xProc[i] < 0.75)\n            counts[2]++;\n        else\n            counts[3]++;\n    }\n\n    MPI::COMM_WORLD.Allreduce(counts.data(), bins.data(), 4, MPI_INT, MPI_SUM);\n}",
            "MPI_Datatype type;\n    MPI_Type_contiguous(x.size(), MPI_DOUBLE, &type);\n    MPI_Type_commit(&type);\n    MPI_Allreduce(x.data(), bins.data(), 4, MPI_UNSIGNED, MPI_SUM, MPI_COMM_WORLD);\n    MPI_Type_free(&type);\n}",
            "//TODO: Replace with appropriate MPI calls\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n\n    return;\n}",
            "// You fill in here\n}",
            "int const rank = MPI_Comm_rank(MPI_COMM_WORLD);\n    int const nproc = MPI_Comm_size(MPI_COMM_WORLD);\n    assert(nproc >= 1);\n\n    if (x.size() == 0) {\n        // Return all zeros\n        for (int i = 0; i < 4; i++)\n            bins[i] = 0;\n        return;\n    }\n\n    // Find the global max value\n    double max_value = x[0];\n    for (int i = 1; i < x.size(); i++) {\n        if (x[i] > max_value)\n            max_value = x[i];\n    }\n\n    // Find the number of processes that will need to compute\n    // the count of the first bin\n    int number_of_procs_first_bin = 0;\n    for (int i = 0; i < nproc; i++) {\n        if (rank <= max_value * i / nproc) {\n            number_of_procs_first_bin++;\n        }\n    }\n\n    // Divide the vector into `number_of_procs_first_bin` chunks\n    int const chunk_size = x.size() / number_of_procs_first_bin;\n    int const chunk_remainder = x.size() % number_of_procs_first_bin;\n\n    // Compute the chunk that this process owns\n    std::vector<double> local_vector;\n    for (int i = 0; i < chunk_size + chunk_remainder; i++) {\n        if (i < chunk_size + chunk_remainder &&\n                i < chunk_size + rank * (chunk_size + chunk_remainder) / number_of_procs_first_bin ||\n                i >= chunk_size + rank * (chunk_size + chunk_remainder) / number_of_procs_first_bin &&\n                i < chunk_size + (rank + 1) * (chunk_size + chunk_remainder) / number_of_procs_first_bin) {\n            local_vector.push_back(x[i]);\n        }\n    }\n\n    std::vector<double> local_results(4, 0);\n\n    // Count in this local vector\n    // Each process in this rank has the same number of values to count\n    for (int i = 0; i < local_vector.size(); i++) {\n        if (local_vector[i] >= 0 && local_vector[i] < 0.25) {\n            local_results[0]++;\n        } else if (local_vector[i] >= 0.25 && local_vector[i] < 0.5) {\n            local_results[1]++;\n        } else if (local_vector[i] >= 0.5 && local_vector[i] < 0.75) {\n            local_results[2]++;\n        } else if (local_vector[i] >= 0.75 && local_vector[i] <= 1) {\n            local_results[3]++;\n        } else {\n            std::cout << \"Error: \" << local_vector[i] << \" is not in [0, 0.25), [0.25, 0.5), [0.5, 0.75), or [0.75, 1)\\n\";\n        }\n    }\n\n    // Alltoallv for all the results\n    // Since we are using a single global array, we can't use MPI_Alltoall\n    // Instead, we use MPI_Alltoallv to send and receive to everyone else\n    // The data to send/receive is local_results\n    // The counts for the send and receive are global_sizes\n    std::vector<int> global_sizes(nproc);\n    std::vector<int> global_displacements(nproc);\n    global_sizes[0] = local_results.size();\n    global_displacements[0] = 0;\n    for (int i = 1; i < nproc; i++) {\n        global_sizes[i] = global_sizes[i - 1] + 4;",
            "std::array<size_t, 4> local_bins;\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tstd::vector<double> local_x;\n\tlocal_x.reserve(x.size() / size);\n\tfor (int i = rank; i < x.size(); i += size) {\n\t\tlocal_x.push_back(x[i]);\n\t}\n\tfor (int i = 0; i < local_x.size(); i++) {\n\t\tif (local_x[i] < 0.25) {\n\t\t\tlocal_bins[0]++;\n\t\t} else if (local_x[i] < 0.5) {\n\t\t\tlocal_bins[1]++;\n\t\t} else if (local_x[i] < 0.75) {\n\t\t\tlocal_bins[2]++;\n\t\t} else {\n\t\t\tlocal_bins[3]++;\n\t\t}\n\t}\n\tint disp[4] = { 0 };\n\tfor (int i = 0; i < 4; i++) {\n\t\tdisp[i] = local_bins[i];\n\t}\n\tMPI_Allreduce(local_bins.data(), bins.data(), 4, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\tMPI_Reduce(disp, bins.data(), 4, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // compute global min and max\n  double max = *std::max_element(x.begin(), x.end());\n  double min = *std::min_element(x.begin(), x.end());\n\n  // global range\n  double range = max - min;\n\n  // compute local min and max\n  double local_min = *std::min_element(x.begin(), x.end());\n  double local_max = *std::max_element(x.begin(), x.end());\n\n  // local range\n  double local_range = local_max - local_min;\n\n  // number of values each rank has\n  int local_count = x.size();\n\n  // average of local values\n  double local_avg = std::accumulate(x.begin(), x.end(), 0.0) / local_count;\n\n  // compute local bins\n  std::array<int, 4> local_bins{0, 0, 0, 0};\n  for (auto i = x.begin(); i!= x.end(); ++i) {\n    if (*i < local_avg - 0.25 * local_range) {\n      local_bins[0]++;\n    } else if (*i < local_avg - 0.5 * local_range) {\n      local_bins[1]++;\n    } else if (*i < local_avg - 0.75 * local_range) {\n      local_bins[2]++;\n    } else if (*i < local_avg + 0.25 * local_range) {\n      local_bins[3]++;\n    } else {\n      std::cerr << \"Fractional part out of range\" << std::endl;\n    }\n  }\n\n  // compute global bins\n  if (rank == 0) {\n    bins[0] = local_bins[0];\n    bins[1] = local_bins[1];\n    bins[2] = local_bins[2];\n    bins[3] = local_bins[3];\n  }\n  MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // double-check\n  if (rank == 0) {\n    for (int i = 0; i < 4; ++i) {\n      if (bins[i]!= local_bins[i]) {\n        std::cerr << \"Wrong bins at rank 0\" << std::endl;\n      }\n    }\n  }\n\n  // compute global avg\n  double global_avg = std::accumulate(x.begin(), x.end(), 0.0) / (x.size() * size);\n\n  // print the results\n  if (rank == 0) {\n    std::cout << \"bins = \" << bins[0] << \" \" << bins[1] << \" \" << bins[2] << \" \" << bins[3] << std::endl;\n    std::cout << \"min = \" << min << std::endl;\n    std::cout << \"max = \" << max << std::endl;\n    std::cout << \"avg = \" << global_avg << std::endl;\n  }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Your code here\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\t//get the total number of double in the x vector\n\tint n = x.size();\n\t//get the chunk of data to be worked on\n\tint chunk_size = n / size;\n\tint remainder = n % size;\n\tint begin, end;\n\tif (rank == 0) {\n\t\tbegin = 0;\n\t\tend = chunk_size;\n\t}\n\telse {\n\t\tbegin = rank * chunk_size + remainder;\n\t\tend = begin + chunk_size;\n\t}\n\t//create the partial sums array\n\tstd::array<int, 4> partial_sums;\n\tfor (int i = 0; i < 4; i++)\n\t\tpartial_sums[i] = 0;\n\n\t//find the number of items in each category of values in the x vector\n\tfor (int i = begin; i < end; i++) {\n\t\tif (x[i] < 0.25)\n\t\t\tpartial_sums[0]++;\n\t\telse if (x[i] >= 0.25 && x[i] < 0.5)\n\t\t\tpartial_sums[1]++;\n\t\telse if (x[i] >= 0.5 && x[i] < 0.75)\n\t\t\tpartial_sums[2]++;\n\t\telse if (x[i] >= 0.75 && x[i] < 1)\n\t\t\tpartial_sums[3]++;\n\t}\n\t//get the final sums array using MPI_Allreduce\n\tstd::array<int, 4> final_sums;\n\tMPI_Allreduce(partial_sums.data(), final_sums.data(), 4, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\t//set the bins\n\tbins[0] = final_sums[0];\n\tbins[1] = final_sums[1];\n\tbins[2] = final_sums[2];\n\tbins[3] = final_sums[3];\n\treturn;\n}",
            "// TODO\n}",
            "int nprocs, proc_id;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &proc_id);\n    size_t n = x.size();\n    size_t nperproc = n / nprocs;\n    size_t extra = n % nprocs;\n    size_t start = proc_id * nperproc;\n    size_t end = start + nperproc;\n    if (proc_id < extra) {\n        end += 1;\n    }\n    end += start;\n    size_t i = start;\n    size_t ndouble = 0;\n    size_t ndouble25 = 0;\n    size_t ndouble5 = 0;\n    size_t ndouble75 = 0;\n    while (i < end) {\n        if (x[i] - std::floor(x[i]) < 0.25) {\n            ndouble25 += 1;\n            ndouble += 1;\n        } else if (x[i] - std::floor(x[i]) < 0.5) {\n            ndouble5 += 1;\n            ndouble += 1;\n        } else if (x[i] - std::floor(x[i]) < 0.75) {\n            ndouble75 += 1;\n            ndouble += 1;\n        } else {\n            ndouble += 1;\n        }\n        i += 1;\n    }\n    if (proc_id == 0) {\n        bins[0] = ndouble25;\n        bins[1] = ndouble5;\n        bins[2] = ndouble75;\n        bins[3] = n - ndouble;\n    }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n  int rank;\n  MPI_Comm_rank(comm, &rank);\n\n  size_t n = x.size();\n  size_t n_per_rank = n / comm_size;\n\n  size_t n_local = n_per_rank;\n  size_t start_idx = rank * n_per_rank;\n\n  if (n_per_rank > n - start_idx) {\n    n_local = n - start_idx;\n  }\n\n  size_t n_per_rank_local = n_local / comm_size;\n  size_t start_idx_local = start_idx + rank * n_per_rank_local;\n\n  std::vector<double> x_local(x.begin() + start_idx_local, x.begin() + start_idx_local + n_per_rank_local);\n\n  // Sorting x_local\n  std::sort(x_local.begin(), x_local.end());\n\n  size_t bins_local[4] = {0, 0, 0, 0};\n\n  // counting numbers with 0.00 ~ 0.24\n  for (size_t i = 0; i < n_local; ++i) {\n    if (x_local[i] < 0.25) {\n      ++bins_local[0];\n    }\n  }\n\n  // counting numbers with 0.25 ~ 0.50\n  for (size_t i = 0; i < n_local; ++i) {\n    if (0.25 <= x_local[i] && x_local[i] < 0.50) {\n      ++bins_local[1];\n    }\n  }\n\n  // counting numbers with 0.50 ~ 0.75\n  for (size_t i = 0; i < n_local; ++i) {\n    if (0.50 <= x_local[i] && x_local[i] < 0.75) {\n      ++bins_local[2];\n    }\n  }\n\n  // counting numbers with 0.75 ~ 1.00\n  for (size_t i = 0; i < n_local; ++i) {\n    if (0.75 <= x_local[i] && x_local[i] < 1.00) {\n      ++bins_local[3];\n    }\n  }\n\n  bins[0] = bins_local[0];\n  bins[1] = bins_local[1];\n  bins[2] = bins_local[2];\n  bins[3] = bins_local[3];\n\n  MPI_Reduce(MPI_IN_PLACE, bins_local, 4, MPI_UNSIGNED, MPI_SUM, 0, comm);\n  bins[0] = bins_local[0];\n  bins[1] = bins_local[1];\n  bins[2] = bins_local[2];\n  bins[3] = bins_local[3];\n}",
            "// TODO: Your code here\n    return;\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int rank = 0;\n    MPI_Comm_rank(comm, &rank);\n    int numprocs = 1;\n    MPI_Comm_size(comm, &numprocs);\n    int tag = 99;\n\n    size_t n = x.size();\n    std::array<size_t, 4> bins_part = {0, 0, 0, 0};\n\n    if (n == 0) {\n        bins[0] = 0;\n        bins[1] = 0;\n        bins[2] = 0;\n        bins[3] = 0;\n        return;\n    }\n\n    if (n <= 2) {\n        if (rank == 0) {\n            bins[0] = (n > 0) && (x[0] < 0.25);\n            bins[1] = (n > 1) && (x[0] >= 0.25) && (x[1] < 0.5);\n            bins[2] = (n > 1) && (x[1] >= 0.5) && (x[0] < 0.75);\n            bins[3] = (n > 1) && (x[1] >= 0.75);\n        }\n        return;\n    }\n\n    if (n <= 3) {\n        if (rank == 0) {\n            bins[0] = x[0] < 0.25;\n            bins[1] = x[0] >= 0.25 && x[1] < 0.5;\n            bins[2] = x[1] >= 0.5 && x[0] < 0.75;\n            bins[3] = x[1] >= 0.75;\n        }\n        return;\n    }\n\n    // split the input into four vectors\n    // for the first vector, first rank takes the first element\n    // for the second vector, first rank takes the second element\n    // for the third vector, last rank takes the third element\n    // for the fourth vector, last rank takes the forth element\n    // all other ranks get the rest of the elements\n    size_t n_part1 = (rank * n) / numprocs;\n    size_t n_part2 = ((rank + 1) * n) / numprocs - n_part1;\n    size_t n_part3 = (rank + 2) * n / numprocs - n_part1 - n_part2;\n\n    std::vector<double> x_part1(n_part1), x_part2(n_part2), x_part3(n_part3);\n\n    if (rank == 0) {\n        x_part1[0] = x[0];\n        x_part2[0] = x[1];\n        x_part3[0] = x[2];\n    }\n\n    if (rank == numprocs - 1) {\n        x_part1[n_part1 - 1] = x[n - 1];\n        x_part2[n_part2 - 1] = x[n - 2];\n        x_part3[n_part3 - 1] = x[n - 3];\n    }\n\n    MPI_Send(&x_part1[0], n_part1, MPI_DOUBLE, rank + 1, tag, comm);\n    MPI_Send(&x_part2[0], n_part2, MPI_DOUBLE, rank + 1, tag, comm);\n    MPI_Send(&x_part3[0], n_part3, MPI_DOUBLE, rank + 2, tag, comm);\n\n    MPI_Status status;\n\n    if (rank == numprocs - 1) {\n        MPI_Recv(&bins_part[0], 1, MPI_INT, rank + 1, tag, comm, &status);\n        MPI_Recv(&bins_part[1], 1, MPI_INT, rank + 1, tag, comm, &status);\n        MPI_Recv(&bins_part[",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if (size == 1) {\n        bins.fill(0);\n        for (auto& xi : x) {\n            if (xi < 0.25)\n                bins[0]++;\n            else if (xi < 0.5)\n                bins[1]++;\n            else if (xi < 0.75)\n                bins[2]++;\n            else if (xi < 1)\n                bins[3]++;\n            else\n                throw std::invalid_argument(\"x must be in [0, 1].\");\n        }\n        return;\n    }\n    size_t count_per_rank = x.size() / size;\n    std::vector<size_t> local_counts(4);\n    local_counts.fill(0);\n    for (int i = 0; i < count_per_rank; ++i) {\n        auto xi = x[i + rank * count_per_rank];\n        if (xi < 0.25)\n            local_counts[0]++;\n        else if (xi < 0.5)\n            local_counts[1]++;\n        else if (xi < 0.75)\n            local_counts[2]++;\n        else if (xi < 1)\n            local_counts[3]++;\n        else\n            throw std::invalid_argument(\"x must be in [0, 1].\");\n    }\n    std::vector<size_t> global_counts(4);\n    MPI_Allreduce(local_counts.data(), global_counts.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n    bins = global_counts;\n}",
            "// Your code here\n}",
            "int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Each rank receives a section of the input vector x.\n    size_t sizePerRank = x.size() / size;\n    size_t offset = rank * sizePerRank;\n    std::vector<double> xSection(x.begin() + offset, x.begin() + offset + sizePerRank);\n\n    // Count the number of doubles in the vector xSection that have a fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    for (double value : xSection) {\n        int index = 0;\n        if (value < 0.25)\n            index = 0;\n        else if (value < 0.5)\n            index = 1;\n        else if (value < 0.75)\n            index = 2;\n        else\n            index = 3;\n        bins[index]++;\n    }\n\n    // Use MPI to compute the sum of the counts in bins.\n    MPI_Allreduce(MPI_IN_PLACE, &bins[0], 4, MPI_UNSIGNED, MPI_SUM, MPI_COMM_WORLD);\n\n    // Store the sum on rank 0.\n    if (rank == 0) {\n        bins[0] /= sizePerRank;\n        bins[1] /= sizePerRank;\n        bins[2] /= sizePerRank;\n        bins[3] /= sizePerRank;\n    }\n}",
            "int nproc;\n    int rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (nproc > 1 && rank == 0) {\n        // rank 0 counts\n        size_t n = x.size();\n        double lower_bound = 0.0;\n        double upper_bound = 0.25;\n        std::array<size_t, 4> rank0_bins = count_quartiles_serial(x, lower_bound, upper_bound);\n\n        // MPI scatter\n        std::vector<size_t> sendcounts(nproc);\n        for (int i = 0; i < nproc; i++) {\n            sendcounts[i] = (i == 0)? rank0_bins[0] : n / (nproc - 1) + 1;\n        }\n        std::vector<size_t> displs(nproc);\n        displs[0] = 0;\n        for (int i = 1; i < nproc; i++) {\n            displs[i] = displs[i-1] + sendcounts[i-1];\n        }\n        std::vector<size_t> recvcounts(nproc);\n        for (int i = 0; i < nproc; i++) {\n            recvcounts[i] = (i == 0)? n : n / (nproc - 1);\n        }\n        std::vector<size_t> recvbuf(recvcounts[0] + recvcounts[1] + recvcounts[2] + recvcounts[3]);\n\n        MPI_Scatterv(rank0_bins.data(), sendcounts.data(), displs.data(),\n                     MPI_INT, recvbuf.data(), recvcounts[0], MPI_INT, 0, MPI_COMM_WORLD);\n\n        bins[0] = recvbuf[0];\n        bins[1] = recvbuf[recvcounts[0]];\n        bins[2] = recvbuf[recvcounts[0] + recvcounts[1]];\n        bins[3] = recvbuf[recvcounts[0] + recvcounts[1] + recvcounts[2]];\n    } else {\n        bins = count_quartiles_serial(x, 0.0, 0.25);\n    }\n}",
            "int n = x.size();\n    bins = std::array<size_t, 4>{0};\n    if(n==0) return;\n\n    // MPI_Allreduce is a collective communication operation, which means\n    // every MPI process must be involved in it. This operation returns\n    // the minimum number of ranks to call MPI_Allreduce.\n    int min_ranks = MPI_Allreduce(nullptr, nullptr, 0, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n\n    int ranks, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // if there's only one rank, then there's no point doing MPI,\n    // and calling MPI_Allreduce will throw an error. To avoid this,\n    // we use this \"if\" statement to call MPI_Allreduce only if\n    // there's more than one rank.\n    if(ranks > 1)\n    {\n        // we'll need an array of size 4 to receive the bins\n        // on each MPI rank. We'll initialize them all to 0.\n        std::array<size_t, 4> bins_local = std::array<size_t, 4>{0};\n        for(int i = 0; i < n; i++)\n        {\n            // we'll need the fractional part of x[i]\n            // to decide where to bin the number\n            int index = 0;\n            if(x[i] >= 0.75) index = 3;\n            else if(x[i] >= 0.5) index = 2;\n            else if(x[i] >= 0.25) index = 1;\n\n            // then, we increment the local bins\n            bins_local[index]++;\n        }\n\n        // MPI_Allreduce is a collective communication operation,\n        // which means every MPI process must be involved in it.\n        // The last argument, MPI_SUM, means we want the result to\n        // be the sum of the values on each rank.\n        //\n        // We also need to specify the data type of the data we're\n        // sending. The data type is a struct that contains the size of\n        // the data, the data, and a type (MPI_INT in this case).\n        // This struct is defined in mpi_type.h\n        MPI_Allreduce(bins_local.data(), bins.data(), 4, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    }\n\n    // if there's only one rank, we still need to increment the bins\n    // so that we get the correct values. In this case, every rank\n    // has a complete copy of the x vector, and we can just loop over\n    // the numbers in x and bin them normally.\n    else if(ranks == 1)\n    {\n        for(int i = 0; i < n; i++)\n        {\n            // we'll need the fractional part of x[i]\n            // to decide where to bin the number\n            int index = 0;\n            if(x[i] >= 0.75) index = 3;\n            else if(x[i] >= 0.5) index = 2;\n            else if(x[i] >= 0.25) index = 1;\n\n            // then, we increment the local bins\n            bins[index]++;\n        }\n    }\n}",
            "// The following function will be called by all ranks\n    auto count_quartiles = [] (std::vector<double> const& x, std::array<size_t, 4> &bins) {\n        auto const N = x.size();\n        for (size_t i = 0; i < 4; ++i) {\n            bins[i] = 0;\n        }\n\n        for (size_t i = 0; i < N; ++i) {\n            if (x[i] >= 0 && x[i] < 0.25) {\n                ++bins[0];\n            } else if (x[i] >= 0.25 && x[i] < 0.5) {\n                ++bins[1];\n            } else if (x[i] >= 0.5 && x[i] < 0.75) {\n                ++bins[2];\n            } else if (x[i] >= 0.75 && x[i] < 1) {\n                ++bins[3];\n            }\n        }\n    };\n\n    // MPI part\n    auto const rank = MPI::COMM_WORLD.Get_rank();\n    auto const size = MPI::COMM_WORLD.Get_size();\n\n    // Every rank gets a copy of the original vector\n    std::vector<double> x_copy = x;\n    if (rank == 0) {\n        std::cout << \"vector size: \" << x_copy.size() << std::endl;\n    }\n\n    std::array<size_t, 4> bins_copy;\n    // Every rank calls the function\n    count_quartiles(x_copy, bins_copy);\n\n    // Compute the number of ranks that will be used to count the vector\n    auto const nranks = (size + 3) / 4;\n\n    // Create a new communicator (the sub-communicator)\n    MPI::Intracomm comm;\n    if (rank < nranks) {\n        MPI::Group g = MPI::COMM_WORLD.Get_group();\n        std::vector<int> r;\n        for (int i = rank; i < size; i += nranks) {\n            r.push_back(i);\n        }\n        MPI::Group g_nranks = g.Incl(r.size(), r.data());\n        comm = MPI::COMM_WORLD.Create(g_nranks);\n        std::cout << \"created a sub-communicator with \" << comm.Get_size() << \" ranks\" << std::endl;\n    } else {\n        comm = MPI::COMM_NULL;\n        std::cout << \"the rank is not in the sub-communicator with \" << nranks << \" ranks\" << std::endl;\n    }\n\n    // Every rank in the sub-communicator will compute the same vector bins, but\n    // only rank 0 will write the vector bins to the original vector bins\n    std::array<size_t, 4> bins_new;\n    comm.Allreduce(&bins_copy, &bins_new, 4, MPI::SUM);\n    if (rank == 0) {\n        bins = bins_new;\n    }\n\n    comm.Free();\n\n    if (rank == 0) {\n        std::cout << \"vector size: \" << x.size() << std::endl;\n        for (size_t i = 0; i < 4; ++i) {\n            std::cout << \"bins[\" << i << \"] = \" << bins[i] << std::endl;\n        }\n    }\n\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int rank;\n    int nprocs;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &nprocs);\n\n    std::vector<double> xlocal;\n    if(rank == 0) {\n        // Rank 0 has the full vector.\n        xlocal = x;\n    }\n    else {\n        // All other ranks have their local vectors.\n        int n = x.size() / nprocs;\n        int remainder = x.size() % nprocs;\n        xlocal.resize(n + (rank < remainder? 1 : 0));\n        for(size_t i = 0; i < xlocal.size(); ++i) {\n            xlocal[i] = x[i + (rank * n)];\n        }\n    }\n    size_t n = xlocal.size();\n    // MPI_Alltoall is the function used to collect all the local results on rank 0.\n    double *temp = new double[4];\n    // rank 0 will store the result of the local count, send the result to all the processes.\n    if(rank == 0) {\n        size_t c0 = 0;\n        size_t c1 = 0;\n        size_t c2 = 0;\n        size_t c3 = 0;\n        for(size_t i = 0; i < n; ++i) {\n            if(xlocal[i] < 0.25) {\n                ++c0;\n            }\n            else if(xlocal[i] < 0.5) {\n                ++c1;\n            }\n            else if(xlocal[i] < 0.75) {\n                ++c2;\n            }\n            else {\n                ++c3;\n            }\n        }\n        temp[0] = c0;\n        temp[1] = c1;\n        temp[2] = c2;\n        temp[3] = c3;\n        MPI_Alltoall(temp, 4, MPI_DOUBLE, bins.data(), 4, MPI_DOUBLE, comm);\n    }\n    else {\n        // Other processes send the results to rank 0.\n        size_t c0 = 0;\n        size_t c1 = 0;\n        size_t c2 = 0;\n        size_t c3 = 0;\n        for(size_t i = 0; i < n; ++i) {\n            if(xlocal[i] < 0.25) {\n                ++c0;\n            }\n            else if(xlocal[i] < 0.5) {\n                ++c1;\n            }\n            else if(xlocal[i] < 0.75) {\n                ++c2;\n            }\n            else {\n                ++c3;\n            }\n        }\n        temp[0] = c0;\n        temp[1] = c1;\n        temp[2] = c2;\n        temp[3] = c3;\n        MPI_Alltoall(temp, 4, MPI_DOUBLE, bins.data(), 4, MPI_DOUBLE, comm);\n    }\n    // Free the memory.\n    delete[] temp;\n    return;\n}",
            "int n = x.size();\n    std::vector<size_t> counts(n);\n\n    MPI_Allreduce(MPI_IN_PLACE, counts.data(), n, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    size_t k = 0;\n    for (size_t i = 0; i < n; i++) {\n        if (x[i] < 0.25) {\n            bins[0] += counts[i];\n            k += counts[i];\n        } else if (x[i] < 0.5) {\n            bins[1] += counts[i];\n            k += counts[i];\n        } else if (x[i] < 0.75) {\n            bins[2] += counts[i];\n            k += counts[i];\n        } else {\n            bins[3] += counts[i];\n            k += counts[i];\n        }\n    }\n\n    MPI_Reduce(MPI_IN_PLACE, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (k!= n) {\n        throw std::runtime_error(\"Total number of elements in the vector x is not equal to the sum of all the counts.\");\n    }\n}",
            "size_t n = x.size();\n  bins = {0, 0, 0, 0};\n\n  MPI_Datatype double_type;\n  MPI_Type_contiguous(sizeof(double), MPI_BYTE, &double_type);\n  MPI_Type_commit(&double_type);\n\n  MPI_Allreduce(MPI_IN_PLACE, x.data(), n, double_type, MPI_SUM, MPI_COMM_WORLD);\n\n  for (size_t i = 0; i < n; ++i) {\n    double tmp = x[i];\n    if (tmp < 0.25) {\n      ++bins[0];\n    } else if (tmp < 0.5) {\n      ++bins[1];\n    } else if (tmp < 0.75) {\n      ++bins[2];\n    } else {\n      ++bins[3];\n    }\n  }\n\n  MPI_Type_free(&double_type);\n}",
            "size_t numRanks;\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int binWidth = n / numRanks;\n\n    // determine local bins\n    std::array<size_t, 4> localBins;\n    for (size_t i = 0; i < binWidth; i++) {\n        if (x[i] >= 0 && x[i] <= 0.25) {\n            localBins[0]++;\n        } else if (x[i] >= 0.25 && x[i] <= 0.5) {\n            localBins[1]++;\n        } else if (x[i] >= 0.5 && x[i] <= 0.75) {\n            localBins[2]++;\n        } else if (x[i] >= 0.75 && x[i] <= 1) {\n            localBins[3]++;\n        }\n    }\n    MPI_Allreduce(&localBins, &bins, 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        bins.fill(0);\n    }\n\n    int count = x.size();\n    int blockSize = count / size;\n    int remainder = count % size;\n\n    if (rank < remainder) {\n        blockSize++;\n    }\n\n    std::vector<double> buffer;\n    buffer.reserve(blockSize);\n    int start = rank * blockSize;\n\n    for (int i = 0; i < blockSize; i++) {\n        int index = start + i;\n        if (index < count) {\n            buffer.push_back(x[index]);\n        }\n    }\n\n    double lowerLimit = 0, upperLimit = 0, fraction = 0;\n\n    if (rank == 0) {\n        std::sort(buffer.begin(), buffer.end());\n\n        lowerLimit = buffer[0];\n        upperLimit = buffer[blockSize - 1];\n        fraction = buffer[blockSize - 1] - buffer[0];\n    }\n\n    MPI_Bcast(&lowerLimit, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Bcast(&upperLimit, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Bcast(&fraction, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < buffer.size(); i++) {\n        double val = buffer[i];\n        if (val >= lowerLimit && val < lowerLimit + fraction / 4) {\n            bins[0]++;\n        } else if (val >= lowerLimit + fraction / 4 && val < lowerLimit + fraction / 2) {\n            bins[1]++;\n        } else if (val >= lowerLimit + fraction / 2 && val < lowerLimit + 3 * fraction / 4) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n\n    if (rank == 0) {\n        std::cout << \"Fraction: \" << fraction << std::endl;\n        std::cout << \"Counts: \" << bins[0] << \" \" << bins[1] << \" \" << bins[2] << \" \" << bins[3] << std::endl;\n    }\n}",
            "// YOUR CODE HERE\n    // Note: The size of the input vector is not necessarily divisible by the size of the MPI communicator.\n    // If the size of the input vector is not evenly divisible by the size of the communicator, \n    // then a single MPI rank will have a different number of elements to process than the others.\n    // If your MPI communicator has an even number of ranks, then the \"extra\" elements will be processed by rank 0.\n    // If your MPI communicator has an odd number of ranks, then the \"extra\" elements will be processed by rank 0 or 1 (the first or last rank).\n    return;\n}",
            "int mpi_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n    int mpi_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    size_t total_size = x.size();\n    int size_per_rank = total_size / mpi_size;\n    int extra_size = total_size % mpi_size;\n    std::vector<size_t> x_local(size_per_rank);\n    if (mpi_rank == 0) {\n        for (int i = 1; i < mpi_size; i++) {\n            MPI_Recv(&x_local[0], size_per_rank, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            total_size += size_per_rank;\n        }\n    } else {\n        MPI_Send(&x[mpi_rank*size_per_rank], size_per_rank, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n        x_local.resize(size_per_rank + extra_size);\n    }\n\n    // Fill the local vector with appropriate values\n    for (int i = 0; i < size_per_rank; i++) {\n        x_local[i] = x[mpi_rank*size_per_rank + i];\n    }\n    std::sort(x_local.begin(), x_local.end());\n\n    if (x_local.size() == 0) {\n        for (int i = 0; i < 4; i++)\n            bins[i] = 0;\n        return;\n    }\n\n    bins.at(0) = x_local[0];\n    bins.at(1) = x_local[size_per_rank / 4];\n    bins.at(2) = x_local[size_per_rank / 2];\n    bins.at(3) = x_local[size_per_rank * 3 / 4];\n    bins.at(4) = x_local[x_local.size() - 1];\n    for (int i = 0; i < 4; i++) {\n        MPI_Reduce(&bins[i], &bins[i], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n\n    if (mpi_rank == 0) {\n        std::sort(bins.begin(), bins.end());\n    } else {\n        MPI_Bcast(&bins, 4, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    //TODO: Compute bins\n}",
            "MPI_Status status;\n    int nbins[4] = {0, 0, 0, 0};\n    int rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int count = x.size();\n    int chunk = count / size;\n    int mod = count % size;\n    int begin = 0;\n    int end = 0;\n    if (rank < mod) {\n        begin = chunk * rank;\n        end = begin + chunk;\n    }\n    else {\n        begin = chunk * rank + mod;\n        end = begin + chunk;\n    }\n    for (int i = begin; i < end; ++i) {\n        if (x[i] < 0.25) nbins[0] += 1;\n        else if (x[i] < 0.5) nbins[1] += 1;\n        else if (x[i] < 0.75) nbins[2] += 1;\n        else nbins[3] += 1;\n    }\n    MPI_Gather(&nbins, 4, MPI_INT, &bins, 4, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n  return;\n}",
            "// TODO: Your code here\n}",
            "auto n = x.size();\n    auto n_rank = (n + 1) / 2; // the number of elements that each rank owns\n\n    // bins = {0, 0, 0, 0}\n    std::fill(std::begin(bins), std::end(bins), 0);\n\n    // set up the communicator\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int n_rank_world;\n    MPI_Comm_size(comm, &n_rank_world);\n\n    // the world rank\n    int world_rank;\n    MPI_Comm_rank(comm, &world_rank);\n\n    // find the left-most rank that owns data\n    int left_rank = world_rank - 1;\n    while (left_rank >= 0) {\n        if (left_rank * n_rank + n_rank <= n) break;\n        left_rank--;\n    }\n    if (left_rank < 0) {\n        // we don't own any data\n        return;\n    }\n\n    // find the right-most rank that owns data\n    int right_rank = world_rank + 1;\n    while (right_rank < n_rank_world) {\n        if (right_rank * n_rank + n_rank <= n) break;\n        right_rank++;\n    }\n    if (right_rank >= n_rank_world) {\n        // we don't own any data\n        return;\n    }\n\n    // left rank should send its data\n    if (left_rank == world_rank) {\n        // the world rank owns data left to the left_rank\n        // determine how many elements should be sent\n        auto n_left = std::min(n_rank, n - (left_rank * n_rank));\n        // the local indices that the left_rank owns\n        std::vector<size_t> local_indices_left;\n        local_indices_left.reserve(n_left);\n        std::iota(std::back_inserter(local_indices_left),\n                  left_rank * n_rank, left_rank * n_rank + n_left);\n        // send the local indices to the left_rank\n        MPI_Send(local_indices_left.data(), n_left, MPI_INT, left_rank, 0, comm);\n    }\n\n    // right rank should receive data\n    if (right_rank == world_rank) {\n        // the world rank owns data right to the right_rank\n        // receive the data from the right_rank\n        std::vector<size_t> local_indices_right(n_rank);\n        MPI_Recv(local_indices_right.data(), n_rank, MPI_INT, right_rank, 0, comm, MPI_STATUS_IGNORE);\n        // the local indices that the right_rank owns\n        auto n_right = std::min(n_rank, n - (right_rank * n_rank));\n        // determine the local indices that the right_rank owns\n        std::vector<size_t> local_indices_right_corrected;\n        local_indices_right_corrected.reserve(n_right);\n        std::iota(std::back_inserter(local_indices_right_corrected),\n                  right_rank * n_rank, right_rank * n_rank + n_right);\n        // compare the received data to the right_rank's local data\n        std::sort(local_indices_right.begin(), local_indices_right.end());\n        std::sort(local_indices_right_corrected.begin(), local_indices_right_corrected.end());\n        if (local_indices_right!= local_indices_right_corrected) {\n            std::cout << \"MPI: right_rank's data is incorrect!\" << std::endl;\n            exit(-1);\n        }\n    }\n\n    // left_rank and right_rank should be updated to the ranks of their neighbors\n    if (left_rank == world_rank) left_rank = right_rank;\n    if (right_rank == world_rank) right_rank = left_rank;\n\n    //",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  // Your code here\n\n  std::array<size_t, 4> counts;\n  for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] >= 0 && x[i] < 0.25) {\n      counts[0]++;\n    }\n    if (x[i] >= 0.25 && x[i] < 0.5) {\n      counts[1]++;\n    }\n    if (x[i] >= 0.5 && x[i] < 0.75) {\n      counts[2]++;\n    }\n    if (x[i] >= 0.75 && x[i] <= 1) {\n      counts[3]++;\n    }\n  }\n\n  // Calculate the sum of counts, and store in bins.\n  std::array<size_t, 4> sums;\n  for (size_t i = 0; i < 4; i++) {\n    sums[i] = counts[i] + (rank > 0? bins[i] : 0);\n  }\n  MPI_Reduce(counts.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: YOUR CODE HERE\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> partial{};\n\n    for (auto i = 0; i < x.size(); i++){\n        if (x[i] < 0.25) partial[0]++;\n        else if (x[i] < 0.50) partial[1]++;\n        else if (x[i] < 0.75) partial[2]++;\n        else partial[3]++;\n    }\n\n    std::array<size_t, 4> temp;\n    MPI_Allreduce(&partial, &temp, 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n    bins = temp;\n}",
            "int rank, nprocs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    //TODO\n\n\n    std::array<double, 4> x_part{0};\n    int count_x_part = x.size() / nprocs;\n    int rest_x_part = x.size() % nprocs;\n\n    std::array<double, 4> x_part_result{0};\n\n    for (int i = 0; i < count_x_part; i++) {\n        if (x[i] < 0.25) x_part[0] = x[i];\n        else if (x[i] >= 0.25 && x[i] < 0.5) x_part[1] = x[i];\n        else if (x[i] >= 0.5 && x[i] < 0.75) x_part[2] = x[i];\n        else x_part[3] = x[i];\n    }\n    for (int i = 0; i < rest_x_part; i++) {\n        if (x[i + count_x_part * nprocs] < 0.25) x_part[0] = x[i + count_x_part * nprocs];\n        else if (x[i + count_x_part * nprocs] >= 0.25 && x[i + count_x_part * nprocs] < 0.5) x_part[1] = x[i + count_x_part * nprocs];\n        else if (x[i + count_x_part * nprocs] >= 0.5 && x[i + count_x_part * nprocs] < 0.75) x_part[2] = x[i + count_x_part * nprocs];\n        else x_part[3] = x[i + count_x_part * nprocs];\n    }\n\n    std::array<size_t, 4> x_part_result{0};\n    for (int i = 0; i < 4; i++) {\n        x_part_result[i] = std::count_if(x_part.begin(), x_part.end(), [i](double e) { return e >= i * 0.25 && e < (i + 1) * 0.25; });\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; i++) {\n            bins[i] = 0;\n        }\n    }\n\n    std::array<int, 4> x_part_result_result{0};\n    MPI_Reduce(&x_part_result[0], &x_part_result_result[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; i++) {\n            bins[i] = x_part_result_result[i];\n        }\n    }\n\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, size;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &size);\n    int const n = x.size();\n    int const block_size = n / size;\n    int const rem = n % size;\n    int const n_local = (rank < rem)? block_size + 1 : block_size;\n    // allocate temporary memory for the local block of data\n    auto *x_local = new double[n_local];\n    // copy the data from x into local memory\n    std::copy(x.begin() + rank * block_size, x.begin() + (rank + 1) * block_size, x_local);\n    // check if this is the last rank in the communicator\n    bool is_last = rank == size - 1;\n    // if it is, then we need to copy the remaining values into local memory\n    if (is_last) {\n        std::copy(x.begin() + rank * block_size + block_size, x.begin() + n, x_local);\n    }\n    // compute the bin counts in the local block\n    std::array<size_t, 4> bins_local = {{0, 0, 0, 0}};\n    for (int i = 0; i < n_local; ++i) {\n        double const x_local_i = x_local[i];\n        if (x_local_i < 0.25) {\n            ++bins_local[0];\n        } else if (x_local_i < 0.5) {\n            ++bins_local[1];\n        } else if (x_local_i < 0.75) {\n            ++bins_local[2];\n        } else {\n            ++bins_local[3];\n        }\n    }\n    // free the memory allocated for the local block\n    delete[] x_local;\n    // reduce the local counts to get the global counts\n    MPI_Allreduce(MPI_IN_PLACE, bins_local.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, comm);\n    // copy the global bins into the output parameter\n    std::copy(bins_local.data(), bins_local.data() + 4, bins.data());\n    return;\n}",
            "// TODO: fill in your code here\n    size_t size = x.size();\n    bins = {};\n\n    std::vector<double> x_sorted(x.begin(), x.end());\n    std::sort(x_sorted.begin(), x_sorted.end());\n\n    size_t count = 0;\n    for (size_t i = 0; i < size; i++) {\n        if (x_sorted[i] < 0.25) {\n            count++;\n        }\n        if (x_sorted[i] >= 0.25 && x_sorted[i] < 0.5) {\n            count++;\n        }\n        if (x_sorted[i] >= 0.5 && x_sorted[i] < 0.75) {\n            count++;\n        }\n        if (x_sorted[i] >= 0.75 && x_sorted[i] < 1) {\n            count++;\n        }\n    }\n\n    bins = {count, size - count, (size - count) / 2, size / 4};\n\n    // MPI_Allgather is a collective operation and executes all processes at once.\n    // The number of arguments is equal to the number of processes participating in the operation,\n    // so we need to know how many ranks we are dealing with.\n    // std::vector<double> x_sorted(x.begin(), x.end());\n    // std::sort(x_sorted.begin(), x_sorted.end());\n\n    // size_t count = 0;\n    // MPI_Allgather(&count, 1, MPI_INT, &bins, 4, MPI_INT, MPI_COMM_WORLD);\n    // for (size_t i = 0; i < size; i++) {\n    //     if (x_sorted[i] < 0.25) {\n    //         count++;\n    //     }\n    //     if (x_sorted[i] >= 0.25 && x_sorted[i] < 0.5) {\n    //         count++;\n    //     }\n    //     if (x_sorted[i] >= 0.5 && x_sorted[i] < 0.75) {\n    //         count++;\n    //     }\n    //     if (x_sorted[i] >= 0.75 && x_sorted[i] < 1) {\n    //         count++;\n    //     }\n    // }\n    // return bins;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Split the data into equal pieces for each rank\n  size_t n = x.size();\n  size_t chunk_size = n / size;\n  size_t remainder = n % size;\n  std::vector<double> local_vec(chunk_size + (rank < remainder));\n  for (size_t i = 0; i < local_vec.size(); i++) {\n    local_vec[i] = x[i + chunk_size * rank + (rank < remainder) * remainder];\n  }\n\n  std::vector<double> send_buffer(chunk_size);\n  std::vector<double> recv_buffer(size * chunk_size);\n  MPI_Allgather(local_vec.data(), chunk_size, MPI_DOUBLE, recv_buffer.data(), chunk_size, MPI_DOUBLE, MPI_COMM_WORLD);\n\n  for (int i = 0; i < size; i++) {\n    int start_idx = i * chunk_size;\n    int end_idx = (i + 1) * chunk_size;\n    for (int j = 0; j < chunk_size; j++) {\n      send_buffer[j] = recv_buffer[start_idx + j];\n    }\n\n    double sum = 0;\n    for (int j = 0; j < chunk_size; j++) {\n      sum += send_buffer[j];\n    }\n    double avg = sum / chunk_size;\n\n    for (int j = 0; j < chunk_size; j++) {\n      if (send_buffer[j] > avg) {\n        bins[i]++;\n      }\n    }\n  }\n\n  if (rank == 0) {\n    std::cout << bins[0] << \" \" << bins[1] << \" \" << bins[2] << \" \" << bins[3] << \"\\n\";\n  }\n\n}",
            "size_t N = x.size();\n    size_t Nproc = 1;\n    MPI_Comm_size(MPI_COMM_WORLD, &Nproc);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // sort x on all ranks\n    std::vector<double> xs(N);\n    MPI_Allgather(&x[0], N, MPI_DOUBLE, &xs[0], N, MPI_DOUBLE, MPI_COMM_WORLD);\n\n    // calculate number of values in each bin\n    auto xmin = xs[0];\n    auto xmax = xs[N-1];\n    auto dx = (xmax - xmin)/4.0;\n    bins[0] = 0;\n    for (size_t i=0; i<N; ++i) {\n        auto v = xs[i];\n        if (v >= xmin && v < xmin+dx) {\n            ++bins[0];\n        }\n        else if (v >= xmin+dx && v < xmin+2*dx) {\n            ++bins[1];\n        }\n        else if (v >= xmin+2*dx && v < xmin+3*dx) {\n            ++bins[2];\n        }\n        else if (v >= xmin+3*dx && v < xmax) {\n            ++bins[3];\n        }\n    }\n\n    // gather the counts from each rank\n    std::array<size_t, 4> bins_all;\n    MPI_Reduce(&bins[0], &bins_all[0], 4, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // copy the counts to the output array\n    if (rank == 0) {\n        for (size_t i=0; i<4; ++i) {\n            bins[i] = bins_all[i];\n        }\n    }\n}",
            "size_t n=x.size();\n\n    if (n<=0) {\n        bins = {0, 0, 0, 0};\n        return;\n    }\n\n    if (n==1) {\n        bins = {0, 1, 0, 0};\n        return;\n    }\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<double> x_local(x.begin(), x.begin()+n/size);\n    std::vector<double> x_global(n);\n\n    std::vector<double> x_part(n/size);\n\n    std::vector<size_t> bin_local(4);\n    std::vector<size_t> bin_global(4);\n\n    std::vector<size_t> displs(size);\n    displs[0] = 0;\n    for (int i=1; i<size; ++i) {\n        displs[i] = displs[i-1]+n/size;\n    }\n\n    MPI_Allgatherv(x_local.data(), x_local.size(), MPI_DOUBLE, \n        x_global.data(), displs.data(), displs.data()+1, MPI_DOUBLE, MPI_COMM_WORLD);\n\n    for (size_t i=0; i<4; ++i) {\n        bin_local[i] = 0;\n    }\n    for (size_t i=0; i<x_local.size(); ++i) {\n        if (x_local[i] >= 0.0 && x_local[i] < 0.25) {\n            bin_local[0]++;\n        }\n        if (x_local[i] >= 0.25 && x_local[i] < 0.5) {\n            bin_local[1]++;\n        }\n        if (x_local[i] >= 0.5 && x_local[i] < 0.75) {\n            bin_local[2]++;\n        }\n        if (x_local[i] >= 0.75 && x_local[i] <= 1.0) {\n            bin_local[3]++;\n        }\n    }\n    MPI_Reduce(MPI_IN_PLACE, bin_local.data(), bin_local.size(), MPI_UNSIGNED_LONG_LONG,\n               MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = {bin_local[0], bin_local[1], bin_local[2], bin_local[3]};\n    }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n\tint rank;\n\tint nProcs;\n\tMPI_Comm_size(comm, &nProcs);\n\tMPI_Comm_rank(comm, &rank);\n\tint count = x.size();\n\tint rem = count % nProcs;\n\tint n = count / nProcs + (rank < rem? 1 : 0);\n\tint start = rank * n + std::min(rank, rem);\n\tint end = start + n - 1;\n\tstd::vector<double> x1(x.begin() + start, x.begin() + end + 1);\n\tbins.fill(0);\n\tint rank1 = rank;\n\tint i = 0;\n\tfor (auto it = x1.begin(); it!= x1.end(); ++it) {\n\t\tif (*it <= 0.25) {\n\t\t\tbins[0]++;\n\t\t}\n\t\telse if (*it <= 0.5) {\n\t\t\tbins[1]++;\n\t\t}\n\t\telse if (*it <= 0.75) {\n\t\t\tbins[2]++;\n\t\t}\n\t\telse {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n\n\tif (nProcs > 1) {\n\t\tstd::vector<std::array<size_t, 4>> bins1(nProcs);\n\t\tMPI_Allgather(&bins, 4, MPI_UNSIGNED_LONG_LONG, &bins1[0], 4, MPI_UNSIGNED_LONG_LONG, comm);\n\n\t\tstd::array<size_t, 4> bins2{};\n\t\tfor (int i = 0; i < nProcs; i++) {\n\t\t\tfor (int j = 0; j < 4; j++) {\n\t\t\t\tbins2[j] += bins1[i][j];\n\t\t\t}\n\t\t}\n\n\t\tMPI_Bcast(&bins2[0], 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\t\tif (rank == 0) {\n\t\t\tbins = bins2;\n\t\t}\n\t}\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint binsize = x.size()/size;\n\tint remain = x.size() % size;\n\tsize_t bins_local[4];\n\tmemset(bins_local, 0, sizeof(size_t)*4);\n\tdouble xmin = x[0], xmax = x[0];\n\tfor (auto i = 1; i < x.size(); ++i) {\n\t\txmin = std::min(xmin, x[i]);\n\t\txmax = std::max(xmax, x[i]);\n\t}\n\n\t// binsize for each process\n\tint binsize_proc = 0;\n\tif (rank == size - 1) {\n\t\tbinsize_proc = binsize + remain;\n\t} else {\n\t\tbinsize_proc = binsize;\n\t}\n\n\tdouble dx = (xmax - xmin) / 4;\n\n\tfor (int i = 0; i < binsize_proc; ++i) {\n\t\tif (x[i] < xmin + dx) {\n\t\t\tbins_local[0]++;\n\t\t} else if (x[i] < xmin + 2 * dx) {\n\t\t\tbins_local[1]++;\n\t\t} else if (x[i] < xmin + 3 * dx) {\n\t\t\tbins_local[2]++;\n\t\t} else {\n\t\t\tbins_local[3]++;\n\t\t}\n\t}\n\n\tstd::array<size_t, 4> bins_global;\n\tMPI_Reduce(bins_local, bins_global.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\tbins = bins_global;\n}",
            "size_t const n = x.size();\n    std::sort(x.begin(), x.end());\n    size_t low = n/4;\n    size_t high = 3*low;\n    size_t high_plus_one = high + 1;\n    bins[0] = std::count_if(x.begin(), x.begin()+low, [](double d) { return d-std::floor(d) < 0.25; });\n    bins[1] = std::count_if(x.begin()+low, x.begin()+high, [](double d) { return d-std::floor(d) >= 0.25 && d-std::floor(d) < 0.5; });\n    bins[2] = std::count_if(x.begin()+high, x.begin()+high_plus_one, [](double d) { return d-std::floor(d) >= 0.5 && d-std::floor(d) < 0.75; });\n    bins[3] = std::count_if(x.begin()+high_plus_one, x.end(), [](double d) { return d-std::floor(d) >= 0.75; });\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        int size;\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n        int buffer = 0;\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&buffer, 1, MPI_INT, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            bins[0] += buffer;\n            MPI_Send(&bins[0], 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n        }\n    }\n    else {\n        MPI_Send(&bins[0], 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n        MPI_Recv(&bins[0], 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n}",
            "int mpi_size;\n    int mpi_rank;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    size_t num_doubles = x.size();\n    size_t my_num_doubles = num_doubles / mpi_size;\n\n    size_t start_index = my_num_doubles * mpi_rank;\n    size_t end_index = my_num_doubles * (mpi_rank + 1);\n\n    if(mpi_rank == mpi_size - 1)\n        end_index = num_doubles;\n\n    int count_in_zero_to_025 = 0;\n    int count_in_025_to_05 = 0;\n    int count_in_05_to_075 = 0;\n    int count_in_075_to_1 = 0;\n\n    for(size_t i = start_index; i < end_index; i++) {\n        double val = x[i];\n        double fraction = val - floor(val);\n        if(fraction <= 0.25) {\n            count_in_zero_to_025++;\n        } else if(fraction <= 0.5) {\n            count_in_025_to_05++;\n        } else if(fraction <= 0.75) {\n            count_in_05_to_075++;\n        } else {\n            count_in_075_to_1++;\n        }\n    }\n\n    if(mpi_rank == 0) {\n        bins[0] = count_in_zero_to_025;\n        bins[1] = count_in_025_to_05;\n        bins[2] = count_in_05_to_075;\n        bins[3] = count_in_075_to_1;\n    }\n}",
            "// TODO: Your code here.\n  int N = x.size();\n  int mpi_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n  int mpi_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n  int num_bins = 4;\n  int N_per_rank = N/mpi_size;\n  int N_to_fill = N_per_rank-1;\n  int N_remainder = N%mpi_size;\n  int mpi_rank_index = mpi_rank*N_per_rank;\n  int mpi_rank_index_end = mpi_rank_index + N_to_fill;\n  if (mpi_rank == mpi_size-1)\n    mpi_rank_index_end = N_to_fill + mpi_rank_index;\n  if (mpi_rank == 0)\n    std::fill(bins.begin(), bins.end(), 0);\n  if (N_to_fill == 0)\n    std::fill(bins.begin(), bins.end(), 0);\n\n  for (int i=mpi_rank_index; i<mpi_rank_index_end; i++) {\n    if (x[i] >= 0 && x[i] < 0.25) {\n      bins[0]++;\n    }\n    else if (x[i] >= 0.25 && x[i] < 0.5) {\n      bins[1]++;\n    }\n    else if (x[i] >= 0.5 && x[i] < 0.75) {\n      bins[2]++;\n    }\n    else {\n      bins[3]++;\n    }\n  }\n\n  // Add up the bins to fill the remainder:\n  if (mpi_rank!= 0 && N_to_fill < N_remainder) {\n    int bins_to_send[num_bins] = {0};\n    MPI_Status status;\n    for (int i = 0; i < num_bins; i++)\n      bins_to_send[i] = bins[i];\n    MPI_Reduce(bins_to_send, bins.data(), num_bins, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  if (mpi_rank == 0) {\n    for (int i = 0; i < num_bins; i++) {\n      std::cout << bins[i] << \" \";\n    }\n    std::cout << \"\\n\";\n  }\n}",
            "if (x.empty()) return;\n\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<double> x_sorted = x;\n    std::sort(x_sorted.begin(), x_sorted.end());\n\n    double q1 = x_sorted.size() * 0.25;\n    double q2 = x_sorted.size() * 0.5;\n    double q3 = x_sorted.size() * 0.75;\n\n    if (rank == 0) {\n        bins = {0, 0, 0, 0};\n    }\n\n    std::vector<double> bins_temp(4);\n\n    size_t q1_pos = std::distance(x_sorted.begin(), std::upper_bound(x_sorted.begin(), x_sorted.end(), q1));\n    size_t q2_pos = std::distance(x_sorted.begin(), std::upper_bound(x_sorted.begin(), x_sorted.end(), q2));\n    size_t q3_pos = std::distance(x_sorted.begin(), std::upper_bound(x_sorted.begin(), x_sorted.end(), q3));\n\n    size_t bins_size = x.size() / world_size;\n\n    std::vector<double> bins_send(bins_size);\n    if (rank == 0) {\n        bins_send[q1_pos] = 1;\n        bins_send[q2_pos] = 1;\n        bins_send[q3_pos] = 1;\n    }\n\n    std::vector<double> bins_recv(bins_size);\n    MPI_Allreduce(bins_send.data(), bins_recv.data(), bins_size, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins[0] = static_cast<size_t>(std::floor(bins_recv[0]));\n        bins[1] = static_cast<size_t>(std::floor(bins_recv[1]));\n        bins[2] = static_cast<size_t>(std::floor(bins_recv[2]));\n        bins[3] = x_sorted.size() - bins[0] - bins[1] - bins[2];\n    }\n\n}",
            "int size;\n  int rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  std::vector<double> localX(x.begin()+rank, x.begin()+rank+size);\n\n  size_t c0 = std::count_if(localX.begin(), localX.end(), [](double x){ return x <= 0.25;});\n  size_t c1 = std::count_if(localX.begin(), localX.end(), [](double x){ return x > 0.25 && x <= 0.5;});\n  size_t c2 = std::count_if(localX.begin(), localX.end(), [](double x){ return x > 0.5 && x <= 0.75;});\n  size_t c3 = std::count_if(localX.begin(), localX.end(), [](double x){ return x > 0.75 && x <= 1.0;});\n\n  std::array<size_t, 4> localBins = {c0, c1, c2, c3};\n  std::vector<size_t> globalBins;\n  MPI_Gather(&localBins, 4, MPI_UNSIGNED, &globalBins, 4, MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n\n  if(rank == 0) {\n    bins = {0,0,0,0};\n    for (size_t i = 0; i < globalBins.size(); i++) {\n      bins[i] = globalBins[i];\n    }\n  }\n}",
            "//TODO: replace this with a call to MPI_Allreduce.\n\tsize_t n = x.size();\n\tstd::vector<size_t> counts(4);\n\tfor (size_t i=0; i<4; i++)\n\t\tcounts[i] = 0;\n\n\tsize_t total = 0;\n\n\tfor (size_t i=0; i<n; i++) {\n\t\tdouble v = x[i];\n\t\tdouble k = (v - floor(v))*4;\n\t\tsize_t index = k;\n\t\tcounts[index]++;\n\t}\n\n\tfor (size_t i=0; i<4; i++) {\n\t\ttotal += counts[i];\n\t}\n\n\tif (0 == MPI::COMM_WORLD.Get_rank()) {\n\t\tfor (size_t i=0; i<4; i++) {\n\t\t\tbins[i] = counts[i];\n\t\t}\n\t}\n\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        int size;\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n        int quotient = x.size() / size;\n        int remainder = x.size() % size;\n\n        MPI_Request req[size];\n        MPI_Status status[size];\n\n        std::array<size_t, 4> recv_bins;\n\n        for (int i = 0; i < size; ++i) {\n            MPI_Irecv(&recv_bins, sizeof(recv_bins), MPI_BYTE, i, 0, MPI_COMM_WORLD, &req[i]);\n        }\n\n        size_t current_index = 0;\n\n        for (int i = 0; i < size; ++i) {\n\n            size_t start = current_index;\n            size_t end = (start + quotient + 1) - 1;\n\n            if (i < remainder) {\n                ++end;\n            }\n\n            double min = x[start];\n            double max = x[end];\n\n            if (rank!= i) {\n                MPI_Isend(&bins, sizeof(bins), MPI_BYTE, i, 0, MPI_COMM_WORLD, &req[rank]);\n                MPI_Wait(&req[i], &status[i]);\n            } else {\n                for (int j = start; j <= end; ++j) {\n                    if (x[j] >= min && x[j] <= max) {\n                        ++bins[0];\n                    } else if (x[j] > max && x[j] <= max + 0.25) {\n                        ++bins[1];\n                    } else if (x[j] > max + 0.25 && x[j] <= max + 0.5) {\n                        ++bins[2];\n                    } else if (x[j] > max + 0.5 && x[j] <= max + 0.75) {\n                        ++bins[3];\n                    }\n                }\n            }\n            current_index = end + 1;\n        }\n        for (int i = 0; i < size; ++i) {\n            MPI_Wait(&req[i], &status[i]);\n        }\n\n    } else {\n        int size;\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n        int quotient = x.size() / size;\n        int remainder = x.size() % size;\n\n        std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n\n        size_t current_index = quotient * rank;\n\n        for (int i = 0; i < quotient + remainder; ++i) {\n            if (x[current_index] >= 0 && x[current_index] < 0.25) {\n                ++local_bins[0];\n            } else if (x[current_index] >= 0.25 && x[current_index] < 0.5) {\n                ++local_bins[1];\n            } else if (x[current_index] >= 0.5 && x[current_index] < 0.75) {\n                ++local_bins[2];\n            } else if (x[current_index] >= 0.75 && x[current_index] <= 1) {\n                ++local_bins[3];\n            }\n            ++current_index;\n        }\n\n        MPI_Send(&local_bins, sizeof(local_bins), MPI_BYTE, 0, 0, MPI_COMM_WORLD);\n    }\n\n\n}",
            "bins = {};\n    size_t N = x.size();\n    std::vector<double> xCopy = x;\n    std::sort(xCopy.begin(), xCopy.end());\n    size_t bins_size = bins.size();\n\n    for (int i = 0; i < bins_size; i++) {\n        bins[i] = 0;\n    }\n\n    for (int i = 0; i < N; i++) {\n        if (xCopy[i] <= 0.25) {\n            bins[0] += 1;\n        } else if (xCopy[i] > 0.25 && xCopy[i] <= 0.5) {\n            bins[1] += 1;\n        } else if (xCopy[i] > 0.5 && xCopy[i] <= 0.75) {\n            bins[2] += 1;\n        } else if (xCopy[i] > 0.75 && xCopy[i] <= 1) {\n            bins[3] += 1;\n        }\n    }\n}",
            "// MPI_COMM_WORLD is the MPI communicator for all ranks\n    int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    int rank_num = x.size() / world_size;\n\n    // If the size of vector isn't divisible by number of ranks, add to the last rank\n    if (x.size() % world_size!= 0) {\n        rank_num += 1;\n    }\n\n    // Get the rank number of the first element of vector\n    int first_index = rank_num * world_rank;\n\n    // Check the index of last element of vector\n    if (rank_num * (world_rank + 1) > x.size()) {\n        rank_num = x.size() - rank_num * world_rank;\n    }\n\n    // Calculate the number of elements in the vector for each rank\n    std::vector<double> local_x;\n    local_x.resize(rank_num);\n    for (int i = 0; i < rank_num; i++) {\n        local_x[i] = x[i + first_index];\n    }\n\n    // Split the array into 4 parts\n    std::vector<double> first_quartile(rank_num / 4);\n    std::vector<double> second_quartile(rank_num / 4);\n    std::vector<double> third_quartile(rank_num / 4);\n    std::vector<double> fourth_quartile(rank_num / 4);\n\n    // Check if vector size is smaller than 4 and set the size of first 3 vectors to be equal to vector size\n    // If vector size is smaller than 4, then the last array element will be filled with the last element of vector\n    // and the other arrays will be filled with the last element of vector\n    if (rank_num < 4) {\n        first_quartile.resize(rank_num);\n        second_quartile.resize(rank_num);\n        third_quartile.resize(rank_num);\n        fourth_quartile.resize(rank_num);\n    }\n\n    // Fill the 4 vectors\n    for (int i = 0; i < rank_num; i++) {\n        if (local_x[i] < 0.25) {\n            first_quartile[i] = local_x[i];\n        }\n        else if (local_x[i] >= 0.25 && local_x[i] < 0.5) {\n            second_quartile[i] = local_x[i];\n        }\n        else if (local_x[i] >= 0.5 && local_x[i] < 0.75) {\n            third_quartile[i] = local_x[i];\n        }\n        else if (local_x[i] >= 0.75 && local_x[i] < 1) {\n            fourth_quartile[i] = local_x[i];\n        }\n    }\n\n    // Get the counts of numbers in each quartile\n    MPI_Allreduce(first_quartile.data(), bins.data(), 4, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n}",
            "auto MPI_Comm = MPI_COMM_WORLD;\n  int MPI_Comm_size = 0;\n  MPI_Comm_size = MPI_Comm_size(MPI_Comm, &MPI_Comm_size);\n\n  size_t xSize = x.size();\n  if (MPI_Comm_size == 1) {\n    bins[0] = xSize;\n    bins[1] = xSize;\n    bins[2] = xSize;\n    bins[3] = xSize;\n    return;\n  }\n\n  size_t binsSize = 4;\n  size_t binSize = xSize / MPI_Comm_size;\n\n  int rank = 0;\n  MPI_Comm_rank(MPI_Comm, &rank);\n\n  std::array<size_t, 4> localBins;\n  std::vector<double> localX;\n  if (rank == 0) {\n    localX.assign(x.begin(), x.begin() + binSize);\n    localBins[0] = localX.size();\n    localX.assign(x.begin() + binSize, x.begin() + 2 * binSize);\n    localBins[1] = localX.size();\n    localX.assign(x.begin() + 2 * binSize, x.begin() + 3 * binSize);\n    localBins[2] = localX.size();\n    localX.assign(x.begin() + 3 * binSize, x.begin() + 4 * binSize);\n    localBins[3] = localX.size();\n  } else {\n    localX.assign(x.begin() + rank * binSize, x.begin() + (rank + 1) * binSize);\n    localBins[0] = localX.size();\n    localX.assign(x.begin() + (rank + 1) * binSize, x.begin() + (rank + 2) * binSize);\n    localBins[1] = localX.size();\n    localX.assign(x.begin() + (rank + 2) * binSize, x.begin() + (rank + 3) * binSize);\n    localBins[2] = localX.size();\n    localX.assign(x.begin() + (rank + 3) * binSize, x.begin() + (rank + 4) * binSize);\n    localBins[3] = localX.size();\n  }\n\n  std::array<size_t, 4> tmpBins;\n  MPI_Allreduce(localBins.data(), tmpBins.data(), binsSize, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n  bins = tmpBins;\n}",
            "// TODO\n}",
            "// TODO\n}",
            "MPI_Datatype dt;\n    MPI_Type_contiguous(sizeof(double), MPI_CHAR, &dt);\n    MPI_Type_commit(&dt);\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    // count number of doubles per rank\n    size_t count = 0;\n    for (size_t i = world_rank; i < x.size(); i += world_size) {\n        if (x[i] - floor(x[i]) < 0.25) {\n            ++bins[0];\n        }\n        else if (x[i] - floor(x[i]) < 0.5) {\n            ++bins[1];\n        }\n        else if (x[i] - floor(x[i]) < 0.75) {\n            ++bins[2];\n        }\n        else {\n            ++bins[3];\n        }\n        ++count;\n    }\n    MPI_Allreduce(&count, &bins[0], 1, dt, MPI_SUM, MPI_COMM_WORLD);\n    MPI_Type_free(&dt);\n}",
            "int rank, numprocs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\n    // The problem is partitioned into numprocs pieces.\n    // The rank of the process is assigned to a piece.\n    // The range of elements assigned to a piece is numprocs * piece to (numprocs * piece) + piece\n    // numprocs * piece + (rank + 1) is the index of the first element assigned to this process\n    // The last element in a piece is numprocs * piece + (piece - 1)\n    // The last element in the vector is numprocs * piece + piece - 1\n\n    // Compute the starting and ending indices assigned to this process\n    int const piece = x.size() / numprocs;\n    int const start = numprocs * rank + (rank + 1);\n    int const end = std::min(start + piece, x.size());\n    assert(start >= 0 && start <= x.size());\n    assert(end >= 0 && end <= x.size());\n    assert(end >= start);\n\n    for (int i = 0; i < 4; ++i)\n        bins[i] = 0;\n\n    // Go through each element in the piece assigned to this process\n    // and determine to which bin the element should be added\n    for (int i = start; i < end; ++i) {\n        if (x[i] < 0.25) {\n            bins[0] += 1;\n        } else if (x[i] < 0.5) {\n            bins[1] += 1;\n        } else if (x[i] < 0.75) {\n            bins[2] += 1;\n        } else {\n            bins[3] += 1;\n        }\n    }\n\n    // Sum the number of elements in each bin across the entire vector\n    // from the ranks below\n    MPI_Reduce(&bins[0], &bins[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // get number of cores\n    int num_cores;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_cores);\n\n    // make a vector to store local copy of x\n    std::vector<double> x_local;\n    x_local.reserve(x.size());\n    x_local.assign(x.begin(), x.begin() + (x.size() / num_cores));\n\n    // make vector to store the result of each core\n    std::vector<int> bins_local(4);\n\n    // divide the vector x into 4 equal parts\n    std::vector<double> quartiles;\n    quartiles.reserve(4);\n\n    if (x_local.size() == 0) {\n        bins_local = {0, 0, 0, 0};\n    } else {\n        for (size_t i = 0; i < 4; i++) {\n            quartiles.push_back(x_local[x_local.size() / 4 * i]);\n        }\n\n        // make a vector to store the partial sum of elements of x in each core\n        std::vector<double> partial_sums;\n        partial_sums.reserve(x_local.size());\n        partial_sums.assign(x_local.begin(), x_local.end());\n\n        // compute the partial sum of x in each core\n        for (size_t i = 1; i < partial_sums.size(); i++) {\n            partial_sums[i] = partial_sums[i - 1] + partial_sums[i];\n        }\n\n        // compute the fractional parts of each element in x\n        std::vector<double> fractional_parts(x_local.size());\n        for (size_t i = 0; i < fractional_parts.size(); i++) {\n            fractional_parts[i] = (x_local[i] - quartiles[i % 4]) / quartiles[i % 4];\n        }\n\n        // compute the counts of elements in each core\n        for (size_t i = 0; i < fractional_parts.size(); i++) {\n            if (fractional_parts[i] < 0.25) {\n                bins_local[0] += 1;\n            } else if (fractional_parts[i] < 0.5) {\n                bins_local[1] += 1;\n            } else if (fractional_parts[i] < 0.75) {\n                bins_local[2] += 1;\n            } else if (fractional_parts[i] < 1) {\n                bins_local[3] += 1;\n            }\n        }\n    }\n\n    // make a vector to store the total sum of each core\n    std::vector<int> total_sums;\n    total_sums.reserve(num_cores);\n\n    // compute the total sum of each core\n    MPI_Reduce(bins_local.data(), total_sums.data(), bins_local.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // make a vector to store the final result\n    std::vector<int> bins_final;\n    bins_final.reserve(4);\n\n    // compute the final result\n    if (rank == 0) {\n        for (size_t i = 0; i < 4; i++) {\n            bins_final.push_back(total_sums[i]);\n        }\n    }\n\n    // print the result\n    if (rank == 0) {\n        for (size_t i = 0; i < 4; i++) {\n            std::cout << bins_final[i] << std::endl;\n        }\n    }\n}",
            "int n = x.size();\n\tint rank;\n\tint world_size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n\tstd::vector<double> tmp(x.begin(), x.begin() + n / world_size);\n\tbins = { 0, 0, 0, 0 };\n\n\tint s = 1;\n\tfor (int i = 0; i < 4; ++i) {\n\t\tif (rank >= s) {\n\t\t\tbins[i] = tmp.size();\n\t\t\tbreak;\n\t\t}\n\t\ts *= 2;\n\t}\n\n\tfor (auto it = tmp.begin(); it!= tmp.end(); ++it) {\n\t\tdouble t = *it;\n\t\tif (t < 0.25) {\n\t\t\t++bins[0];\n\t\t}\n\t\telse if (t < 0.5) {\n\t\t\t++bins[1];\n\t\t}\n\t\telse if (t < 0.75) {\n\t\t\t++bins[2];\n\t\t}\n\t\telse {\n\t\t\t++bins[3];\n\t\t}\n\t}\n\tMPI_Reduce(MPI_IN_PLACE, &bins[0], 4, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n\treturn;\n}",
            "}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  bins.fill(0);\n  if (rank == 0) {\n    for (size_t i = 0; i < x.size(); ++i) {\n      if (x[i] >= 0 && x[i] < 0.25) {\n        bins[0]++;\n      } else if (x[i] >= 0.25 && x[i] < 0.5) {\n        bins[1]++;\n      } else if (x[i] >= 0.5 && x[i] < 0.75) {\n        bins[2]++;\n      } else if (x[i] >= 0.75 && x[i] <= 1) {\n        bins[3]++;\n      }\n    }\n  } else {\n    std::vector<double> vec;\n    for (size_t i = 0; i < x.size(); ++i) {\n      if (x[i] >= 0 && x[i] < 0.25) {\n        vec.push_back(x[i]);\n      } else if (x[i] >= 0.25 && x[i] < 0.5) {\n        vec.push_back(x[i]);\n      } else if (x[i] >= 0.5 && x[i] < 0.75) {\n        vec.push_back(x[i]);\n      } else if (x[i] >= 0.75 && x[i] <= 1) {\n        vec.push_back(x[i]);\n      }\n    }\n    MPI_Reduce(&vec[0], &bins[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n}",
            "// TODO\n}",
            "// Your code here\n\n  int nprocs, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // determine local subrange of input vector for each rank\n  size_t n = x.size();\n  size_t local_size = n / nprocs;\n  size_t extra_size = n % nprocs;\n  size_t start = rank * local_size + std::min(extra_size, rank);\n  size_t end = start + local_size + (rank < extra_size? 1 : 0);\n\n  // count number of elements in each bin\n  size_t local_bins[4] = { 0 };\n  for (size_t i = start; i < end; i++) {\n    if (x[i] < 0.25) {\n      local_bins[0]++;\n    }\n    else if (x[i] < 0.5) {\n      local_bins[1]++;\n    }\n    else if (x[i] < 0.75) {\n      local_bins[2]++;\n    }\n    else {\n      local_bins[3]++;\n    }\n  }\n\n  // collect local bins on rank 0 and store in bins\n  if (rank == 0) {\n    for (int i = 0; i < nprocs; i++) {\n      int source = i;\n      MPI_Recv(bins.data(), 4, MPI_UNSIGNED_LONG_LONG, source, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  }\n  else {\n    MPI_Send(local_bins, 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n}",
            "size_t numDoubles = x.size();\n  if (numDoubles == 0) {\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    return;\n  }\n\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  if (rank == 0) {\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n  }\n  int count = 0;\n  for (size_t i = rank; i < numDoubles; i += size) {\n    if (x[i] < 0.25) {\n      count++;\n    } else if (x[i] < 0.5) {\n      count++;\n    } else if (x[i] < 0.75) {\n      count++;\n    } else {\n      count++;\n    }\n  }\n\n  int bin = rank;\n  int recvCount = 0;\n  MPI_Status status;\n  if (rank == 0) {\n    bin = 0;\n    recvCount = size - 1;\n  }\n\n  MPI_Reduce(&count, &bins[bin], recvCount, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, num_procs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n    int binSize = x.size()/num_procs;\n    int binStart = binSize*rank;\n    int binEnd = binSize*(rank+1)-1;\n\n    std::array<size_t, 4> binsLocal;\n    for (int i = 0; i < 4; ++i) {\n        binsLocal[i] = 0;\n    }\n\n    double lower = 0.0;\n    double upper = 0.25;\n    for (int i = binStart; i < binEnd; ++i) {\n        if (x[i] >= lower && x[i] < upper) {\n            binsLocal[0] += 1;\n        }\n    }\n    for (int i = 0; i < 4; ++i) {\n        if (i == 3) {\n            upper = 1.0;\n        } else {\n            lower = upper;\n            upper = lower + 0.25;\n        }\n        for (int j = binStart; j < binEnd; ++j) {\n            if (x[j] >= lower && x[j] < upper) {\n                binsLocal[i+1] += 1;\n            }\n        }\n    }\n    MPI_Reduce(&binsLocal, &bins, 4, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n  int rank = 0;\n  MPI_Comm_rank(comm, &rank);\n  int size = 0;\n  MPI_Comm_size(comm, &size);\n\n  // split the vector into n/p chunks\n  size_t n = x.size();\n  size_t p = size;\n  size_t chunk = n / p;\n  size_t mod = n % p;\n\n  // set up the receive buffer\n  std::vector<size_t> buf(chunk);\n  for (size_t i = 0; i < chunk; ++i) buf[i] = 0;\n\n  // split the work\n  int source = rank - 1;\n  int dest = rank + 1;\n  if (rank == 0) dest = size - 1;\n  if (rank == size - 1) source = 0;\n\n  // get the number of doubles in each bin\n  if (rank == 0) {\n    for (size_t i = 0; i < chunk + mod; ++i) {\n      size_t idx = i;\n      double value = x[idx];\n      size_t bin = (size_t) floor(value);\n      ++bins[bin];\n    }\n  }\n  if (rank > 0 && rank < size - 1) {\n    MPI_Send(&x[rank * chunk], chunk, MPI_UNSIGNED_LONG_LONG, source, 0, comm);\n    MPI_Recv(&buf, chunk, MPI_UNSIGNED_LONG_LONG, dest, 0, comm, MPI_STATUS_IGNORE);\n    for (size_t i = 0; i < chunk; ++i) {\n      size_t bin = (size_t) floor(x[rank * chunk + i]);\n      ++bins[bin];\n    }\n  }\n  if (rank == size - 1) {\n    MPI_Send(&x[rank * chunk], mod, MPI_UNSIGNED_LONG_LONG, source, 0, comm);\n    for (size_t i = 0; i < mod; ++i) {\n      size_t idx = mod + rank * chunk + i;\n      double value = x[idx];\n      size_t bin = (size_t) floor(value);\n      ++bins[bin];\n    }\n  }\n\n  // count the fractions\n  size_t i = 0;\n  for (size_t j = 1; j < chunk; ++j) {\n    size_t idx = i + rank * chunk + mod;\n    double value = x[idx];\n    size_t bin = (size_t) floor(value);\n    if (value < 0.25) {\n      ++bins[bin];\n    } else if (value < 0.5) {\n      ++bins[bin + 1];\n    } else if (value < 0.75) {\n      ++bins[bin + 2];\n    } else {\n      ++bins[bin + 3];\n    }\n    ++i;\n  }\n}",
            "bins = {};\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, n_procs;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &n_procs);\n    int chunk_size = x.size()/n_procs;\n    int left_over = x.size()%n_procs;\n\n    size_t lower = 0, upper = 0;\n    size_t lower_idx, upper_idx;\n    size_t count = 0;\n    size_t start = rank*chunk_size;\n    size_t end = start + chunk_size;\n\n    if(rank == n_procs - 1){\n        end += left_over;\n    }\n    for(int i = start; i < end; i++){\n        double val = x[i];\n        if(val < 0.25){\n            lower++;\n        }else if(val < 0.50){\n            lower_idx = 1;\n            upper++;\n        }else if(val < 0.75){\n            lower_idx = 2;\n            upper_idx = 1;\n            upper++;\n        }else{\n            lower_idx = 2;\n            upper_idx = 2;\n            upper++;\n        }\n    }\n\n    // get sum of the bins of each proc\n    std::vector<size_t> lower_bins(n_procs, 0);\n    std::vector<size_t> upper_bins(n_procs, 0);\n    MPI_Allgather(&lower, 1, MPI_UNSIGNED_LONG_LONG, lower_bins.data(), 1, MPI_UNSIGNED_LONG_LONG, comm);\n    MPI_Allgather(&upper, 1, MPI_UNSIGNED_LONG_LONG, upper_bins.data(), 1, MPI_UNSIGNED_LONG_LONG, comm);\n\n    // get sum of the bins of each proc\n    for(int i = 0; i < n_procs; i++){\n        count += lower_bins[i];\n    }\n\n    // get the rank that has the least sum\n    double min = DBL_MAX;\n    int min_rank = -1;\n    for(int i = 0; i < n_procs; i++){\n        double diff = min - upper_bins[i];\n        if(diff > 0){\n            min = upper_bins[i];\n            min_rank = i;\n        }\n    }\n\n    // send the bin count to the rank with the least sum\n    if(rank == min_rank){\n        MPI_Send(&count, 1, MPI_UNSIGNED_LONG_LONG, 0, 0, comm);\n    }else{\n        MPI_Recv(&count, 1, MPI_UNSIGNED_LONG_LONG, min_rank, 0, comm, MPI_STATUS_IGNORE);\n    }\n\n    // get the sum of counts from each rank\n    std::vector<size_t> bins_count(n_procs, 0);\n    MPI_Allgather(&count, 1, MPI_UNSIGNED_LONG_LONG, bins_count.data(), 1, MPI_UNSIGNED_LONG_LONG, comm);\n\n    // distribute the counts among the bins\n    std::vector<size_t> temp(4);\n    std::vector<size_t> bins_distrib(4, 0);\n    for(int i = 0; i < n_procs; i++){\n        bins_distrib[0] += (lower_idx == 0)? bins_count[i] : 0;\n        bins_distrib[1] += (lower_idx == 1)? bins_count[i] : 0;\n        bins_distrib[2] += (lower_idx == 2)? bins_count[i] : 0;\n        bins_distrib[3] += (lower_idx == 3)? bins_count[i] : 0;",
            "}",
            "size_t n = x.size();\n    size_t rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    size_t n_per_rank = n/world_size;\n    size_t remainder = n%world_size;\n    size_t start = rank*n_per_rank;\n    size_t end = start + n_per_rank;\n\n    if (rank < remainder) {\n        end += 1;\n    }\n\n    size_t counts[4] = {0, 0, 0, 0};\n    for (size_t i = start; i < end; i++) {\n        double temp = x[i];\n        size_t index = 0;\n        if (temp < 0.25) {\n            index = 0;\n        } else if (temp >= 0.25 && temp < 0.5) {\n            index = 1;\n        } else if (temp >= 0.5 && temp < 0.75) {\n            index = 2;\n        } else if (temp >= 0.75 && temp <= 1) {\n            index = 3;\n        }\n        counts[index]++;\n    }\n\n    size_t sums[4] = {0, 0, 0, 0};\n    for (int i = 0; i < world_size; i++) {\n        size_t index = i*4;\n        sums[0] += counts[index];\n        sums[1] += counts[index+1];\n        sums[2] += counts[index+2];\n        sums[3] += counts[index+3];\n    }\n\n    if (rank == 0) {\n        bins[0] = sums[0];\n        bins[1] = sums[1];\n        bins[2] = sums[2];\n        bins[3] = sums[3];\n    }\n}",
            "// TODO: Your code here\n\n}",
            "/*\n    Count the numbers of doubles in the vector x that have a fractional part \n    in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n    The number of doubles in the vector is known to be larger than 0.\n    Use MPI to compute in parallel.\n    Every rank has a complete copy of x.\n    The result is stored in bins on rank 0.\n  */\n  // Your code here\n  int rank;\n  int nproc;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  size_t count;\n  size_t start = 0;\n  size_t end = 0;\n  std::vector<double> local_vec = x;\n\n  for (int i = 0; i < nproc; ++i) {\n    if (i!= rank) {\n      start += local_vec.size() / nproc;\n    } else {\n      break;\n    }\n  }\n  for (int i = 0; i < nproc; ++i) {\n    if (i!= rank) {\n      end += local_vec.size() / nproc;\n    } else {\n      break;\n    }\n  }\n\n  count = 0;\n  for (int i = start; i < end; ++i) {\n    if (local_vec[i] - floor(local_vec[i]) < 0.25) {\n      ++count;\n    } else if (local_vec[i] - floor(local_vec[i]) >= 0.25 &&\n               local_vec[i] - floor(local_vec[i]) < 0.5) {\n      ++count;\n    } else if (local_vec[i] - floor(local_vec[i]) >= 0.5 &&\n               local_vec[i] - floor(local_vec[i]) < 0.75) {\n      ++count;\n    } else if (local_vec[i] - floor(local_vec[i]) >= 0.75 &&\n               local_vec[i] - floor(local_vec[i]) <= 1.0) {\n      ++count;\n    }\n  }\n  if (rank == 0) {\n    for (int i = 0; i < 4; ++i) {\n      bins[i] = count;\n    }\n  }\n  return;\n}",
            "int rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tsize_t N = x.size();\n\tbins.fill(0);\n\tif (N == 0) {\n\t\treturn;\n\t}\n\tint rankNum = rank + 1;\n\tint nRanks = 1 + MPI_Comm_size(MPI_COMM_WORLD, &rank);\n\n\tif (rankNum == nRanks) {\n\t\tstd::vector<double> x0 = x;\n\t\tMPI_Status status;\n\t\tfor (int i = 1; i < nRanks; i++) {\n\t\t\tstd::vector<double> x;\n\t\t\tMPI_Recv(&x, 1, MPI_UNSIGNED, i, i, MPI_COMM_WORLD, &status);\n\t\t\tx0.insert(x0.end(), x.begin(), x.end());\n\t\t}\n\t\tstd::sort(x0.begin(), x0.end());\n\t\tfor (size_t i = 0; i < x0.size(); i++) {\n\t\t\tif (x0[i] < 0.25) {\n\t\t\t\tbins[0]++;\n\t\t\t} else if (x0[i] < 0.5) {\n\t\t\t\tbins[1]++;\n\t\t\t} else if (x0[i] < 0.75) {\n\t\t\t\tbins[2]++;\n\t\t\t} else if (x0[i] < 1) {\n\t\t\t\tbins[3]++;\n\t\t\t}\n\t\t}\n\t\treturn;\n\t} else if (rankNum > 1) {\n\t\tMPI_Send(&x, 1, MPI_UNSIGNED, 0, rankNum, MPI_COMM_WORLD);\n\t\treturn;\n\t} else if (rankNum == 1) {\n\t\tstd::vector<double> x0 = x;\n\t\tMPI_Status status;\n\t\tfor (int i = 2; i < nRanks; i++) {\n\t\t\tstd::vector<double> x;\n\t\t\tMPI_Recv(&x, 1, MPI_UNSIGNED, i, i, MPI_COMM_WORLD, &status);\n\t\t\tx0.insert(x0.end(), x.begin(), x.end());\n\t\t}\n\t\tstd::sort(x0.begin(), x0.end());\n\t\tfor (size_t i = 0; i < x0.size(); i++) {\n\t\t\tif (x0[i] < 0.25) {\n\t\t\t\tbins[0]++;\n\t\t\t} else if (x0[i] < 0.5) {\n\t\t\t\tbins[1]++;\n\t\t\t} else if (x0[i] < 0.75) {\n\t\t\t\tbins[2]++;\n\t\t\t} else if (x0[i] < 1) {\n\t\t\t\tbins[3]++;\n\t\t\t}\n\t\t}\n\t\treturn;\n\t}\n}",
            "if (x.size() == 0) {\n        bins = {{0, 0, 0, 0}};\n        return;\n    }\n    size_t length = x.size();\n    size_t rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Find the bounds for my subrange.\n    double start, end;\n    if (rank == 0) {\n        start = x[0];\n    }\n    MPI_Bcast(&start, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank < size - 1) {\n        end = x[length / (size - 1) * (rank + 1)];\n    }\n    else {\n        end = x[length - 1];\n    }\n    MPI_Bcast(&end, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Find the ranks for the other subranges.\n    std::array<size_t, 4> rank_starts;\n    if (rank == 0) {\n        rank_starts = {{0, 0, 0, 0}};\n    }\n    MPI_Allgather(&rank, 1, MPI_INT, rank_starts.data(), 1, MPI_INT, MPI_COMM_WORLD);\n\n    // Find the bounds of the subranges.\n    std::array<double, 4> subrange_starts, subrange_ends;\n    subrange_starts[0] = start;\n    subrange_ends[3] = end;\n    for (int i = 0; i < 3; i++) {\n        subrange_starts[i + 1] = x[rank_starts[i] * (length / (size - 1))];\n        subrange_ends[i] = x[rank_starts[i + 1] * (length / (size - 1)) - 1];\n    }\n\n    // Count the number of elements in each subrange.\n    std::array<size_t, 4> counts;\n    for (size_t i = 0; i < length; i++) {\n        if (subrange_starts[0] <= x[i] && x[i] < subrange_ends[0]) {\n            counts[0]++;\n        }\n        else if (subrange_starts[1] <= x[i] && x[i] < subrange_ends[1]) {\n            counts[1]++;\n        }\n        else if (subrange_starts[2] <= x[i] && x[i] < subrange_ends[2]) {\n            counts[2]++;\n        }\n        else if (subrange_starts[3] <= x[i] && x[i] < subrange_ends[3]) {\n            counts[3]++;\n        }\n    }\n\n    // Sum the counts for each subrange.\n    std::array<size_t, 4> counts_all;\n    if (rank == 0) {\n        counts_all = {{0, 0, 0, 0}};\n    }\n    MPI_Allgather(counts.data(), 4, MPI_INT, counts_all.data(), 4, MPI_INT, MPI_COMM_WORLD);\n\n    // Output the results.\n    if (rank == 0) {\n        bins = {{0, 0, 0, 0}};\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n    MPI_Reduce(counts_all.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (x.empty()) return;\n\n    size_t block_size = x.size()/size;\n    size_t first_index = rank*block_size;\n\n    size_t left = 0, middle = 0, right = 0, extreme = 0;\n\n    for (int i = first_index; i < first_index + block_size; ++i)\n    {\n        if (x[i] >= 0 && x[i] < 0.25) ++left;\n        if (x[i] >= 0.25 && x[i] < 0.5) ++middle;\n        if (x[i] >= 0.5 && x[i] < 0.75) ++right;\n        if (x[i] >= 0.75 && x[i] <= 1) ++extreme;\n    }\n\n    std::array<size_t, 4> bins_partial = {left, middle, right, extreme};\n\n    MPI_Reduce(MPI_IN_PLACE, bins_partial.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0)\n    {\n        bins = bins_partial;\n    }\n\n}",
            "// TODO: Fill in this function.\n\n    // TODO: use MPI_Bcast to broadcast x to all ranks.\n    //       use MPI_Reduce to compute the bins\n\n    // TODO: use MPI_Gatherv to collect the bins from all ranks to rank 0.\n    //       Use bins.data() as the receive buffer.\n    //       For example, on rank 0, the receive buffer should have 4*nranks doubles.\n    //       For example, on rank 1, the receive buffer should have 4*rank doubles.\n    //       On rank 0, call bins.data() as the receive buffer.\n    //       On rank 1, call bins.data() + 4*rank as the receive buffer.\n    //       On rank 2, call bins.data() + 8*rank as the receive buffer.\n    //      ...\n    //       On rank nranks-1, call bins.data() + 4*(nranks-1) as the receive buffer.\n    //       Do not forget to set bins.data() as the receive buffer on all ranks.\n\n    // TODO: use MPI_Bcast to broadcast bins to all ranks.\n}",
            "// YOUR CODE GOES HERE\n  int mpi_size;\n  int mpi_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n  //get the size of vector x\n  int size = x.size();\n\n  //send the size of vector x to all other ranks\n  MPI_Bcast(&size, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  int mpi_size_half = mpi_size / 2;\n\n  //vector that will hold the elements of x that are in each quartile\n  std::vector<double> x_quartile(mpi_size_half);\n\n  //vector that will hold the count of elements that are in each quartile\n  std::vector<double> count(mpi_size_half);\n\n  //rank 0 will gather the elements of x that are in each quartile\n  if (mpi_rank == 0) {\n    for (int i = 0; i < mpi_size_half; i++) {\n      MPI_Recv(&x_quartile[i], 1, MPI_DOUBLE, i, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      MPI_Recv(&count[i], 1, MPI_INT, i, mpi_size_half + i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  }\n  else {\n    //rank > 0 will send the elements of x that are in each quartile\n    //and the count of elements in each quartile to rank 0\n    double x_quartile_tmp = 0;\n    for (int i = 0; i < size; i++) {\n      if (x[i] < 0.25) {\n        x_quartile_tmp = x[i];\n        break;\n      }\n    }\n    MPI_Send(&x_quartile_tmp, 1, MPI_DOUBLE, 0, mpi_rank, MPI_COMM_WORLD);\n    MPI_Send(&size, 1, MPI_INT, 0, mpi_size_half + mpi_rank, MPI_COMM_WORLD);\n  }\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  //rank 0 will print the count of elements that are in each quartile\n  if (mpi_rank == 0) {\n    std::cout << \"Rank 0: The count of elements that are in each quartile: \";\n    for (int i = 0; i < mpi_size_half; i++) {\n      std::cout << count[i] << \" \";\n    }\n    std::cout << std::endl;\n  }\n\n  //every rank will print the elements of x that are in each quartile\n  if (mpi_rank < mpi_size_half) {\n    std::cout << \"Rank \" << mpi_rank << \": The elements of x that are in each quartile: \";\n    for (int i = 0; i < mpi_size_half; i++) {\n      std::cout << x_quartile[i] << \" \";\n    }\n    std::cout << std::endl;\n  }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Compute local counts\n    std::array<size_t, 4> local_bins = {0};\n    for (auto xi : x) {\n        if (xi >= 0 && xi < 0.25) local_bins[0]++;\n        else if (xi >= 0.25 && xi < 0.5) local_bins[1]++;\n        else if (xi >= 0.5 && xi < 0.75) local_bins[2]++;\n        else if (xi >= 0.75 && xi < 1) local_bins[3]++;\n    }\n\n    // Sum counts\n    std::array<size_t, 4> global_bins;\n    MPI_Reduce(local_bins.data(), global_bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // Send bins to rank 0\n    if (rank == 0) {\n        for (int i = 0; i < size; i++) {\n            if (i == 0)\n                bins[0] = global_bins[0];\n            else if (i == size - 1)\n                bins[1] = global_bins[1];\n            else if (i == (size / 2))\n                bins[2] = global_bins[2];\n            else if (i == (size / 2) + 1)\n                bins[3] = global_bins[3];\n        }\n    }\n}",
            "// Get the number of MPI processes and rank of this process\n\tint mpiSize, mpiRank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &mpiSize);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &mpiRank);\n\t\n\t// Initialize the vector of counts to zero.\n\tbins.fill(0);\n\n\t// Compute the total number of elements in x.\n\tint n = x.size();\n\t// Number of elements assigned to each process.\n\tint nPerProcess = n/mpiSize;\n\t// Number of elements assigned to this process.\n\tint nThisProcess = nPerProcess + n%mpiSize;\n\t// Get a pointer to the first element assigned to this process.\n\tdouble const* first = x.data() + mpiRank*nPerProcess;\n\t// Get a pointer to the last element assigned to this process.\n\tdouble const* last = x.data() + (mpiRank + 1)*nPerProcess;\n\tif (mpiRank == mpiSize - 1) last = x.data() + n;\n\n\t// Compute the number of elements less than, equal to, and greater than \n\t// the quartiles of x, respectively.\n\tint i;\n\tfor (i = 0; first + i < last; i++) {\n\t\tif (first[i] < 0.25) bins[0]++;\n\t\telse if (first[i] < 0.5) bins[1]++;\n\t\telse if (first[i] < 0.75) bins[2]++;\n\t\telse bins[3]++;\n\t}\n\n\t// Aggregate the counts to rank 0.\n\tif (mpiRank == 0) {\n\t\t// Get the counts from all the other processes.\n\t\tstd::array<size_t, 4> counts;\n\t\tMPI_Gather(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, &counts[0], 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\t\t// Combine the counts.\n\t\tfor (int j = 0; j < 4; j++) bins[j] = counts[0] + counts[1] + counts[2] + counts[3];\n\t}\n\telse {\n\t\tMPI_Gather(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, NULL, 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\t}\n\n\treturn;\n}",
            "size_t n = x.size();\n  assert(n!= 0);\n  size_t n_per_rank = n / mpi::size;\n  assert(n_per_rank * mpi::size == n);\n\n  std::vector<double> x_copy(x);\n  std::vector<size_t> counts(4, 0);\n  std::vector<size_t> send_counts(mpi::size);\n\n  auto is_small = [](double x) {return x < 0.25;};\n  auto is_med = [](double x) {return x >= 0.25 && x < 0.5;};\n  auto is_large = [](double x) {return x >= 0.5 && x < 0.75;};\n  auto is_xlarge = [](double x) {return x >= 0.75;};\n\n  for(size_t i = 0; i < n_per_rank; i++) {\n    if(is_small(x_copy[i])) counts[0]++;\n    if(is_med(x_copy[i])) counts[1]++;\n    if(is_large(x_copy[i])) counts[2]++;\n    if(is_xlarge(x_copy[i])) counts[3]++;\n  }\n\n  MPI_Allgather(&counts[0], 4, MPI_UNSIGNED_LONG_LONG, &send_counts[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_COMM_WORLD);\n\n  bins[0] = send_counts[0];\n  bins[1] = send_counts[1] + send_counts[0];\n  bins[2] = send_counts[2] + send_counts[1] + send_counts[0];\n  bins[3] = send_counts[3] + send_counts[2] + send_counts[1] + send_counts[0];\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int n = x.size();\n    // std::cout << \"rank \" << rank << \" \" << n << std::endl;\n    int n_local = n/size;\n    int n_rest = n%size;\n    int begin_index = rank * n_local;\n    int end_index = begin_index + n_local;\n    if (rank < n_rest) {\n        end_index += 1;\n    }\n\n    std::array<size_t, 4> local_bins{0, 0, 0, 0};\n    for (int i=begin_index; i < end_index; ++i) {\n        double value = x[i];\n        if (value >= 0.0 && value < 0.25) {\n            ++local_bins[0];\n        }\n        else if (value >= 0.25 && value < 0.5) {\n            ++local_bins[1];\n        }\n        else if (value >= 0.5 && value < 0.75) {\n            ++local_bins[2];\n        }\n        else if (value >= 0.75 && value <= 1.0) {\n            ++local_bins[3];\n        }\n    }\n    if (rank == 0) {\n        bins = local_bins;\n    }\n    MPI_Reduce(MPI_IN_PLACE, local_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        bins = local_bins;\n    }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, size;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &size);\n\n    int len = x.size();\n    int chunk = len/size;\n    int rem = len%size;\n\n    std::vector<double> x_local(chunk + (rank < rem));\n    std::copy(x.begin() + rank*chunk, x.begin() + (rank+1)*chunk, x_local.begin());\n    if (rank < rem) {\n        x_local.back() = x[rank*chunk + rem];\n    }\n\n    // get lower bound for each bin\n    std::vector<double> bounds(4);\n    if (rank == 0) {\n        bounds[0] = std::floor(x.front()*4);\n        bounds[1] = std::floor(x.front()*4) + 1.0/4.0;\n        bounds[2] = std::floor(x.front()*4) + 2.0/4.0;\n        bounds[3] = std::floor(x.front()*4) + 3.0/4.0;\n    }\n    MPI_Gather(&bounds[0], 1, MPI_DOUBLE, &bins[0], 1, MPI_DOUBLE, 0, comm);\n\n    // count the number of entries in each bin\n    std::vector<int> counts(4);\n    for (int i = 0; i < x_local.size(); ++i) {\n        ++counts[static_cast<size_t>(std::floor(x_local[i]*4))];\n    }\n\n    // sum counts for each bin\n    std::vector<int> recvbuf(4);\n    MPI_Reduce(&counts[0], &recvbuf[0], 4, MPI_INT, MPI_SUM, 0, comm);\n\n    // get counts for each bin from rank 0\n    if (rank == 0) {\n        for (int i = 0; i < 4; ++i) {\n            bins[i] = recvbuf[i];\n        }\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if(x.size() == 0) {\n        for(int i = 0; i < 4; ++i) bins[i] = 0;\n        return;\n    }\n\n    double a = x[0];\n    size_t n = x.size();\n    size_t nPerRank = n/size;\n    size_t remain = n%size;\n    size_t begin = rank*nPerRank;\n    size_t end = begin + nPerRank + (rank < remain? 1 : 0);\n    size_t count = 0;\n\n    for(int i = begin; i < end; ++i) {\n        if(x[i] - a <= 0.25)\n            ++count;\n        else if(x[i] - a > 0.25 && x[i] - a < 0.5)\n            ++count;\n        else if(x[i] - a > 0.5 && x[i] - a < 0.75)\n            ++count;\n        else if(x[i] - a > 0.75)\n            ++count;\n    }\n    MPI_Reduce(&count, &bins[0], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "std::array<size_t, 4> local_bins;\n\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int nprocs;\n    MPI_Comm_size(comm, &nprocs);\n\n    int rank;\n    MPI_Comm_rank(comm, &rank);\n\n    int num_per_proc = x.size() / nprocs;\n    int rem = x.size() % nprocs;\n    int start = rank * num_per_proc + std::min(rank, rem);\n    int end = start + num_per_proc;\n    int len = end - start;\n\n    for (size_t i = 0; i < len; i++) {\n        if (x[i + start] >= 0 && x[i + start] < 0.25)\n            local_bins[0]++;\n        else if (x[i + start] >= 0.25 && x[i + start] < 0.5)\n            local_bins[1]++;\n        else if (x[i + start] >= 0.5 && x[i + start] < 0.75)\n            local_bins[2]++;\n        else if (x[i + start] >= 0.75 && x[i + start] <= 1)\n            local_bins[3]++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_LONG, MPI_SUM, 0, comm);\n}",
            "// TODO: Your code here\n\n}",
            "// TODO\n    // YOUR CODE HERE\n    \n    /////////////////////////////////////////////////////////////////////\n\n    int rank, num_procs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n    int block_size = x.size() / num_procs;\n\n    std::vector<double> my_x(block_size);\n    std::vector<size_t> my_bins(4);\n\n    int rest = x.size() - block_size * num_procs;\n\n    if (rank == 0) {\n        for (int i = 0; i < rest; i++) {\n            my_x.push_back(x.at(i));\n        }\n    }\n\n    MPI_Scatter(x.data() + rest * rank, block_size, MPI_DOUBLE, my_x.data(), block_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < my_x.size(); i++) {\n        if (my_x.at(i) >= 0.0 && my_x.at(i) < 0.25) {\n            my_bins.at(0)++;\n        } else if (my_x.at(i) >= 0.25 && my_x.at(i) < 0.5) {\n            my_bins.at(1)++;\n        } else if (my_x.at(i) >= 0.5 && my_x.at(i) < 0.75) {\n            my_bins.at(2)++;\n        } else if (my_x.at(i) >= 0.75 && my_x.at(i) <= 1.0) {\n            my_bins.at(3)++;\n        }\n    }\n\n    MPI_Reduce(my_bins.data(), bins.data(), 4, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < rest; i++) {\n            x.at(i) = my_x.at(i);\n        }\n    }\n}",
            "// Your code here\n    // std::vector<double> x1(x);\n    // std::sort(x.begin(), x.end());\n    // size_t len = x.size();\n    // if (len == 0) {\n    //     bins[0] = 0;\n    //     bins[1] = 0;\n    //     bins[2] = 0;\n    //     bins[3] = 0;\n    //     return;\n    // }\n    // else if (len == 1) {\n    //     if (x[0] == 1.0) {\n    //         bins[0] = 0;\n    //         bins[1] = 0;\n    //         bins[2] = 0;\n    //         bins[3] = 1;\n    //     }\n    //     else if (x[0] == 0.0) {\n    //         bins[0] = 1;\n    //         bins[1] = 0;\n    //         bins[2] = 0;\n    //         bins[3] = 0;\n    //     }\n    // }\n    // else if (len > 1) {\n    //     if (x[0] == 1.0) {\n    //         bins[0] = 0;\n    //         bins[1] = 0;\n    //         bins[2] = 0;\n    //         bins[3] = 1;\n    //     }\n    //     else if (x[0] == 0.0) {\n    //         bins[0] = 1;\n    //         bins[1] = 0;\n    //         bins[2] = 0;\n    //         bins[3] = 0;\n    //     }\n    //     else if (x[0] >= 0.0 && x[0] < 0.25) {\n    //         bins[0] = 1;\n    //         bins[1] = 0;\n    //         bins[2] = 0;\n    //         bins[3] = 0;\n    //     }\n    //     else if (x[0] >= 0.25 && x[0] < 0.5) {\n    //         bins[0] = 0;\n    //         bins[1] = 1;\n    //         bins[2] = 0;\n    //         bins[3] = 0;\n    //     }\n    //     else if (x[0] >= 0.5 && x[0] < 0.75) {\n    //         bins[0] = 0;\n    //         bins[1] = 0;\n    //         bins[2] = 1;\n    //         bins[3] = 0;\n    //     }\n    //     else if (x[0] >= 0.75 && x[0] < 1.0) {\n    //         bins[0] = 0;\n    //         bins[1] = 0;\n    //         bins[2] = 0;\n    //         bins[3] = 1;\n    //     }\n    //     else {\n    //         bins[0] = 0;\n    //         bins[1] = 0;\n    //         bins[2] = 0;\n    //         bins[3] = 0;\n    //     }\n    //     for (size_t i = 1; i < len; i++) {\n    //         if (x[i] >= 0.0 && x[i] < 0.25) {\n    //             bins[0]++;\n    //         }\n    //         else if (x[i] >= 0.25 && x[i] < 0.5) {\n    //             bins[1]++;\n    //         }\n    //         else if (x[i] >= 0.5 && x[i] < 0.75) {\n    //             bins[2]++;\n    //         }\n    //         else if (x[i] >= 0.75 && x",
            "const size_t n = x.size();\n    double* x_ptr = x.data();\n\n    // MPI_Allreduce expects a single double, so we convert the array\n    // to a single double and add the elements together.\n    double reduction_buffer = 0;\n    for (size_t i = 0; i < n; i++) {\n        reduction_buffer += x[i];\n    }\n\n    MPI_Allreduce(&reduction_buffer, &bins[0], 4, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    // MPI_Allreduce computes in place. To get the original values we must copy them back.\n    bins[0] /= n;\n    bins[1] = (bins[0] + bins[1]) / 2;\n    bins[2] = (bins[1] + bins[2]) / 2;\n    bins[3] = (bins[2] + bins[3]) / 2;\n}",
            "// your code here\n\n}",
            "// TODO: fill in the gaps\n  //\n  // Note: the following code does not work in MPI and is included for testing\n  // purposes only.\n  auto const n = x.size();\n  std::array<size_t, 4> result {0, 0, 0, 0};\n  for(size_t i=0; i<n; ++i) {\n    if(x[i] >= 0.0 && x[i] < 0.25) {\n      result[0] += 1;\n    } else if(x[i] >= 0.25 && x[i] < 0.5) {\n      result[1] += 1;\n    } else if(x[i] >= 0.5 && x[i] < 0.75) {\n      result[2] += 1;\n    } else if(x[i] >= 0.75 && x[i] <= 1.0) {\n      result[3] += 1;\n    }\n  }\n  return result;\n}",
            "// TODO: Your code goes here\n\n    //int rank = 0;\n    //int nproc = 1;\n    //MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    //MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    int rank, nproc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    std::vector<double> x_p;\n    x_p = x;\n    if (rank == 0) {\n        //std::cout << \"rank = \" << rank << std::endl;\n        //std::cout << \"nproc = \" << nproc << std::endl;\n        //std::cout << \"x = \" << x << std::endl;\n        std::cout << \"x_p = \" << x_p << std::endl;\n    }\n    int n;\n    MPI_Status status;\n    if (rank == 0) {\n        n = x_p.size();\n        bins.fill(0);\n        //std::cout << \"n = \" << n << std::endl;\n    }\n    MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    int x_size = n / nproc;\n    if (rank == 0) {\n        std::cout << \"x_size = \" << x_size << std::endl;\n    }\n    std::vector<double> x_p_send(x_size);\n    std::vector<double> x_p_recv(x_size);\n    if (rank == 0) {\n        for (int i = 0; i < x_size; i++) {\n            x_p_send[i] = x_p[i];\n            //std::cout << \"x_p_send[\" << i << \"] = \" << x_p_send[i] << std::endl;\n        }\n        //std::cout << \"x_p_send = \" << x_p_send << std::endl;\n    }\n\n    MPI_Scatter(&x_p_send[0], x_size, MPI_DOUBLE, &x_p_recv[0], x_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    //MPI_Scatter(x_p_send, x_size, MPI_DOUBLE, x_p_recv, x_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    //std::cout << \"x_p_recv = \" << x_p_recv << std::endl;\n\n    if (rank!= 0) {\n        std::cout << \"x_p_recv = \" << x_p_recv << std::endl;\n    }\n\n    for (int i = 0; i < x_size; i++) {\n        if (x_p_recv[i] >= 0 && x_p_recv[i] < 0.25) {\n            bins[0] += 1;\n        }\n        else if (x_p_recv[i] >= 0.25 && x_p_recv[i] < 0.5) {\n            bins[1] += 1;\n        }\n        else if (x_p_recv[i] >= 0.5 && x_p_recv[i] < 0.75) {\n            bins[2] += 1;\n        }\n        else if (x_p_recv[i] >= 0.75 && x_p_recv[i] < 1.0) {\n            bins[3] += 1;\n        }\n        else {\n            std::cout << \"error\" << std::endl;\n            exit(0);\n        }\n    }\n\n    std::vector<size_t> bins_p;\n    if (rank == 0) {\n        bins_p = bins;\n    }\n\n    MPI_Gather(&bins[0], 4, MPI_",
            "// TODO: replace with MPI calls\n    size_t x_size = x.size();\n\n    size_t bin_size = x_size / 4;\n\n    std::array<size_t, 4> temp_bins{};\n\n    for (size_t i = 0; i < 4; ++i)\n        temp_bins[i] = 0;\n\n    for (size_t i = 0; i < x_size; ++i) {\n        if (x[i] >= 0 && x[i] <= 0.25)\n            ++temp_bins[0];\n        else if (x[i] > 0.25 && x[i] <= 0.5)\n            ++temp_bins[1];\n        else if (x[i] > 0.5 && x[i] <= 0.75)\n            ++temp_bins[2];\n        else if (x[i] > 0.75 && x[i] <= 1)\n            ++temp_bins[3];\n    }\n\n    for (size_t i = 0; i < 4; ++i)\n        bins[i] = 0;\n\n    for (size_t i = 0; i < 4; ++i) {\n        if (i < 3) {\n            for (size_t j = 0; j < bin_size; ++j)\n                bins[i] += temp_bins[i];\n        }\n        else\n            for (size_t j = 0; j < x_size % 4; ++j)\n                bins[3] += temp_bins[3];\n    }\n}",
            "// TODO\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] < 0.25) {\n            bins[0]++;\n        } else if (x[i] >= 0.25 && x[i] < 0.5) {\n            bins[1]++;\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            bins[2]++;\n        } else if (x[i] >= 0.75 && x[i] <= 1) {\n            bins[3]++;\n        }\n    }\n}",
            "size_t myRank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n    int size = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int dim = x.size();\n    double sum = 0.0;\n    double partial = 0.0;\n    int i = 0;\n\n    int n = 0;\n    MPI_Allreduce(&dim, &n, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    bins[0] = n/size;\n    bins[1] = n/size;\n    bins[2] = n/size;\n    bins[3] = n/size;\n    for(size_t j = 0; j < x.size(); j++) {\n        partial = std::floor(x[j]);\n        sum += partial;\n    }\n    for(size_t j = 0; j < x.size(); j++) {\n        if(sum >= 0.25 * n && sum < 0.5 * n) {\n            bins[0]--;\n        }\n        else if(sum >= 0.5 * n && sum < 0.75 * n) {\n            bins[1]--;\n        }\n        else if(sum >= 0.75 * n && sum < 1 * n) {\n            bins[2]--;\n        }\n        else if(sum >= 1 * n && sum <= 1 * n) {\n            bins[3]--;\n        }\n        else if(sum < 0.25 * n) {\n            bins[0]++;\n        }\n    }\n    int temp;\n    MPI_Allreduce(&bins[0], &temp, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    if(myRank == 0) {\n        bins[0] = temp;\n        MPI_Send(&temp, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);\n    }\n    else if(myRank == 1) {\n        MPI_Recv(&temp, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        bins[0] = temp;\n        MPI_Send(&temp, 1, MPI_INT, 2, 0, MPI_COMM_WORLD);\n    }\n    else if(myRank == 2) {\n        MPI_Recv(&temp, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        bins[0] = temp;\n        MPI_Send(&temp, 1, MPI_INT, 3, 0, MPI_COMM_WORLD);\n    }\n    else if(myRank == 3) {\n        MPI_Recv(&temp, 1, MPI_INT, 2, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        bins[0] = temp;\n        MPI_Recv(&temp, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        bins[0] += temp;\n    }\n    std::cout << \"Rank \" << myRank << \" has \" << bins[0] << \" \" << bins[1] << \" \" << bins[2] << \" \" << bins[3] << std::endl;\n}",
            "// TODO\n    size_t n = x.size();\n    size_t n_per_rank = n/MPI::COMM_WORLD.size();\n    int rank = MPI::COMM_WORLD.Get_rank();\n\n    // sort\n    std::vector<double> x_copy(x);\n    std::sort(x_copy.begin(), x_copy.end());\n\n    // compute\n    size_t bin_start = n_per_rank*rank;\n    size_t bin_end = std::min(n_per_rank*rank+n_per_rank, n);\n    size_t bin_idx = 0;\n    double bin_start_val = x_copy[bin_start];\n    for(size_t i=bin_start; i<bin_end; i++) {\n        if(x_copy[i] - bin_start_val > 0.25) {\n            bin_idx++;\n            bin_start_val = x_copy[i];\n        }\n    }\n\n    // store on rank 0\n    if(rank == 0) {\n        bins[0] = 0;\n        bins[1] = 0;\n        bins[2] = 0;\n        bins[3] = 0;\n        for(int i=1; i<MPI::COMM_WORLD.size(); i++) {\n            MPI::COMM_WORLD.Recv(&bins[0], 4, MPI::INT, i, i);\n        }\n        for(int i=0; i<n_per_rank; i++) {\n            if(x[i] - bin_start_val > 0.25) {\n                bins[bin_idx]++;\n                bin_idx++;\n                bin_start_val = x[i];\n            }\n        }\n    }\n    else {\n        MPI::COMM_WORLD.Send(&bins[0], 4, MPI::INT, 0, rank);\n    }\n}",
            "// You can assume that x is sorted in ascending order\n  assert(isSorted(x));\n  // You can assume that x has at least one element\n  assert(!x.empty());\n\n  // This is not a good design, but it is OK for this exercise\n  // TODO: Fix this design issue\n  size_t N = x.size();\n\n  size_t mpi_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\n  std::vector<double> local_x;\n  for (size_t i = mpi_rank(); i < N; i+=mpi_size) {\n    local_x.push_back(x[i]);\n  }\n\n  double local_min = *std::min_element(local_x.begin(), local_x.end());\n  double local_max = *std::max_element(local_x.begin(), local_x.end());\n\n  // Step 0: Split the interval [0, 1] into 4 equal intervals\n  double local_width = (local_max - local_min) / 4.0;\n\n  // Step 1: Assign each xi to the corresponding interval\n  std::array<size_t, 4> local_bins;\n  for (auto xi : local_x) {\n    size_t i = 0;\n    if (xi <= local_min + local_width) {\n      i = 0;\n    }\n    else if (xi <= local_min + 2 * local_width) {\n      i = 1;\n    }\n    else if (xi <= local_min + 3 * local_width) {\n      i = 2;\n    }\n    else {\n      i = 3;\n    }\n    local_bins[i]++;\n  }\n\n  // Step 2: Sum up the bins from all the ranks\n  size_t global_bins[4] = {0};\n\n  // Step 3: Use MPI to communicate bins across ranks\n  MPI_Allreduce(local_bins.data(), global_bins, 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  // Step 4: Output the result\n  bins[0] = global_bins[0];\n  bins[1] = global_bins[1];\n  bins[2] = global_bins[2];\n  bins[3] = global_bins[3];\n\n  return;\n}",
            "// TODO: implement\n}",
            "size_t length = x.size();\n    size_t n_partitions = 4;\n    size_t my_partition = MPI_UNDEFINED;\n    size_t i = 0;\n    size_t number = 0;\n    size_t size = 0;\n    double *x_part = nullptr;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_partition);\n\n    if (size == 1) {\n        for (double element : x) {\n            if (element >= 0.0 && element < 0.25) {\n                bins[i] += 1;\n            }\n            else if (element >= 0.25 && element < 0.5) {\n                bins[i+1] += 1;\n            }\n            else if (element >= 0.5 && element < 0.75) {\n                bins[i+2] += 1;\n            }\n            else if (element >= 0.75 && element <= 1.0) {\n                bins[i+3] += 1;\n            }\n        }\n    }\n    else {\n        if (my_partition < n_partitions) {\n            MPI_Scatter(&length, 1, MPI_UNSIGNED_LONG, &number, 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n            x_part = new double[number];\n            MPI_Scatter(&x[0], number, MPI_DOUBLE, &x_part[0], number, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        }\n        if (my_partition == 0) {\n            MPI_Gather(&bins[0], n_partitions, MPI_UNSIGNED_LONG, &bins[0], n_partitions, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n        }\n\n        for (int j = 0; j < my_partition; j++) {\n            bins[j] += bins[j+1];\n        }\n\n        if (my_partition < n_partitions) {\n            for (double element : x_part) {\n                if (element >= 0.0 && element < 0.25) {\n                    bins[my_partition] += 1;\n                }\n                else if (element >= 0.25 && element < 0.5) {\n                    bins[my_partition+1] += 1;\n                }\n                else if (element >= 0.5 && element < 0.75) {\n                    bins[my_partition+2] += 1;\n                }\n                else if (element >= 0.75 && element <= 1.0) {\n                    bins[my_partition+3] += 1;\n                }\n            }\n\n            if (my_partition == 0) {\n                delete[] x_part;\n                x_part = nullptr;\n            }\n        }\n    }\n}",
            "int rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tsize_t size = x.size();\n\tsize_t start = rank * size / MPI_COMM_WORLD_SIZE;\n\tsize_t end = start + size / MPI_COMM_WORLD_SIZE;\n\n\tbins[0] = 0;\n\tbins[1] = 0;\n\tbins[2] = 0;\n\tbins[3] = 0;\n\tfor (size_t i = start; i < end; i++) {\n\t\tdouble val = x[i];\n\t\tif (val < 0.25)\n\t\t\tbins[0]++;\n\t\telse if (val < 0.5)\n\t\t\tbins[1]++;\n\t\telse if (val < 0.75)\n\t\t\tbins[2]++;\n\t\telse\n\t\t\tbins[3]++;\n\t}\n\n\tMPI_Reduce(MPI_IN_PLACE, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\treturn;\n}",
            "// The fractional part of a double can be represented as the fraction between the double\n  // and its integer part. For example, the fractional part of 1.5 = 0.5\n  auto fracPart = [](double val) { return val - static_cast<int>(val); };\n\n  // The lower bound for the fractional part of a double. For example, the lower bound of 1.5 is 0.5\n  auto lowerBound = [](double val) { return val - fracPart(val); };\n\n  // The upper bound for the fractional part of a double. For example, the upper bound of 1.5 is 1.0\n  auto upperBound = [](double val) { return val + (1 - fracPart(val)); };\n\n  // Create a vector of the fractional parts\n  std::vector<double> fracs;\n  std::transform(x.begin(), x.end(), std::back_inserter(fracs), fracPart);\n\n  // Initialize the bins to 0\n  std::fill(bins.begin(), bins.end(), 0);\n\n  // Each rank receives a different chunk of the input vector\n  std::vector<double> chunk(fracs.size() / nprocs);\n  MPI_Scatter(fracs.data(), chunk.size(), MPI_DOUBLE, chunk.data(), chunk.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  // For each double in the chunk, we increment the corresponding bin.\n  // We also use the chunk bounds to determine if we are in a bin\n  for (size_t i = 0; i < chunk.size(); ++i) {\n    double frac = chunk[i];\n    if (frac < lowerBound(0.25)) {\n      ++bins[0];\n    }\n    else if (frac < lowerBound(0.5)) {\n      ++bins[1];\n    }\n    else if (frac < lowerBound(0.75)) {\n      ++bins[2];\n    }\n    else if (frac <= 1.0) {\n      ++bins[3];\n    }\n  }\n\n  // Since we are using MPI, we need to sum the bins on rank 0\n  if (rank == 0) {\n    for (int r = 1; r < nprocs; ++r) {\n      MPI_Recv(bins.data(), 4, MPI_LONG, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    std::partial_sum(bins.begin(), bins.end(), bins.begin());\n  } else {\n    MPI_Send(bins.data(), 4, MPI_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size_t size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    bins = {0,0,0,0};\n    // MPI_Bcast(bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n    // if(rank == 0){\n    //     std::cout << \"bins: \" << bins[0] << \" \" << bins[1] << \" \" << bins[2] << \" \" << bins[3] << std::endl;\n    // }\n    std::vector<double> input_x(x);\n    if (rank == 0) {\n        bins = {0, 0, 0, 0};\n        for (auto xi : input_x) {\n            // if (xi >= 0 && xi < 0.25) {\n            //     bins[0] += 1;\n            // } else if (xi >= 0.25 && xi < 0.5) {\n            //     bins[1] += 1;\n            // } else if (xi >= 0.5 && xi < 0.75) {\n            //     bins[2] += 1;\n            // } else if (xi >= 0.75 && xi < 1) {\n            //     bins[3] += 1;\n            // } else {\n            //     std::cout << xi << \" is not in [0, 1]\" << std::endl;\n            // }\n            if (xi >= 0 && xi < 0.25) {\n                bins[0] += 1;\n            } else if (xi >= 0.25 && xi < 0.5) {\n                bins[1] += 1;\n            } else if (xi >= 0.5 && xi < 0.75) {\n                bins[2] += 1;\n            } else if (xi >= 0.75 && xi < 1) {\n                bins[3] += 1;\n            } else {\n                std::cout << xi << \" is not in [0, 1]\" << std::endl;\n            }\n        }\n        std::cout << \"bins[0]: \" << bins[0] << std::endl;\n        std::cout << \"bins[1]: \" << bins[1] << std::endl;\n        std::cout << \"bins[2]: \" << bins[2] << std::endl;\n        std::cout << \"bins[3]: \" << bins[3] << std::endl;\n    }\n    MPI_Bcast(bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n    if(rank == 0){\n        std::cout << \"bins: \" << bins[0] << \" \" << bins[1] << \" \" << bins[2] << \" \" << bins[3] << std::endl;\n    }\n    std::vector<double> bins_part(4);\n    std::vector<int> disp(4);\n    // disp[0] = 0;\n    // disp[1] = disp[0] + bins[0];\n    // disp[2] = disp[1] + bins[1];\n    // disp[3] = disp[2] + bins[2];\n    disp[0] = 0;\n    disp[1] = bins[0];\n    disp[2] = disp[1] + bins[1];\n    disp[3] = disp[2] + bins[2];\n    // std::cout << \"rank = \" << rank << \" disp = \" << disp[0] << \" \" << disp[1] << \" \" << disp[2] << \" \" << disp[3] << std::endl;\n    for(int i = 0; i < size; ++i){\n        if(i == rank){\n            MPI_Scatterv(input_x.data(), bins.data(), disp.data(), MPI_DOUBLE, bins_part.data(), bins[0",
            "const size_t n = x.size();\n    bins.fill(0);\n\n    if (n == 0)\n        return;\n\n    std::vector<double> sorted(n);\n    std::copy(x.begin(), x.end(), sorted.begin());\n    std::sort(sorted.begin(), sorted.end());\n\n    const double quartile_start = sorted[n/4];\n    const double quartile_end = sorted[3*n/4];\n    const double midpoint = sorted[n/2];\n\n    for (size_t i = 0; i < n; i++) {\n        double const x_i = x[i];\n        if (x_i < quartile_start)\n            bins[0]++;\n        else if (x_i >= quartile_start && x_i < quartile_end)\n            bins[1]++;\n        else if (x_i >= quartile_end && x_i < midpoint)\n            bins[2]++;\n        else if (x_i >= midpoint)\n            bins[3]++;\n    }\n}",
            "// TODO: Your code goes here\n\n  // 1. Calculate the number of elements in the vector\n  int rank = 0, numProcesses = 1;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n\n  // 2. Split vector into equal parts\n  size_t elementsPerProcess = x.size() / numProcesses;\n  size_t remainder = x.size() % numProcesses;\n\n  // 3. Compute quartiles\n  std::vector<double> quartiles;\n  quartiles.push_back(0.0);\n  quartiles.push_back(0.25);\n  quartiles.push_back(0.5);\n  quartiles.push_back(0.75);\n  quartiles.push_back(1.0);\n\n  std::vector<size_t> binsTmp;\n  for (int i = 0; i < quartiles.size(); ++i) {\n    binsTmp.push_back(0);\n  }\n  std::vector<double> lowerBound, upperBound;\n\n  // 4. Calculate the range of the quartiles\n  if (rank == 0) {\n    for (int i = 0; i < quartiles.size(); ++i) {\n      lowerBound.push_back(quartiles[i] * elementsPerProcess);\n    }\n    for (int i = 0; i < quartiles.size(); ++i) {\n      upperBound.push_back(quartiles[i + 1] * elementsPerProcess);\n    }\n  }\n\n  // 5. Calculate the number of elements in each subvector\n  std::vector<int> elementsTmp;\n  MPI_Gather(&elementsPerProcess, 1, MPI_INT, elementsTmp.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i = 0; i < elementsTmp.size(); ++i) {\n      elementsTmp[i] += remainder;\n    }\n  }\n\n  // 6. Calculate the indices where each quartile begins\n  std::vector<int> lowerIdxTmp, upperIdxTmp;\n  MPI_Gather(&lowerBound, 1, MPI_DOUBLE, lowerIdxTmp.data(), 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  MPI_Gather(&upperBound, 1, MPI_DOUBLE, upperIdxTmp.data(), 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i = 0; i < elementsTmp.size(); ++i) {\n      lowerIdxTmp[i] = lowerIdxTmp[i] + elementsTmp[i];\n      upperIdxTmp[i] = upperIdxTmp[i] + elementsTmp[i];\n    }\n  }\n\n  // 7. Loop through each subvector and calculate the number of elements for each quartile\n  std::vector<double> elements;\n  std::vector<double> fractional;\n  std::vector<int> indices;\n  if (rank == 0) {\n    for (int i = 0; i < elementsTmp.size(); ++i) {\n      elements.push_back(elementsTmp[i]);\n    }\n  }\n\n  for (int i = 0; i < x.size(); ++i) {\n    fractional.push_back(x[i] - floor(x[i]));\n  }\n\n  for (int i = 0; i < elements.size(); ++i) {\n    indices.push_back(i);\n  }\n\n  for (int i = 0; i < elements.size(); ++i) {\n    for (int j = 0; j < quartiles.size(); ++j) {\n      if (lowerIdxTmp[j] <= i && upperIdxTmp[j] > i) {\n        binsTmp[j]++;\n      }",
            "int rank, nprocs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n  if(rank==0) {\n    bins = {0,0,0,0};\n    size_t nx = x.size();\n    size_t offset = nx/nprocs;\n    size_t remaining = nx % nprocs;\n    std::vector<size_t> displacements(nprocs);\n    std::iota(displacements.begin(), displacements.end(), 0);\n    displacements[rank] += remaining;\n    for(int i = 1; i < nprocs; ++i) {\n      displacements[i] += offset;\n    }\n    std::vector<int> sendcounts(nprocs);\n    std::fill(sendcounts.begin(), sendcounts.end(), offset);\n    if(remaining > 0) {\n      sendcounts[rank] += remaining;\n    }\n    // scatter x\n    std::vector<double> recvbuf(offset);\n    MPI_Scatterv(x.data(), sendcounts.data(), displacements.data(), MPI_DOUBLE, recvbuf.data(), offset, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    // count values in each range\n    for(auto x: recvbuf) {\n      if(x < 0.25) {\n        ++bins[0];\n      }\n      else if(x < 0.5) {\n        ++bins[1];\n      }\n      else if(x < 0.75) {\n        ++bins[2];\n      }\n      else if(x < 1.0) {\n        ++bins[3];\n      }\n    }\n  }\n  else {\n    std::vector<double> recvbuf(0);\n    MPI_Scatterv(x.data(), nullptr, nullptr, MPI_DOUBLE, recvbuf.data(), 0, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  }\n  MPI_Barrier(MPI_COMM_WORLD);\n}",
            "size_t N = x.size();\n\n  // Use a std::vector<size_t> to avoid having to deal with\n  // dynamic allocation and memory management.\n  std::vector<size_t> counts(4);\n\n  for (auto x_i : x) {\n    int b = 0;\n    if (x_i >= 0.75) b = 3;\n    if (x_i >= 0.5) b = 2;\n    if (x_i >= 0.25) b = 1;\n    counts[b]++;\n  }\n\n  MPI_Allreduce(counts.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // get minimum and maximum\n        auto it = std::minmax_element(x.begin(), x.end());\n        double min = *it.first, max = *it.second;\n\n        // compute the width of each bin\n        double binWidth = (max - min) / (double) size;\n\n        // initialize bins to 0\n        bins = {0, 0, 0, 0};\n\n        for (auto i : x) {\n            size_t iBin = (i - min) / binWidth;\n            bins[iBin]++;\n        }\n\n    }\n    MPI_Bcast(bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "int rank, nprocs;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    std::vector<double> local_x(n);\n    if(rank==0){\n        local_x = x;\n    }\n    std::vector<int> local_bins(4);\n    MPI_Scatter(local_x.data(), n/nprocs, MPI_DOUBLE, local_x.data(), n/nprocs, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    for(int i=0; i<n/nprocs; i++){\n        if(local_x[i] < 0.25){\n            local_bins[0]++;\n        }else if(local_x[i] < 0.5){\n            local_bins[1]++;\n        }else if(local_x[i] < 0.75){\n            local_bins[2]++;\n        }else if(local_x[i] < 1.0){\n            local_bins[3]++;\n        }\n    }\n    MPI_Gather(local_bins.data(), 4, MPI_INT, bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n    if(rank==0){\n        for(int i=0; i<4; i++){\n            std::cout << bins[i] << \" \";\n        }\n        std::cout << std::endl;\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // calculate the size of each chunk, then do the counting\n    size_t chunk_size = x.size() / size;\n    size_t last_chunk_size = x.size() % size;\n\n    std::array<size_t, 4> counts_from_rank;\n    if (rank == 0) {\n        counts_from_rank.fill(0);\n    }\n\n    // calculate the chunk indices\n    std::vector<size_t> chunk_indices;\n    chunk_indices.reserve(size);\n    for (size_t i = 0; i < size; ++i) {\n        chunk_indices.emplace_back(chunk_size * i);\n        if (i == size - 1 && last_chunk_size > 0) {\n            chunk_indices.back() += last_chunk_size;\n        }\n    }\n\n    // count the quartiles on each rank\n    for (size_t i = 0; i < chunk_indices.size(); ++i) {\n        std::array<size_t, 4> counts = {0, 0, 0, 0};\n\n        for (size_t j = chunk_indices[i]; j < chunk_indices[i] + chunk_size; ++j) {\n            double x_val = x[j];\n            if (x_val < 0.25) {\n                ++counts[0];\n            }\n            else if (x_val < 0.5) {\n                ++counts[1];\n            }\n            else if (x_val < 0.75) {\n                ++counts[2];\n            }\n            else {\n                ++counts[3];\n            }\n        }\n\n        MPI_Gather(&counts, 4, MPI_LONG_LONG_INT, &counts_from_rank, 4, MPI_LONG_LONG_INT, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        bins = counts_from_rank;\n    }\n}",
            "// Write your code here\n    int my_rank;\n    int comm_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n    if (my_rank == 0) {\n        size_t total = x.size();\n        bins = {0, 0, 0, 0};\n\n        std::vector<size_t> bins_receive(4);\n\n        for (int i = 0; i < comm_size; i++) {\n            if (i == 0)\n                MPI_Recv(bins_receive.data(), 4, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            else\n                MPI_Recv(bins_receive.data(), 4, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            bins[0] += bins_receive[0];\n            bins[1] += bins_receive[1];\n            bins[2] += bins_receive[2];\n            bins[3] += bins_receive[3];\n        }\n\n        size_t total_receive = total - bins[0] - bins[1] - bins[2] - bins[3];\n        bins[0] += total_receive;\n        bins[1] += total_receive;\n        bins[2] += total_receive;\n        bins[3] += total_receive;\n\n    } else {\n        int my_rank_offset = 0;\n        while (my_rank_offset < x.size()) {\n            size_t my_rank_count = 0;\n            while (my_rank_offset + my_rank_count < x.size() && x[my_rank_offset + my_rank_count] < 0.25) {\n                my_rank_count++;\n            }\n            my_rank_count -= my_rank_offset;\n            my_rank_offset += my_rank_count;\n            MPI_Send(&my_rank_count, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n        }\n\n        std::vector<size_t> bins_send(4);\n        bins_send[0] = 0;\n        bins_send[1] = 0;\n        bins_send[2] = 0;\n        bins_send[3] = 0;\n\n        MPI_Send(bins_send.data(), 4, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n}",
            "//...\n}",
            "auto count_quartiles = [](double val) {\n        double fractional_part = fmod(val, 1.0);\n        if (fractional_part <= 0.25) return 0;\n        if (fractional_part <= 0.5) return 1;\n        if (fractional_part <= 0.75) return 2;\n        return 3;\n    };\n\n    size_t num_procs;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    size_t rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size_t x_size = x.size();\n\n    // split work to each process\n    std::vector<std::vector<double>> x_split(num_procs);\n    size_t split_size = x_size / num_procs;\n    size_t rest = x_size % num_procs;\n    size_t begin = rank * split_size;\n    size_t end = begin + split_size;\n    if (rank < rest) {\n        end += 1;\n    }\n    for (size_t i = begin; i < end; ++i) {\n        x_split[rank].push_back(x[i]);\n    }\n\n    // do local count\n    std::array<size_t, 4> bins_local = { 0, 0, 0, 0 };\n    for (double val : x_split[rank]) {\n        ++bins_local[count_quartiles(val)];\n    }\n\n    // all reduce to get global count\n    MPI_Allreduce(MPI_IN_PLACE, &bins_local, 4, MPI_UNSIGNED, MPI_SUM, MPI_COMM_WORLD);\n\n    // set final result\n    bins[0] = bins_local[0];\n    bins[1] = bins_local[1];\n    bins[2] = bins_local[2];\n    bins[3] = bins_local[3];\n\n    if (rank == 0) {\n        std::cout << \"Rank 0: \" << std::endl;\n        for (size_t i = 0; i < bins.size(); ++i) {\n            std::cout << \"q\" << i << \": \" << bins[i] << std::endl;\n        }\n    }\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // split the vector into sub-vectors\n    int num_blocks = 4;\n    int block_size = x.size() / num_blocks;\n    int remainder = x.size() % num_blocks;\n\n    std::vector<std::vector<double>> sub_vectors(num_blocks);\n    for (int i = 0; i < num_blocks; i++) {\n        sub_vectors[i].reserve(block_size + (i < remainder? 1 : 0));\n    }\n    for (size_t i = 0; i < x.size(); i++) {\n        sub_vectors[i / block_size].push_back(x[i]);\n    }\n\n    // count the number of values in each sub-vector that fall into each\n    // fractional interval\n    std::array<int, 4> bins_local;\n    for (int i = 0; i < num_blocks; i++) {\n        int size = sub_vectors[i].size();\n        bins_local[0] = 0;\n        for (int j = 0; j < size; j++) {\n            if (sub_vectors[i][j] >= 0 && sub_vectors[i][j] < 0.25) {\n                bins_local[0]++;\n            } else if (sub_vectors[i][j] >= 0.25 && sub_vectors[i][j] < 0.5) {\n                bins_local[1]++;\n            } else if (sub_vectors[i][j] >= 0.5 && sub_vectors[i][j] < 0.75) {\n                bins_local[2]++;\n            } else {\n                bins_local[3]++;\n            }\n        }\n    }\n\n    // compute the sum of the local counts\n    int bins_local_sum;\n    bins_local_sum = bins_local[0] + bins_local[1] + bins_local[2] + bins_local[3];\n\n    // compute the sum of the counts across all MPI ranks\n    int bins_global_sum;\n    MPI_Allreduce(&bins_local_sum, &bins_global_sum, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    // compute the global fractions\n    for (int i = 0; i < 4; i++) {\n        bins[i] = static_cast<size_t>(bins_global_sum / 4.0 * bins_local[i]);\n    }\n}",
            "assert(x.size() > 0);\n  assert(x.size() == x.capacity());\n  assert(bins.size() == 4);\n  assert(bins[0] == 0);\n  assert(bins[1] == 0);\n  assert(bins[2] == 0);\n  assert(bins[3] == 0);\n\n  int rank = 0;\n  int nprocs = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n  if (nprocs > 1) {\n    size_t n = x.size();\n    std::vector<double> x_local(n);\n    if (rank == 0) {\n      std::copy(x.begin(), x.end(), x_local.begin());\n      std::vector<std::vector<double> > x_split;\n      splitVector(x_local, x_split, nprocs);\n      for (size_t i = 1; i < nprocs; i++) {\n        MPI_Send(x_split[i].data(), x_split[i].size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n      }\n    } else {\n      MPI_Status status;\n      std::vector<double> x_recv(n);\n      MPI_Recv(x_recv.data(), x_recv.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);\n      std::copy(x_recv.begin(), x_recv.end(), x_local.begin());\n    }\n\n    bins = countQuartiles(x_local);\n\n    if (rank == 0) {\n      MPI_Status status;\n      for (size_t i = 1; i < nprocs; i++) {\n        std::vector<size_t> bins_recv(4);\n        MPI_Recv(bins_recv.data(), 4, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, &status);\n        bins[0] += bins_recv[0];\n        bins[1] += bins_recv[1];\n        bins[2] += bins_recv[2];\n        bins[3] += bins_recv[3];\n      }\n    } else {\n      MPI_Send(bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n  } else {\n    bins = countQuartiles(x);\n  }\n}",
            "int size = MPI_Comm_size(MPI_COMM_WORLD);\n    int rank = MPI_Comm_rank(MPI_COMM_WORLD);\n    double * x_arr;\n\n    x_arr = new double[x.size()];\n\n    for (int i = 0; i < x.size(); i++){\n        x_arr[i] = x[i];\n    }\n\n    double dp = size*0.25;\n\n    double * x_min, *x_max;\n\n    MPI_Reduce(&x_arr[0], &x_min, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&x_arr[x.size()-1], &x_max, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    std::vector<double> x_v;\n\n    if(rank == 0){\n        x_v.assign(x_min, x_max + 1);\n    }\n\n    MPI_Bcast(&x_v, x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    x_arr = &x_v[0];\n\n    int s = x_v.size();\n\n    double * x_sorted;\n\n    if(rank == 0){\n        std::sort(x_v.begin(), x_v.end());\n    }\n\n    MPI_Bcast(&x_v[0], s, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    x_arr = &x_v[0];\n\n    if(rank == 0){\n        bins[0] = 0;\n        bins[1] = 0;\n        bins[2] = 0;\n        bins[3] = 0;\n    }\n\n    double * x_partial;\n    x_partial = new double[s];\n\n    for (int i = 0; i < s; i++){\n        x_partial[i] = x_arr[i];\n    }\n\n    MPI_Scatter(x_partial, size/4, MPI_DOUBLE, &x_partial[0], size/4, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    int j = 0;\n\n    if(rank == 0){\n        for(int i = 0; i < size/4; i++){\n            while(j < s && x_partial[i] < x_arr[j]){\n                j++;\n            }\n            bins[i] = j;\n        }\n    }\n\n    MPI_Gather(&bins[0], size/4, MPI_INT, &bins[0], size/4, MPI_INT, 0, MPI_COMM_WORLD);\n\n    delete [] x_arr;\n    delete [] x_min;\n    delete [] x_max;\n    delete [] x_partial;\n\n}",
            "size_t n = x.size();\n    size_t n_half = n/2;\n    std::vector<double> local_x(n);\n    MPI_Status status;\n    MPI_Recv(&local_x[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);\n    size_t n_my = n_half;\n    if (status.MPI_SOURCE!= MPI_PROC_NULL) {\n        n_my = n_half + status.MPI_SOURCE*n_half;\n    }\n    int my_rank = status.MPI_SOURCE;\n    for (size_t i = n_half; i < n_my; i++) {\n        if (0.0 <= x[i] && x[i] < 0.25) bins[0]++;\n        else if (0.25 <= x[i] && x[i] < 0.5) bins[1]++;\n        else if (0.5 <= x[i] && x[i] < 0.75) bins[2]++;\n        else if (0.75 <= x[i]) bins[3]++;\n    }\n    if (my_rank!= 0) {\n        MPI_Send(&local_x[n_half], n_half, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // number of doubles per process\n  int n = x.size();\n\n  // number of processes\n  int nprocs;\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n  // size of the last process\n  int last_size = n % nprocs;\n\n  // get the index of the first double of process\n  int start = 0;\n  if (rank > 0) {\n    start = n / nprocs * (rank - 1) + std::min(rank - 1, last_size);\n  }\n\n  // get the index of the last double of process\n  int end = 0;\n  if (rank < nprocs) {\n    end = n / nprocs * rank + std::min(rank, last_size);\n  }\n\n  // sum of the count of doubles for each bin\n  size_t sum = 0;\n\n  // loop on the doubles and count the fractional part\n  for (int i = start; i < end; ++i) {\n    if (x[i] < 0.25) {\n      sum += 1;\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      sum += 2;\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      sum += 3;\n    } else if (x[i] >= 0.75 && x[i] < 1) {\n      sum += 4;\n    }\n  }\n\n  // send the counts to the process 0\n  MPI_Send(&sum, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n\n  // if the process is not the process 0, then receive the counts from process 0\n  if (rank!= 0) {\n    MPI_Status status;\n    size_t bins_buffer[4];\n    MPI_Recv(bins_buffer, 4, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD, &status);\n    bins[0] = bins_buffer[0];\n    bins[1] = bins_buffer[1];\n    bins[2] = bins_buffer[2];\n    bins[3] = bins_buffer[3];\n  }\n}",
            "std::array<size_t, 4> bins_local = {};\n\tsize_t n = x.size();\n\n\tfor (int i = 0; i < n; i++) {\n\t\tif (x.at(i) < 0.25) bins_local[0]++;\n\t\telse if (x.at(i) < 0.50) bins_local[1]++;\n\t\telse if (x.at(i) < 0.75) bins_local[2]++;\n\t\telse bins_local[3]++;\n\t}\n\n\tMPI_Allreduce(&bins_local, &bins, 4, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n}",
            "int num_procs, my_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n  std::array<size_t, 4> local_bins = {};\n  for (double x_val : x) {\n    if (x_val < 0.25) {\n      ++local_bins[0];\n    } else if (x_val < 0.5) {\n      ++local_bins[1];\n    } else if (x_val < 0.75) {\n      ++local_bins[2];\n    } else {\n      ++local_bins[3];\n    }\n  }\n\n  MPI_Allreduce(MPI_IN_PLACE, local_bins.data(), 4, MPI_UNSIGNED_LONG_LONG,\n                MPI_SUM, MPI_COMM_WORLD);\n\n  if (my_rank == 0) {\n    bins = local_bins;\n  }\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  std::vector<double> x_local(x.begin(), x.begin() + x.size() / size);\n  double* x_local_ptr = x_local.data();\n\n  size_t count_0_25 = 0;\n  size_t count_0_5 = 0;\n  size_t count_0_75 = 0;\n  size_t count_1 = 0;\n  for (size_t i = 0; i < x_local.size(); i++) {\n    if (x_local_ptr[i] > 0.75) count_1++;\n    else if (x_local_ptr[i] > 0.5) count_0_75++;\n    else if (x_local_ptr[i] > 0.25) count_0_5++;\n    else if (x_local_ptr[i] > 0) count_0_25++;\n  }\n\n  size_t* count_0_25_ptr = new size_t;\n  *count_0_25_ptr = count_0_25;\n  MPI_Bcast(count_0_25_ptr, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  count_0_25 = *count_0_25_ptr;\n\n  size_t* count_0_5_ptr = new size_t;\n  *count_0_5_ptr = count_0_5;\n  MPI_Bcast(count_0_5_ptr, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  count_0_5 = *count_0_5_ptr;\n\n  size_t* count_0_75_ptr = new size_t;\n  *count_0_75_ptr = count_0_75;\n  MPI_Bcast(count_0_75_ptr, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  count_0_75 = *count_0_75_ptr;\n\n  size_t* count_1_ptr = new size_t;\n  *count_1_ptr = count_1;\n  MPI_Bcast(count_1_ptr, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  count_1 = *count_1_ptr;\n\n  bins[0] = count_0_25;\n  bins[1] = count_0_5;\n  bins[2] = count_0_75;\n  bins[3] = count_1;\n\n  delete count_0_25_ptr;\n  delete count_0_5_ptr;\n  delete count_0_75_ptr;\n  delete count_1_ptr;\n}",
            "// TODO: Your code here\n  bins.fill(0);\n  size_t count = 0;\n  for(double i : x) {\n    if(i < 0.25) {\n      bins.at(0)++;\n    }\n    else if(i < 0.5) {\n      bins.at(1)++;\n    }\n    else if(i < 0.75) {\n      bins.at(2)++;\n    }\n    else {\n      bins.at(3)++;\n    }\n    count++;\n  }\n\n  // TODO: Your code here\n  size_t total = count;\n  std::array<double, 4> avg;\n  MPI_Reduce(&count, &total, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  MPI_Reduce(&bins, &avg, 4, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  if(rank == 0) {\n    for(int i = 0; i < 4; i++) {\n      bins[i] = avg[i] / total;\n    }\n  }\n}",
            "// TODO\n\n}",
            "if (bins.size()!= 4) {\n        bins = {0, 0, 0, 0};\n    }\n    size_t n = x.size();\n    double q1, q2, q3;\n    q1 = x[n / 4];\n    q2 = x[n / 2];\n    q3 = x[(3 * n) / 4];\n    int my_rank = 0;\n    int num_ranks = 1;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    for (size_t i = 0; i < n; i++) {\n        if (x[i] >= q3) {\n            bins[3]++;\n        }\n        else if (x[i] >= q2) {\n            bins[2]++;\n        }\n        else if (x[i] >= q1) {\n            bins[1]++;\n        }\n        else {\n            bins[0]++;\n        }\n    }\n    std::array<size_t, 4> temp_bins = bins;\n    MPI_Reduce(temp_bins.data(), bins.data(), 4, MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (my_rank == 0) {\n        std::cout << \"bins: [\";\n        for (size_t i = 0; i < 4; i++) {\n            std::cout << bins[i];\n            if (i!= 3) {\n                std::cout << \", \";\n            }\n        }\n        std::cout << \"]\" << std::endl;\n    }\n}",
            "// TODO: Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // make sure input is valid\n    if (x.size() == 0) {\n        for (int i = 0; i < 4; i++) {\n            bins[i] = 0;\n        }\n        return;\n    }\n\n    // assign chunk size to each rank\n    size_t chunkSize = x.size()/size;\n\n    // check if last chunk is smaller than chunk size\n    if (x.size() % size!= 0) {\n        chunkSize++;\n    }\n\n    // assign each rank a portion of input array\n    std::vector<double> subArr;\n    int i = rank * chunkSize;\n    int j = i + chunkSize;\n    for (int k = i; k < j; k++) {\n        subArr.push_back(x[k]);\n    }\n\n    // sort sub array\n    std::sort(subArr.begin(), subArr.end());\n\n    // calculate number of values in each bin\n    for (int k = 0; k < subArr.size(); k++) {\n        if (subArr[k] < 0.25) {\n            bins[0]++;\n        } else if (subArr[k] < 0.5) {\n            bins[1]++;\n        } else if (subArr[k] < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n\n    // find median of each bin\n    double median = 0.0;\n    size_t counter = 0;\n    for (int i = 0; i < 4; i++) {\n        if (bins[i] == 0) {\n            continue;\n        }\n        median = median + i;\n        counter++;\n    }\n    median = median/counter;\n\n    // send to rank 0\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&bins, 4, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            median = median + bins[0];\n            counter++;\n        }\n        median = median/counter;\n    } else {\n        MPI_Send(&bins, 4, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "size_t const size = x.size();\n    std::vector<double> x_sorted = x;\n\n    // Sort x\n    std::sort(x_sorted.begin(), x_sorted.end());\n\n    // Count the number of values less than x[i] for each i in [0, size).\n    std::vector<int> bins_i;\n    bins_i.push_back(0);\n    for (size_t i = 0; i < size; ++i) {\n        bins_i.push_back(std::count_if(x_sorted.begin(), x_sorted.begin() + i + 1,\n                                       [&](double const& val) { return val < x_sorted[i]; }));\n    }\n\n    // Compute the number of values that fall in each quartile\n    for (size_t i = 0; i < 4; ++i) {\n        bins[i] = bins_i[size * i / 4];\n    }\n\n    // Communicate the results\n    //TODO\n\n    return;\n}",
            "}",
            "assert(x.size() > 0);\n    size_t num_procs;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    assert(num_procs > 0);\n    size_t num_doubles = x.size();\n    size_t num_doubles_per_proc = num_doubles / num_procs;\n    size_t extra = num_doubles % num_procs;\n    int proc_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);\n    std::vector<double> x_proc(num_doubles_per_proc);\n    if (proc_rank == 0) {\n        x_proc.resize(num_doubles_per_proc + extra);\n    }\n    std::vector<double> local_x(num_doubles_per_proc + extra);\n    for (size_t i = 0; i < num_doubles_per_proc + extra; ++i) {\n        if (proc_rank == 0) {\n            local_x[i] = x[i];\n        } else {\n            local_x[i] = x[i + num_doubles_per_proc * proc_rank];\n        }\n    }\n    std::vector<int> bins_proc(4);\n    int num_bins = 4;\n    MPI_Reduce(local_x.data(), x_proc.data(), num_doubles_per_proc, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(local_x.data(), bins_proc.data(), num_bins, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    bins = std::array<size_t, 4>();\n    if (proc_rank == 0) {\n        bins[0] = bins_proc[0];\n        bins[1] = bins_proc[1];\n        bins[2] = bins_proc[2];\n        bins[3] = bins_proc[3];\n        for (size_t i = 1; i < num_procs; ++i) {\n            bins[0] += bins_proc[0 + i * 4];\n            bins[1] += bins_proc[1 + i * 4];\n            bins[2] += bins_proc[2 + i * 4];\n            bins[3] += bins_proc[3 + i * 4];\n        }\n    }\n}",
            "bins = {0, 0, 0, 0};\n  if (x.empty()) { return; }\n\n  int mpi_size;\n  int mpi_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n  double n = x.size();\n  double rank_size = n / mpi_size;\n  std::vector<double> tmp(x.begin() + mpi_rank * rank_size, x.begin() + (m",
            "// your code here\n\n}",
            "// TODO: Your code here\n    bins = {};\n    size_t n = x.size();\n    size_t local_size = n / mpi::nprocs;\n    double local_start = (mpi::rank * local_size) / n;\n    double local_end = ((mpi::rank + 1) * local_size) / n;\n    size_t low = 0, mid = 0, high = 0, max = 0;\n    for (size_t i = 0; i < n; i++) {\n        if (x[i] >= local_start && x[i] < (local_start + 0.25)) {\n            low++;\n        } else if (x[i] >= (local_start + 0.25) && x[i] < (local_start + 0.5)) {\n            mid++;\n        } else if (x[i] >= (local_start + 0.5) && x[i] < (local_start + 0.75)) {\n            high++;\n        } else if (x[i] >= (local_start + 0.75) && x[i] <= local_end) {\n            max++;\n        }\n    }\n    bins[0] = low;\n    bins[1] = mid;\n    bins[2] = high;\n    bins[3] = max;\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  if (rank == 0) {\n    for (size_t i = 0; i < x.size(); i += 2) {\n      if (x[i] >= 0 && x[i] < 0.25) {\n        bins[0]++;\n      } else if (x[i] >= 0.25 && x[i] < 0.5) {\n        bins[1]++;\n      } else if (x[i] >= 0.5 && x[i] < 0.75) {\n        bins[2]++;\n      } else if (x[i] >= 0.75 && x[i] < 1) {\n        bins[3]++;\n      }\n    }\n  }\n\n  // Everyone else does nothing.\n\n  return;\n}",
            "int rank, nranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n    int n;\n    if (rank == 0) {\n        n = x.size();\n    }\n    MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    int chunk = n / nranks;\n    int remainder = n % nranks;\n    std::vector<size_t> counts(4, 0);\n    std::vector<double> local_x(chunk);\n    for (int i = 0; i < chunk; i++) {\n        local_x[i] = x[rank * chunk + i];\n    }\n    std::sort(local_x.begin(), local_x.end());\n    for (int i = 0; i < local_x.size(); i++) {\n        if (local_x[i] < 0.25) {\n            counts[0]++;\n        } else if (local_x[i] < 0.5) {\n            counts[1]++;\n        } else if (local_x[i] < 0.75) {\n            counts[2]++;\n        } else if (local_x[i] < 1.0) {\n            counts[3]++;\n        }\n    }\n    MPI_Reduce(counts.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int numprocs;\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\n    // if rank is 0, it is responsible for distributing work to other ranks\n    if (rank == 0) {\n        // split vector x into numprocs blocks of x.size()/numprocs each\n        std::vector<size_t> start(numprocs + 1);\n        start[0] = 0;\n        size_t block = x.size() / numprocs;\n        size_t remaining = x.size() % numprocs;\n\n        for (int i = 1; i < numprocs + 1; i++) {\n            start[i] = start[i - 1] + block + (i - 1 < remaining? 1 : 0);\n        }\n\n        // send to each rank the part of x it is responsible for\n        for (int i = 1; i < numprocs + 1; i++) {\n            MPI_Send(x.data() + start[i - 1], start[i] - start[i - 1], MPI_DOUBLE, i - 1, 0, MPI_COMM_WORLD);\n        }\n\n        // compute each rank's own bins\n        bins = countQuartilesRank(x, start[numprocs]);\n\n    } else {\n        // receive a vector from rank 0 containing x\n        std::vector<double> x_sub(x.size() / numprocs + (rank < x.size() % numprocs? 1 : 0));\n        MPI_Recv(x_sub.data(), x_sub.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n        // compute bins\n        bins = countQuartilesRank(x_sub, x_sub.size());\n\n        // send bins back to rank 0\n        MPI_Send(bins.data(), 4, MPI_SIZE_T, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "size_t global_size = x.size();\n\n    // check size\n    if (global_size == 0) {\n        std::cout << \"Error: vector is empty\" << std::endl;\n        exit(EXIT_FAILURE);\n    }\n\n    // if global size is not multiple of 4\n    // add the missing values\n    if (global_size % 4!= 0) {\n        global_size = global_size + 4 - (global_size % 4);\n    }\n\n    // each rank receives local_size\n    size_t local_size = global_size / mpi::size();\n\n    // each rank has a local vector\n    std::vector<double> local_x;\n    local_x.resize(local_size);\n\n    // each rank gets a local vector\n    for (int i = 0; i < local_size; i++) {\n        local_x[i] = x[mpi::rank() * local_size + i];\n    }\n\n    // sort local vector\n    std::sort(local_x.begin(), local_x.end());\n\n    // compute local bins\n    std::array<size_t, 4> local_bins;\n\n    size_t count_0 = 0;\n    size_t count_1 = 0;\n    size_t count_2 = 0;\n    size_t count_3 = 0;\n\n    // check the values of the local vector\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] < 0.25) {\n            count_0++;\n        } else if (local_x[i] < 0.5) {\n            count_1++;\n        } else if (local_x[i] < 0.75) {\n            count_2++;\n        } else if (local_x[i] <= 1) {\n            count_3++;\n        } else {\n            std::cout << \"Error: a value larger than 1 was found in the local vector\" << std::endl;\n            exit(EXIT_FAILURE);\n        }\n    }\n\n    // store local bins\n    local_bins[0] = count_0;\n    local_bins[1] = count_1;\n    local_bins[2] = count_2;\n    local_bins[3] = count_3;\n\n    // store bins in global vector\n    if (mpi::rank() == 0) {\n        // each rank has a local vector\n        std::vector<double> local_bins_temp;\n        local_bins_temp.resize(4);\n\n        for (int i = 0; i < mpi::size(); i++) {\n            if (i == mpi::rank()) {\n                local_bins_temp[0] = local_bins[0];\n                local_bins_temp[1] = local_bins[1];\n                local_bins_temp[2] = local_bins[2];\n                local_bins_temp[3] = local_bins[3];\n            }\n            // receive bins from the i-th rank\n            MPI_Status status;\n            MPI_Recv(local_bins_temp.data(), 4, MPI_DOUBLE, i, 1, MPI_COMM_WORLD, &status);\n\n            // check rank\n            if (i!= 0) {\n                // check if the received values are valid\n                if (local_bins_temp[0] > 0 || local_bins_temp[1] > 0 || local_bins_temp[2] > 0 || local_bins_temp[3] > 0) {\n                    std::cout << \"Error: a value larger than 0 was found in the bins of the local vector\" << std::endl;\n                    exit(EXIT_FAILURE);\n                }\n            }\n\n            // add received values to the global bins\n            bins[0] = bins[0] + local_bins_temp[0];\n            bins[1] = bins[1] + local_bins_temp[1];\n            bins[2] = bins[2] + local",
            "bins.fill(0);\n\n    size_t N = x.size();\n    double *x_ptr = x.data();\n    MPI_Status status;\n\n    MPI_Allreduce(&N, &bins[0], 1, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    if (N == 0) return;\n\n    double *x_sorted = new double[N];\n    for (size_t i = 0; i < N; i++) {\n        x_sorted[i] = x_ptr[i];\n    }\n\n    std::sort(x_sorted, x_sorted + N);\n\n    // get the quartile counts\n    size_t q1_idx = N * 0.25;\n    size_t q2_idx = N * 0.5;\n    size_t q3_idx = N * 0.75;\n\n    bins[0] = count_if(x_sorted, x_sorted + q1_idx, [](double x) {\n        return (x - std::floor(x)) < 0.25;\n    });\n\n    bins[1] = count_if(x_sorted + q1_idx, x_sorted + q2_idx, [](double x) {\n        return (x - std::floor(x)) >= 0.25 && (x - std::floor(x)) < 0.5;\n    });\n\n    bins[2] = count_if(x_sorted + q2_idx, x_sorted + q3_idx, [](double x) {\n        return (x - std::floor(x)) >= 0.5 && (x - std::floor(x)) < 0.75;\n    });\n\n    bins[3] = count_if(x_sorted + q3_idx, x_sorted + N, [](double x) {\n        return (x - std::floor(x)) >= 0.75;\n    });\n\n    delete[] x_sorted;\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &bins.size());\n  MPI_Comm_rank(MPI_COMM_WORLD, &bins.size());\n\n  size_t n = x.size();\n\n  if (n < 1) {\n    return;\n  }\n\n  size_t bin_size = n / bins.size();\n\n  for (size_t i = 0; i < n; ++i) {\n    size_t rank = i / bin_size;\n    if (rank < bins.size()) {\n      double frac = x[i] - (int)x[i];\n      if (frac < 0.25) {\n        ++bins[0];\n      } else if (frac < 0.5) {\n        ++bins[1];\n      } else if (frac < 0.75) {\n        ++bins[2];\n      } else {\n        ++bins[3];\n      }\n    }\n  }\n\n  MPI_Reduce(&bins[0], &bins[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (0 == rank) {\n    printf(\"[%i]: [%lu, %lu, %lu, %lu]\\n\", rank, bins[0], bins[1], bins[2], bins[3]);\n  }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n\n\t// Get the size of the MPI process grid\n\tint size;\n\tMPI_Comm_size(comm, &size);\n\n\t// Get the rank of this process\n\tint rank;\n\tMPI_Comm_rank(comm, &rank);\n\n\t// We are going to do four MPI_Send/Recv operations in this function\n\tconstexpr int numberOfSends = 4;\n\tconstexpr int numberOfReceives = 4;\n\n\t// These four arrays contain the number of doubles in each bin\n\tstd::array<int, 4> sendCounts = { 0, 0, 0, 0 };\n\tstd::array<int, 4> receiveCounts = { 0, 0, 0, 0 };\n\n\t// The two MPI_Datatype objects that will be used\n\tMPI_Datatype sendType, receiveType;\n\n\t// Find the bin that each double in x falls into\n\tfor (double const& d : x) {\n\t\t// if (d >= 0 && d < 0.25) {\n\t\tif (d < 0.25) {\n\t\t\tsendCounts[0] += 1;\n\t\t}\n\t\t// else if (d >= 0.25 && d < 0.5) {\n\t\telse if (d < 0.5) {\n\t\t\tsendCounts[1] += 1;\n\t\t}\n\t\t// else if (d >= 0.5 && d < 0.75) {\n\t\telse if (d < 0.75) {\n\t\t\tsendCounts[2] += 1;\n\t\t}\n\t\t// else if (d >= 0.75 && d < 1) {\n\t\telse {\n\t\t\tsendCounts[3] += 1;\n\t\t}\n\t}\n\n\t// Send the bins to the other processes\n\t// -----------------------------------\n\n\t// Create the send datatype\n\tMPI_Type_create_subarray(numberOfSends,\n\t\t\t\t\t\t\t \t\t\t\t\t\t\t&sendCounts[0],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t&sendCounts[numberOfSends],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t&numberOfSends,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tMPI_INT,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t&sendType);\n\n\t// Commit the datatype\n\tMPI_Type_commit(&sendType);\n\n\t// Determine the send displacement\n\tint sendDisplacement[4] = {0, 0, 0, 0};\n\tfor (int i = 0; i < sendDisplacement[0]; i++) {\n\t\tsendDisplacement[1] += sendCounts[0];\n\t}\n\tfor (int i = 0; i < sendDisplacement[1]; i++) {\n\t\tsendDisplacement[2] += sendCounts[1];\n\t}\n\tfor (int i = 0; i < sendDisplacement[2]; i++) {\n\t\tsendDisplacement[3] += sendCounts[2];\n\t}\n\n\t// Send the bins to the other processes\n\tfor (int i = 0; i < numberOfSends; i++) {\n\t\tMPI_Send(&sendCounts[i],\n\t\t\t\t\t\t 1,\n\t\t\t\t\t\t sendType,\n\t\t\t\t\t\t rank,\n\t\t\t\t\t\t i,\n\t\t\t\t\t\t comm);\n\t}\n\n\t// Free the send datatype\n\tMPI_Type_free(&sendType);\n\n\t// Receive the bins from the other processes\n\t// -----------------------------------------\n\n\t// Create the receive datatype\n\tMPI_Type_create_subarray(numberOfReceives,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t&receiveCounts[0],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t&receiveCounts[numberOfReceives],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t&",
            "// TODO: Your code here\n}",
            "size_t n = x.size();\n  if (n == 0) {\n    return;\n  }\n  size_t nranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n  int myrank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n  std::vector<double> x_loc(n);\n  MPI_Scatter(x.data(), n/nranks, MPI_DOUBLE, x_loc.data(), n/nranks, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  size_t bins_local[4] = {0};\n  for (size_t i = 0; i < n/nranks; ++i) {\n    double x_i = x_loc[i];\n    if (x_i < 0.25) {\n      ++bins_local[0];\n    }\n    else if (x_i < 0.5) {\n      ++bins_local[1];\n    }\n    else if (x_i < 0.75) {\n      ++bins_local[2];\n    }\n    else {\n      ++bins_local[3];\n    }\n  }\n\n  size_t bins_global[4] = {0};\n  MPI_Reduce(bins_local, bins_global, 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  bins[0] = bins_global[0];\n  bins[1] = bins_global[1];\n  bins[2] = bins_global[2];\n  bins[3] = bins_global[3];\n}",
            "// Fill in code here\n}",
            "const size_t nprocs = MPI_Comm_size(MPI_COMM_WORLD);\n    const size_t proc_id = MPI_Comm_rank(MPI_COMM_WORLD);\n    const double eps = 0.25;\n\n    // Step 1: scatter the array so that each processor has a chunk of the array\n    std::vector<double> local_x(x.size()/nprocs);\n    MPI_Scatter(x.data(), local_x.size(), MPI_DOUBLE,\n        local_x.data(), local_x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Step 2: compute the bins for the local data\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    for (size_t i = 0; i < local_x.size(); ++i) {\n        if (local_x[i] < 0) {\n            bins[0] += 1;\n        }\n        else if (local_x[i] < eps) {\n            bins[1] += 1;\n        }\n        else if (local_x[i] < eps*2) {\n            bins[2] += 1;\n        }\n        else if (local_x[i] < eps*3) {\n            bins[3] += 1;\n        }\n    }\n\n    // Step 3: Reduce the bins into a vector of four elements, the sum of the bins\n    // from each processor.\n    std::vector<size_t> total(4);\n    MPI_Allreduce(bins.data(), total.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    // Step 4: Compute the bins of the global data.\n    // This assumes that the input data was already sorted.\n    // If the input data was not sorted, sort it using MPI_Alltoallv to exchange\n    // the chunks, and then sort the local data.\n    if (proc_id == 0) {\n        bins = total;\n    }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Compute how many doubles each rank has in the vector\n    size_t sizeOfX = x.size() / size;\n    size_t remaining = x.size() % size;\n\n    // Compute the start and end index for each rank in the vector\n    size_t startIndex = rank * sizeOfX;\n    size_t endIndex = (rank + 1) * sizeOfX + (rank < remaining? 1 : 0);\n\n    // Compute the number of doubles in the vector\n    size_t count = endIndex - startIndex;\n\n    // Compute the quartile bins\n    std::array<double, 4> quartiles = {0.25, 0.5, 0.75, 1.0};\n    bins.fill(0);\n    for (size_t i = startIndex; i < endIndex; i++) {\n        int idx = 0;\n        for (double q : quartiles) {\n            if (x[i] < q) {\n                break;\n            }\n            idx++;\n        }\n        bins[idx]++;\n    }\n\n    // Compute the global bins\n    std::array<size_t, 4> globalBins;\n    globalBins.fill(0);\n    MPI_Reduce(bins.data(), globalBins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // Print out the quartile bins\n    if (rank == 0) {\n        for (int i = 0; i < 4; i++) {\n            std::cout << \"(\" << quartiles[i] << \", \" << quartiles[i + 1] << \"]: \" << globalBins[i] << std::endl;\n        }\n    }\n}",
            "// YOUR CODE HERE\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int remainder = size % 4;\n    int batchSize = size / 4;\n    MPI_Status status;\n    std::vector<double> xRecv;\n    std::vector<size_t> binsRecv(4);\n\n    if (rank < remainder)\n    {\n        MPI_Send(&x[rank * batchSize], batchSize, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n        MPI_Recv(&bins[rank], 4, MPI_LONG, 0, 0, MPI_COMM_WORLD, &status);\n    }\n    else\n    {\n        int index = remainder;\n        int start = index * batchSize;\n        int end = start + batchSize;\n        std::vector<double> xSend(x.begin() + start, x.begin() + end);\n        MPI_Send(&xSend[0], batchSize, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n        MPI_Recv(&binsRecv[0], 4, MPI_LONG, 0, 0, MPI_COMM_WORLD, &status);\n        bins[index - remainder] = binsRecv[0];\n        index = index + 1;\n        start = index * batchSize;\n        end = start + batchSize;\n        std::vector<double> xSend1(x.begin() + start, x.begin() + end);\n        MPI_Send(&xSend1[0], batchSize, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n        MPI_Recv(&binsRecv[1], 4, MPI_LONG, 0, 0, MPI_COMM_WORLD, &status);\n        bins[index - remainder] = binsRecv[1];\n        index = index + 1;\n        start = index * batchSize;\n        end = start + batchSize;\n        std::vector<double> xSend2(x.begin() + start, x.begin() + end);\n        MPI_Send(&xSend2[0], batchSize, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n        MPI_Recv(&binsRecv[2], 4, MPI_LONG, 0, 0, MPI_COMM_WORLD, &status);\n        bins[index - remainder] = binsRecv[2];\n        index = index + 1;\n        start = index * batchSize;\n        end = start + batchSize;\n        std::vector<double> xSend3(x.begin() + start, x.begin() + end);\n        MPI_Send(&xSend3[0], batchSize, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n        MPI_Recv(&binsRecv[3], 4, MPI_LONG, 0, 0, MPI_COMM_WORLD, &status);\n        bins[index - remainder] = binsRecv[3];\n    }\n\n    return;\n}",
            "assert(x.size() == 7);\n\n  // TODO: add code here\n\n  // Create a vector to contain the counts for each rank\n  std::vector<size_t> counts(x.size());\n\n  // Initialize counts to zero for each element\n  std::fill(counts.begin(), counts.end(), 0);\n\n  // Compute the total number of elements for each rank\n  int nElements = x.size();\n  int nRanks = 1;\n  MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n  int rank = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Compute the number of elements per rank\n  int nElementsPerRank = nElements / nRanks;\n\n  // Compute the starting index for this rank\n  int indexStart = nElementsPerRank * rank;\n\n  // Compute the ending index for this rank\n  int indexEnd = (rank == (nRanks - 1))? nElements : (nElementsPerRank * (rank + 1));\n\n  // Count the elements in each of the four bins\n  for (int i = indexStart; i < indexEnd; i++) {\n    if (x[i] < 0.25)\n      counts[rank] += 1;\n    else if (x[i] >= 0.25 && x[i] < 0.5)\n      counts[rank] += 1;\n    else if (x[i] >= 0.5 && x[i] < 0.75)\n      counts[rank] += 1;\n    else\n      counts[rank] += 1;\n  }\n\n  // Sum the counts from all ranks\n  MPI_Allreduce(counts.data(), bins.data(), nRanks, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n  // Set the counts to zero\n  std::fill(counts.begin(), counts.end(), 0);\n\n  // Compute the starting index for each rank\n  indexStart = 0;\n\n  // Compute the ending index for each rank\n  indexEnd = (rank == 0)? nElementsPerRank : (nElementsPerRank * (rank + 1));\n\n  // Set the counts in the four bins\n  for (int i = indexStart; i < indexEnd; i++) {\n    if (x[i] < 0.25)\n      counts[rank] += 1;\n    else if (x[i] >= 0.25 && x[i] < 0.5)\n      counts[rank] += 1;\n    else if (x[i] >= 0.5 && x[i] < 0.75)\n      counts[rank] += 1;\n    else\n      counts[rank] += 1;\n  }\n\n  // Sum the counts from all ranks\n  MPI_Allreduce(counts.data(), bins.data(), nRanks, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    double *buf;\n    buf = (double *) malloc(x.size() * sizeof(double));\n    std::vector<double> *xptr;\n    xptr = &x;\n    int lens = x.size();\n    MPI_Allgather(&lens, 1, MPI_INT, buf, 1, MPI_INT, MPI_COMM_WORLD);\n    int offset = 0;\n    for (int i = 0; i < rank; i++) {\n        offset += buf[i];\n    }\n    MPI_Allgatherv(xptr->data(), lens, MPI_DOUBLE, buf, buf, MPI_INT, MPI_COMM_WORLD);\n    for (int i = 0; i < rank; i++) {\n        offset += buf[i];\n    }\n    for (int i = offset; i < offset + lens; i++) {\n        if (buf[i] < 0.25) {\n            bins[0]++;\n        } else if (buf[i] < 0.5) {\n            bins[1]++;\n        } else if (buf[i] < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n    free(buf);\n}",
            "MPI_Comm world;\n    MPI_Comm_dup(MPI_COMM_WORLD, &world);\n\n    int nprocs, proc_id;\n    MPI_Comm_size(world, &nprocs);\n    MPI_Comm_rank(world, &proc_id);\n\n    int n = x.size();\n    size_t part = (n + nprocs - 1) / nprocs;\n    if (part < n) {\n        // not a multiple of nprocs\n        part++;\n    }\n\n    std::vector<double> buffer(part);\n    std::copy(x.begin(), x.begin() + part, buffer.begin());\n\n    std::vector<size_t> bin_counts(4);\n\n    for (int i = 0; i < 4; i++) {\n        MPI_Bcast(&buffer[0], buffer.size(), MPI_DOUBLE, 0, world);\n        for (int j = 0; j < part; j++) {\n            if (buffer[j] < 0.25) {\n                bin_counts[i]++;\n            } else if (buffer[j] < 0.5) {\n                bin_counts[i + 1]++;\n            } else if (buffer[j] < 0.75) {\n                bin_counts[i + 2]++;\n            } else {\n                bin_counts[i + 3]++;\n            }\n        }\n    }\n\n    MPI_Reduce(bin_counts.data(), bins.data(), 4, MPI_UNSIGNED, MPI_SUM, 0, world);\n    MPI_Comm_free(&world);\n}",
            "// TODO\n}",
            "MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    std::vector<double> my_x;\n    std::vector<double> my_bins;\n\n    if(rank == 0){\n        bins = {0, 0, 0, 0};\n        my_x = x;\n    }\n    else{\n        my_x = x;\n    }\n\n    double step = 1.0/size;\n    double start = rank*step;\n    double stop = start + step;\n\n    for(size_t i = 0; i < my_x.size(); i++){\n        if(start <= my_x[i] && my_x[i] < stop){\n            if(my_x[i] < start + 0.25*step){\n                bins[0]++;\n            }\n            else if(my_x[i] >= start + 0.25*step && my_x[i] < start + 0.5*step){\n                bins[1]++;\n            }\n            else if(my_x[i] >= start + 0.5*step && my_x[i] < start + 0.75*step){\n                bins[2]++;\n            }\n            else{\n                bins[3]++;\n            }\n        }\n    }\n\n    MPI_Reduce(MPI_IN_PLACE, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    return;\n}",
            "int rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        std::sort(x.begin(), x.end());\n        size_t size = x.size();\n        bins.fill(0);\n\n        for (size_t i = 0; i < size; i++) {\n            double x_i = x[i];\n            if (x_i >= 0.25 * size && x_i < 0.5 * size) {\n                bins[0]++;\n            } else if (x_i >= 0.5 * size && x_i < 0.75 * size) {\n                bins[1]++;\n            } else if (x_i >= 0.75 * size && x_i < size) {\n                bins[2]++;\n            } else if (x_i == size) {\n                bins[3]++;\n            }\n        }\n\n        std::cout << \"x = \" << x << std::endl;\n        std::cout << \"bins = \" << bins << std::endl;\n    }\n\n    return;\n}",
            "/*\n        // TODO: complete this function\n        //\n        //\n        //\n        //\n        //\n        //\n    */\n\n    MPI_Bcast(&x, x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    int nranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    MPI_Status status;\n    int count;\n    int source;\n\n    for (int i = 0; i < x.size(); i += nranks) {\n\n        if (rank == 0) {\n            source = i % nranks;\n        } else {\n            source = 0;\n        }\n        MPI_Recv(&count, 1, MPI_INT, source, 0, MPI_COMM_WORLD, &status);\n\n        if (rank == 0) {\n            bins[0] += count;\n        }\n        std::cout << \"rank: \" << rank << \"count: \" << count << std::endl;\n        std::cout << \"rank: \" << rank << \"bins: \" << bins[0] << std::endl;\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; i++) {\n            std::cout << \"rank: \" << rank << \" bin: \" << i << \" count: \" << bins[i] << std::endl;\n        }\n    }\n\n    std::cout << std::endl << std::endl << std::endl << std::endl;\n\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int len = x.size();\n    int chunk_size = len / size;\n    int chunk_rem = len % size;\n\n    std::vector<double> my_data;\n    if (rank == 0)\n        my_data = x;\n    else {\n        my_data.resize(chunk_size);\n        MPI_Scatter(x.data(), chunk_size, MPI_DOUBLE, my_data.data(), chunk_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        if (rank < chunk_rem) {\n            my_data.push_back(x[rank * chunk_size + chunk_rem]);\n            len++;\n        }\n    }\n\n    std::array<double, 4> bin_counts = { 0, 0, 0, 0 };\n    for (int i = 0; i < len; ++i) {\n        int index = (int)((x[i] - std::floor(x[i])) * 4);\n        bin_counts[index]++;\n    }\n\n    std::vector<double> counts;\n    counts.resize(4 * size);\n\n    MPI_Gather(bin_counts.data(), 4, MPI_DOUBLE, counts.data(), 4, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (int i = 0; i < size; ++i) {\n            for (int j = 0; j < 4; ++j) {\n                bins[j] += counts[i * 4 + j];\n            }\n        }\n    }\n}",
            "bins = std::array<size_t, 4>{0};\n\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n\n  std::vector<double> local_x(n);\n\n  int block_size = n / size;\n  if (rank == 0) {\n    local_x = x;\n  } else {\n    int start = rank * block_size;\n    int end = start + block_size - 1;\n    local_x = std::vector<double>(x.begin() + start, x.begin() + end);\n  }\n\n  std::array<size_t, 4> local_bins = std::array<size_t, 4>{0};\n\n  for (size_t i = 0; i < local_x.size(); i++) {\n    if (local_x[i] < 0.25) {\n      local_bins[0]++;\n    } else if (local_x[i] < 0.5) {\n      local_bins[1]++;\n    } else if (local_x[i] < 0.75) {\n      local_bins[2]++;\n    } else {\n      local_bins[3]++;\n    }\n  }\n\n  // Gather\n  std::array<size_t, 4> global_bins;\n  MPI_Reduce(local_bins.data(), global_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  bins = global_bins;\n}",
            "if (bins.size()!= 4)\n    throw std::runtime_error(\"Size of bins array must be 4.\");\n  size_t n = x.size();\n  if (n == 0)\n    throw std::runtime_error(\"Vector size must be non-zero.\");\n\n  // Create a vector of size n*size, in which each element contains the index \n  // of an element in x.\n  std::vector<int> indices(n*size);\n  for (size_t i = 0; i < n; i++)\n    indices[i*size] = i;\n\n  // Create a vector of size n*size, in which each element contains a copy of \n  // the corresponding element from x.\n  std::vector<double> values(n*size);\n  for (size_t i = 0; i < n; i++)\n    values[i*size] = x[i];\n\n  // Compute indices and values for each processor.\n  for (size_t i = 1; i < n*size; i++) {\n    indices[i] = indices[(i-1)%n];\n    values[i] = values[(i-1)%n];\n  }\n\n  // Create a vector of size n*size, in which each element contains the \n  // fractional part of the corresponding element from values.\n  std::vector<double> fraction(n*size);\n  for (size_t i = 0; i < n; i++)\n    fraction[i*size] = values[i] - floor(values[i]);\n\n  for (size_t i = 1; i < n*size; i++) {\n    fraction[i] = fraction[(i-1)%n];\n  }\n\n  // Compute the bins for the fractional parts of values.\n  std::vector<int> fraction_bins(n*size);\n  for (size_t i = 0; i < n; i++) {\n    if (fraction[i*size] < 0.25)\n      fraction_bins[i*size] = 0;\n    else if (fraction[i*size] < 0.5)\n      fraction_bins[i*size] = 1;\n    else if (fraction[i*size] < 0.75)\n      fraction_bins[i*size] = 2;\n    else\n      fraction_bins[i*size] = 3;\n  }\n\n  // Count the number of fractional parts in each bin.\n  std::vector<size_t> fraction_counts(4*size);\n  for (size_t i = 0; i < 4*size; i++)\n    fraction_counts[i] = 0;\n  for (size_t i = 0; i < n; i++)\n    fraction_counts[fraction_bins[i*size]]++;\n\n  // Count the number of fractional parts in each bin for each processor.\n  std::vector<size_t> fraction_proc_counts(4*size);\n  for (size_t i = 0; i < 4*size; i++)\n    fraction_proc_counts[i] = 0;\n  MPI_Allreduce(fraction_counts.data(), fraction_proc_counts.data(), \n    4*size, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  // Compute the bins for the values of x.\n  std::vector<int> value_bins(n*size);\n  for (size_t i = 0; i < n; i++) {\n    if (fraction[i*size] < 0.25)\n      value_bins[i*size] = indices[i*size];\n    else if (fraction[i*size] < 0.5)\n      value_bins[i*size] = indices[i*size] + n;\n    else if (fraction[i*size] < 0.75)\n      value_bins[i*size] = indices[i*size] + 2*n;\n    else\n      value_bins[i*size] = indices[i*size] + 3*n;",
            "size_t num_procs, my_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n    int bins_per_proc = x.size() / num_procs;\n    int last_rank_bins = x.size() % num_procs;\n    int my_first_bin = my_rank * bins_per_proc;\n    int my_last_bin = my_first_bin + bins_per_proc;\n    if (my_rank == num_procs - 1) {\n        my_last_bin += last_rank_bins;\n    }\n\n    for (int i = 0; i < 4; i++) {\n        bins[i] = 0;\n    }\n\n    for (int i = my_first_bin; i < my_last_bin; i++) {\n        bins[my_rank]++;\n        if (x[i] < 0.25) {\n            bins[0]++;\n        }\n        else if (x[i] < 0.5) {\n            bins[1]++;\n        }\n        else if (x[i] < 0.75) {\n            bins[2]++;\n        }\n        else {\n            bins[3]++;\n        }\n    }\n\n    for (int i = 0; i < 4; i++) {\n        if (my_rank == 0) {\n            std::cout << bins[i] << \" \";\n        }\n    }\n    if (my_rank == 0) {\n        std::cout << \"\\n\";\n    }\n\n    MPI_Reduce(&bins[0], &bins[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, size;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &size);\n\n    // get the number of elements for each rank\n    size_t elements_per_rank = x.size()/size;\n\n    // get the starting position of the array for each rank\n    size_t offset = rank*elements_per_rank;\n\n    // get the number of elements for this rank\n    size_t local_elements = (rank == size-1)? x.size() - offset : elements_per_rank;\n\n    // get the number of elements in each bin\n    std::vector<size_t> local_bins(4);\n\n    // get the number of elements in each bin for this rank\n    for(size_t i = 0; i < local_elements; ++i){\n        size_t index = 0;\n        if(x[i] < 0.25){\n            index = 0;\n        }\n        else if(x[i] < 0.5){\n            index = 1;\n        }\n        else if(x[i] < 0.75){\n            index = 2;\n        }\n        else{\n            index = 3;\n        }\n\n        ++local_bins[index];\n    }\n\n    // get the total number of elements for each bin\n    std::vector<size_t> global_bins(4);\n\n    MPI_Allreduce(local_bins.data(), global_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, comm);\n\n    // store the number of elements in each bin to bins\n    bins[0] = global_bins[0];\n    bins[1] = global_bins[1];\n    bins[2] = global_bins[2];\n    bins[3] = global_bins[3];\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n_per_rank = x.size() / size;\n  int remainder = x.size() % size;\n  int send_count = (rank < remainder? n_per_rank + 1 : n_per_rank);\n  int recv_count = (rank < remainder? n_per_rank : n_per_rank + 1);\n\n  std::vector<double> send_buf(send_count);\n  std::vector<double> recv_buf(recv_count);\n  for (int i = 0; i < send_count; i++) {\n    send_buf[i] = x[rank * n_per_rank + i];\n  }\n\n  MPI_Allgather(&send_buf[0], send_count, MPI_DOUBLE, &recv_buf[0], recv_count, MPI_DOUBLE, MPI_COMM_WORLD);\n\n  std::sort(recv_buf.begin(), recv_buf.end());\n\n  bins[0] = recv_buf.size() * 0.25;\n  bins[1] = recv_buf.size() * 0.5;\n  bins[2] = recv_buf.size() * 0.75;\n  bins[3] = recv_buf.size();\n\n  if (rank == 0) {\n    for (int i = 0; i < 4; i++) {\n      std::cout << bins[i] << \" \";\n    }\n    std::cout << std::endl;\n  }\n}",
            "MPI_Comm world = MPI_COMM_WORLD;\n    int world_size;\n    MPI_Comm_size(world, &world_size);\n    int rank;\n    MPI_Comm_rank(world, &rank);\n    std::vector<double> send_buffer;\n    send_buffer.assign(x.begin() + rank*x.size()/world_size, x.begin() + (rank+1)*x.size()/world_size);\n    MPI_Request request;\n    std::array<size_t, 4> recv_buffer = {0, 0, 0, 0};\n    MPI_Iallreduce(&send_buffer[0], &recv_buffer[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, world, &request);\n    MPI_Wait(&request, MPI_STATUS_IGNORE);\n    bins = recv_buffer;\n}",
            "// TODO: insert code here\n}",
            "/* \n    The master rank collects all the data from all the ranks and computes the quartiles\n  */\n  // TODO 1: collect all the data from all the ranks\n  std::vector<double> data(x.size());\n  MPI_Gather(&x[0], x.size(), MPI_DOUBLE, &data[0], x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  \n  // TODO 2: sort all the data\n  std::sort(data.begin(), data.end());\n\n  if(rank == 0) {\n    // TODO 3: count the number of doubles in the vector x that have a fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n    // TODO 4: store the counts in bins\n    int i = 0;\n    int size = data.size();\n    int r = rank;\n    bins[0] = (size * r + 1) * 0.25;\n    while (i < size && data[i] < bins[0]) {\n      i++;\n    }\n    bins[1] = (size * r + 2) * 0.25;\n    while (i < size && data[i] < bins[1]) {\n      i++;\n    }\n    bins[2] = (size * r + 3) * 0.25;\n    while (i < size && data[i] < bins[2]) {\n      i++;\n    }\n    bins[3] = (size * r + 4) * 0.25;\n    while (i < size && data[i] < bins[3]) {\n      i++;\n    }\n  }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    size_t num_doubles_per_rank = x.size() / size;\n    size_t start_index = rank * num_doubles_per_rank;\n    size_t end_index = start_index + num_doubles_per_rank;\n    if (rank == (size - 1)) {\n        end_index = x.size();\n    }\n    std::vector<double> local_x(x.begin() + start_index, x.begin() + end_index);\n\n    std::array<size_t, 4> local_bins;\n    local_bins[0] = std::count_if(local_x.begin(), local_x.end(), [](double d) { return (d - (int)d) < 0.25; });\n    local_bins[1] = std::count_if(local_x.begin(), local_x.end(), [](double d) { return (d - (int)d) >= 0.25 && (d - (int)d) < 0.5; });\n    local_bins[2] = std::count_if(local_x.begin(), local_x.end(), [](double d) { return (d - (int)d) >= 0.5 && (d - (int)d) < 0.75; });\n    local_bins[3] = std::count_if(local_x.begin(), local_x.end(), [](double d) { return (d - (int)d) >= 0.75; });\n\n    MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int nprocs, proc_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);\n\n  if (x.size() % nprocs!= 0) {\n    throw std::logic_error(\"Number of values not divisible by the number of processes.\");\n  }\n\n  size_t n = x.size() / nprocs;\n\n  std::vector<size_t> partial(n);\n  std::vector<double> partial_x(n);\n  std::vector<size_t> counts_partial(n, 0);\n\n  if (proc_rank == 0) {\n    for (int rank = 0; rank < nprocs; ++rank) {\n      std::vector<double> x_partial(n);\n      MPI_Recv(x_partial.data(), n, MPI_DOUBLE, rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t i = 0; i < n; ++i) {\n        if (x_partial[i] >= 0 && x_partial[i] < 0.25) {\n          ++counts_partial[i];\n        } else if (x_partial[i] >= 0.25 && x_partial[i] < 0.5) {\n          ++counts_partial[i];\n        } else if (x_partial[i] >= 0.5 && x_partial[i] < 0.75) {\n          ++counts_partial[i];\n        } else if (x_partial[i] >= 0.75 && x_partial[i] <= 1) {\n          ++counts_partial[i];\n        }\n      }\n    }\n    for (size_t i = 0; i < n; ++i) {\n      bins[0] += counts_partial[i];\n    }\n  } else {\n    for (size_t i = 0; i < n; ++i) {\n      partial_x[i] = x[n * proc_rank + i];\n      if (partial_x[i] >= 0 && partial_x[i] < 0.25) {\n        ++partial[i];\n      } else if (partial_x[i] >= 0.25 && partial_x[i] < 0.5) {\n        ++partial[i];\n      } else if (partial_x[i] >= 0.5 && partial_x[i] < 0.75) {\n        ++partial[i];\n      } else if (partial_x[i] >= 0.75 && partial_x[i] <= 1) {\n        ++partial[i];\n      }\n    }\n    MPI_Send(partial_x.data(), n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    MPI_Send(partial.data(), n, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "// initialize bins to zeros\n  bins.fill(0);\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: use MPI to distribute and count x\n  // TODO: gather bins on rank 0\n\n  // TODO: output bins on rank 0\n}",
            "//TODO: Your code goes here\n    return;\n}",
            "size_t size = x.size();\n    bins.fill(0);\n    MPI_Datatype MPI_DOUBLE_VECTOR = MPI_Type_vector(size, 1, 1, MPI_DOUBLE);\n    double *data = x.data();\n    MPI_Reduce(data, bins.data(), bins.size(), MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Type_free(&MPI_DOUBLE_VECTOR);\n}",
            "//TODO\n    // 1) split x in the 4 parts\n    // 2) count the number of elements in each part\n    // 3) scatter the counts to the processors\n    // 4) gather the counts in bins on the root\n    int rank, commSize;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &commSize);\n    int commSize1 = commSize + 1;\n    // bins[0] = bins[1] = bins[2] = bins[3] = 0;\n    // for (double const& el: x) {\n    //     if (el < 0.25) {\n    //         bins[0]++;\n    //     } else if (el < 0.5) {\n    //         bins[1]++;\n    //     } else if (el < 0.75) {\n    //         bins[2]++;\n    //     } else {\n    //         bins[3]++;\n    //     }\n    // }\n    // if (rank == 0) {\n    //     std::cout << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << std::endl;\n    // }\n    // MPI_Gather(bins.data(), 4, MPI_INT, bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n    std::vector<int> binsV;\n    if (rank == 0) {\n        binsV = bins;\n    }\n    std::vector<int> localBins;\n    localBins.resize(4);\n    for (double const& el: x) {\n        if (el < 0.25) {\n            localBins[0]++;\n        } else if (el < 0.5) {\n            localBins[1]++;\n        } else if (el < 0.75) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n    // std::cout << \"rank \" << rank << \" localBins: \" << localBins[0] << \", \" << localBins[1] << \", \" << localBins[2] << \", \" << localBins[3] << std::endl;\n    MPI_Gather(localBins.data(), 4, MPI_INT, binsV.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n    // std::cout << \"rank \" << rank << \" binsV: \" << binsV[0] << \", \" << binsV[1] << \", \" << binsV[2] << \", \" << binsV[3] << std::endl;\n    if (rank == 0) {\n        std::cout << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << std::endl;\n    }\n}",
            "auto size = x.size();\n\n    size_t const n = 4;\n    size_t const nperproc = size / n;\n    size_t const nrem = size % n;\n    size_t const binsize = nperproc + (nrem > 0? 1 : 0);\n\n    bins.fill(0);\n\n    std::vector<double> myx(binsize);\n    if (size > 0) {\n        std::copy_n(x.begin(), binsize, myx.begin());\n    }\n    std::partial_sort(myx.begin(), myx.begin() + nrem, myx.end());\n    bins[0] = binsize - nrem;\n\n    for (int i = 1; i < n; ++i) {\n        if (myx[i-1] < 0.25 && myx[i] >= 0.25) {\n            bins[i] += 1;\n        }\n    }\n\n    auto mybin = bins[0];\n    if (size > 0) {\n        MPI_Reduce(&mybin, &bins[0], n, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n    else {\n        MPI_Reduce(MPI_IN_PLACE, &bins[0], n, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n\n    std::sort(myx.begin(), myx.end());\n    for (int i = 1; i < n; ++i) {\n        if (myx[i-1] < 0.5 && myx[i] >= 0.5) {\n            bins[i] += 1;\n        }\n    }\n\n    mybin = bins[0];\n    if (size > 0) {\n        MPI_Reduce(&mybin, &bins[0], n, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n    else {\n        MPI_Reduce(MPI_IN_PLACE, &bins[0], n, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n\n    std::sort(myx.begin(), myx.end());\n    for (int i = 1; i < n; ++i) {\n        if (myx[i-1] < 0.75 && myx[i] >= 0.75) {\n            bins[i] += 1;\n        }\n    }\n\n    mybin = bins[0];\n    if (size > 0) {\n        MPI_Reduce(&mybin, &bins[0], n, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n    else {\n        MPI_Reduce(MPI_IN_PLACE, &bins[0], n, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n\n    std::sort(myx.begin(), myx.end());\n    for (int i = 1; i < n; ++i) {\n        if (myx[i-1] < 1 && myx[i] >= 1) {\n            bins[i] += 1;\n        }\n    }\n}",
            "int num_rank = 1;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_rank);\n\n    std::array<double, 4> local_counts = {0};\n    for (double x_i : x) {\n        if (x_i < 0.25) {\n            local_counts[0]++;\n        } else if (x_i < 0.5) {\n            local_counts[1]++;\n        } else if (x_i < 0.75) {\n            local_counts[2]++;\n        } else if (x_i < 1) {\n            local_counts[3]++;\n        }\n    }\n    std::array<double, 4> total_counts = {0};\n    MPI_Allreduce(local_counts.data(), total_counts.data(), 4, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    bins[0] = static_cast<size_t>(total_counts[0]);\n    bins[1] = static_cast<size_t>(total_counts[1]);\n    bins[2] = static_cast<size_t>(total_counts[2]);\n    bins[3] = static_cast<size_t>(total_counts[3]);\n\n}",
            "int numProcs = 0;\n    int myRank = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n\n    if (myRank == 0) {\n        for (int i = 0; i < numProcs; i++) {\n            std::vector<double> binsVec;\n            MPI_Recv(&binsVec, 4, MPI_DOUBLE, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            bins[0] += binsVec[0];\n            bins[1] += binsVec[1];\n            bins[2] += binsVec[2];\n            bins[3] += binsVec[3];\n        }\n    } else {\n        for (auto const& v : x) {\n            if (v < 0.25)\n                bins[0]++;\n            else if (v < 0.5)\n                bins[1]++;\n            else if (v < 0.75)\n                bins[2]++;\n            else\n                bins[3]++;\n        }\n        std::vector<double> binsVec(bins.begin(), bins.end());\n        MPI_Send(&binsVec, 4, MPI_DOUBLE, 0, 1, MPI_COMM_WORLD);\n    }\n}",
            "std::vector<int> binCount(bins.size(), 0);\n\tstd::vector<int> binIndices(bins.size());\n\n\tsize_t n = x.size();\n\tsize_t nPerRank = n / MPI_Comm_size(MPI_COMM_WORLD);\n\n\tstd::vector<double> localX(nPerRank);\n\n\t// Gather local vector into a complete one\n\tMPI_Scatter(x.data(), nPerRank, MPI_DOUBLE, localX.data(), nPerRank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n\tfor (size_t i = 0; i < nPerRank; ++i) {\n\t\tint bin = -1;\n\n\t\tif (localX[i] <= 0.25) {\n\t\t\tbin = 0;\n\t\t}\n\t\telse if (localX[i] <= 0.5) {\n\t\t\tbin = 1;\n\t\t}\n\t\telse if (localX[i] <= 0.75) {\n\t\t\tbin = 2;\n\t\t}\n\t\telse if (localX[i] <= 1.0) {\n\t\t\tbin = 3;\n\t\t}\n\n\t\tbinCount[bin] += 1;\n\t}\n\n\t// Reduce counts\n\tMPI_Reduce(binCount.data(), binIndices.data(), binCount.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\t// Write results\n\tif (MPI_Comm_rank(MPI_COMM_WORLD) == 0) {\n\t\tfor (size_t i = 0; i < binIndices.size(); ++i) {\n\t\t\tbins[i] = static_cast<size_t>(binIndices[i]);\n\t\t}\n\t}\n}",
            "// TODO: Your code here\n}",
            "// your code here\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    size_t start = 0;\n    size_t end = x.size() / size;\n    size_t i = start;\n    for(; i < end; i++) {\n        if (x[i] >= 0 && x[i] <= 0.25)\n            bins[0]++;\n        else if (x[i] > 0.25 && x[i] <= 0.5)\n            bins[1]++;\n        else if (x[i] > 0.5 && x[i] <= 0.75)\n            bins[2]++;\n        else if (x[i] > 0.75 && x[i] <= 1.0)\n            bins[3]++;\n    }\n    if(rank == size - 1)\n        end = x.size();\n    else\n        end = end + rank * (x.size() / size);\n    for(; i < end; i++) {\n        if (x[i] >= 0 && x[i] <= 0.25)\n            bins[0]++;\n        else if (x[i] > 0.25 && x[i] <= 0.5)\n            bins[1]++;\n        else if (x[i] > 0.5 && x[i] <= 0.75)\n            bins[2]++;\n        else if (x[i] > 0.75 && x[i] <= 1.0)\n            bins[3]++;\n    }\n\n    int count;\n    MPI_Reduce(&bins, &count, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if(rank == 0) {\n        bins[0] = count[0];\n        bins[1] = count[1];\n        bins[2] = count[2];\n        bins[3] = count[3];\n    }\n}",
            "// TODO:\n    // Compute the fractional part of the double.\n    // Compute the bin number.\n    // Increment the bin count.\n    // Return the bin counts.\n\n    return;\n}",
            "// TO DO: Add MPI calls here\n\n  // TO DO: Fill in the results in the bins array here\n\n  return;\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int nproc;\n    MPI_Comm_size(comm, &nproc);\n    int procid;\n    MPI_Comm_rank(comm, &procid);\n    if (procid == 0) {\n        bins.fill(0);\n    }\n    std::vector<double> x_local = x;\n    MPI_Bcast(&x_local[0], x.size(), MPI_DOUBLE, 0, comm);\n    double min = std::numeric_limits<double>::max();\n    double max = std::numeric_limits<double>::min();\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        if (x[i] > max) {\n            max = x[i];\n        }\n    }\n    MPI_Bcast(&min, 1, MPI_DOUBLE, 0, comm);\n    MPI_Bcast(&max, 1, MPI_DOUBLE, 0, comm);\n    double binSize = (max - min) / 4.0;\n    std::vector<double> x_bins;\n    for (int i = 0; i < 4; i++) {\n        x_bins.push_back(min + i * binSize);\n    }\n    for (int i = 0; i < x_local.size(); i++) {\n        int idx = 0;\n        while (x_local[i] >= x_bins[idx]) {\n            idx++;\n        }\n        bins[idx - 1] += 1;\n    }\n    std::vector<size_t> bins_local = bins;\n    MPI_Reduce(&bins_local[0], &bins[0], 4, MPI_UNSIGNED, MPI_SUM, 0, comm);\n}",
            "int nproc = 0, rank = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (nproc < 2) {\n    printf(\"Error: Need at least 2 MPI ranks to perform the exercise.\\n\");\n    exit(1);\n  }\n\n  size_t size = x.size(), chunk = size / nproc, offset = rank * chunk;\n\n  if (rank == nproc - 1) {\n    chunk = size - chunk * (nproc - 1);\n  }\n\n  std::vector<double> x_proc(chunk);\n  for (int i = 0; i < chunk; ++i) {\n    x_proc[i] = x[offset + i];\n  }\n\n  bins.fill(0);\n  for (int i = 0; i < chunk; ++i) {\n    if (x_proc[i] < 0.25) {\n      bins[0]++;\n    }\n    else if (x_proc[i] < 0.5) {\n      bins[1]++;\n    }\n    else if (x_proc[i] < 0.75) {\n      bins[2]++;\n    }\n    else {\n      bins[3]++;\n    }\n  }\n\n  int status = 0;\n  MPI_Reduce(&bins[0], &bins[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i = 0; i < 4; ++i) {\n      printf(\"%llu, \", bins[i]);\n    }\n    printf(\"\\n\");\n  }\n  MPI_Finalize();\n}",
            "size_t n = x.size();\n    bins = std::array<size_t, 4>{0, 0, 0, 0};\n    std::vector<double> r_v(n);\n    MPI_Allreduce(x.data(), r_v.data(), n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    //count in each range\n    for (size_t i = 0; i < n; ++i) {\n        if (r_v[i] < 0.25)\n            bins[0]++;\n        else if (r_v[i] >= 0.25 && r_v[i] < 0.5)\n            bins[1]++;\n        else if (r_v[i] >= 0.5 && r_v[i] < 0.75)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "int world_size = 1;\n    int world_rank = 0;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    std::vector<double> x_chunk(x.begin(), x.end());\n    if (world_size == 1) {\n        bins[0] = std::count_if(x_chunk.begin(), x_chunk.end(),\n                                [](double x) { return x >= 0 && x < 0.25; });\n        bins[1] = std::count_if(x_chunk.begin(), x_chunk.end(),\n                                [](double x) { return x >= 0.25 && x < 0.5; });\n        bins[2] = std::count_if(x_chunk.begin(), x_chunk.end(),\n                                [](double x) { return x >= 0.5 && x < 0.75; });\n        bins[3] = std::count_if(x_chunk.begin(), x_chunk.end(),\n                                [](double x) { return x >= 0.75 && x < 1; });\n        return;\n    }\n\n    size_t chunk_size = x_chunk.size() / world_size;\n    std::vector<size_t> chunk_bins(4);\n\n    MPI_Scatter(x_chunk.data() + chunk_size * world_rank, chunk_size, MPI_DOUBLE,\n                chunk_bins.data(), chunk_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    bins[0] = std::count_if(chunk_bins.begin(), chunk_bins.end(),\n                            [](size_t x) { return x!= 0; });\n\n    std::vector<double> reduced_x(x.begin(), x.begin() + chunk_size);\n    std::vector<double> reduced_chunk(chunk_bins.begin(), chunk_bins.begin() + chunk_size);\n\n    MPI_Reduce(reduced_chunk.data(), reduced_x.data(), chunk_size, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    bins[0] = std::count_if(reduced_x.begin(), reduced_x.end(),\n                            [](double x) { return x >= 0 && x < 0.25; });\n\n    std::vector<double> reduced_chunk2(chunk_bins.begin() + chunk_size, chunk_bins.end());\n\n    MPI_Reduce(reduced_chunk2.data(), reduced_x.data() + chunk_size, chunk_size, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    bins[1] = std::count_if(reduced_x.begin() + chunk_size, reduced_x.end(),\n                            [](double x) { return x >= 0.25 && x < 0.5; });\n    bins[2] = std::count_if(reduced_x.begin() + chunk_size * 2, reduced_x.end(),\n                            [](double x) { return x >= 0.5 && x < 0.75; });\n    bins[3] = std::count_if(reduced_x.begin() + chunk_size * 3, reduced_x.end(),\n                            [](double x) { return x >= 0.75 && x < 1; });\n}",
            "MPI_Barrier(MPI_COMM_WORLD);\n\n  // \n  MPI_Datatype mpi_dbl_t;\n  MPI_Type_contiguous(sizeof(double), MPI_BYTE, &mpi_dbl_t);\n  MPI_Type_commit(&mpi_dbl_t);\n\n  // Get the number of processes in MPI_COMM_WORLD\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  // Get the rank of this process in MPI_COMM_WORLD\n  int world_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  // Figure out the number of doubles per process\n  int doubles_per_proc = x.size() / world_size;\n\n  // Make a vector for each process\n  std::vector<double> x_vec(x.begin() + world_rank * doubles_per_proc, x.begin() + (world_rank + 1) * doubles_per_proc);\n\n  // Make a vector for each process with the counts\n  std::vector<size_t> counts(4);\n\n  // Count the number of doubles that fall in each category\n  for (int i = 0; i < x_vec.size(); ++i) {\n    if (x_vec[i] < 0.25) {\n      counts[0]++;\n    } else if (x_vec[i] < 0.5) {\n      counts[1]++;\n    } else if (x_vec[i] < 0.75) {\n      counts[2]++;\n    } else {\n      counts[3]++;\n    }\n  }\n\n  // Gather the counts on rank 0\n  MPI_Gather(&counts, 4, mpi_dbl_t, &bins, 4, mpi_dbl_t, 0, MPI_COMM_WORLD);\n\n  // Print the output on rank 0\n  if (world_rank == 0) {\n    std::cout << \"input: \";\n    for (auto const& i : x) {\n      std::cout << i << \", \";\n    }\n    std::cout << \"\\noutput: [\";\n    for (auto const& i : bins) {\n      std::cout << i << \", \";\n    }\n    std::cout << \"]\" << std::endl;\n  }\n\n  MPI_Type_free(&mpi_dbl_t);\n  MPI_Barrier(MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n  int numprocs, proc_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);\n  int local_num = x.size()/numprocs;\n  int reminder = x.size() % numprocs;\n  int start = proc_rank * local_num;\n  int end = start + local_num;\n  if (proc_rank == numprocs - 1) {\n    end = end + reminder;\n  }\n  size_t b0 = 0, b1 = 0, b2 = 0, b3 = 0;\n\n  if (proc_rank == 0) {\n    for (int i = 0; i < end; i++) {\n      if (x[i] < 0.25) {\n        b0 = b0 + 1;\n      } else if (x[i] >= 0.25 && x[i] < 0.5) {\n        b1 = b1 + 1;\n      } else if (x[i] >= 0.5 && x[i] < 0.75) {\n        b2 = b2 + 1;\n      } else if (x[i] >= 0.75 && x[i] <= 1) {\n        b3 = b3 + 1;\n      }\n    }\n  }\n  MPI_Reduce(&b0, &bins[0], 1, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n  MPI_Reduce(&b1, &bins[1], 1, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n  MPI_Reduce(&b2, &bins[2], 1, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n  MPI_Reduce(&b3, &bins[3], 1, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "//TODO: Your code here\n    int rank, size;\n    double fractional_part, integral_part;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    std::vector<int> quartiles(4);\n\n    for(int i = 0; i < x.size(); i++)\n    {\n        fractional_part = modf(x[i], &integral_part);\n\n        if(fractional_part > 0.25 && fractional_part <= 0.5)\n        {\n            quartiles[1]++;\n        }\n        else if(fractional_part > 0.5 && fractional_part <= 0.75)\n        {\n            quartiles[2]++;\n        }\n        else if(fractional_part > 0.75 && fractional_part <= 1)\n        {\n            quartiles[3]++;\n        }\n        else\n        {\n            quartiles[0]++;\n        }\n    }\n\n    if(rank == 0)\n    {\n        for(int i = 0; i < quartiles.size(); i++)\n        {\n            bins[i] = 0;\n            for(int j = 0; j < size; j++)\n            {\n                bins[i] += quartiles[i];\n            }\n            quartiles[i] = 0;\n        }\n    }\n    MPI_Gather(quartiles.data(), 4, MPI_INT, bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement me!\n  MPI_Comm comm = MPI_COMM_WORLD;\n  int rank;\n  MPI_Comm_rank(comm, &rank);\n  int size;\n  MPI_Comm_size(comm, &size);\n\n  size_t total_size = x.size();\n  std::vector<size_t> start_indices(size+1);\n\n  if(rank == 0){\n    start_indices[0] = 0;\n  }\n\n  MPI_Scatter(x.data(), 1, MPI_UNSIGNED_LONG_LONG, start_indices.data()+1, 1, MPI_UNSIGNED_LONG_LONG, 0, comm);\n\n  for(int i = 0; i < size; ++i){\n    std::pair<double, double> range = std::make_pair(start_indices[i], start_indices[i+1]);\n    bins[0] += std::count_if(x.begin()+start_indices[i], x.begin()+start_indices[i+1], [&](double d){return d < range.first+0.25;});\n    bins[1] += std::count_if(x.begin()+start_indices[i], x.begin()+start_indices[i+1], [&](double d){return d >= range.first+0.25 && d < range.first+0.5;});\n    bins[2] += std::count_if(x.begin()+start_indices[i], x.begin()+start_indices[i+1], [&](double d){return d >= range.first+0.5 && d < range.first+0.75;});\n    bins[3] += std::count_if(x.begin()+start_indices[i], x.begin()+start_indices[i+1], [&](double d){return d >= range.first+0.75 && d < range.first+1;});\n  }\n\n  if(rank == 0){\n    std::vector<size_t> all_bins(size*4);\n    MPI_Gather(bins.data(), 4, MPI_UNSIGNED_LONG_LONG, all_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, comm);\n    if(size > 0){\n      for(int i = 0; i < 4; ++i){\n        bins[i] = 0;\n        for(int j = 0; j < size; ++j){\n          bins[i] += all_bins[i+j*4];\n        }\n      }\n    }\n  }\n  else{\n    MPI_Gather(bins.data(), 4, MPI_UNSIGNED_LONG_LONG, nullptr, 0, MPI_UNSIGNED_LONG_LONG, 0, comm);\n  }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n\tint rank, size;\n\tMPI_Comm_rank(comm, &rank);\n\tMPI_Comm_size(comm, &size);\n\n\t// split the data in equal chunks\n\tint splitSize = x.size() / size;\n\tint remainder = x.size() % size;\n\tstd::vector<double> localPart;\n\tif (rank == 0) {\n\t\tlocalPart.insert(localPart.end(), x.begin(), x.begin() + splitSize + remainder);\n\t}\n\telse {\n\t\tlocalPart.resize(splitSize);\n\t}\n\n\t// do the counting\n\tsize_t totalBins[4];\n\tstd::fill(totalBins, totalBins + 4, 0);\n\n\tif (rank == 0) {\n\t\tMPI_Gather(&localPart[0], splitSize + remainder, MPI_DOUBLE, &totalBins[0], splitSize + remainder, MPI_DOUBLE, 0, comm);\n\t}\n\telse {\n\t\tMPI_Gather(&localPart[0], splitSize, MPI_DOUBLE, &totalBins[0], splitSize, MPI_DOUBLE, 0, comm);\n\t}\n\n\t// all counts are now on rank 0\n\tif (rank == 0) {\n\t\tbins[0] = 0;\n\t\tbins[1] = 0;\n\t\tbins[2] = 0;\n\t\tbins[3] = 0;\n\t\tfor (size_t i = 0; i < x.size(); i++) {\n\t\t\tint binIndex = 3;\n\t\t\tif (x[i] >= 0.75) {\n\t\t\t\tbinIndex = 0;\n\t\t\t}\n\t\t\telse if (x[i] >= 0.5) {\n\t\t\t\tbinIndex = 1;\n\t\t\t}\n\t\t\telse if (x[i] >= 0.25) {\n\t\t\t\tbinIndex = 2;\n\t\t\t}\n\t\t\ttotalBins[binIndex] += 1;\n\t\t}\n\t\t// output the bins on rank 0\n\t\tstd::copy(totalBins, totalBins + 4, bins.begin());\n\t}\n}",
            "}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    std::array<size_t, 4> tmp = {0, 0, 0, 0};\n\n    size_t num_elements = x.size();\n    size_t num_per_process = num_elements / size;\n    size_t remainder = num_elements % size;\n    size_t start = rank * num_per_process;\n    size_t end = (rank + 1) * num_per_process;\n\n    if(rank == size - 1) {\n        end = num_elements;\n    }\n\n    if(rank == 0) {\n        tmp[0] = std::count_if(x.begin(), x.begin() + num_per_process, [](auto a) { return a < 0.25; });\n        tmp[1] = std::count_if(x.begin() + num_per_process, x.begin() + num_per_process + remainder, [](auto a) { return a < 0.25; });\n    }\n\n    std::vector<double> temp(x.begin() + start, x.begin() + end);\n\n    if(rank == 0) {\n        std::array<size_t, 4> total_bins{0, 0, 0, 0};\n        MPI_Reduce(MPI_IN_PLACE, total_bins.data(), 4, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n        bins = total_bins;\n    } else {\n        MPI_Reduce(temp.data(), tmp.data(), 4, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n        bins = tmp;\n    }\n}",
            "// TODO\n}",
            "int rank, nproc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    // get local count\n    int local_count = 0;\n    for (auto xi: x) {\n        if (xi >= 0.0 && xi < 0.25)\n            local_count++;\n        else if (xi >= 0.25 && xi < 0.5)\n            local_count++;\n        else if (xi >= 0.5 && xi < 0.75)\n            local_count++;\n        else if (xi >= 0.75 && xi < 1.0)\n            local_count++;\n    }\n\n    // get global count\n    int global_count;\n    MPI_Allreduce(&local_count, &global_count, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    // find local indices of quartiles\n    std::vector<int> indices;\n    int n = (int)x.size();\n    int local_n = n/nproc;\n    int start = rank*local_n;\n    int end = start+local_n;\n    for (int i = start; i < end; ++i) {\n        if (x[i] >= 0.0 && x[i] < 0.25)\n            indices.push_back(i);\n        else if (x[i] >= 0.25 && x[i] < 0.5)\n            indices.push_back(i);\n        else if (x[i] >= 0.5 && x[i] < 0.75)\n            indices.push_back(i);\n        else if (x[i] >= 0.75 && x[i] < 1.0)\n            indices.push_back(i);\n    }\n\n    // get bins\n    if (rank == 0) {\n        for (int i = 0; i < 4; ++i) {\n            bins[i] = 0;\n            for (auto index: indices) {\n                if (x[index] >= 0.0 && x[index] < 0.25)\n                    bins[i]++;\n                else if (x[index] >= 0.25 && x[index] < 0.5)\n                    bins[i]++;\n                else if (x[index] >= 0.5 && x[index] < 0.75)\n                    bins[i]++;\n                else if (x[index] >= 0.75 && x[index] < 1.0)\n                    bins[i]++;\n            }\n        }\n    }\n}",
            "size_t n = x.size();\n\n    // Checks if x is a vector\n    if (!(n > 0)) {\n        std::cerr << \"Error: Empty vector\\n\";\n        return;\n    }\n\n    // Check if x is sorted\n    if (!std::is_sorted(x.begin(), x.end())) {\n        std::cerr << \"Error: Vector x is not sorted\\n\";\n        return;\n    }\n\n    // Determine the number of processes\n    int mpi_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\n    // Determine the current process rank\n    int mpi_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    // Determine the number of elements per process\n    size_t elements_per_process = n / mpi_size;\n\n    // Checks if x is an even vector\n    if (n % 2!= 0) {\n        std::cerr << \"Error: Vector x is not an even vector\\n\";\n        return;\n    }\n\n    // Create a vector with the elements that each process will calculate\n    std::vector<double> x_process;\n    if (mpi_rank == mpi_size - 1) {\n        x_process = x.begin() + elements_per_process * mpi_rank;\n    } else {\n        x_process = x.begin() + elements_per_process * mpi_rank;\n        x_process.resize(elements_per_process);\n    }\n\n    // Count the fractional part of the elements\n    size_t fractional_part = 0;\n    for (auto it = x_process.begin(); it!= x_process.end(); ++it) {\n        if (*it < 0.25) {\n            fractional_part += 2;\n        } else if (*it < 0.5) {\n            fractional_part += 1;\n        } else if (*it < 0.75) {\n            fractional_part += 0;\n        } else {\n            fractional_part += 3;\n        }\n    }\n\n    // Calculate the sum of the fractional parts\n    std::vector<double> sum_fractional_part(mpi_size, 0);\n    MPI_Allreduce(&fractional_part, &sum_fractional_part[0], 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    // Determine the number of elements of each rank\n    std::vector<int> elements_per_rank(mpi_size, 0);\n    elements_per_rank[0] = sum_fractional_part[0];\n    for (int i = 1; i < mpi_size; i++) {\n        elements_per_rank[i] = elements_per_rank[i - 1] + sum_fractional_part[i];\n    }\n\n    // Calculate the bins of the fractional parts\n    for (int i = 0; i < mpi_size; i++) {\n        bins[i] = elements_per_rank[i];\n        for (auto it = x.begin() + elements_per_rank[i - 1]; it!= x.begin() + elements_per_rank[i]; ++it) {\n            if (*it < 0.25) {\n                bins[i] += 2;\n            } else if (*it < 0.5) {\n                bins[i] += 1;\n            } else if (*it < 0.75) {\n                bins[i] += 0;\n            } else {\n                bins[i] += 3;\n            }\n        }\n    }\n\n    // Check if the result is correct\n    std::array<size_t, 4> expected_bins{2, 1, 2, 2};\n    if (bins!= expected_bins) {\n        std::cerr << \"Error: The result is not correct\\n\";\n        return;\n    }\n}",
            "// TODO\n    size_t n = x.size();\n    std::vector<double> myx(x);\n    std::sort(myx.begin(), myx.end());\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if(rank == 0){\n        bins.fill(0);\n        std::array<double, 4> quart{0, 0.25, 0.5, 0.75};\n        std::vector<double> myx2;\n        size_t i = 0, j = 0;\n        for(size_t k = 0; k < n; ++k){\n            if(myx[k] > quart[j]){\n                i++;\n                if(j < 3){\n                    ++j;\n                }\n                else{\n                    ++bins[j];\n                }\n            }\n            else{\n                ++bins[j];\n            }\n        }\n        MPI_Reduce(&bins[0], &bins[0], 4, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n    else{\n        MPI_Reduce(&myx[0], &myx2[0], n, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n        std::array<double, 4> quart{0, 0.25, 0.5, 0.75};\n        size_t i = 0, j = 0;\n        for(size_t k = 0; k < n; ++k){\n            if(myx2[k] > quart[j]){\n                i++;\n                if(j < 3){\n                    ++j;\n                }\n                else{\n                    ++bins[j];\n                }\n            }\n            else{\n                ++bins[j];\n            }\n        }\n    }\n\n}",
            "// TODO: implement\n}",
            "/* Your code here.\n       See lab_05.cpp for a hint.\n       Note that you cannot use any std::vector methods.\n       Note that you must compute the number of values in the four\n       different bins separately.\n       Note that x.size() must be an even number.\n    */\n\n    // TODO: compute bins on all ranks\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: sum the counts on rank 0\n\n    // TODO: broadcast bins to all ranks\n}",
            "if (x.empty()) return;\n\n    // find the size of each bins\n    size_t size = x.size() / 4;\n    // the rest is assigned to the first bin\n    size_t rest = x.size() % 4;\n    bins[0] = rest + size;\n    for (int i = 1; i < 4; ++i) {\n        bins[i] = size;\n    }\n\n    // MPI_Allreduce to get the sum of all the bins from all ranks\n    std::vector<size_t> global_bins(4, 0);\n    MPI_Allreduce(bins.data(), global_bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    // divide each bins by the total size to get the fractions\n    for (int i = 0; i < 4; ++i) {\n        bins[i] = (double)global_bins[i] / x.size();\n    }\n\n    // compute the quartiles\n    std::vector<double> quartiles;\n    for (int i = 0; i < 4; ++i) {\n        quartiles.push_back(x[global_bins[i] - 1]);\n    }\n    std::nth_element(x.begin(), x.begin() + global_bins[0] - 1, x.end());\n    for (int i = 0; i < 4; ++i) {\n        quartiles.push_back(*(x.begin() + global_bins[i] - 1));\n    }\n\n    // get the quartiles of all ranks\n    std::vector<double> global_quartiles(4);\n    MPI_Allreduce(quartiles.data(), global_quartiles.data(), 4, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    // compute the median\n    double median = (global_quartiles[0] + global_quartiles[2]) / 2.0;\n\n    // output the result\n    if (rank == 0) {\n        std::cout << median << '\\n';\n        for (int i = 0; i < 4; ++i) {\n            std::cout << global_quartiles[i] <<'';\n        }\n        std::cout << '\\n';\n    }\n}",
            "// your code goes here\n\t\n}",
            "// TODO: Your code here\n\n}",
            "size_t n = x.size();\n\n  // calculate quartiles\n  std::array<double, 4> quartiles = {x[n / 4], x[n / 4 + n / 8], x[n / 4 + n / 8 + n / 16], x[n / 4 + n / 8 + n / 16 + n / 32]};\n\n  // determine which ranks have the quartiles\n  std::array<int, 4> ranks = {0, 0, 0, 0};\n  for (int i = 0; i < 4; ++i) {\n    for (int j = 0; j < n; ++j) {\n      if (x[j] == quartiles[i]) {\n        ranks[i] = j / n;\n        break;\n      }\n    }\n  }\n\n  // determine which ranks are necessary to communicate with\n  int requiredRanks[4] = {0, 0, 0, 0};\n  for (int i = 0; i < 4; ++i) {\n    int rank = (i + 1) % 4;\n    int rank1 = (i + 3) % 4;\n    if (quartiles[i] < quartiles[rank] && quartiles[rank] < quartiles[rank1]) {\n      requiredRanks[i] = ranks[rank];\n    }\n  }\n\n  // determine which ranks are necessary to communicate with\n  std::array<int, 4> sendRanks = {0, 0, 0, 0};\n  std::array<int, 4> recvRanks = {0, 0, 0, 0};\n  int mpiRanks[4] = {0, 0, 0, 0};\n  for (int i = 0; i < 4; ++i) {\n    int rank = (i + 1) % 4;\n    int rank1 = (i + 3) % 4;\n    if (requiredRanks[i] > ranks[rank] && requiredRanks[i] < ranks[rank1]) {\n      sendRanks[i] = ranks[i];\n      recvRanks[i] = requiredRanks[i];\n      mpiRanks[i] = ranks[i];\n    }\n  }\n\n  // gather counts in order of ranks (0, 1, 2, 3)\n  for (int i = 0; i < 4; ++i) {\n    int size = 0;\n    for (int j = 0; j < 4; ++j) {\n      size += j == i;\n    }\n    std::vector<size_t> counts(size);\n    int currentRank = 0;\n    MPI_Gather(&bins[currentRank], 1, MPI_UNSIGNED_LONG_LONG, counts.data(), 1, MPI_UNSIGNED_LONG_LONG, i, MPI_COMM_WORLD);\n    if (i == mpiRanks[0]) {\n      bins[0] = 0;\n    }\n    for (int j = 0; j < size; ++j) {\n      bins[i] += counts[j];\n    }\n  }\n\n  // scatter counts to appropriate ranks\n  for (int i = 0; i < 4; ++i) {\n    int rank = (i + 1) % 4;\n    int rank1 = (i + 3) % 4;\n    if (sendRanks[i]!= 0) {\n      std::vector<size_t> sendCounts(4);\n      for (int j = 0; j < 4; ++j) {\n        sendCounts[j] = j == i? bins[sendRanks[i]] : 0;\n      }\n      std::vector<size_t> recvCounts(4);\n      MPI_Scatter(sendCounts.data(), 1, MPI_UNSIGNED_LONG_LONG, recvCounts.data(), 1, MPI_UNSIGNED_LONG_LONG, mpiRanks[i], MPI_COMM_WORLD);\n      for (int j =",
            "assert(0 < x.size());\n  bins.fill(0);\n\n  int rank;\n  int nproc;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (nproc > 1) {\n\n    size_t const n = x.size();\n\n    std::vector<size_t> count(nproc);\n    std::vector<size_t> idx(n);\n\n    std::iota(idx.begin(), idx.end(), 0);\n\n    size_t const r = n % nproc;\n    size_t const c = n / nproc;\n\n    for (int i = 0; i < n; ++i) {\n      count[rank] += x[i] - floor(x[i]) < 0.25;\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, count.data(), nproc, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    for (int i = 0; i < nproc; ++i) {\n      idx[i * c + r] += count[i];\n    }\n\n    MPI_Allgather(idx.data(), nproc, MPI_INT, bins.data(), nproc, MPI_INT, MPI_COMM_WORLD);\n\n  } else {\n\n    for (int i = 0; i < x.size(); ++i) {\n      bins[0] += x[i] - floor(x[i]) < 0.25;\n      bins[1] += x[i] - floor(x[i]) < 0.5;\n      bins[2] += x[i] - floor(x[i]) < 0.75;\n      bins[3] += x[i] - floor(x[i]) < 1;\n    }\n  }\n\n  return;\n}",
            "// Fill in your code here\n    return;\n}",
            "// TODO: add your code here\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "cuda",
        "prompt": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\n__global__ void countQuartiles(const double *x, size_t N, size_t[4] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "/*\n       HINT:\n       The `int` variable `i` should take on values [0, 1, 2, 3].\n       The thread with index `threadIdx.x` should look at the\n       `threadIdx.x`-th entry of `x` and compute which `i`\n       the value belongs to (i.e. the index in `bins`).\n       After that, the thread should increment the appropriate\n       entry of `bins` by 1.\n\n       Example:\n       Suppose the `x` passed in is [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8].\n       Suppose threadIdx.x is 2.\n       Then threadIdx.x-th element of x is 7.8, which belongs to index 2 (because 7.8 >= 0.25 and 7.8 < 0.5).\n       So we should increment the second element of `bins`.\n       So if `bins` starts out as [0, 0, 0, 0], it will end up as [0, 1, 2, 0].\n    */\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    for (size_t i = 0; i < N; ++i) {\n        double tmp = x[i] - (int)x[i];\n        if (tmp >= 0 && tmp < 0.25)\n            atomicAdd(&bins[0], 1);\n        else if (tmp >= 0.25 && tmp < 0.5)\n            atomicAdd(&bins[1], 1);\n        else if (tmp >= 0.5 && tmp < 0.75)\n            atomicAdd(&bins[2], 1);\n        else if (tmp >= 0.75 && tmp <= 1)\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "//TODO\n}",
            "const double step = 0.25;\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) {\n        return;\n    }\n    double val = x[tid];\n    size_t idx = (size_t) (floorf(val / step));\n    atomicAdd(&bins[idx], 1);\n}",
            "// TODO: implement\n}",
            "// TODO\n  //...\n}",
            "// TODO: Implement me\n\n\t// You should only need 1 thread per double in the input vector x.\n\t// Note that you can assume N is divisible by 4.\n\n\t// The global thread index is the index of the double in the vector x\n\t// that the thread will compute the value of.\n\tsize_t i = blockDim.x * blockIdx.x + threadIdx.x;\n\tsize_t index = i / 4;\n\n\t// Each thread is responsible for computing one bin.\n\t// The thread id indicates which bin is being computed.\n\t// The number of bins is 4.\n\tint id = threadIdx.x;\n\n\tif (index < N) {\n\t\tdouble fract = modf(x[index], &x[index]);\n\t\tif (fract <= 0.25)\n\t\t\tatomicAdd(&bins[0], 1);\n\t\telse if (fract <= 0.5)\n\t\t\tatomicAdd(&bins[1], 1);\n\t\telse if (fract <= 0.75)\n\t\t\tatomicAdd(&bins[2], 1);\n\t\telse\n\t\t\tatomicAdd(&bins[3], 1);\n\t}\n\n}",
            "// Initialize bins to 0\n    for (size_t i = 0; i < 4; i++) {\n        bins[i] = 0;\n    }\n    // TODO:\n    // Your code here\n    // x[0] is a double number\n    // bins[i] is a size_t number\n    // 1. get the first element of the vector x\n    // 2. use if-else statements to categorize the value of x[0] into four categories\n    // 3. increment the index of the appropriate category in the bins array\n    // 4. loop through the rest of the elements in the vector and repeat step 2 and 3\n\n    // 1. Get the first element of the vector\n    // 2. Use if-else statements to categorize the value of x[0] into four categories\n    // 3. Increment the index of the appropriate category in the bins array\n    if (blockIdx.x == 0 && threadIdx.x == 0) {\n        if (x[0] >= 0 && x[0] < 0.25) {\n            atomicAdd(&bins[0], 1);\n        }\n        else if (x[0] >= 0.25 && x[0] < 0.5) {\n            atomicAdd(&bins[1], 1);\n        }\n        else if (x[0] >= 0.5 && x[0] < 0.75) {\n            atomicAdd(&bins[2], 1);\n        }\n        else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n    // 4. Loop through the rest of the elements in the vector and repeat step 2 and 3\n    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        if (x[i] >= 0 && x[i] < 0.25) {\n            atomicAdd(&bins[0], 1);\n        }\n        else if (x[i] >= 0.25 && x[i] < 0.5) {\n            atomicAdd(&bins[1], 1);\n        }\n        else if (x[i] >= 0.5 && x[i] < 0.75) {\n            atomicAdd(&bins[2], 1);\n        }\n        else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    bins[0] += (x[tid] % 1.0 < 0.25)? 1 : 0;\n    bins[1] += (x[tid] % 1.0 >= 0.25 && x[tid] % 1.0 < 0.5)? 1 : 0;\n    bins[2] += (x[tid] % 1.0 >= 0.5 && x[tid] % 1.0 < 0.75)? 1 : 0;\n    bins[3] += (x[tid] % 1.0 >= 0.75)? 1 : 0;\n  }\n}",
            "const size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    const size_t stride = blockDim.x * gridDim.x;\n    size_t i, j, bin;\n\n    for (i = tid; i < N; i += stride) {\n        if (x[i] >= 0 && x[i] < 0.25) {\n            bin = 0;\n        } else if (x[i] >= 0.25 && x[i] < 0.5) {\n            bin = 1;\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            bin = 2;\n        } else if (x[i] >= 0.75 && x[i] < 1) {\n            bin = 3;\n        } else {\n            bin = 4;\n        }\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO: YOUR CODE HERE\n}",
            "// TODO\n}",
            "size_t threadIndex = threadIdx.x + blockIdx.x * blockDim.x;\n    size_t blockIndex = blockIdx.x + blockIdx.y * gridDim.x;\n    size_t gridSize = gridDim.x * blockDim.x;\n    // your code here\n    size_t i;\n    for(i = threadIndex; i < N; i+=gridSize){\n        bins[int(x[i]*4)] += 1;\n    }\n}",
            "// Replace this statement with a block-level parallel for loop\n   // using N threads.\n   __syncthreads();\n}",
            "// TODO: Implement your code here\n}",
            "__shared__ size_t sBins[4];\n\n\tif (threadIdx.x == 0) {\n\t\tsBins[0] = 0;\n\t\tsBins[1] = 0;\n\t\tsBins[2] = 0;\n\t\tsBins[3] = 0;\n\t}\n\t__syncthreads();\n\n\t// TODO\n\n\tsize_t threadBlockSize = blockDim.x;\n\tint threadId = threadIdx.x;\n\tint threadBlockId = blockIdx.x;\n\n\t// if (threadId < N) {\n\t// \tif (x[threadId] < 0.25) sBins[0] += 1;\n\t// \tif (x[threadId] < 0.5) sBins[1] += 1;\n\t// \tif (x[threadId] < 0.75) sBins[2] += 1;\n\t// \tif (x[threadId] < 1.0) sBins[3] += 1;\n\t// }\n\n\tfor (size_t i = threadId; i < N; i += threadBlockSize) {\n\t\tsize_t idx = (x[i] < 0.25) + (x[i] < 0.5) + (x[i] < 0.75) + (x[i] < 1.0);\n\t\tsBins[idx] += 1;\n\t}\n\t__syncthreads();\n\n\tif (threadId == 0) {\n\t\tfor (int i = 1; i < threadBlockSize; i++) {\n\t\t\tsBins[0] += sBins[i];\n\t\t}\n\t\tfor (int i = 1; i < 4; i++) {\n\t\t\tbins[i - 1] = sBins[i];\n\t\t}\n\t}\n\n}",
            "/*\n\t__shared__ int shared_counts[4];\n\n\tint tid = threadIdx.x;\n\tint start_index = blockIdx.x * N + tid;\n\tint stride = blockDim.x * gridDim.x;\n\t\n\t// Fill the shared array\n\tfor (int i = start_index; i < N; i += stride)\n\t{\n\t\tint idx = (int) (x[i] * 4);\n\t\tshared_counts[idx]++;\n\t}\n\n\t// Each thread writes to one element of the global array\n\tfor (int i = 0; i < 4; i++)\n\t{\n\t\tbins[i] = shared_counts[i];\n\t}\n\t*/\n\t__shared__ int s_count[4];\n\t\n\t// Compute the starting and ending point of the array segment\n\t// for the current thread\n\tint i = threadIdx.x + blockDim.x * blockIdx.x;\n\tint stride = gridDim.x * blockDim.x;\n\t\n\tfor (; i < N; i += stride) {\n\t\tint idx = (int) (x[i] * 4);\n\t\tatomicAdd(&s_count[idx], 1);\n\t}\n\n\t__syncthreads();\n\t\n\t// Each thread writes to one element of the global array\n\tif (threadIdx.x == 0)\n\t{\n\t\tfor (int j = 0; j < 4; j++)\n\t\t{\n\t\t\tbins[j] = s_count[j];\n\t\t}\n\t}\n}",
            "const int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        const double val = x[i];\n        const double frac = val - (int) val;\n        if (frac < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (frac < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (frac < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "__shared__ size_t sm_bins[4];\n\n  // TODO: Your code here\n\n  sm_bins[threadIdx.x] = 0;\n  __syncthreads();\n\n  if (threadIdx.x == 0) {\n    for (size_t i = 0; i < N; i++) {\n      if (x[i] >= 0 && x[i] < 0.25) {\n        atomicAdd(&(sm_bins[0]), 1);\n      } else if (x[i] >= 0.25 && x[i] < 0.5) {\n        atomicAdd(&(sm_bins[1]), 1);\n      } else if (x[i] >= 0.5 && x[i] < 0.75) {\n        atomicAdd(&(sm_bins[2]), 1);\n      } else if (x[i] >= 0.75 && x[i] <= 1) {\n        atomicAdd(&(sm_bins[3]), 1);\n      }\n    }\n  }\n\n  __syncthreads();\n\n  for (size_t i = 0; i < 4; i++) {\n    atomicAdd(&(bins[i]), sm_bins[i]);\n  }\n}",
            "/*\n       Add your kernel here.\n\n       You will need to create an array of size N*4 and store the fractional parts\n       of the values of x in that array.\n\n       The fractional part of a value x is computed as x - floor(x).\n\n       You will then need to launch a CUDA kernel that will iterate over the\n       elements of the array and use atomicAdd to increment the value in the \n       bins array corresponding to the index in the array.\n    */\n\n    return;\n}",
            "// Fill in your code here\n  int tID = threadIdx.x;\n  if(tID < N){\n    int i = floor((x[tID]-0.25)/0.5);\n    if(i == -2) i = 0;\n    if(i == 4) i = 3;\n    atomicAdd(&bins[i], 1);\n  }\n}",
            "// TODO\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int i = floor(x[tid]*4);\n        atomicAdd(&bins[i], 1);\n    }\n}",
            "// TODO\n}",
            "int tid = threadIdx.x;\n    int i = tid;\n    int j = 0;\n    int start = i * N;\n    int k = 0;\n    int quartiles = 4;\n\n    while (i < quartiles) {\n        if (x[start + j] < 0.25) {\n            bins[k] = 0;\n        } else if (x[start + j] >= 0.25 && x[start + j] < 0.5) {\n            bins[k] = 1;\n        } else if (x[start + j] >= 0.5 && x[start + j] < 0.75) {\n            bins[k] = 2;\n        } else if (x[start + j] >= 0.75) {\n            bins[k] = 3;\n        }\n        j++;\n        if (j == N) {\n            k++;\n            j = 0;\n        }\n        i++;\n    }\n}",
            "size_t i = threadIdx.x;\n    if (i >= N) return;\n    double frac = fmod(x[i], 1.0);\n    bins[frac < 0.25]++;\n    bins[frac >= 0.25 && frac < 0.5]++;\n    bins[frac >= 0.5 && frac < 0.75]++;\n    bins[frac >= 0.75]++;\n}",
            "//TODO\n}",
            "// Get thread ID\n    int thread_id = threadIdx.x + blockDim.x * blockIdx.x;\n\n    // Thread with 0 index is responsible for storing the counts in bins\n    if (thread_id == 0) {\n        // Get the start of the array\n        double *start_array = (double *)x;\n\n        // Compute the number of threads in the block\n        int block_num_threads = blockDim.x * gridDim.x;\n\n        // Initialize the bins\n        bins[0] = 0;\n        bins[1] = 0;\n        bins[2] = 0;\n        bins[3] = 0;\n\n        // Count the quartiles\n        for (int i = thread_id; i < N; i += block_num_threads) {\n            double number = start_array[i];\n\n            // Find the number of the quartile it belongs to\n            if (number >= 0.75)\n                bins[3] += 1;\n            else if (number >= 0.5)\n                bins[2] += 1;\n            else if (number >= 0.25)\n                bins[1] += 1;\n            else\n                bins[0] += 1;\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n\tif (i >= N)\n\t\treturn;\n\tif (i < 0)\n\t\treturn;\n\n\tbins[0] += (double)(x[i] - (int)(x[i])) <= 0.25;\n\tbins[1] += (double)(x[i] - (int)(x[i])) <= 0.5;\n\tbins[2] += (double)(x[i] - (int)(x[i])) <= 0.75;\n\tbins[3] += (double)(x[i] - (int)(x[i])) <= 1.0;\n\n}",
            "// Insert code here\n\n}",
            "/*\n       TODO: Your code here.\n\n       Note that the vector `x` has size N, but the vector `bins` has\n       size 4, so you should avoid writing beyond the last element of `bins`.\n    */\n    int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i > N) return;\n    if (i == 0) {\n        bins[0] = 0;\n        bins[1] = 0;\n        bins[2] = 0;\n        bins[3] = 0;\n    }\n\n    if (i <= N) {\n        if (x[i] >= 0 && x[i] < 0.25) {\n            bins[0] += 1;\n        } else if (x[i] >= 0.25 && x[i] < 0.5) {\n            bins[1] += 1;\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            bins[2] += 1;\n        } else if (x[i] >= 0.75 && x[i] < 1.0) {\n            bins[3] += 1;\n        }\n    }\n}",
            "/*\n    * TODO\n    *\n    *\n    */\n    return;\n}",
            "for (size_t i = 0; i < 4; ++i) {\n    bins[i] = 0;\n  }\n  __syncthreads();\n  for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n    int i1 = i * 100;\n    int i2 = (i + 1) * 100;\n    if ((x[i] - i1) <= 0) {\n      if ((i1 - x[i]) <= 0.25) {\n        atomicAdd(&(bins[0]), 1);\n      }\n      else if ((i1 - x[i]) <= 0.5) {\n        atomicAdd(&(bins[1]), 1);\n      }\n      else if ((i1 - x[i]) <= 0.75) {\n        atomicAdd(&(bins[2]), 1);\n      }\n      else {\n        atomicAdd(&(bins[3]), 1);\n      }\n    }\n    else {\n      if ((i2 - x[i]) <= 0.25) {\n        atomicAdd(&(bins[0]), 1);\n      }\n      else if ((i2 - x[i]) <= 0.5) {\n        atomicAdd(&(bins[1]), 1));\n      }\n      else if ((i2 - x[i]) <= 0.75) {\n        atomicAdd(&(bins[2]), 1));\n      }\n      else {\n        atomicAdd(&(bins[3]), 1));\n      }\n    }\n  }\n}",
            "// TODO\n}",
            "int tid = threadIdx.x;\n\n    if (tid < N) {\n        size_t idx = floor(x[tid] * 4);\n        if (idx < 4) {\n            atomicAdd(&bins[idx], 1);\n        }\n    }\n}",
            "int threadId = threadIdx.x + blockIdx.x * blockDim.x;\n  int blockSize = blockDim.x * gridDim.x;\n\n  int i = threadId;\n  int b = 0;\n  int bin_count = 0;\n\n  while (i < N) {\n    if ((x[i] - int(x[i])) < 0.25) {\n      b = 0;\n      bin_count++;\n    } else if ((x[i] - int(x[i])) < 0.5) {\n      b = 1;\n      bin_count++;\n    } else if ((x[i] - int(x[i])) < 0.75) {\n      b = 2;\n      bin_count++;\n    } else {\n      b = 3;\n      bin_count++;\n    }\n    i += blockSize;\n  }\n\n  __shared__ int shm[32];\n  shm[threadId] = bin_count;\n  __syncthreads();\n\n  // Reduction\n  int warp = threadIdx.x / warpSize;\n  for (int i = 1; i < blockDim.x / warpSize; i++) {\n    shm[warp] += shm[warp * warpSize + i];\n  }\n  __syncthreads();\n  //\n\n  if (threadId == 0) {\n    bins[0] = shm[0];\n    bins[1] = shm[blockDim.x / warpSize];\n    bins[2] = shm[blockDim.x / warpSize * 2];\n    bins[3] = shm[blockDim.x / warpSize * 3];\n  }\n}",
            "size_t blockIdx = blockIdx.x;\n  size_t threadIdx = threadIdx.x;\n\n  size_t bin = 0;\n  for (size_t i = blockIdx; i < N; i += gridDim.x) {\n    if (x[i] < 0.25) {\n      bin = 0;\n    } else if (x[i] < 0.5) {\n      bin = 1;\n    } else if (x[i] < 0.75) {\n      bin = 2;\n    } else {\n      bin = 3;\n    }\n    atomicAdd(bins + bin, 1);\n  }\n}",
            "const int tid = threadIdx.x;\n    const int nTid = threadIdx.x;\n    const int nB = blockDim.x;\n    const int bId = blockIdx.x;\n    const int nbB = blockDim.x * gridDim.x;\n    const int stride = nbB * N;\n\n    const int startIdx = bId * stride + nTid * N;\n    const int stopIdx = bId * stride + nTid * N + N;\n\n    int n = 0;\n\n    // Process one bin at a time\n    for (int i = startIdx; i < stopIdx; i += stride) {\n        double val = x[i];\n        double fract = modf(val, &val);\n        if (fract >= 0.25 && fract < 0.5) n++;\n        else if (fract >= 0.5 && fract < 0.75) n++;\n        else if (fract >= 0.75 && fract < 1) n++;\n        else if (fract >= 0.0 && fract < 0.25) n++;\n    }\n\n    // Reduce the values in the block to the warp\n    int blockResult = n;\n    for (int stride = nB / 2; stride > 0; stride /= 2) {\n        __syncthreads();\n        if (nTid < stride) {\n            blockResult += shfl_down(blockResult, stride);\n        }\n    }\n\n    // Reduce the values in the warp to the first thread\n    if (nTid == 0) {\n        // Write to the output vector\n        bins[bId] = blockResult;\n    }\n}",
            "__shared__ double sdata[512]; // shared memory for block\n\n  // Load data into shared memory\n  size_t t = threadIdx.x;\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t stride = blockDim.x * gridDim.x;\n\n  if (i < N) {\n    sdata[t] = x[i];\n  }\n  __syncthreads();\n\n  // Merge shared memory\n  if (t < 256) {\n    sdata[t] += sdata[t + 256];\n  }\n  __syncthreads();\n\n  if (t < 128) {\n    sdata[t] += sdata[t + 128];\n  }\n  __syncthreads();\n\n  if (t < 64) {\n    sdata[t] += sdata[t + 64];\n  }\n  __syncthreads();\n\n  // Write the last 4 numbers to the output bins\n  if (t == 0) {\n    bins[0] = sdata[64];\n    bins[1] = sdata[128];\n    bins[2] = sdata[192];\n    bins[3] = sdata[256];\n  }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// YOUR CODE HERE\n}",
            "/*\n  TODO: implement the kernel.\n\n  Your code should look like:\n\n  size_t thread_id = threadIdx.x;\n  size_t stride = blockDim.x;\n\n  for (int i = threadIdx.x; i < N; i += blockDim.x) {\n    if (x[i] < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (x[i] < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (x[i] < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n  */\n}",
            "// Declare the shared memory\n    __shared__ double shared[BLOCK_SIZE];\n    // Block index\n    const int blockIdx_x = blockIdx.x;\n    // Thread index\n    const int threadIdx_x = threadIdx.x;\n    // Thread index in x-dimension\n    const int threadIdx_x_x = threadIdx_x;\n    // Thread index in y-dimension\n    const int threadIdx_y_x = blockIdx_x;\n    // Index in shared memory\n    const int index = threadIdx_y_x * BLOCK_SIZE + threadIdx_x_x;\n    // Initialise the shared memory\n    shared[index] = 0;\n    // Number of elements in each block\n    const int block_size = BLOCK_SIZE;\n    // Number of elements in x\n    const int n = N;\n    // Number of blocks\n    const int n_blocks = ceil(n / block_size);\n    // Size of shared memory in bytes\n    const int shared_bytes = BLOCK_SIZE * sizeof(double);\n\n    // Copy the data from global memory to shared memory\n    if (threadIdx_x_x < ceil(n / block_size)) {\n        shared[index] = x[threadIdx_y_x * block_size + threadIdx_x_x];\n    }\n    // Synchronise before using shared memory\n    __syncthreads();\n    // Fill the shared memory\n    for (int i = threadIdx_x_x; i < n; i += block_size) {\n        // Calculate the index\n        const int index = threadIdx_y_x * block_size + i;\n        // Check if the value is between 0 and 0.25\n        if (x[index] >= 0 && x[index] < 0.25) {\n            shared[0]++;\n        }\n        // Check if the value is between 0.25 and 0.5\n        if (x[index] >= 0.25 && x[index] < 0.5) {\n            shared[1]++;\n        }\n        // Check if the value is between 0.5 and 0.75\n        if (x[index] >= 0.5 && x[index] < 0.75) {\n            shared[2]++;\n        }\n        // Check if the value is between 0.75 and 1\n        if (x[index] >= 0.75 && x[index] <= 1) {\n            shared[3]++;\n        }\n    }\n    // Synchronise before using shared memory\n    __syncthreads();\n    // Copy the shared memory to global memory\n    if (threadIdx_x_x < ceil(n / block_size)) {\n        bins[threadIdx_y_x] = shared[index];\n    }\n}",
            "// The first thread in the block counts the number of doubles in the block\n\t// that have a fractional part in the first quartile.\n\tif (threadIdx.x == 0) {\n\t\tsize_t count = 0;\n\t\tfor (int i = blockIdx.x * blockDim.x; i < N && i < blockDim.x * gridDim.x; i++) {\n\t\t\tdouble frac = modf(x[i], NULL);\n\t\t\tif (frac < 0.25) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t\tbins[0] = count;\n\t}\n\n\t// Each thread in the block counts the number of doubles in the block\n\t// that have a fractional part in the second quartile.\n\tif (threadIdx.x == 1) {\n\t\tsize_t count = 0;\n\t\tfor (int i = blockIdx.x * blockDim.x; i < N && i < blockDim.x * gridDim.x; i++) {\n\t\t\tdouble frac = modf(x[i], NULL);\n\t\t\tif (frac >= 0.25 && frac < 0.5) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t\tbins[1] = count;\n\t}\n\n\t// Each thread in the block counts the number of doubles in the block\n\t// that have a fractional part in the third quartile.\n\tif (threadIdx.x == 2) {\n\t\tsize_t count = 0;\n\t\tfor (int i = blockIdx.x * blockDim.x; i < N && i < blockDim.x * gridDim.x; i++) {\n\t\t\tdouble frac = modf(x[i], NULL);\n\t\t\tif (frac >= 0.5 && frac < 0.75) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t\tbins[2] = count;\n\t}\n\n\t// Each thread in the block counts the number of doubles in the block\n\t// that have a fractional part in the fourth quartile.\n\tif (threadIdx.x == 3) {\n\t\tsize_t count = 0;\n\t\tfor (int i = blockIdx.x * blockDim.x; i < N && i < blockDim.x * gridDim.x; i++) {\n\t\t\tdouble frac = modf(x[i], NULL);\n\t\t\tif (frac >= 0.75 && frac <= 1.0) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t\tbins[3] = count;\n\t}\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int stride = blockDim.x * gridDim.x;\n    for (size_t i = tid; i < N; i += stride) {\n        double xi = x[i];\n        int index = 0;\n        if (xi < 0.25) {\n            index = 0;\n        }\n        else if (xi >= 0.25 && xi < 0.5) {\n            index = 1;\n        }\n        else if (xi >= 0.5 && xi < 0.75) {\n            index = 2;\n        }\n        else if (xi >= 0.75 && xi < 1) {\n            index = 3;\n        }\n        atomicAdd(&bins[index], 1);\n    }\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t stride = blockDim.x * gridDim.x;\n  double dx, fp, t1, t2;\n  t1 = 0.25; t2 = 0.75;\n  for (size_t i = tid; i < N; i += stride) {\n    dx = x[i];\n    fp = fmod(dx, 1);\n    if (fp <= t1) bins[0] += 1;\n    else if (fp <= t2) bins[1] += 1;\n    else if (fp <= 0.5) bins[2] += 1;\n    else bins[3] += 1;\n  }\n}",
            "int id = threadIdx.x;\n    int lane = id % 32;\n    int warp = id / 32;\n    int laneId = __shfl_sync(0xffffffff, id, lane);\n    int warpId = __shfl_sync(0xffffffff, id, 32);\n    int num = 1;\n\n    __shared__ double val[1024];\n    __shared__ int count[4];\n    val[warpId] = x[warp * 32 + laneId];\n    __syncthreads();\n\n    if (laneId < 32) {\n        double val2 = val[laneId];\n        int binId = 0;\n        if (val2 >= 0.0 && val2 < 0.25) {\n            binId = 0;\n        } else if (val2 >= 0.25 && val2 < 0.5) {\n            binId = 1;\n        } else if (val2 >= 0.5 && val2 < 0.75) {\n            binId = 2;\n        } else if (val2 >= 0.75 && val2 <= 1.0) {\n            binId = 3;\n        }\n        count[binId] += 1;\n    }\n\n    for (int i = 0; i < 4; i++) {\n        bins[i] = count[i];\n    }\n}",
            "}",
            "// TODO\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n\n    for (int i = index; i < N; i += stride) {\n        // find the range that current element is in\n        int range = floor((x[i] - 0) * 4);\n        atomicAdd(&bins[range], 1);\n    }\n}",
            "/*\n   * TODO: Your code here\n   */\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  for(size_t i = 0; i < N; i++){\n    if(x[i] >= 0 && x[i] < 0.25){\n      bins[0]++;\n    }\n    else if(x[i] >= 0.25 && x[i] < 0.5){\n      bins[1]++;\n    }\n    else if(x[i] >= 0.5 && x[i] < 0.75){\n      bins[2]++;\n    }\n    else if(x[i] >= 0.75 && x[i] <= 1){\n      bins[3]++;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        // TODO: Use if-statements and floating-point comparisons to compute the quartiles of x[i].\n        //       Store the number of elements smaller than x[i] in the first element of bins.\n        //       Store the number of elements smaller than x[i] and equal to x[i] in the second element of bins.\n        //       Store the number of elements smaller than or equal to x[i] in the third element of bins.\n        //       Store the number of elements larger than x[i] in the fourth element of bins.\n    }\n}",
            "// This function does not return a value, but rather a set of 4 integer values\n    // It is thus not possible to write bins = countQuartiles(x, N)\n    //...\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N)\n        return;\n\n    if (x[tid] >= 0.0 && x[tid] < 0.25)\n        atomicAdd(&bins[0], 1);\n    else if (x[tid] >= 0.25 && x[tid] < 0.5)\n        atomicAdd(&bins[1], 1);\n    else if (x[tid] >= 0.5 && x[tid] < 0.75)\n        atomicAdd(&bins[2], 1);\n    else if (x[tid] >= 0.75 && x[tid] < 1.0)\n        atomicAdd(&bins[3], 1);\n}",
            "// Write your code here\n}",
            "// TODO\n}",
            "}",
            "size_t threadId = threadIdx.x + blockIdx.x * blockDim.x;\n\n  // Only the threads with id in [0, N) will be used\n  if (threadId < N) {\n    size_t bin = (size_t)(floor(x[threadId] * 4));\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "size_t id = blockIdx.x * blockDim.x + threadIdx.x;\n  if (id < N) {\n    double frac = modf(x[id], &x[id]);\n    if (frac < 0.25)\n      atomicAdd(&bins[0], 1);\n    else if (frac < 0.5)\n      atomicAdd(&bins[1], 1);\n    else if (frac < 0.75)\n      atomicAdd(&bins[2], 1);\n    else\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "// TODO: implement the kernel\n\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n    // TODO\n}",
            "size_t i = threadIdx.x + blockIdx.x*blockDim.x;\n  size_t offset = 0;\n  if (i < N) {\n    double v = x[i];\n    double r = modf(v, &offset);\n    bins[0] += (r >= 0 && r < 0.25);\n    bins[1] += (r >= 0.25 && r < 0.5);\n    bins[2] += (r >= 0.5 && r < 0.75);\n    bins[3] += (r >= 0.75 && r < 1);\n  }\n}",
            "// TODO\n  // 1. Use atomicAdd to increment bins[0], bins[1], bins[2], and bins[3] in each thread\n  //    when the thread's value of x is in the appropriate quartile.\n  // 2. Launch a kernel with at least N threads\n}",
            "//...\n}",
            "size_t idx = threadIdx.x + blockDim.x * blockIdx.x;\n    if (idx >= N) return;\n    double d = x[idx];\n    bins[0] += d >= 0 && d < 0.25;\n    bins[1] += d >= 0.25 && d < 0.5;\n    bins[2] += d >= 0.5 && d < 0.75;\n    bins[3] += d >= 0.75 && d < 1;\n}",
            "bins[0] = bins[1] = bins[2] = bins[3] = 0;\n    for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n        if (x[i] >= 0 && x[i] < 0.25)\n            atomicAdd(&bins[0], 1);\n        else if (x[i] >= 0.25 && x[i] < 0.5)\n            atomicAdd(&bins[1], 1);\n        else if (x[i] >= 0.5 && x[i] < 0.75)\n            atomicAdd(&bins[2], 1);\n        else\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\tsize_t stride = blockDim.x * gridDim.x;\n\t// For each element in the array, check if its fractional part is in [0, 0.25), [0.25, 0.5), [0.5, 0.75), [0.75, 1)\n\tfor (size_t k = i; k < N; k += stride) {\n\t\tif (fmod(x[k], 1.0) < 0.25) {\n\t\t\tatomicAdd(&bins[0], 1);\n\t\t} else if (fmod(x[k], 1.0) < 0.5) {\n\t\t\tatomicAdd(&bins[1], 1);\n\t\t} else if (fmod(x[k], 1.0) < 0.75) {\n\t\t\tatomicAdd(&bins[2], 1);\n\t\t} else {\n\t\t\tatomicAdd(&bins[3], 1);\n\t\t}\n\t}\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = blockDim.x * gridDim.x;\n\n    for(size_t i = tid; i < N; i += stride) {\n        if (x[i] >= 0.0 && x[i] < 0.25) {\n            bins[0]++;\n        } else if (x[i] >= 0.25 && x[i] < 0.5) {\n            bins[1]++;\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            bins[2]++;\n        } else if (x[i] >= 0.75 && x[i] < 1.0) {\n            bins[3]++;\n        } else {\n            // do nothing\n        }\n    }\n}",
            "// TODO: add your code here\n    // Note that the thread that calls the kernel will block until all threads complete\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int stride = blockDim.x * gridDim.x;\n    int i;\n\n    for (i = tid; i < N; i += stride) {\n        if (x[i] < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (x[i] < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (x[i] < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "//...\n}",
            "size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n    if (i >= N) return;\n\n    const size_t nbins = 4;\n    double *bins_d = (double *) malloc(nbins * sizeof(double));\n    bins_d[0] = 0;\n    bins_d[1] = 0;\n    bins_d[2] = 0;\n    bins_d[3] = 0;\n    if (i < N)\n    {\n        if (x[i] >= 0.0 && x[i] < 0.25)\n        {\n            bins_d[0]++;\n        }\n        else if (x[i] >= 0.25 && x[i] < 0.5)\n        {\n            bins_d[1]++;\n        }\n        else if (x[i] >= 0.5 && x[i] < 0.75)\n        {\n            bins_d[2]++;\n        }\n        else if (x[i] >= 0.75 && x[i] <= 1.0)\n        {\n            bins_d[3]++;\n        }\n    }\n    bins[0] = (size_t) bins_d[0];\n    bins[1] = (size_t) bins_d[1];\n    bins[2] = (size_t) bins_d[2];\n    bins[3] = (size_t) bins_d[3];\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    for (int i = threadIdx.x; i < N; i += blockDim.x) {\n        if (x[i] < 0.25)\n            bins[0]++;\n        else if (x[i] < 0.5)\n            bins[1]++;\n        else if (x[i] < 0.75)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "int threadID = threadIdx.x + blockDim.x*blockIdx.x;\n    int numThreads = blockDim.x * gridDim.x;\n    size_t bin = 0;\n\n    for (size_t i = threadID; i < N; i += numThreads) {\n        if (x[i] >= 0.0 && x[i] < 0.25) bin = 0;\n        else if (x[i] >= 0.25 && x[i] < 0.5) bin = 1;\n        else if (x[i] >= 0.5 && x[i] < 0.75) bin = 2;\n        else if (x[i] >= 0.75 && x[i] <= 1.0) bin = 3;\n    }\n\n    atomicAdd(&bins[bin], 1);\n}",
            "//TODO: Implement\n}",
            "bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // thread index\n    int tid = threadIdx.x;\n\n    // threads are split into 4 groups, each with N/4 threads\n    int group = tid / (N/4);\n\n    // threads in group 1 get assigned to quartile 1\n    int quartile = 0;\n    if(group == 1) quartile = 1;\n\n    // threads in group 2 get assigned to quartile 2\n    if(group == 2) quartile = 2;\n\n    // threads in group 3 get assigned to quartile 3\n    if(group == 3) quartile = 3;\n\n    // threads in group 4 get assigned to quartile 4\n    if(group == 4) quartile = 4;\n\n    // if thread's quartile is not 0, then it's an even thread. Otherwise, it's an uneven thread\n    if(quartile!= 0) {\n        // iterate through array, counting even threads\n        for(int i = 0; i < N; i++) {\n            // assign thread quartile and add 1 to the count\n            if(x[i] % 1 == 0) bins[quartile]++;\n        }\n    }\n    else {\n        // iterate through array, counting uneven threads\n        for(int i = 0; i < N; i++) {\n            // assign thread quartile and add 1 to the count\n            if(x[i] % 1!= 0) bins[quartile]++;\n        }\n    }\n}",
            "__shared__ double q[4];\n\t__shared__ double b;\n\n\tsize_t i, j;\n\tdouble z;\n\tsize_t k;\n\n\t// initialize the shared memory\n\tq[0] = q[1] = q[2] = q[3] = 0;\n\tb = 0;\n\n\t// sort\n\tfor (i = threadIdx.x; i < N; i += blockDim.x) {\n\t\tz = x[i];\n\t\tif (z > q[3]) {\n\t\t\tif (q[3] > q[2]) {\n\t\t\t\tif (q[2] > q[1]) {\n\t\t\t\t\tif (q[1] > q[0]) {\n\t\t\t\t\t\tq[0] = z;\n\t\t\t\t\t}\n\t\t\t\t\telse if (q[0] < q[1]) {\n\t\t\t\t\t\tq[1] = z;\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tq[2] = z;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse if (q[1] < q[2]) {\n\t\t\t\t\tq[2] = z;\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tq[3] = z;\n\t\t\t\t}\n\t\t\t}\n\t\t\telse if (q[3] > q[2]) {\n\t\t\t\tif (q[2] > q[1]) {\n\t\t\t\t\tif (q[0] < q[1]) {\n\t\t\t\t\t\tq[0] = z;\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tq[1] = z;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse if (q[1] < q[2]) {\n\t\t\t\t\tq[1] = z;\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tq[2] = z;\n\t\t\t\t}\n\t\t\t}\n\t\t\telse if (q[2] > q[1]) {\n\t\t\t\tif (q[0] < q[1]) {\n\t\t\t\t\tq[0] = z;\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tq[1] = z;\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\tq[2] = z;\n\t\t\t}\n\t\t}\n\t}\n\n\t// count\n\tfor (i = threadIdx.x; i < 4; i += blockDim.x) {\n\t\tz = q[i];\n\t\tk = 0;\n\n\t\tif (z < q[1]) {\n\t\t\tif (z < q[0]) {\n\t\t\t\tk = 1;\n\t\t\t}\n\t\t}\n\t\telse if (z < q[2]) {\n\t\t\tif (z < q[1]) {\n\t\t\t\tk = 2;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tk = 3;\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\tif (z < q[2]) {\n\t\t\t\tk = 4;\n\t\t\t}\n\t\t\telse if (z < q[3]) {\n\t\t\t\tk = 5;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tk = 6;\n\t\t\t}\n\t\t}\n\n\t\tatomicAdd(&bins[k - 1], 1);\n\t}\n\n\t// set b\n\tif (threadIdx.x == 0) {\n\t\tb = q[3];\n\t}\n\t__syncthreads();\n\n\t// get the number of non-zero terms in the quartiles\n\tif (threadIdx.x == 0) {\n\t\tfor (i = 1; i < 4; i++) {\n\t\t\tj = i - 1;\n\t\t\tif (q[i]!= b && bins[j] > 0) {",
            "__shared__ double shmem[4];\n  // The thread that will write to shmem[0] should be the first one to enter the critical section.\n  const int thread0 = threadIdx.x + blockIdx.x*blockDim.x;\n  const int num_threads = blockDim.x * gridDim.x;\n  if (threadIdx.x == 0) {\n    // Initialize the shared memory\n    shmem[0] = 0.0;\n    shmem[1] = 0.0;\n    shmem[2] = 0.0;\n    shmem[3] = 0.0;\n  }\n  __syncthreads();\n  int thread_offset = 0;\n  for (; thread_offset + threadIdx.x < N; thread_offset += num_threads) {\n    // First determine which bin this element falls into\n    double x_val = x[thread_offset];\n    int bin = 0;\n    if (x_val < 0.25) {\n      bin = 0;\n    } else if (x_val < 0.5) {\n      bin = 1;\n    } else if (x_val < 0.75) {\n      bin = 2;\n    } else {\n      bin = 3;\n    }\n    // Count the value in the shared memory\n    atomicAdd(&shmem[bin], 1.0);\n  }\n  // Write to global memory\n  if (threadIdx.x == 0) {\n    bins[0] = (size_t)shmem[0];\n    bins[1] = (size_t)shmem[1];\n    bins[2] = (size_t)shmem[2];\n    bins[3] = (size_t)shmem[3];\n  }\n}",
            "// Allocate memory for the thread to store a value for each quartile\n  double *quartiles = (double *)malloc(sizeof(double) * 4);\n\n  // Compute the quartiles\n  quartiles[0] = x[0];\n  quartiles[1] = x[N / 4];\n  quartiles[2] = x[N / 2];\n  quartiles[3] = x[3 * N / 4];\n\n  // Sort the quartiles\n  int sorted[4] = { 0, 1, 2, 3 };\n  sort(sorted, 4, quartiles);\n\n  // Compute the fractional part of each quartile and store the counts in bins\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n\n  // Compute the fractional part of each quartile\n  double fraction[4];\n  fraction[0] = quartiles[0] - floor(quartiles[0]);\n  fraction[1] = quartiles[1] - floor(quartiles[1]);\n  fraction[2] = quartiles[2] - floor(quartiles[2]);\n  fraction[3] = quartiles[3] - floor(quartiles[3]);\n\n  for (int i = 0; i < 4; i++) {\n    // Count the number of elements with the fractional part in the appropriate range\n    if (fraction[i] >= 0 && fraction[i] < 0.25) {\n      bins[0]++;\n    } else if (fraction[i] >= 0.25 && fraction[i] < 0.5) {\n      bins[1]++;\n    } else if (fraction[i] >= 0.5 && fraction[i] < 0.75) {\n      bins[2]++;\n    } else if (fraction[i] >= 0.75 && fraction[i] <= 1) {\n      bins[3]++;\n    }\n  }\n\n  free(quartiles);\n}",
            "int tid = threadIdx.x;\n    double frac;\n    double xj = x[tid];\n    if (xj > 0.75) {\n        frac = 1.0;\n    }\n    else if (xj > 0.5) {\n        frac = (xj - 0.5) / 0.25;\n    }\n    else if (xj > 0.25) {\n        frac = (xj - 0.25) / 0.25;\n    }\n    else {\n        frac = xj / 0.25;\n    }\n    int i = (int)floor(frac);\n    bins[i]++;\n}",
            "int idx = threadIdx.x + blockDim.x * blockIdx.x;\n\tint stride = blockDim.x * gridDim.x;\n\n\tint i;\n\tdouble xi, q0, q1, q2, q3;\n\tfor (i = idx; i < N; i += stride) {\n\t\txi = x[i];\n\t\tif (xi < 0.0) xi = -xi;\n\t\tq0 = floor(xi);\n\t\tq1 = q0 + 0.25;\n\t\tq2 = q1 + 0.25;\n\t\tq3 = q2 + 0.25;\n\t\tif (xi - q0 < 0.25) {\n\t\t\tatomicAdd(&(bins[0]), 1);\n\t\t}\n\t\telse if (xi - q1 < 0.25) {\n\t\t\tatomicAdd(&(bins[1]), 1);\n\t\t}\n\t\telse if (xi - q2 < 0.25) {\n\t\t\tatomicAdd(&(bins[2]), 1);\n\t\t}\n\t\telse if (xi - q3 < 0.25) {\n\t\t\tatomicAdd(&(bins[3]), 1);\n\t\t}\n\t}\n}",
            "int threadID = blockDim.x * blockIdx.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n    int threadCount = 0;\n    for (size_t i = threadID; i < N; i += stride) {\n        if (x[i] >= 0 && x[i] < 0.25) {\n            atomicAdd(&bins[0], 1);\n            threadCount++;\n        } else if (x[i] >= 0.25 && x[i] < 0.5) {\n            atomicAdd(&bins[1], 1);\n            threadCount++;\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            atomicAdd(&bins[2], 1);\n            threadCount++;\n        } else if (x[i] >= 0.75 && x[i] <= 1) {\n            atomicAdd(&bins[3], 1);\n            threadCount++;\n        }\n    }\n    if (threadID == 0) {\n        printf(\"%d\\n\", threadCount);\n    }\n}",
            "const int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] < 0.25) bins[0]++;\n        else if (x[i] < 0.5) bins[1]++;\n        else if (x[i] < 0.75) bins[2]++;\n        else bins[3]++;\n    }\n}",
            "__shared__ double s_x[1024];\n    int i = threadIdx.x + blockDim.x*blockIdx.x;\n    s_x[threadIdx.x] = i < N? x[i] : -1;\n    __syncthreads();\n\n    double fractional_part;\n    double integer_part;\n    for (int j = 0; j < N; j += blockDim.x) {\n        if (threadIdx.x + j < N && threadIdx.x + j < 1024) {\n            fractional_part = modf(s_x[threadIdx.x + j], &integer_part);\n            bins[0] += integer_part > 1.75 || (integer_part == 1.75 && fractional_part > 0.25)? 1 : 0;\n            bins[1] += integer_part > 0.75 || (integer_part == 0.75 && fractional_part > 0.25)? 1 : 0;\n            bins[2] += integer_part > 0.25 || (integer_part == 0.25 && fractional_part > 0.25)? 1 : 0;\n            bins[3] += integer_part > 0.0 || (integer_part == 0.0 && fractional_part > 0.25)? 1 : 0;\n        }\n    }\n}",
            "const size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    __shared__ size_t b[4];\n    if (tid == 0) {\n        b[0] = 0;\n        b[1] = 0;\n        b[2] = 0;\n        b[3] = 0;\n    }\n    __syncthreads();\n    for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n        if (x[i] < 0.25) {\n            atomicAdd(&b[0], 1);\n        } else if (x[i] < 0.5) {\n            atomicAdd(&b[1], 1);\n        } else if (x[i] < 0.75) {\n            atomicAdd(&b[2], 1);\n        } else {\n            atomicAdd(&b[3], 1);\n        }\n    }\n    __syncthreads();\n    if (tid == 0) {\n        for (int i = 0; i < 4; i++) {\n            atomicAdd(&bins[i], b[i]);\n        }\n    }\n}",
            "// Compute the number of elements less than 0.25, 0.5, 0.75, 1 in x.\n    // (Notice that this is the same as counting the number of elements less than each bin.)\n    // Add the results to bins[].\n}",
            "/* Compute a local index for the thread in the grid\n    Note that blockDim.x is fixed at 32 so the block index can be computed\n    */\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] >= 0 && x[i] < 0.25) bins[0]++;\n        else if (x[i] >= 0.25 && x[i] < 0.5) bins[1]++;\n        else if (x[i] >= 0.5 && x[i] < 0.75) bins[2]++;\n        else if (x[i] >= 0.75 && x[i] <= 1) bins[3]++;\n    }\n}",
            "size_t myBin = blockDim.x*blockIdx.x + threadIdx.x;\n\tsize_t nBins = blockDim.x * gridDim.x;\n\n\tif (myBin >= N) {\n\t\treturn;\n\t}\n\n\t// For the given thread, compute the fractional part.\n\tdouble frac = fmod(x[myBin], 1.0);\n\n\t// Which bin does it belong to?\n\tif (frac < 0.25) {\n\t\tbins[0]++;\n\t} else if (frac < 0.5) {\n\t\tbins[1]++;\n\t} else if (frac < 0.75) {\n\t\tbins[2]++;\n\t} else {\n\t\tbins[3]++;\n\t}\n}",
            "// TODO\n}",
            "// TODO: your code here\n\n  for (int i = threadIdx.x; i < N; i += blockDim.x) {\n    if (x[i] >= 0 && x[i] < 0.25)\n      atomicAdd(&bins[0], 1);\n    else if (x[i] >= 0.25 && x[i] < 0.5)\n      atomicAdd(&bins[1], 1);\n    else if (x[i] >= 0.5 && x[i] < 0.75)\n      atomicAdd(&bins[2], 1);\n    else if (x[i] >= 0.75 && x[i] <= 1.0)\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i >= N) return;\n    double val = x[i];\n    int digit = (int)(val * 4.0);\n    //__syncthreads();\n    atomicAdd(&bins[digit], 1);\n    return;\n}",
            "// TODO: add code to count the number of doubles in the vector x that \n    // have a fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and\n    // [0.75, 1).\n    // Store the counts in `bins`.\n}",
            "/*\n\t * TODO: Implement the kernel.\n\t *\n\t * HINT:\n\t *\n\t *   1. Use `threadIdx.x` to access an element of `x`.\n\t *   2. Use `threadIdx.x + blockDim.x * blockIdx.x` to access an element of `bins`.\n\t */\n\tint tx = threadIdx.x;\n\tint idx = threadIdx.x + blockDim.x * blockIdx.x;\n\tdouble val;\n\tif (idx < N)\n\t{\n\t\tval = x[idx];\n\t\tif (val < 1.0)\n\t\t{\n\t\t\tatomicAdd(&bins[0], 1);\n\t\t}\n\t\telse if (val < 2.0)\n\t\t{\n\t\t\tatomicAdd(&bins[1], 1);\n\t\t}\n\t\telse if (val < 3.0)\n\t\t{\n\t\t\tatomicAdd(&bins[2], 1);\n\t\t}\n\t\telse\n\t\t{\n\t\t\tatomicAdd(&bins[3], 1);\n\t\t}\n\t}\n\n}",
            "// TODO: implement on GPU\n    for(int i=threadIdx.x; i<N; i+=blockDim.x) {\n        if (x[i] >= 0.25 && x[i] < 0.5) {\n            atomicAdd(&bins[0], 1);\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            atomicAdd(&bins[1], 1);\n        } else if (x[i] >= 0.75 && x[i] < 1) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n    __syncthreads();\n}",
            "// allocate a shared memory array to store all the thread's results\n    extern __shared__ int counts[];\n    for (int i = threadIdx.x; i < 4; i += blockDim.x) {\n        counts[i] = 0;\n    }\n    // initialize the global memory array\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    // each thread is responsible for finding the fractional parts of the\n    // corresponding x values and then adding them to the corresponding bin\n    for (int i = threadIdx.x; i < N; i += blockDim.x) {\n        double x_i = x[i];\n        int bin_index = (int)((x_i - floor(x_i)) * 4);\n        counts[bin_index]++;\n    }\n    // each thread is responsible for updating the global memory array with\n    // the local counts array\n    for (int i = threadIdx.x; i < 4; i += blockDim.x) {\n        atomicAdd(&bins[i], counts[i]);\n    }\n}",
            "const double epsilon = 0.25;\n  // TODO: fill in this function\n\n  double temp;\n  for (size_t i = 0; i < 4; i++)\n    bins[i] = 0;\n  for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n    temp = x[i] % 1;\n    if (temp < epsilon)\n      bins[0] += 1;\n    else if (temp < epsilon + 0.25)\n      bins[1] += 1;\n    else if (temp < epsilon + 0.5)\n      bins[2] += 1;\n    else\n      bins[3] += 1;\n  }\n  __syncthreads();\n\n  if (threadIdx.x == 0) {\n    for (int i = 1; i < blockDim.x; i++) {\n      for (int j = 0; j < 4; j++) {\n        bins[j] += bins[j + 4 * i];\n      }\n    }\n  }\n  __syncthreads();\n}",
            "// TODO: YOUR CODE HERE\n}",
            "int b = threadIdx.x;\n  for (int i = 0; i < N; i++)\n    if (x[i] < b+0.25)\n      atomicAdd(bins+b, 1);\n}",
            "__shared__ double localX[N];\n\n  size_t tid = threadIdx.x;\n  size_t bid = blockIdx.x;\n  size_t nt = blockDim.x;\n\n  // Copy input vector to local memory\n  if (tid < N) {\n    localX[tid] = x[tid];\n  }\n\n  // Wait for all threads in the block to finish their copy\n  __syncthreads();\n\n  // Sort the elements in local memory\n  for (int j = 1; j < N; j++) {\n    int i = tid - j;\n    if (i >= 0 && localX[i] > localX[i + 1]) {\n      double tmp = localX[i];\n      localX[i] = localX[i + 1];\n      localX[i + 1] = tmp;\n    }\n    __syncthreads();\n  }\n\n  // Copy sorted vector to global memory\n  if (tid < N) {\n    x[tid] = localX[tid];\n  }\n\n  // Wait for all threads in the block to finish their copy\n  __syncthreads();\n\n  // Count the number of doubles in the vector x that have a fractional part\n  // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n  for (size_t i = 0; i < 4; i++) {\n    size_t begin = i * N / 4;\n    size_t end = (i + 1) * N / 4;\n\n    bins[i] = 0;\n\n    for (size_t j = begin; j < end; j++) {\n      if (x[j] - floor(x[j]) > 0) {\n        bins[i]++;\n      }\n    }\n  }\n}",
            "/*\n       In the kernel,\n       - the thread block is assumed to be of size N, so that\n         each thread can process an element of x in turn.\n       - use atomic operations to update the bins\n       - use a shared memory array of size 4 to store the bins\n       - the block is divided into 4 groups of threads, each group\n         responsible for updating a different bin\n    */\n\n}",
            "// TODO: implement this function\n}",
            "__shared__ size_t s_bins[4];\n    if (threadIdx.x < 4) {\n        s_bins[threadIdx.x] = 0;\n    }\n    __syncthreads();\n    for (size_t i = blockDim.x*blockIdx.x + threadIdx.x; i < N; i += blockDim.x*gridDim.x) {\n        size_t index = (size_t)((x[i] - floor(x[i])) * 4);\n        atomicAdd(&s_bins[index], 1);\n    }\n    __syncthreads();\n    for (size_t i = threadIdx.x; i < 4; i += blockDim.x) {\n        atomicAdd(&bins[i], s_bins[i]);\n    }\n}",
            "// TODO\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    size_t threadIdx = blockDim.x * blockIdx.x + threadIdx.x;\n    if (threadIdx > N)\n        return;\n\n    if (threadIdx < N) {\n        double fractional_part = fmod(x[threadIdx], 1);\n        size_t bin_index = 0;\n        if (fractional_part >= 0 && fractional_part < 0.25)\n            bin_index = 0;\n        else if (fractional_part >= 0.25 && fractional_part < 0.5)\n            bin_index = 1;\n        else if (fractional_part >= 0.5 && fractional_part < 0.75)\n            bin_index = 2;\n        else if (fractional_part >= 0.75 && fractional_part < 1)\n            bin_index = 3;\n        atomicAdd(&bins[bin_index], 1);\n    }\n}",
            "//TODO\n\n}",
            "// YOUR CODE HERE\n  double *d_x = (double*) x;\n  unsigned int tid = threadIdx.x;\n  unsigned int offset = blockIdx.x * blockDim.x;\n  unsigned int i = tid + offset;\n\n  __shared__ size_t shmem[4];\n\n  if (i < N) {\n    int bin = 0;\n    if (x[i] >= 0 && x[i] < 0.25) bin = 0;\n    if (x[i] >= 0.25 && x[i] < 0.5) bin = 1;\n    if (x[i] >= 0.5 && x[i] < 0.75) bin = 2;\n    if (x[i] >= 0.75 && x[i] < 1) bin = 3;\n\n    atomicAdd(&shmem[bin], 1);\n  }\n\n  __syncthreads();\n\n  if (tid == 0) {\n    for (int i = 0; i < 4; i++) {\n      atomicAdd(&bins[i], shmem[i]);\n    }\n  }\n}",
            "// TODO: your code here\n}",
            "// YOUR CODE GOES HERE\n\n}",
            "}",
            "// compute the thread index for the current thread\n    size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n    // compute the total number of threads for the entire grid\n    size_t total = blockDim.x * gridDim.x;\n\n    // declare variables\n    int start = 0, end = 0, mid = 0;\n    double fraction = 0;\n\n    // loop until tid is out of bounds\n    while(tid < N){\n        // check if the thread is outside the bounds of the array\n        if(tid < N){\n            // store the value of the current element in the array in the element\n            // fraction\n            fraction = (x[tid] - floor(x[tid]));\n            // store the value of the current element's index in start\n            start = tid;\n            // increment the thread index\n            tid += total;\n            // store the value of the current element's index in end\n            end = tid;\n            // check if the thread is out of bounds of the array\n            while((tid < N) && (x[tid] - floor(x[tid]) == fraction)){\n                // increment the thread index\n                tid += total;\n                // store the value of the current element's index in end\n                end = tid;\n            }\n            // check if the thread is out of bounds of the array\n            if(tid >= N){\n                // assign the value of 1 to mid\n                mid = 1;\n            }\n            // check if the fraction part of the element is in [0, 0.25)\n            else if(fraction >= 0 && fraction < 0.25){\n                // assign the value of 1 to mid\n                mid = 0;\n            }\n            // check if the fraction part of the element is in [0.25, 0.5)\n            else if(fraction >= 0.25 && fraction < 0.5){\n                // assign the value of 1 to mid\n                mid = 1;\n            }\n            // check if the fraction part of the element is in [0.5, 0.75)\n            else if(fraction >= 0.5 && fraction < 0.75){\n                // assign the value of 1 to mid\n                mid = 2;\n            }\n            // check if the fraction part of the element is in [0.75, 1)\n            else{\n                // assign the value of 1 to mid\n                mid = 3;\n            }\n            // increase the value of the appropriate bin by the number of elements\n            // in the array from start to end\n            bins[mid] += end - start;\n        }\n    }\n}",
            "// TODO\n\n    return;\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n   size_t i = tid;\n\n   // Initialize the `bins` array.\n   for (int j = 0; j < 4; j++) bins[j] = 0;\n\n   // Do not do anything if the array is empty.\n   if (N == 0) return;\n\n   // Compute the quartiles.\n   double Q1 = N/4;\n   double Q2 = N/2;\n   double Q3 = 3 * N/4;\n\n   // Iterate through each element and increment the right bin.\n   while (i < N) {\n      double d = x[i];\n\n      // If the element is the smallest, assign it to the first bin.\n      if (d < Q1) {\n         atomicAdd(&bins[0], 1);\n      }\n      else if (d < Q2) {\n         atomicAdd(&bins[1], 1);\n      }\n      else if (d < Q3) {\n         atomicAdd(&bins[2], 1);\n      }\n      else {\n         atomicAdd(&bins[3], 1);\n      }\n\n      i += blockDim.x * gridDim.x;\n   }\n}",
            "int id = threadIdx.x + blockIdx.x * blockDim.x;\n    if (id < N) {\n        bins[0] += (x[id] >= 0.0) && (x[id] < 0.25);\n        bins[1] += (x[id] >= 0.25) && (x[id] < 0.5);\n        bins[2] += (x[id] >= 0.5) && (x[id] < 0.75);\n        bins[3] += (x[id] >= 0.75) && (x[id] <= 1.0);\n    }\n}",
            "// TODO: launch a block of threads to compute the quartile count in parallel\n\t// Hint: you can determine the index of the first element in the thread's\n\t// block by using `blockIdx.x * blockDim.x`\n}",
            "int tid = threadIdx.x;\n  int totalThreads = blockDim.x;\n  int blockOffset = blockIdx.x * blockDim.x;\n  int gridSize = gridDim.x * blockDim.x;\n\n  if (tid < N) {\n    double value = x[tid + blockOffset];\n    int index = (value < 0.25) + (value >= 0.25 && value < 0.5) +\n                 (value >= 0.5 && value < 0.75) + (value >= 0.75);\n    atomicAdd(&bins[index], 1);\n  }\n\n  for (int i = tid; i < 4; i += totalThreads) {\n    __syncthreads();\n    if (tid == 0) {\n      int value;\n      for (int i = 1; i < totalThreads; i++) {\n        atomicAdd(&value, bins[i]);\n        atomicAdd(&bins[i], value);\n      }\n    }\n  }\n}",
            "}",
            "// compute bin index for the current thread\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid >= N)\n    return;\n\n  // compute the corresponding bin index\n  int bin = 0;\n  if (x[tid] < 0.25)\n    bin = 0;\n  else if (x[tid] < 0.5)\n    bin = 1;\n  else if (x[tid] < 0.75)\n    bin = 2;\n  else\n    bin = 3;\n\n  // atomically increment the appropriate bin\n  atomicAdd(&bins[bin], 1);\n}",
            "// Implement this function\n}",
            "// Allocate two shared memory arrays of size N/blockDim.x \n  __shared__ size_t shMem1[1024];\n  __shared__ size_t shMem2[1024];\n\n  // Initialize the bins\n  if (threadIdx.x == 0) {\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n  }\n  __syncthreads();\n\n  // Determine the index of the thread in the current block\n  size_t thIdx = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t blockSize = blockDim.x * gridDim.x;\n\n  // Loop through the data and put each value in a specific bin\n  for (size_t i = thIdx; i < N; i += blockSize) {\n    double val = x[i];\n    if (val < 0.25) {\n      atomicAdd(&shMem1[threadIdx.x], 1);\n    } else if (val >= 0.25 && val < 0.5) {\n      atomicAdd(&shMem2[threadIdx.x], 1);\n    } else if (val >= 0.5 && val < 0.75) {\n      atomicAdd(&shMem1[threadIdx.x], 1);\n    } else if (val >= 0.75 && val <= 1) {\n      atomicAdd(&shMem2[threadIdx.x], 1);\n    }\n  }\n  __syncthreads();\n\n  // Write the values back to the bins\n  for (size_t i = threadIdx.x; i < 1024; i += blockDim.x) {\n    atomicAdd(&bins[0], shMem1[i]);\n    atomicAdd(&bins[1], shMem2[i]);\n  }\n}",
            "unsigned int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    unsigned int num_threads = blockDim.x * gridDim.x;\n    unsigned int start_index = tid * 0.25 * N;\n    unsigned int end_index = start_index + N * 0.25;\n    double low = 0.0, high = 1.0, midpoint;\n    for (unsigned int i = 0; i < 4; i++) {\n        bins[i] = 0;\n    }\n    for (unsigned int i = start_index; i < end_index; i++) {\n        if (x[i] >= low && x[i] < (midpoint = low + 0.25 * (high - low)))\n            bins[0]++;\n        else if (x[i] >= (midpoint) && x[i] < (midpoint = low + 0.5 * (high - low)))\n            bins[1]++;\n        else if (x[i] >= (midpoint) && x[i] < (midpoint = low + 0.75 * (high - low)))\n            bins[2]++;\n        else if (x[i] >= (midpoint) && x[i] < (high = 1.0))\n            bins[3]++;\n        low = midpoint;\n    }\n}",
            "// TODO: implement\n}",
            "if (threadIdx.x >= N)\n        return;\n    size_t i = threadIdx.x;\n    // Your code here\n    if (i < N)\n    {\n        if (x[i] >= 0 && x[i] < 0.25)\n            atomicAdd(&bins[0], 1);\n        if (x[i] >= 0.25 && x[i] < 0.5)\n            atomicAdd(&bins[1], 1);\n        if (x[i] >= 0.5 && x[i] < 0.75)\n            atomicAdd(&bins[2], 1);\n        if (x[i] >= 0.75 && x[i] < 1.0)\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "int i = threadIdx.x + blockDim.x * blockIdx.x;\n\tif (i >= N) return;\n\n\t__shared__ size_t counts[4];\n\tcounts[0] = 0;\n\tcounts[1] = 0;\n\tcounts[2] = 0;\n\tcounts[3] = 0;\n\n\tdouble frac = fmod(x[i], 1.0);\n\tint index = 0;\n\tif (frac >= 0.0 && frac < 0.25) {\n\t\tindex = 0;\n\t} else if (frac >= 0.25 && frac < 0.5) {\n\t\tindex = 1;\n\t} else if (frac >= 0.5 && frac < 0.75) {\n\t\tindex = 2;\n\t} else if (frac >= 0.75 && frac <= 1.0) {\n\t\tindex = 3;\n\t}\n\tatomicAdd(&counts[index], 1);\n\n\tbins[0] = counts[0];\n\tbins[1] = counts[1];\n\tbins[2] = counts[2];\n\tbins[3] = counts[3];\n}",
            "// TODO\n}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (index < N) {\n    if (x[index] >= 0 && x[index] < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (x[index] >= 0.25 && x[index] < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (x[index] >= 0.5 && x[index] < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else if (x[index] >= 0.75 && x[index] <= 1.0) {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "int i = threadIdx.x + blockDim.x * blockIdx.x;\n  if (i >= N)\n    return;\n\n  int index = (x[i] * 4);\n  atomicAdd(&bins[index], 1);\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n\n\tif (i < N) {\n\t\tif (x[i] >= 0.0 && x[i] <= 0.25) bins[0]++;\n\t\telse if (x[i] > 0.25 && x[i] <= 0.5) bins[1]++;\n\t\telse if (x[i] > 0.5 && x[i] <= 0.75) bins[2]++;\n\t\telse if (x[i] > 0.75 && x[i] <= 1.0) bins[3]++;\n\t\telse bins[4]++;\n\t}\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    if (threadIdx.x < N) {\n        if (x[threadIdx.x] >= 0.0 && x[threadIdx.x] <= 0.25)\n            atomicAdd(&bins[0], 1);\n        else if (x[threadIdx.x] >= 0.25 && x[threadIdx.x] <= 0.50)\n            atomicAdd(&bins[1], 1);\n        else if (x[threadIdx.x] >= 0.50 && x[threadIdx.x] <= 0.75)\n            atomicAdd(&bins[2], 1);\n        else if (x[threadIdx.x] >= 0.75 && x[threadIdx.x] <= 1.0)\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "// TODO: Implement this function\n  // NOTE: Use the block index to determine which bin to update\n  int b = blockIdx.x;\n  int thread_id = threadIdx.x;\n  int block_size = blockDim.x;\n\n  if (b >= 4) return;\n\n  bins[b] = 0;\n\n  for (int i = thread_id; i < N; i += block_size) {\n    if (x[i] - floor(x[i]) <= 0.25 && x[i] - floor(x[i]) > 0)\n      bins[b]++;\n    else if (x[i] - floor(x[i]) > 0.25 && x[i] - floor(x[i]) <= 0.5)\n      bins[b]++;\n    else if (x[i] - floor(x[i]) > 0.5 && x[i] - floor(x[i]) <= 0.75)\n      bins[b]++;\n    else if (x[i] - floor(x[i]) > 0.75 && x[i] - floor(x[i]) <= 1)\n      bins[b]++;\n  }\n\n  __syncthreads();\n\n  for (int i = blockDim.x / 2; i > 0; i >>= 1) {\n    if (thread_id < i) {\n      bins[b] += bins[b + i];\n    }\n    __syncthreads();\n  }\n\n  if (thread_id == 0) {\n    bins[b] = 0;\n  }\n}",
            "}",
            "// TODO\n    size_t stride = blockDim.x * gridDim.x;\n    for(int i = threadIdx.x + blockIdx.x * blockDim.x; i < N; i += stride) {\n        size_t j = (int)(4*x[i]);\n        atomicAdd(&bins[j], 1);\n    }\n    __syncthreads();\n}",
            "// Fill this in.\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        int j = (int)(x[i] * 4);\n        bins[j]++;\n    }\n}",
            "// TODO\n}",
            "// TODO: complete this function\n    \n}",
            "// TODO: Fill out this function\n}",
            "/*\n     * TODO: Implement the function\n     *\n     *     bins[0] = count the fraction of doubles in x that have a fractional part\n     *               in [0, 0.25)\n     *     bins[1] = count the fraction of doubles in x that have a fractional part\n     *               in [0.25, 0.5)\n     *     bins[2] = count the fraction of doubles in x that have a fractional part\n     *               in [0.5, 0.75)\n     *     bins[3] = count the fraction of doubles in x that have a fractional part\n     *               in [0.75, 1)\n     *\n     * Use the function ceil to compute the next nearest integer.\n     *\n     */\n\n}",
            "int tid = threadIdx.x;\n\tint stride = blockDim.x;\n\n\tfor (int i = tid; i < N; i += stride) {\n\t\tbins[0] += x[i] >= 0 && x[i] < 0.25;\n\t\tbins[1] += x[i] >= 0.25 && x[i] < 0.5;\n\t\tbins[2] += x[i] >= 0.5 && x[i] < 0.75;\n\t\tbins[3] += x[i] >= 0.75 && x[i] <= 1.0;\n\t}\n}",
            "//TODO: fill in the array bins with the count of doubles in x with fractional \n  //      parts in each of the four intervals [0, 0.25), [0.25, 0.5), [0.5, 0.75), \n  //      and [0.75, 1)\n  return;\n}",
            "/*\n        Note: the compiler will pack the 4 bins into one 4-byte word.\n              You can also use an array of 4 ints, but it will be slower.\n    */\n\n    // 1. Load the data from global memory to the local memory.\n    //    Use 1 thread for 1 double.\n    double local_x[1];\n    for(int i = 0; i < 1; i++) {\n        local_x[i] = x[i];\n    }\n\n    // 2. Declare the variables to store the quartiles.\n    double quartiles[4];\n\n    // 3. Compute the quartiles.\n    quartiles[0] = compute_quartile(local_x, N, 0.0, 0.25);\n    quartiles[1] = compute_quartile(local_x, N, 0.25, 0.5);\n    quartiles[2] = compute_quartile(local_x, N, 0.5, 0.75);\n    quartiles[3] = compute_quartile(local_x, N, 0.75, 1.0);\n\n    // 4. Write the results to the shared memory.\n    //    Use 1 thread for 1 double.\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    for(int i = 0; i < 4; i++) {\n        int index = i;\n        bins[index] = count_fractional(quartiles[i]);\n    }\n}",
            "// TODO: Replace with your implementation\n}",
            "// implement this function\n}",
            "//TODO: Implement this function!\n\n    // get the index of the thread\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int stride = blockDim.x * gridDim.x;\n\n    // get the global id of the thread\n    int global_id = threadIdx.x + blockIdx.x * blockDim.x;\n\n    // start the counting\n    for (int i = 0; i < 4; i++)\n        bins[i] = 0;\n\n    for (int i = 0; i < N; i++) {\n        // each thread will check the value of x[i]\n        // if it belongs to the current bin, then count it\n        // check x[i] if it is bigger than the lower bound\n        // and smaller than the upper bound\n        if (x[i] > 0 && x[i] < 0.25)\n            atomicAdd(&bins[0], 1);\n        else if (x[i] >= 0.25 && x[i] < 0.5)\n            atomicAdd(&bins[1], 1);\n        else if (x[i] >= 0.5 && x[i] < 0.75)\n            atomicAdd(&bins[2], 1);\n        else if (x[i] >= 0.75 && x[i] <= 1.0)\n            atomicAdd(&bins[3], 1);\n    }\n\n    // synchronize the threads\n    __syncthreads();\n}",
            "// TODO: add your code here\n}",
            "// YOUR CODE HERE\n  \n}",
            "if (threadIdx.x == 0) {\n        bins[0] = bins[1] = bins[2] = bins[3] = 0;\n    }\n    __syncthreads();\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    while (tid < N) {\n        int bin = 0;\n        if (x[tid] < 0.25) {\n            bin = 0;\n        } else if (x[tid] >= 0.25 && x[tid] < 0.5) {\n            bin = 1;\n        } else if (x[tid] >= 0.5 && x[tid] < 0.75) {\n            bin = 2;\n        } else if (x[tid] >= 0.75 && x[tid] <= 1.0) {\n            bin = 3;\n        }\n        atomicAdd(&bins[bin], 1);\n        tid += blockDim.x * gridDim.x;\n    }\n    __syncthreads();\n}",
            "// TODO: implement countQuartiles kernel\n}",
            "int tid = threadIdx.x;\n    int blockSize = blockDim.x;\n    int bid = blockIdx.x;\n    int idx = bid * blockSize + tid;\n\n    int i;\n    int numOfQuartiles = 4;\n    int numOfThreads = blockSize * gridDim.x;\n\n    int step = ceil((double)N / (double)numOfThreads);\n    for (i = 0; i < N; i += step) {\n        if (x[idx] >= i && x[idx] < i + step) {\n            if (x[idx] - i <= 0.25) {\n                atomicAdd(&bins[0], 1);\n            } else if (x[idx] - i <= 0.5) {\n                atomicAdd(&bins[1], 1);\n            } else if (x[idx] - i <= 0.75) {\n                atomicAdd(&bins[2], 1);\n            } else {\n                atomicAdd(&bins[3], 1);\n            }\n        }\n    }\n}",
            "bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    // TODO: Use CUDA to parallelize the counting\n    // HINT:\n    //     1. If the device function is not a device function, it will be called once per thread.\n    //        If it is a device function, it will be called once per thread block.\n    //     2. If the device function is not a device function, it must be called from a kernel.\n    //        If it is a device function, it may be called from a kernel or from the host.\n    //     3. The function will be evaluated elementwise, and the results stored in `bins`.\n    //        Do not assume that the entire vector is passed to the function.\n    //     4. To determine the number of threads and blocks to use, consider the number of elements.\n    //        (Note: this is a toy problem so the number of elements is small).\n}",
            "// TODO\n}",
            "int index = threadIdx.x + blockIdx.x*blockDim.x;\n    if (index >= N) return;\n    int b = (int) (4 * x[index]);\n    atomicAdd(&bins[b], 1);\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    double frac = fmod(x[i], 1.0);\n    if (frac < 0.25)\n      atomicAdd(&bins[0], 1);\n    else if (frac < 0.50)\n      atomicAdd(&bins[1], 1);\n    else if (frac < 0.75)\n      atomicAdd(&bins[2], 1);\n    else\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "size_t t = threadIdx.x;\n    size_t offset = blockIdx.x*blockDim.x + t;\n\n    // If this thread is in the last block, and there are not enough elements to fill the\n    // threadblock, then offset will be greater than N. In this case, we don't do anything.\n    if (offset < N) {\n        // Determine which bin this value is in.\n        double val = x[offset];\n        double lowerBound = (val/4) - 0.25;\n        size_t bin;\n        if (val < 1) {\n            bin = 0;\n        } else if (val < 2) {\n            bin = 1;\n        } else if (val < 3) {\n            bin = 2;\n        } else {\n            bin = 3;\n        }\n\n        // Increment the count for the bin in which this value resides.\n        bins[bin] += 1;\n    }\n}",
            "// TODO\n}",
            "int thread_id = threadIdx.x + blockIdx.x * blockDim.x;\n    int stride = blockDim.x * gridDim.x;\n\n    double bins_values[4] = {0.0, 0.25, 0.5, 0.75};\n\n    for (size_t i = thread_id; i < N; i += stride) {\n        double value = x[i];\n\n        for (int j = 0; j < 4; j++) {\n            if (value < bins_values[j + 1]) {\n                atomicAdd(&bins[j], 1);\n            }\n        }\n    }\n}",
            "__shared__ size_t shared[N];\n    size_t i = threadIdx.x;\n    size_t j = threadIdx.y;\n    while (i < N) {\n        double f = fmod(x[i], 1.0);\n        if (f < 0.25) {\n            if (j < 4) {\n                atomicAdd(&shared[j], 1);\n            }\n        } else if (f < 0.5) {\n            if (j < 3) {\n                atomicAdd(&shared[j + 1], 1);\n            }\n        } else if (f < 0.75) {\n            if (j < 2) {\n                atomicAdd(&shared[j + 2], 1);\n            }\n        } else {\n            if (j < 1) {\n                atomicAdd(&shared[j + 3], 1);\n            }\n        }\n        i += blockDim.x;\n    }\n    __syncthreads();\n    if (threadIdx.y == 0) {\n        for (int j = 0; j < 4; j++) {\n            atomicAdd(&bins[j], shared[j]);\n        }\n    }\n}",
            "// compute the thread index\n  size_t thread_idx = threadIdx.x + blockIdx.x * blockDim.x;\n\n  // make sure that the thread index is valid\n  if (thread_idx < N) {\n\n    // compute the fractional part of the number stored in x\n    double fraction = fmod(x[thread_idx], 1.0);\n\n    // compute which bin the fractional part belongs to\n    int bin;\n    if (fraction < 0.25) {\n      bin = 0;\n    } else if (fraction < 0.5) {\n      bin = 1;\n    } else if (fraction < 0.75) {\n      bin = 2;\n    } else {\n      bin = 3;\n    }\n\n    // update the bin counts\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO\n}",
            "// insert code here\n}",
            "//TODO\n}",
            "int blockId = blockIdx.x;\n    int threadId = threadIdx.x;\n    int i;\n    if(threadId==0) {\n        bins[0]=0;\n        bins[1]=0;\n        bins[2]=0;\n        bins[3]=0;\n    }\n    __syncthreads();\n    double *myBins = (double*) malloc(4*sizeof(double));\n    myBins[0]=0;\n    myBins[1]=0;\n    myBins[2]=0;\n    myBins[3]=0;\n    for (i=0; i<N; i++) {\n        if ((x[i]+0.25)/1.0 < x[i]) {\n            myBins[0] += 1;\n        }\n        else if ((x[i]+0.25)/1.0 < x[i]+0.25) {\n            myBins[1] += 1;\n        }\n        else if ((x[i]+0.25)/1.0 < x[i]+0.5) {\n            myBins[2] += 1;\n        }\n        else if ((x[i]+0.25)/1.0 < x[i]+0.75) {\n            myBins[3] += 1;\n        }\n    }\n    __syncthreads();\n    for (i=0; i<4; i++) {\n        if (threadId==0) {\n            atomicAdd(&bins[i], myBins[i]);\n        }\n    }\n    __syncthreads();\n    if (threadId==0) {\n        free(myBins);\n    }\n}",
            "}",
            "// TODO: Implement this function!\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    // TODO: Fill in this kernel.\n    size_t bins_idx = 0;\n    if (idx < N) {\n        double fractional_part = modf(x[idx], &bins_idx);\n        if (fractional_part >= 0 && fractional_part <= 0.25) {\n            bins[0]++;\n        }\n        else if (fractional_part >= 0.25 && fractional_part <= 0.5) {\n            bins[1]++;\n        }\n        else if (fractional_part >= 0.5 && fractional_part <= 0.75) {\n            bins[2]++;\n        }\n        else if (fractional_part >= 0.75 && fractional_part <= 1) {\n            bins[3]++;\n        }\n    }\n}",
            "// Allocate shared memory for the thread's histogram data\n\t__shared__ double hist[4];\n\n\t// threadIdx.x is in the range [0, blockDim.x). \n\t// Thus, we have a total of blockDim.x threads running in parallel\n\t// In our case, we have 1024 threads.\n\tint index = threadIdx.x;\n\n\thist[index] = 0; // initalize the histogram to 0\n\n\t// Each thread loads a fractional part from the array x\n\tdouble fractional_part = fmod(x[index], 1.0);\n\n\t// Each thread adds 1 to its bin if it belongs in that bin.\n\tif (fractional_part < 0.25) {\n\t\tatomicAdd(&hist[0], 1.0);\n\t}\n\telse if (fractional_part < 0.5) {\n\t\tatomicAdd(&hist[1], 1.0);\n\t}\n\telse if (fractional_part < 0.75) {\n\t\tatomicAdd(&hist[2], 1.0);\n\t}\n\telse {\n\t\tatomicAdd(&hist[3], 1.0);\n\t}\n\n\t__syncthreads();\n\n\t// Store the histogram in the output array\n\tfor (int i = 0; i < 4; ++i) {\n\t\tbins[i] = (size_t)hist[i];\n\t}\n}",
            "// TODO\n}",
            "// TODO\n\n}",
            "}",
            "// TODO\n\n}",
            "/*\n       Use a similar approach to countVotes() above.\n    */\n    /*\n       After writing your code, be sure to compare your code with\n       countVotes() above to ensure that your code is correct.\n    */\n}",
            "size_t index = threadIdx.x + blockIdx.x * blockDim.x;\n  if (index < N) {\n    double v = x[index];\n    double v0 = floor(v);\n    double v1 = floor(v + 0.25);\n    size_t b0 = (size_t)v0;\n    size_t b1 = (size_t)v1;\n    atomicAdd(&bins[b0], (index + 1) - b0);\n    atomicAdd(&bins[b1], b1 - b0);\n  }\n}",
            "__shared__ double partialSums[4];\n    __shared__ size_t partialCounts[4];\n\n    // Initialize partial sums to 0\n    if(threadIdx.x == 0) {\n        partialSums[0] = 0;\n        partialSums[1] = 0;\n        partialSums[2] = 0;\n        partialSums[3] = 0;\n    }\n    __syncthreads();\n\n    size_t myIdx = threadIdx.x;\n    size_t blockSize = blockDim.x;\n\n    // Compute quartile indices\n    size_t startQuartile = myIdx * (N / blockSize);\n    size_t endQuartile = (myIdx + 1) * (N / blockSize);\n\n    // Find the number of doubles in each quartile\n    size_t count = 0;\n    for (size_t i = startQuartile; i < endQuartile; i++) {\n        if (i >= N) break;\n\n        double frac = modf(x[i], NULL);\n        if (frac > 0.25 && frac < 0.5) {\n            count++;\n        } else if (frac > 0.5 && frac < 0.75) {\n            count++;\n        } else if (frac > 0.75 && frac <= 1.0) {\n            count++;\n        }\n    }\n\n    // Compute partial sums\n    partialSums[myIdx] = count;\n    __syncthreads();\n\n    // Sum partial sums\n    for (size_t i = blockDim.x / 2; i > 0; i /= 2) {\n        if (myIdx < i) {\n            partialSums[myIdx] += partialSums[myIdx + i];\n        }\n        __syncthreads();\n    }\n\n    // Store sum at the appropriate index\n    if (myIdx == 0) {\n        partialCounts[0] = partialSums[0];\n        partialCounts[1] = partialSums[1];\n        partialCounts[2] = partialSums[2];\n        partialCounts[3] = partialSums[3];\n    }\n    __syncthreads();\n\n    // Reduce the partial sums to the desired output\n    if (myIdx < 4) {\n        bins[myIdx] = partialCounts[myIdx];\n    }\n}",
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    size_t i = bid * blockDim.x + tid;\n    if (i < N) {\n        if (x[i] < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (x[i] < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (x[i] < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "// TODO: write this function!\n}",
            "// Compute the thread index\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    // Get the bin index\n    int bin = tid / N;\n    // Get the element in x that this thread should work on\n    int element = tid % N;\n    // Initialize the local count to zero\n    int localCount = 0;\n    // Check if the element falls in the appropriate bin\n    if (x[element] < 0.25) {\n        localCount += 1;\n    } else if (x[element] >= 0.25 && x[element] < 0.5) {\n        localCount += 1;\n    } else if (x[element] >= 0.5 && x[element] < 0.75) {\n        localCount += 1;\n    } else if (x[element] >= 0.75) {\n        localCount += 1;\n    }\n    // Store the local count in the shared memory\n    int *sharedCount = (int *)&(sdata[blockIdx.x][0]);\n    // Store the local count in shared memory\n    sharedCount[element] = localCount;\n    // Wait for all threads to finish\n    __syncthreads();\n    // Add the local counts\n    bins[0] = (int)sdata[0][0];\n    bins[1] = (int)sdata[0][1];\n    bins[2] = (int)sdata[0][2];\n    bins[3] = (int)sdata[0][3];\n    // Wait for all threads to finish\n    __syncthreads();\n}",
            "int tx = threadIdx.x;\n    int ty = threadIdx.y;\n    int tz = threadIdx.z;\n    int bx = blockIdx.x;\n    int by = blockIdx.y;\n    int bz = blockIdx.z;\n    int bw = blockDim.x;\n    int bh = blockDim.y;\n    int bd = blockDim.z;\n\n    int id = (tz*bh + ty)*bw + tx + bx*(bw*bh*bw) + by*(bw*bh*bw*bh) + bz*(bw*bh*bw*bh*bw);\n    if (id >= N) return;\n\n    double d = x[id];\n    if (d >= 0.0 && d < 0.25) bins[0]++;\n    else if (d >= 0.25 && d < 0.5) bins[1]++;\n    else if (d >= 0.5 && d < 0.75) bins[2]++;\n    else bins[3]++;\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    int totalThreads = gridDim.x * blockDim.x;\n    int i;\n\n    for (i = tid; i < N; i += totalThreads) {\n        if (x[i] < 0.25)\n            bins[0]++;\n        else if (x[i] < 0.5)\n            bins[1]++;\n        else if (x[i] < 0.75)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    for (int i = blockIdx.x*blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        if (x[i] < 0.25) {\n            bins[0]++;\n        } else if (x[i] < 0.5) {\n            bins[1]++;\n        } else if (x[i] < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "// The index of the thread\n  size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  // The index of the thread block\n  size_t j = blockIdx.x;\n\n  if (i >= N) return;\n\n  // If the thread is within N, then increment the appropriate bin\n  if (x[i] < 0.25)\n    atomicAdd(&bins[0], 1);\n  else if (x[i] < 0.5)\n    atomicAdd(&bins[1], 1);\n  else if (x[i] < 0.75)\n    atomicAdd(&bins[2], 1);\n  else\n    atomicAdd(&bins[3], 1);\n}",
            "// TODO\n}",
            "/*\n       Your code here\n    */\n    \n    __shared__ size_t count[4];\n\n    if(threadIdx.x == 0){\n        count[0] = count[1] = count[2] = count[3] = 0;\n    }\n\n    __syncthreads();\n\n    for(size_t i = threadIdx.x + blockDim.x * blockIdx.x; i < N; i += blockDim.x * gridDim.x){\n        if(x[i] >= 0.75 && x[i] < 1){\n            atomicAdd(&count[3], 1);\n        }\n        else if(x[i] >= 0.5 && x[i] < 0.75){\n            atomicAdd(&count[2], 1);\n        }\n        else if(x[i] >= 0.25 && x[i] < 0.5){\n            atomicAdd(&count[1], 1);\n        }\n        else if(x[i] >= 0 && x[i] < 0.25){\n            atomicAdd(&count[0], 1);\n        }\n    }\n\n    if(threadIdx.x == 0){\n        bins[0] = count[0];\n        bins[1] = count[1];\n        bins[2] = count[2];\n        bins[3] = count[3];\n    }\n}",
            "// TODO\n}",
            "unsigned int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        if (x[i] >= 0.0 && x[i] < 0.25)\n            atomicAdd(&bins[0], 1);\n        else if (x[i] >= 0.25 && x[i] < 0.5)\n            atomicAdd(&bins[1], 1);\n        else if (x[i] >= 0.5 && x[i] < 0.75)\n            atomicAdd(&bins[2], 1);\n        else if (x[i] >= 0.75 && x[i] <= 1.0)\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N) {\n        if (0 <= x[idx] && x[idx] < 0.25) {\n            bins[0] += 1;\n        }\n        else if (0.25 <= x[idx] && x[idx] < 0.5) {\n            bins[1] += 1;\n        }\n        else if (0.5 <= x[idx] && x[idx] < 0.75) {\n            bins[2] += 1;\n        }\n        else {\n            bins[3] += 1;\n        }\n    }\n}",
            "}",
            "}",
            "/*\n    This is a basic implementation of this function. You will\n    need to extend it to fill in the missing lines.\n  */\n  /*\n    The idea is to use if-else if-else-if logic to count the\n    number of doubles in the vector x that have a fractional part\n    in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n    Store the counts in `bins`.\n    For instance, suppose `x = [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]`.\n    Then, `bin[0]` will count how many doubles in `x` have a fractional part\n    in [0, 0.25), `bin[1]` will count how many doubles in `x` have a fractional part\n    in [0.25, 0.5), `bin[2]` will count how many doubles in `x` have a fractional part\n    in [0.5, 0.75), and `bin[3]` will count how many doubles in `x` have a fractional part\n    in [0.75, 1).\n  */\n\n  int tid = threadIdx.x;\n  if (tid >= N) {\n    return;\n  }\n\n  double frac;\n  for (int i = 0; i < 4; i++) {\n    if (x[tid] < (i + 0.25) * 2) {\n      bins[i]++;\n      break;\n    }\n  }\n}",
            "// TODO\n}",
            "// TODO: Your code here. \n\t// This is an example to get you started.\n\tsize_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n\tif (tid < N) {\n\t\tdouble element = x[tid];\n\t\tif (element > 0 && element < 1) {\n\t\t\tif (element > 0 && element < 0.25)\n\t\t\t\tatomicAdd(&bins[0], 1);\n\t\t\telse if (element > 0.25 && element < 0.5)\n\t\t\t\tatomicAdd(&bins[1], 1);\n\t\t\telse if (element > 0.5 && element < 0.75)\n\t\t\t\tatomicAdd(&bins[2], 1);\n\t\t\telse\n\t\t\t\tatomicAdd(&bins[3], 1);\n\t\t}\n\t}\n}",
            "// TODO\n}",
            "}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    for(int i = threadIdx.x; i < N; i += blockDim.x){\n        if(x[i] < 0.25){\n            atomicAdd(&bins[0], 1);\n        } else if(x[i] >= 0.25 && x[i] < 0.5){\n            atomicAdd(&bins[1], 1);\n        } else if(x[i] >= 0.5 && x[i] < 0.75){\n            atomicAdd(&bins[2], 1);\n        } else{\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx >= N) {\n    return;\n  }\n  if (idx < N) {\n    double x_idx = x[idx];\n    if (x_idx < 0.25) {\n      atomicAdd(&bins[0], 1);\n    }\n    else if (x_idx < 0.5) {\n      atomicAdd(&bins[1], 1);\n    }\n    else if (x_idx < 0.75) {\n      atomicAdd(&bins[2], 1);\n    }\n    else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "// TODO: YOUR CODE HERE\n}",
            "int i = threadIdx.x;\n\n    if (i > N) return;\n    double d = x[i];\n\n    if ((d >= 0.0) && (d < 0.25)) {\n        atomicAdd(&bins[0], 1);\n    }\n    else if ((d >= 0.25) && (d < 0.5)) {\n        atomicAdd(&bins[1], 1);\n    }\n    else if ((d >= 0.5) && (d < 0.75)) {\n        atomicAdd(&bins[2], 1);\n    }\n    else if ((d >= 0.75) && (d <= 1.0)) {\n        atomicAdd(&bins[3], 1);\n    }\n}",
            "// your code here\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        double val = x[tid];\n        if (val < 0.25) bins[0]++;\n        else if (val < 0.5) bins[1]++;\n        else if (val < 0.75) bins[2]++;\n        else bins[3]++;\n    }\n}",
            "// Insert code here\n    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid >= N)\n        return;\n    unsigned int b = 0;\n    double d = x[tid];\n    if (d >= 0.0 && d < 0.25) {\n        atomicAdd(&bins[b++], 1);\n    }\n    if (d >= 0.25 && d < 0.5) {\n        atomicAdd(&bins[b++], 1);\n    }\n    if (d >= 0.5 && d < 0.75) {\n        atomicAdd(&bins[b++], 1);\n    }\n    if (d >= 0.75 && d < 1.0) {\n        atomicAdd(&bins[b++], 1);\n    }\n}",
            "// start with bins initialized to zero\n\tbins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n\t// TODO: YOUR CODE HERE\n\n\n}",
            "__shared__ double buffer[BLOCK_SIZE];\n\n    /*\n        Each block is assigned a thread range within [0, N)\n        Assign the thread to a quartile:\n        - if index < 0.25*N, quartile = 0\n        - if index < 0.75*N, quartile = 1\n        - if index < N, quartile = 2\n        - else quartile = 3\n    */\n    const size_t index = threadIdx.x + blockDim.x * blockIdx.x;\n    const size_t N25 = N / 4;\n\n    // Copy block range of data from global memory to shared memory\n    if (threadIdx.x < N) {\n        buffer[threadIdx.x] = x[threadIdx.x];\n    } else {\n        buffer[threadIdx.x] = 0;\n    }\n    __syncthreads();\n\n    size_t quartile = 3;\n    if (index < N25) {\n        quartile = 0;\n    } else if (index < N25 * 2) {\n        quartile = 1;\n    } else if (index < N25 * 3) {\n        quartile = 2;\n    }\n\n    // Increment the count of the appropriate quartile\n    bins[quartile] += __popc(quartile == 2 || quartile == 1 || quartile == 0);\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use MPI and OpenMP to compute in parallel. Assume that MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO\n  int num_procs, my_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n  int size_per_rank = x.size()/num_procs;\n  int remainder = x.size()%num_procs;\n\n  int start = my_rank*size_per_rank;\n  int end = start+size_per_rank-1;\n\n  if(my_rank == num_procs-1){\n    end += remainder;\n  }\n\n  std::vector<double> my_x;\n  for(int i=start; i<=end; ++i){\n    my_x.push_back(x[i]);\n  }\n\n  std::array<size_t, 4> my_bins;\n\n#pragma omp parallel for\n  for(int i=0; i<my_x.size(); ++i){\n    if(my_x[i] < 0.25){\n      my_bins[0]++;\n    }\n    else if(my_x[i] < 0.5){\n      my_bins[1]++;\n    }\n    else if(my_x[i] < 0.75){\n      my_bins[2]++;\n    }\n    else{\n      my_bins[3]++;\n    }\n  }\n\n  MPI_Gather(my_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "// Get the number of ranks and this rank\n    int rank, num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    // If this rank is 0, set the bins to zero\n    if (rank == 0) {\n        bins.fill(0);\n    }\n\n    // Count the number of doubles in the vector x\n    const int n = x.size();\n    // Compute the chunk size for each rank\n    int chunk_size = ceil(n / (double) num_ranks);\n\n    // Split the vector x into n_local chunks\n    std::vector<double> x_local;\n    for (int i = 0; i < chunk_size; ++i) {\n        if (i + rank * chunk_size < n) {\n            x_local.push_back(x.at(i + rank * chunk_size));\n        }\n    }\n\n    // Count the number of fractions in the vector x_local\n    const int n_local = x_local.size();\n\n    // Compute the fractional part of each double in x_local\n    std::vector<double> fracs;\n    for (int i = 0; i < n_local; ++i) {\n        fracs.push_back(x_local.at(i) - floor(x_local.at(i)));\n    }\n\n    // Count the number of fractions in each range\n    std::array<int, 4> counts = {0, 0, 0, 0};\n    for (int i = 0; i < n_local; ++i) {\n        if (fracs.at(i) < 0.25) {\n            counts.at(0)++;\n        } else if (fracs.at(i) >= 0.25 && fracs.at(i) < 0.5) {\n            counts.at(1)++;\n        } else if (fracs.at(i) >= 0.5 && fracs.at(i) < 0.75) {\n            counts.at(2)++;\n        } else if (fracs.at(i) >= 0.75 && fracs.at(i) <= 1) {\n            counts.at(3)++;\n        }\n    }\n\n    // Add the local counts to the global counts\n    bins[0] += counts[0];\n    bins[1] += counts[1];\n    bins[2] += counts[2];\n    bins[3] += counts[3];\n\n    // Reduce the counts on all the ranks\n    std::array<int, 4> total_counts = {0, 0, 0, 0};\n    MPI_Reduce(&bins, &total_counts, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // Set the bins if this is the root rank\n    if (rank == 0) {\n        bins[0] = total_counts[0];\n        bins[1] = total_counts[1];\n        bins[2] = total_counts[2];\n        bins[3] = total_counts[3];\n    }\n\n    // Synchronize all the ranks\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "auto num_ranks = mpi::getSize();\n  auto rank = mpi::getRank();\n\n  if (rank == 0) {\n    bins.fill(0);\n  }\n\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double d = x[i] - floor(x[i]);\n    if (d >= 0 && d < 0.25) {\n      bins[0]++;\n    }\n    else if (d >= 0.25 && d < 0.5) {\n      bins[1]++;\n    }\n    else if (d >= 0.5 && d < 0.75) {\n      bins[2]++;\n    }\n    else if (d >= 0.75) {\n      bins[3]++;\n    }\n  }\n\n  // compute the global bins from each rank's bin\n  mpi::allReduce(&bins[0], 4, MPI_SUM);\n\n  if (rank == 0) {\n    for (size_t i = 0; i < bins.size(); ++i) {\n      printf(\"%zu \", bins[i]);\n    }\n    printf(\"\\n\");\n  }\n}",
            "const size_t N = x.size();\n    const double p0 = 0.0;\n    const double p1 = 0.25;\n    const double p2 = 0.5;\n    const double p3 = 0.75;\n    size_t mpi_size, mpi_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n    std::vector<size_t> bins_local(4);\n    if(mpi_rank == 0) {\n        std::sort(x.begin(), x.end());\n        for(size_t i = 0; i < N; i++) {\n            if(x[i] >= p0 && x[i] < p1) bins_local[0]++;\n            else if(x[i] >= p1 && x[i] < p2) bins_local[1]++;\n            else if(x[i] >= p2 && x[i] < p3) bins_local[2]++;\n            else bins_local[3]++;\n        }\n    } else {\n        std::vector<double> x_local(N);\n        MPI_Scatter(x.data(), N/mpi_size, MPI_DOUBLE, x_local.data(), N/mpi_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        #pragma omp parallel for\n        for(size_t i = 0; i < N; i++) {\n            if(x_local[i] >= p0 && x_local[i] < p1) bins_local[0]++;\n            else if(x_local[i] >= p1 && x_local[i] < p2) bins_local[1]++;\n            else if(x_local[i] >= p2 && x_local[i] < p3) bins_local[2]++;\n            else bins_local[3]++;\n        }\n    }\n    std::array<size_t, 4> bins_global;\n    MPI_Gather(bins_local.data(), 4, MPI_UNSIGNED_LONG_LONG, bins_global.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n    if(mpi_rank == 0) bins = bins_global;\n}",
            "size_t n = x.size();\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, num_ranks;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &num_ranks);\n    double lo = 0, hi = 1;\n    size_t bin_size = n/num_ranks;\n    if(rank == 0) {\n        for(size_t i = 0; i < num_ranks; i++) {\n            lo = i * bin_size;\n            hi = lo + bin_size;\n        }\n    }\n    std::array<size_t, 4> counts;\n    for(size_t i = 0; i < num_ranks; i++) {\n        if(rank == i) {\n            //compute the fractional part of each element of x that is in the range [lo, hi]\n            std::vector<double> temp;\n            for(size_t j = 0; j < n; j++) {\n                if(x[j] >= lo && x[j] < hi) {\n                    temp.push_back(x[j]);\n                }\n            }\n            counts[0] = countFractionalPart(temp, 0, 0.25);\n            counts[1] = countFractionalPart(temp, 0.25, 0.5);\n            counts[2] = countFractionalPart(temp, 0.5, 0.75);\n            counts[3] = countFractionalPart(temp, 0.75, 1);\n        }\n        MPI_Bcast(&counts, 4, MPI_UNSIGNED_LONG_LONG, i, comm);\n        bins[0] += counts[0];\n        bins[1] += counts[1];\n        bins[2] += counts[2];\n        bins[3] += counts[3];\n    }\n}",
            "int nranks, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  assert(nranks > 0);\n\n  // Compute the local count of items in the interval [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n  int nlocal = x.size();\n  std::array<size_t, 4> local_bins;\n  for (int i = 0; i < 4; ++i) {\n    local_bins[i] = 0;\n    for (int j = 0; j < nlocal; ++j) {\n      double val = x[j];\n      if (val >= (double)(i) / 4.0 && val < (double)(i + 1) / 4.0) {\n        local_bins[i]++;\n      }\n    }\n  }\n\n  // Compute the total number of items in each of the intervals.\n  std::array<size_t, 4> total_bins;\n  MPI_Allreduce(&local_bins, &total_bins, 4, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  // Copy the local counts to the output array.\n  for (int i = 0; i < 4; ++i) {\n    bins[i] = local_bins[i];\n  }\n\n}",
            "size_t const local_size = x.size();\n\n    size_t num_bins = 4;\n    double const delta = 1.0 / num_bins;\n\n    std::array<size_t, 4> local_bins = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < local_size; i++) {\n        size_t bin_idx = (size_t)((x[i] - std::floor(x[i])) / delta);\n        if (bin_idx >= num_bins) {\n            bin_idx = num_bins - 1;\n        }\n        local_bins[bin_idx]++;\n    }\n\n    // MPI_Reduce to sum the bins from each rank\n    std::vector<size_t> bins_all(4);\n    MPI_Reduce(&local_bins[0], &bins_all[0], 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    // if rank 0, copy into the output array\n    if (MPI_",
            "/*\n   * TODO\n   *\n   * The fractional parts of doubles are all in the interval [0, 1)\n   * So just check where the fractional part of x is in the interval [0, 0.25), [0.25, 0.5), [0.5, 0.75), [0.75, 1)\n   *\n   * 0.25 is 1/4\n   * 0.5 is 1/2\n   * 0.75 is 3/4\n   *\n   * Note: x.size() is the number of elements in the vector x.\n   */\n\n  // TODO: initialize bins with zeros\n\n  // TODO: use OpenMP to parallelize the loop over the input vector\n\n  // TODO: use MPI to distribute the vector x to all ranks and do the counting\n  //\n  // Note that the number of elements in x is a multiple of the number of ranks.\n  // \n  // The number of elements to be distributed to each rank is \n  // x.size() / world_size\n  //\n  // The starting point of the distributed vector on a given rank is given by\n  //\n  // rank * (x.size() / world_size)\n\n  // TODO: On rank 0, reduce the partial counts bins to the total counts.\n  //\n  // Note: \n  // 1. MPI_SUM is the reduction operation\n  // 2. MPI_Bcast is not needed in this function.\n  // 3. The total count vector is of size 4, so all MPI_Reduce calls need to be done on an array of size 4.\n\n  // TODO: free all MPI allocated memory\n  return;\n}",
            "if (bins.size()!= 4) {\n        throw std::invalid_argument(\"bins should be an array of length 4.\");\n    }\n\n    int comm_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);\n\n    int comm_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n\n    if (comm_size < 2) {\n        throw std::runtime_error(\"Counting must be done in at least 2 ranks.\");\n    }\n\n    if (comm_size!= (int) x.size()) {\n        throw std::runtime_error(\"Communication size should be equal to the vector size.\");\n    }\n\n    // get the number of values in the vector\n    int values_per_rank = (int) x.size() / comm_size;\n\n    // get the number of values for each rank\n    std::vector<int> values_per_rank_vec(comm_size);\n\n    // fill the values per rank vector\n    for (int i = 0; i < comm_size; i++) {\n        values_per_rank_vec[i] = values_per_rank;\n    }\n\n    // get the total number of values\n    int total_values = values_per_rank * comm_size;\n\n    // get the values to be processed by the current rank\n    int start_value = values_per_rank * comm_rank;\n    int end_value = values_per_rank * comm_rank + values_per_rank;\n\n    if (comm_rank == comm_size - 1) {\n        end_value = total_values;\n    }\n\n    // get the total number of values of each bin\n    int total_values_per_bin = total_values / 4;\n\n    // calculate the fractional part of each value\n    std::vector<double> fractional_part(end_value - start_value);\n\n    for (int i = 0; i < end_value - start_value; i++) {\n        fractional_part[i] = std::modf(x[start_value + i], &fractional_part[i]);\n    }\n\n    // get the values of each bin on the current rank\n    std::vector<int> values_per_bin(4);\n\n    // calculate the number of values in each bin\n    for (int i = 0; i < 4; i++) {\n        int bin_start = start_value + i * total_values_per_bin;\n        int bin_end = bin_start + total_values_per_bin;\n\n        values_per_bin[i] = end_value - bin_start;\n\n        if (i == 3) {\n            bin_end = total_values;\n        }\n\n        values_per_bin[i] = end_value - bin_start;\n    }\n\n    // get the number of values per rank for each bin\n    std::vector<std::vector<int>> values_per_rank_per_bin(4, std::vector<int>(comm_size));\n\n    // fill the number of values per rank for each bin\n    for (int i = 0; i < 4; i++) {\n        for (int j = 0; j < comm_size; j++) {\n            values_per_rank_per_bin[i][j] = values_per_rank_vec[j] / 4;\n        }\n    }\n\n    // calculate the index of the bin for each value in the vector\n    std::vector<int> bin_index(end_value - start_value);\n\n    // fill the bin index vector\n    for (int i = 0; i < end_value - start_value; i++) {\n        int bin_index_value = std::floor((double) (i + start_value) / values_per_bin[3]);\n\n        if (bin_index_value == 4) {\n            bin_index_value = 3;\n        }\n\n        bin_index[i] = bin_index_value;\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < end_value - start_value; i++) {\n        // calculate the rank for the current bin\n        int rank_",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t num_x = x.size();\n    size_t bin_size = num_x / size;\n    size_t num_bins = size_t(num_x / bin_size);\n\n    std::vector<size_t> x_bins(num_bins);\n    size_t bin_id = rank;\n    for (size_t i = 0; i < num_bins; ++i) {\n        for (size_t j = 0; j < bin_size; ++j) {\n            size_t idx = bin_id * bin_size + j;\n            double frac = x[idx] - floor(x[idx]);\n            if (frac >= 0 && frac < 0.25) {\n                ++x_bins[i];\n            } else if (frac >= 0.25 && frac < 0.5) {\n                ++x_bins[i + 1];\n            } else if (frac >= 0.5 && frac < 0.75) {\n                ++x_bins[i + 2];\n            } else if (frac >= 0.75 && frac <= 1) {\n                ++x_bins[i + 3];\n            }\n        }\n        bin_id += size;\n    }\n\n    bins = std::array<size_t, 4>();\n    if (rank == 0) {\n        for (size_t i = 0; i < x_bins.size(); ++i) {\n            if (i == 0)\n                bins[0] += x_bins[i];\n            else if (i == 1)\n                bins[1] += x_bins[i];\n            else if (i == 2)\n                bins[2] += x_bins[i];\n            else if (i == 3)\n                bins[3] += x_bins[i];\n            else\n                bins[4] += x_bins[i];\n        }\n    }\n\n#pragma omp parallel for\n    for (int i = 1; i < size; ++i) {\n        MPI_Status status;\n        MPI_Recv(&bins, 4, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "int n_ranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n\n  if(n_ranks > 1){\n    std::vector<double> sub_x(x.begin() + omp_get_thread_num(), x.begin() + (n_ranks-1)*omp_get_num_threads() + omp_get_thread_num());\n    std::array<size_t, 4> sub_bins;\n    MPI_Allreduce(MPI_IN_PLACE, sub_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (rank == 0) {\n      bins = sub_bins;\n    }\n  }\n  else {\n    std::array<size_t, 4> bins;\n    for(size_t i = 0; i < x.size(); i++){\n      if(x[i] < 0.25) {\n        bins[0]++;\n      } else if(x[i] >= 0.25 && x[i] < 0.5) {\n        bins[1]++;\n      } else if(x[i] >= 0.5 && x[i] < 0.75) {\n        bins[2]++;\n      } else {\n        bins[3]++;\n      }\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n  }\n}",
            "// TODO: Your code here\n}",
            "int mpi_size, mpi_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    size_t size = x.size();\n    bins.fill(0);\n\n    // if mpi size is 1, then we are already done\n    if (mpi_size == 1) {\n        return;\n    }\n\n    // calculate the number of elements we need to count\n    size_t elems_to_count = size / mpi_size;\n    // get the rest\n    size_t remaining_elems = size % mpi_size;\n\n    std::vector<double> local_vec;\n\n    // set the size of our local vector\n    if (remaining_elems == 0) {\n        local_vec.resize(elems_to_count);\n    } else {\n        if (mpi_rank < remaining_elems) {\n            local_vec.resize(elems_to_count + 1);\n        } else {\n            local_vec.resize(elems_to_count);\n        }\n    }\n\n    // get our local vector\n    MPI_Scatter(&x[0], elems_to_count, MPI_DOUBLE, &local_vec[0], elems_to_count, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // get the last element if we have it\n    if (remaining_elems!= 0 && mpi_rank == mpi_size - 1) {\n        local_vec.push_back(x[size - 1]);\n    }\n\n    // now we can count\n    #pragma omp parallel for\n    for (size_t i = 0; i < local_vec.size(); ++i) {\n        if (local_vec[i] < 0.25) {\n            bins[0] += 1;\n        } else if (local_vec[i] >= 0.25 && local_vec[i] < 0.5) {\n            bins[1] += 1;\n        } else if (local_vec[i] >= 0.5 && local_vec[i] < 0.75) {\n            bins[2] += 1;\n        } else if (local_vec[i] >= 0.75 && local_vec[i] <= 1.0) {\n            bins[3] += 1;\n        }\n    }\n\n    // now we sum the bins across processes\n    std::array<size_t, 4> bins_total;\n    bins_total.fill(0);\n\n    MPI_Allreduce(bins.data(), bins_total.data(), bins.size(), MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    // assign to our bins\n    bins = bins_total;\n\n    return;\n}",
            "// TODO: Your code here\n    return;\n}",
            "//TODO\n    return;\n}",
            "size_t size = x.size();\n\n    // 1. sort the vector x\n    std::sort(x.begin(), x.end());\n\n    // 2. get the number of doubles in each quartile\n    int num_procs;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    size_t chunk = size / num_procs;\n    size_t num_leaders = size % num_procs;\n    size_t num_workers = chunk + (num_leaders > 0);\n\n    // 3. get the quartile in each worker\n    std::array<size_t, 4> counts = {0, 0, 0, 0};\n    if (num_workers == num_procs) {\n        for (size_t i = 0; i < size; i++) {\n            if (x[i] < 0.25) {\n                counts[0]++;\n            } else if (x[i] < 0.5) {\n                counts[1]++;\n            } else if (x[i] < 0.75) {\n                counts[2]++;\n            } else if (x[i] < 1.0) {\n                counts[3]++;\n            }\n        }\n    } else {\n        size_t offset = 0;\n        for (int i = 0; i < num_procs; i++) {\n            size_t count = (i < num_leaders? chunk + 1 : chunk);\n            std::array<size_t, 4> subcount = {0, 0, 0, 0};\n            for (size_t j = 0; j < count; j++) {\n                if (x[j + offset] < 0.25) {\n                    subcount[0]++;\n                } else if (x[j + offset] < 0.5) {\n                    subcount[1]++;\n                } else if (x[j + offset] < 0.75) {\n                    subcount[2]++;\n                } else if (x[j + offset] < 1.0) {\n                    subcount[3]++;\n                }\n            }\n\n            MPI_Reduce(subcount.data(), counts.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n            if (i < num_leaders) {\n                offset += chunk;\n            } else {\n                offset += chunk + 1;\n            }\n        }\n    }\n\n    // 4. get the quartile in the leader\n    std::array<size_t, 4> sumcounts = {0, 0, 0, 0};\n    MPI_Reduce(counts.data(), sumcounts.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (mpiRank() == 0) {\n        bins = std::array<size_t, 4>{{0, 0, 0, 0}};\n        int num_workers = mpiSize();\n        for (int i = 0; i < 4; i++) {\n            bins[i] = (sumcounts[i] + num_workers - 1) / num_workers;\n        }\n    }\n}",
            "// TODO\n}",
            "size_t const mpi_size = MPI_Comm_size(MPI_COMM_WORLD);\n\tsize_t const mpi_rank = MPI_Comm_rank(MPI_COMM_WORLD);\n\tif (mpi_size <= 1) {\n\t\tthrow std::runtime_error(\"MPI size must be greater than 1\");\n\t}\n\n\tbins = std::array<size_t, 4>{};\n\tint const num_bins = 4;\n\tdouble const lower_bound = 0.0;\n\tdouble const upper_bound = 1.0;\n\tdouble const bin_size = (upper_bound - lower_bound) / num_bins;\n\tdouble const num_total = x.size();\n\tdouble const num_per_bin = num_total / num_bins;\n\n\tstd::vector<double> work(x);\n\tstd::vector<size_t> count(work.size());\n\tstd::vector<size_t> result(num_bins);\n\n\tdouble const *work_begin = work.data();\n\tdouble const *work_end = work_begin + work.size();\n\tsize_t const *count_begin = count.data();\n\tsize_t const *count_end = count_begin + count.size();\n\tsize_t const *result_begin = result.data();\n\tsize_t const *result_end = result_begin + result.size();\n\n\tint const block_size = 1000;\n\tint const block_count = work.size() / block_size + 1;\n\n\tfor (int i = 0; i < block_count; ++i) {\n\t\tsize_t local_count = 0;\n\t\tsize_t start = i * block_size;\n\t\tsize_t end = start + block_size;\n\t\tif (end > work.size()) {\n\t\t\tend = work.size();\n\t\t}\n\t\tdouble const *begin = work_begin + start;\n\t\tdouble const *end_iter = work_begin + end;\n\t\tfor (double const *iter = begin; iter < end_iter; ++iter) {\n\t\t\tif (iter->int() == iter->int()) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (iter->int() > 0 && iter->int() < bin_size) {\n\t\t\t\tlocal_count++;\n\t\t\t} else if (iter->int() > bin_size && iter->int() < bin_size * 2) {\n\t\t\t\tlocal_count++;\n\t\t\t} else if (iter->int() > bin_size * 2 && iter->int() < bin_size * 3) {\n\t\t\t\tlocal_count++;\n\t\t\t} else if (iter->int() > bin_size * 3 && iter->int() < bin_size * 4) {\n\t\t\t\tlocal_count++;\n\t\t\t}\n\t\t}\n\t\tcount[i] = local_count;\n\t}\n\n\tMPI_Allreduce(MPI_IN_PLACE, count.data(), block_count, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n\tfor (int i = 0; i < block_count; ++i) {\n\t\tint const block_start = i * block_size;\n\t\tint const block_end = (i + 1) * block_size;\n\t\tfor (size_t j = block_start; j < block_end; ++j) {\n\t\t\tif (count[i] < j) {\n\t\t\t\twork[j] = 0.0;\n\t\t\t} else {\n\t\t\t\twork[j] = x[j];\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (int i = 0; i < num_bins; ++i) {\n\t\tint const local_start = i * block_size;\n\t\tint const local_end = (i + 1) * block_size;\n\t\tdouble sum = 0.0;\n\t\tfor (int j = local_start; j < local_end; ++j) {\n\t\t\tsum += work[j];\n\t\t}\n\t\tresult[",
            "//\n    // Your code goes here\n    //\n}",
            "const size_t num_threads = omp_get_max_threads();\n    const size_t num_procs = 4;\n    if (num_threads!= num_procs) {\n        std::cout << \"Mismatching number of processors and OpenMP threads: \"\n                  << num_procs << \" vs. \" << num_threads << std::endl;\n        std::exit(EXIT_FAILURE);\n    }\n\n    // Number of elements that each processor holds.\n    size_t const num_elements_per_proc = x.size() / num_procs;\n\n    // Each processor stores a chunk of the x vector.\n    std::vector<double> x_local(num_elements_per_proc);\n\n    // Each processor needs a copy of bins.\n    std::array<size_t, 4> bins_local;\n\n    // Copy the local vector from the global vector.\n    for (size_t i = 0; i < num_elements_per_proc; ++i) {\n        x_local[i] = x[i + (num_elements_per_proc * omp_get_thread_num())];\n    }\n\n    // Count the quartiles.\n    // Here, we use a simple approach with a constant number of bins.\n    for (size_t i = 0; i < num_elements_per_proc; ++i) {\n        if (x_local[i] < 0.25)\n            bins_local[0]++;\n        else if (x_local[i] < 0.5)\n            bins_local[1]++;\n        else if (x_local[i] < 0.75)\n            bins_local[2]++;\n        else\n            bins_local[3]++;\n    }\n\n    // Synchronize all the processors.\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    // Summing the bins for each processor to get the global bins.\n    if (MPI_Get_rank(MPI_COMM_WORLD) == 0) {\n        for (int i = 1; i < num_procs; ++i) {\n            MPI_Recv(&bins_local[0], 4, MPI_UNSIGNED_LONG_LONG, i, 0,\n                     MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < 4; ++j) {\n                bins[j] += bins_local[j];\n            }\n        }\n    } else {\n        MPI_Send(&bins_local[0], 4, MPI_UNSIGNED_LONG_LONG, 0, 0,\n                 MPI_COMM_WORLD);\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        bins = {0,0,0,0};\n    }\n\n    #pragma omp parallel\n    {\n        int thread_id = omp_get_thread_num();\n        int num_threads = omp_get_num_threads();\n        int chunk_size = x.size() / num_threads;\n        int begin = chunk_size * thread_id;\n        int end = begin + chunk_size;\n        if (thread_id == num_threads - 1) end = x.size();\n        for (int i = begin; i < end; ++i) {\n            double xi = x[i];\n            if (xi >= 0 && xi < 0.25) bins[0] += 1;\n            else if (xi >= 0.25 && xi < 0.5) bins[1] += 1;\n            else if (xi >= 0.5 && xi < 0.75) bins[2] += 1;\n            else if (xi >= 0.75 && xi < 1) bins[3] += 1;\n        }\n    }\n\n    // gather data from all ranks\n    int num_values = 4 * size;\n    int * all_data = new int[num_values];\n    MPI_Allgather(bins.data(), 4, MPI_INT, all_data, 4, MPI_INT, MPI_COMM_WORLD);\n\n    // compute results\n    if (rank == 0) {\n        for (int i = 0; i < size; ++i) {\n            for (int j = 0; j < 4; ++j) {\n                bins[j] += all_data[i * 4 + j];\n            }\n        }\n    }\n\n    // free memory\n    delete [] all_data;\n}",
            "int const nranks = omp_get_num_threads();\n   int const rank = omp_get_thread_num();\n   size_t const n = x.size();\n   size_t const n_local = n/nranks;\n   std::vector<size_t> counts(4,0);\n\n   #pragma omp parallel\n   {\n      int const thread = omp_get_thread_num();\n      size_t const start = thread * n_local;\n      size_t const end = (thread == (nranks-1))? n : (thread + 1) * n_local;\n      for (size_t i = start; i < end; i++) {\n         double const val = x[i];\n         if (val < 0.25) { counts[0]++; }\n         else if (val < 0.5) { counts[1]++; }\n         else if (val < 0.75) { counts[2]++; }\n         else { counts[3]++; }\n      }\n   }\n\n   MPI_Reduce(counts.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "size_t size = x.size();\n  size_t rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int mpi_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\n  // compute the number of elements on each rank\n  size_t nlocal = size / mpi_size;\n  if (size % mpi_size!= 0) {\n    nlocal++;\n  }\n\n  size_t i;\n  double min_val = x[0];\n  double max_val = x[0];\n  for (i = 0; i < size; i++) {\n    if (x[i] < min_val) {\n      min_val = x[i];\n    }\n    if (x[i] > max_val) {\n      max_val = x[i];\n    }\n  }\n\n  std::vector<double> x_local(nlocal);\n  std::vector<size_t> bins_local(4);\n  std::vector<double> bin_bounds = {0.0, 0.25, 0.5, 0.75, 1.0};\n\n  // initialize bins\n  for (i = 0; i < 4; i++) {\n    bins_local[i] = 0;\n  }\n\n  // compute local histogram of x\n  #pragma omp parallel for\n  for (i = 0; i < nlocal; i++) {\n    x_local[i] = x[rank * nlocal + i];\n\n    if (x_local[i] >= min_val && x_local[i] <= max_val) {\n      for (int j = 0; j < 4; j++) {\n        if (x_local[i] >= bin_bounds[j] && x_local[i] < bin_bounds[j+1]) {\n          bins_local[j]++;\n        }\n      }\n    }\n  }\n\n  // compute the sum of the local histograms\n  std::vector<size_t> bins_sum(4);\n  MPI_Reduce(bins_local.data(), bins_sum.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  //std::cout << \"bins_sum = \" << bins_sum[0] << \", \" << bins_sum[1] << \", \" << bins_sum[2] << \", \" << bins_sum[3] << std::endl;\n\n  // copy the result back to bins\n  if (rank == 0) {\n    bins[0] = bins_sum[0];\n    bins[1] = bins_sum[1];\n    bins[2] = bins_sum[2];\n    bins[3] = bins_sum[3];\n  }\n\n}",
            "size_t n = x.size();\n    MPI_Comm comm = MPI_COMM_WORLD;\n    MPI_Info info = MPI_INFO_NULL;\n    int rank, size;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &size);\n    size_t chunk = n/size;\n    size_t remainder = n % size;\n    size_t start = rank*chunk + std::min(rank, remainder);\n    size_t end = start + chunk + (rank < remainder? 1 : 0);\n    size_t bsize = bins.size();\n    size_t* h_bins = new size_t[bsize];\n    size_t * d_bins;\n    cudaMalloc(&d_bins, bsize * sizeof(size_t));\n    for (int i = 0; i < bsize; i++) {\n        h_bins[i] = 0;\n    }\n    for (size_t i = start; i < end; i++) {\n        size_t idx = std::min(i, end-1);\n        double d = x[idx];\n        if (d > 0.0 && d < 0.25) {\n            h_bins[0]++;\n        } else if (d >= 0.25 && d < 0.5) {\n            h_bins[1]++;\n        } else if (d >= 0.5 && d < 0.75) {\n            h_bins[2]++;\n        } else if (d >= 0.75 && d <= 1.0) {\n            h_bins[3]++;\n        }\n    }\n    cudaMemcpy(d_bins, h_bins, bsize * sizeof(size_t), cudaMemcpyHostToDevice);\n    int threads_per_block = 256;\n    int blocks_per_grid = (bsize + threads_per_block - 1) / threads_per_block;\n    int shared_mem_size = sizeof(size_t) * bsize;\n    binCount<<<blocks_per_grid, threads_per_block, shared_mem_size>>>(d_bins, bsize);\n    cudaMemcpy(h_bins, d_bins, bsize * sizeof(size_t), cudaMemcpyDeviceToHost);\n    for (int i = 0; i < bsize; i++) {\n        bins[i] = h_bins[i];\n    }\n    cudaFree(d_bins);\n    delete[] h_bins;\n}",
            "size_t N = x.size();\n  int rank = 0, commSize = 1;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &commSize);\n\n  std::vector<double> y = x;\n  int local_size = y.size() / commSize;\n  int local_rank = rank;\n\n  // sort vector y\n  int step = 1;\n  while (step < y.size()) {\n    for (int i = 0; i < step; i++) {\n      for (int j = step; j < y.size(); j++) {\n        if (y[i] > y[j]) {\n          std::swap(y[i], y[j]);\n        }\n      }\n    }\n    step = step << 1;\n  }\n\n  // count number of elements in each interval\n  std::array<double, 4> bounds = {0.25, 0.5, 0.75, 1.0};\n  std::array<size_t, 4> nums{};\n  for (int i = 0; i < 4; i++) {\n    for (int j = 0; j < commSize; j++) {\n      if (i == 0) {\n        if (y[j * local_size] <= bounds[i]) {\n          nums[i]++;\n        }\n      } else if (i == 3) {\n        if (y[(j + 1) * local_size - 1] > bounds[i]) {\n          nums[i]++;\n        }\n      } else {\n        if (y[j * local_size] <= bounds[i] &&\n            y[(j + 1) * local_size - 1] <= bounds[i + 1]) {\n          nums[i]++;\n        }\n      }\n    }\n  }\n\n  // print number of elements in each interval\n  if (rank == 0) {\n    std::cout << \"Number of elements in each interval: \" << std::endl;\n    for (int i = 0; i < 4; i++) {\n      std::cout << nums[i] << \"\\t\";\n    }\n    std::cout << std::endl;\n  }\n\n  // sum number of elements in each interval\n  std::array<size_t, 4> sum;\n  MPI_Reduce(nums.data(), sum.data(), 4, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // compute total number of elements in each interval\n  bins[0] = (sum[0] + (N - sum[0]) * 0.25) / (double) N;\n  bins[1] = (sum[0] + sum[1] + (N - sum[0] - sum[1]) * 0.5) / (double) N;\n  bins[2] = (sum[1] + sum[2] + (N - sum[1] - sum[2]) * 0.75) / (double) N;\n  bins[3] = (sum[2] + sum[3] + (N - sum[2] - sum[3]) * 1.0) / (double) N;\n}",
            "int num_procs;\n  int rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (rank == 0)\n    {\n      int size = x.size();\n      int num_threads = omp_get_max_threads();\n      double *all_x;\n      all_x = new double[size];\n      MPI_Allgather(&x[0], size, MPI_DOUBLE, &all_x[0], size, MPI_DOUBLE, MPI_COMM_WORLD);\n      std::vector<double> tmp_x(all_x, all_x+size);\n\n      int block_size = size/num_threads;\n      int rest = size - block_size * num_threads;\n      std::array<size_t, 4> x_bins{0, 0, 0, 0};\n\n      #pragma omp parallel for num_threads(num_threads)\n      for (int i = 0; i < num_threads; ++i)\n        {\n          int start = i * block_size;\n          int end = start + block_size;\n          for (int j = start; j < end; ++j)\n            {\n              if (tmp_x[j] < 0.25)\n                ++x_bins[0];\n              else if (tmp_x[j] >= 0.25 && tmp_x[j] < 0.5)\n                ++x_bins[1];\n              else if (tmp_x[j] >= 0.5 && tmp_x[j] < 0.75)\n                ++x_bins[2];\n              else if (tmp_x[j] >= 0.75 && tmp_x[j] < 1.0)\n                ++x_bins[3];\n            }\n        }\n      if (rest > 0)\n        {\n          int start = num_threads * block_size;\n          int end = start + rest;\n          for (int i = start; i < end; ++i)\n            {\n              if (tmp_x[i] < 0.25)\n                ++x_bins[0];\n              else if (tmp_x[i] >= 0.25 && tmp_x[i] < 0.5)\n                ++x_bins[1];\n              else if (tmp_x[i] >= 0.5 && tmp_x[i] < 0.75)\n                ++x_bins[2];\n              else if (tmp_x[i] >= 0.75 && tmp_x[i] < 1.0)\n                ++x_bins[3];\n            }\n        }\n\n      bins = x_bins;\n\n      delete[] all_x;\n    }\n\n  int mpi_size = MPI_COMM_WORLD.size();\n\n  int x_size = x.size();\n  int x_block_size = x_size/mpi_size;\n  int x_rest = x_size - x_block_size * mpi_size;\n  int x_start = x_block_size * rank;\n  int x_end = x_start + x_block_size;\n\n  if (x_rest > 0 && rank == mpi_size - 1)\n    {\n      x_end += x_rest;\n    }\n\n  std::array<size_t, 4> mpi_x_bins{0, 0, 0, 0};\n\n  #pragma omp parallel for num_threads(num_threads)\n  for (int i = x_start; i < x_end; ++i)\n    {\n      if (x[i] < 0.25)\n        ++mpi_x_bins[0];\n      else if (x[i] >= 0.25 && x[i] < 0.5)\n        ++mpi_x_bins[1];\n      else if (x[i] >= 0.5 && x[i] < 0.75)\n        ++mpi_x_bins[",
            "int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    size_t myCounts[4];\n    memset(myCounts, 0, sizeof(size_t) * 4);\n\n    // count local\n    size_t n = x.size();\n    for (int i = 0; i < n; i++) {\n        double xi = x[i];\n        if (xi < 0.25) {\n            ++myCounts[0];\n        }\n        else if (xi < 0.5) {\n            ++myCounts[1];\n        }\n        else if (xi < 0.75) {\n            ++myCounts[2];\n        }\n        else if (xi < 1.0) {\n            ++myCounts[3];\n        }\n    }\n\n    // communicate\n    std::array<size_t, 4> counts;\n    MPI_Reduce(&myCounts, &counts, 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        bins = counts;\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<double> xCopy(x);\n\n    int binsPerRank = bins.size() / size;\n    int remainder = bins.size() % size;\n\n    size_t binsSum[4] = {0, 0, 0, 0};\n    if (rank < remainder) {\n        int i;\n#pragma omp parallel for private(i)\n        for (i = 0; i < xCopy.size(); i++) {\n            if (xCopy[i] < 0.25)\n                binsSum[0]++;\n            else if (xCopy[i] < 0.5)\n                binsSum[1]++;\n            else if (xCopy[i] < 0.75)\n                binsSum[2]++;\n            else\n                binsSum[3]++;\n            xCopy[i] = 0.0;\n        }\n    } else {\n        int i;\n#pragma omp parallel for private(i)\n        for (i = 0; i < xCopy.size(); i++) {\n            if (xCopy[i] < 0.25)\n                binsSum[0]++;\n            else if (xCopy[i] < 0.5)\n                binsSum[1]++;\n            else if (xCopy[i] < 0.75)\n                binsSum[2]++;\n            else\n                binsSum[3]++;\n            xCopy[i] = 0.0;\n        }\n    }\n\n    bins[rank * binsPerRank] = binsSum[0];\n    bins[rank * binsPerRank + 1] = binsSum[1];\n    bins[rank * binsPerRank + 2] = binsSum[2];\n    bins[rank * binsPerRank + 3] = binsSum[3];\n\n    if (rank!= 0) {\n        MPI_Gather(bins.data(), binsPerRank, MPI_INT, bins.data(), binsPerRank, MPI_INT, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Gather(bins.data(), binsPerRank, MPI_INT, bins.data(), binsPerRank, MPI_INT, 0, MPI_COMM_WORLD);\n        bins[0] += binsSum[0];\n        bins[1] += binsSum[1];\n        bins[2] += binsSum[2];\n        bins[3] += binsSum[3];\n    }\n}",
            "// your code here\n}",
            "int rank, nRanks;\n  MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  size_t n = x.size();\n\n  size_t n_per_rank = n / nRanks;\n\n  // compute the starting and ending index for this rank\n  size_t my_start = rank * n_per_rank;\n  size_t my_end = my_start + n_per_rank;\n\n  if (rank == nRanks - 1) my_end = n;\n\n  size_t n_bins = 4;\n  size_t n_per_bin = n / n_bins;\n  size_t n_per_bin_extra = n % n_bins;\n\n  std::vector<double> x_local;\n\n  x_local.reserve(my_end - my_start);\n\n#pragma omp parallel for schedule(static)\n  for (int i = 0; i < x.size(); ++i) {\n    x_local.push_back(x[i]);\n  }\n\n  std::vector<size_t> count(n_bins);\n\n  // count the number of doubles in each bin\n  for (size_t i = my_start; i < my_end; ++i) {\n    double xi = x_local[i];\n\n    if (xi >= 0 && xi < 0.25) {\n      ++count[0];\n    } else if (xi >= 0.25 && xi < 0.5) {\n      ++count[1];\n    } else if (xi >= 0.5 && xi < 0.75) {\n      ++count[2];\n    } else if (xi >= 0.75 && xi < 1) {\n      ++count[3];\n    }\n  }\n\n  // distribute the count among the 4 bins\n  for (size_t i = 0; i < n_bins; ++i) {\n    MPI_Allreduce(&count[i], &bins[i], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n  }\n}",
            "size_t mpi_rank, mpi_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    // number of threads per process\n    int num_threads = 1;\n\n    #pragma omp parallel\n    {\n        num_threads = omp_get_num_threads();\n    }\n\n    // number of threads that are used for mpi\n    int mpi_num_threads = 1;\n    MPI_Comm_get_attr(MPI_COMM_WORLD, MPI_THREAD_LEVEL, &mpi_num_threads, &mpi_num_threads);\n    assert(mpi_num_threads == MPI_THREAD_MULTIPLE);\n\n    // determine the size of each process, the size of each chunk, and the index of my first chunk\n    size_t chunk_size = x.size()/mpi_size;\n    size_t reminder = x.size()%mpi_size;\n    size_t chunk_index = (mpi_rank)*chunk_size;\n    if(mpi_rank < reminder) {\n        chunk_index += mpi_rank;\n    } else {\n        chunk_index += reminder;\n    }\n\n    // allocate buffers\n    std::vector<size_t> local_bins(4, 0);\n    std::vector<size_t> buff_bins(4*num_threads, 0);\n    std::vector<double> x_chunk(chunk_size, 0);\n    std::vector<double> x_buff(4*num_threads, 0);\n\n    // determine the index of the end of my chunk\n    size_t chunk_end = chunk_index;\n    if(mpi_rank < reminder) {\n        chunk_end += chunk_size;\n    } else {\n        chunk_end += reminder;\n    }\n\n    // copy the chunk to the buffer\n    for(size_t i=chunk_index; i<chunk_end; i++) {\n        x_chunk[i-chunk_index] = x[i];\n    }\n\n    // determine the index of my buffer\n    size_t buff_start = mpi_rank*4*num_threads;\n    size_t buff_end = (mpi_rank+1)*4*num_threads;\n\n    // copy the chunk to the buffer\n    for(size_t i=chunk_index; i<chunk_end; i++) {\n        x_buff[i-chunk_index] = x[i];\n    }\n\n    // determine the number of elements in each chunk\n    size_t chunk_num = chunk_size;\n    if(mpi_rank < reminder) {\n        chunk_num += 1;\n    }\n\n    // count the bins\n    for(size_t i=0; i<chunk_num; i++) {\n        if((x_chunk[i] >= 0.0) && (x_chunk[i] < 0.25)) {\n            local_bins[0]++;\n        }\n        if((x_chunk[i] >= 0.25) && (x_chunk[i] < 0.5)) {\n            local_bins[1]++;\n        }\n        if((x_chunk[i] >= 0.5) && (x_chunk[i] < 0.75)) {\n            local_bins[2]++;\n        }\n        if((x_chunk[i] >= 0.75) && (x_chunk[i] < 1.0)) {\n            local_bins[3]++;\n        }\n    }\n\n    // gather the results\n    MPI_Allreduce(&local_bins, &buff_bins, 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n    for(int i=0; i<num_threads; i++) {\n        buff_bins[i*4] += buff_bins[i*4+1];\n        buff_bins[i*4+1] += buff_bins[i*4+2];",
            "size_t N = x.size();\n    size_t nbins = bins.size();\n    std::array<size_t, 4> local_bins;\n\n    #pragma omp parallel for\n    for (size_t i=0; i<N; i++) {\n        int rank = omp_get_thread_num();\n        int nthreads = omp_get_num_threads();\n\n        double fraction = x[i] - std::floor(x[i]);\n        size_t bin = fraction * nbins;\n        local_bins[bin]++;\n    }\n\n    std::array<size_t, 4> global_bins;\n    global_bins.fill(0);\n\n    //MPI_Reduce(local_bins.data(), global_bins.data(), nbins, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(local_bins.data(), global_bins.data(), 4, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    //if (rank==0) {\n    bins = global_bins;\n    //}\n}",
            "auto num_ranks = omp_get_num_threads();\n  auto rank = omp_get_thread_num();\n\n  // Get the size of the vector\n  int size = x.size();\n\n  // Create a vector of size equal to the number of ranks\n  std::vector<int> sizes(num_ranks);\n\n  // Gather the size of the vector on each rank\n  MPI_Allgather(&size, 1, MPI_INT, sizes.data(), 1, MPI_INT, MPI_COMM_WORLD);\n\n  // Calculate the offset of each rank\n  std::vector<int> offsets(num_ranks);\n  int sum = 0;\n  for (int i = 0; i < num_ranks; i++) {\n    sum += sizes[i];\n    offsets[i] = sum;\n  }\n\n  // Create a vector of size equal to the number of ranks\n  std::vector<double> local_x(x.begin() + offsets[rank], x.begin() + offsets[rank] + sizes[rank]);\n\n  // Calculate the size of the bins\n  int bin_size = sizes[rank] / 4;\n\n  // Create a vector of size equal to the number of ranks\n  std::vector<int> local_bins(bin_size);\n\n  // Calculate the number of threads in the current rank\n  int num_threads = omp_get_max_threads();\n\n  // Calculate the number of elements per thread in the current rank\n  int size_per_thread = sizes[rank] / num_threads;\n\n  // Calculate the offset of each thread in the current rank\n  std::vector<int> offsets_per_thread(num_threads);\n  for (int i = 0; i < num_threads; i++) {\n    offsets_per_thread[i] = size_per_thread * i;\n  }\n\n  // Calculate the fractional part of each element\n  std::vector<double> local_x_fractional(local_x.begin(), local_x.end());\n  std::vector<double> local_x_fractional_copy(local_x_fractional.begin(), local_x_fractional.end());\n#pragma omp parallel for\n  for (int i = 0; i < local_x_fractional.size(); i++) {\n    local_x_fractional[i] = std::modf(local_x[i], &local_x_fractional_copy[i]);\n  }\n\n  // Calculate the number of elements in each thread\n  std::vector<int> local_x_size_per_thread(num_threads);\n  for (int i = 0; i < num_threads; i++) {\n    local_x_size_per_thread[i] = local_x_fractional.size() - offsets_per_thread[i];\n  }\n\n  // Distribute the elements into the bins\n  for (int i = 0; i < num_threads; i++) {\n    // Get the elements per thread\n    std::vector<double> x_per_thread(local_x_fractional.begin() + offsets_per_thread[i],\n                                     local_x_fractional.begin() + offsets_per_thread[i] + local_x_size_per_thread[i]);\n\n    // Calculate the number of elements in each bin\n    std::vector<int> local_bins_per_thread(bin_size);\n    for (int j = 0; j < bin_size; j++) {\n      int count = 0;\n      for (int k = 0; k < x_per_thread.size(); k++) {\n        if (x_per_thread[k] < 0.25 + j * 0.25) {\n          count++;\n        }\n      }\n      local_bins_per_thread[j] = count;\n    }\n\n    // Accumulate the elements of the local bins into the global bins\n    for (int j = 0; j < bin_size; j++) {\n      local_bins[j] += local_bins_per_thread[j];\n    }\n  }\n}",
            "// Write your code here\n  \n  const int n_bins = 4;\n  bins.fill(0);\n\n  int nproc, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  \n  if(nproc!= 1){\n    int n_per_proc = x.size()/nproc;\n    std::vector<double> part_x;\n    part_x.reserve(n_per_proc);\n    if(rank == 0)\n      part_x = x;\n    else{\n      int prev_rank = rank-1;\n      MPI_Status status;\n      MPI_Recv(part_x.data(), n_per_proc, MPI_DOUBLE, prev_rank, 0, MPI_COMM_WORLD, &status);\n    }\n\n    #pragma omp parallel for\n    for(int i = 0; i < n_per_proc; i++)\n      if(part_x[i] < 0.25) bins[0]++;\n      else if(part_x[i] < 0.5) bins[1]++;\n      else if(part_x[i] < 0.75) bins[2]++;\n      else bins[3]++;\n\n    if(rank!= nproc-1){\n      int next_rank = rank+1;\n      MPI_Send(part_x.data(), n_per_proc, MPI_DOUBLE, next_rank, 0, MPI_COMM_WORLD);\n    }\n\n    if(rank == 0){\n      int i;\n      for(i = 1; i < nproc; i++){\n        MPI_Status status;\n        MPI_Recv(&bins[0], n_bins, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, &status);\n      }\n    }\n  }\n  else{\n    #pragma omp parallel for\n    for(int i = 0; i < x.size(); i++)\n      if(x[i] < 0.25) bins[0]++;\n      else if(x[i] < 0.5) bins[1]++;\n      else if(x[i] < 0.75) bins[2]++;\n      else bins[3]++;\n  }\n}",
            "// TODO: Your code here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int num_bins = 4;\n    int chunk_size = x.size() / size;\n    std::vector<size_t> bins_temp(num_bins);\n\n    #pragma omp parallel\n    {\n        int thread = omp_get_thread_num();\n        int thread_num = omp_get_num_threads();\n        if (thread == 0) {\n            #pragma omp for schedule(static)\n            for (int i = 0; i < x.size(); i += chunk_size) {\n                double val = x[i];\n                if (val < 0.25) {\n                    bins_temp[0]++;\n                } else if (val < 0.5) {\n                    bins_temp[1]++;\n                } else if (val < 0.75) {\n                    bins_temp[2]++;\n                } else {\n                    bins_temp[3]++;\n                }\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < num_bins; ++i) {\n            for (int j = 1; j < size; ++j) {\n                MPI_Recv(&bins_temp[i], 1, MPI_UNSIGNED_LONG_LONG, j, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            }\n            bins[i] = bins_temp[i];\n        }\n    } else {\n        MPI_Send(&bins_temp, 4, MPI_UNSIGNED_LONG_LONG, 0, rank, MPI_COMM_WORLD);\n    }\n}",
            "int numproc, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &numproc);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int block_size = x.size()/numproc;\n   int rem_data = x.size()%numproc;\n   int start = block_size*rank;\n   int end = block_size*rank+block_size;\n\n   if (rank == numproc-1)\n      end = block_size*rank+block_size+rem_data;\n\n   double mysum[4];\n   for (int i=0; i<4; i++)\n      mysum[i] = 0.0;\n   for (int i=start; i<end; i++) {\n      double frac = modf(x[i], &myint);\n      if (frac < 0.25)\n         mysum[0]++;\n      else if (frac < 0.5)\n         mysum[1]++;\n      else if (frac < 0.75)\n         mysum[2]++;\n      else\n         mysum[3]++;\n   }\n\n   #pragma omp parallel\n   {\n      #pragma omp critical\n      for (int i=0; i<4; i++)\n         bins[i] += mysum[i];\n   }\n}",
            "int N = x.size();\n\tdouble *x_d = (double *) malloc(N * sizeof(double));\n\t\n\t//printf(\"x_d: %d\\n\", x_d);\n\n\tfor (int i = 0; i < N; i++)\n\t\tx_d[i] = x[i];\n\n\tint iam = 0;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &iam);\n\tMPI_Barrier(MPI_COMM_WORLD);\n\n\tint rank = 0;\n\tint nprocs = 1;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\tint nranks = nprocs;\n\n\tint N_perproc = N/nranks;\n\tint rest = N - N_perproc*nranks;\n\tint offset = 0;\n\n\tif (iam < rest)\n\t\toffset = iam*N_perproc;\n\telse\n\t\toffset = N_perproc*(rest+1) + (iam-rest)*N_perproc;\n\t\n\t//printf(\"offset: %d\\n\", offset);\n\n\tint *idx_d;\n\tint *size_d;\n\tsize_t *bins_d;\n\n\tidx_d = (int *) malloc(sizeof(int));\n\tsize_d = (int *) malloc(sizeof(int));\n\tbins_d = (size_t *) malloc(sizeof(size_t) * 4);\n\n\tif (offset == 0)\n\t\tidx_d[0] = 0;\n\telse\n\t\tidx_d[0] = offset - N_perproc;\n\t\n\tsize_d[0] = N_perproc;\n\t\n\tMPI_Scatterv(&x_d[offset], size_d, idx_d, MPI_DOUBLE, &x_d[0], N_perproc, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n\t#pragma omp parallel num_threads(4)\n\t{\n\t\tint lane = omp_get_thread_num();\n\t\tsize_t idx = 0;\n\n\t\tfor (int i = 0; i < N_perproc; i++) {\n\t\t\tif (x_d[i] >= lane * 0.25 && x_d[i] <= (lane + 1) * 0.25)\n\t\t\t\tidx++;\n\t\t}\n\t\t\n\t\tbins_d[lane] = idx;\n\t}\n\t\n\tMPI_Gatherv(bins_d, 4, MPI_UNSIGNED_LONG_LONG, &bins[0], size_d, idx_d, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\t\n\tfree(idx_d);\n\tfree(size_d);\n\tfree(bins_d);\n\tfree(x_d);\n}",
            "int mpi_size, mpi_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    int mpi_local_size = x.size() / mpi_size;\n    int mpi_local_remainder = x.size() % mpi_size;\n    int mpi_local_start = (mpi_rank * mpi_local_size) + (mpi_local_remainder < mpi_rank? mpi_local_remainder : 0);\n\n    std::array<size_t, 4> local_bins{};\n\n#pragma omp parallel for schedule(static) reduction(+:local_bins[0], local_bins[1], local_bins[2], local_bins[3])\n    for (int i = 0; i < mpi_local_size; ++i) {\n        double x_val = x[mpi_local_start + i];\n        if (x_val < 0.25) {\n            ++local_bins[0];\n        } else if (x_val < 0.5) {\n            ++local_bins[1];\n        } else if (x_val < 0.75) {\n            ++local_bins[2];\n        } else {\n            ++local_bins[3];\n        }\n    }\n\n    MPI_Allreduce(&local_bins, &bins, 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n}",
            "// compute how many elements we have\n\tsize_t N = x.size();\n\n\t// number of elements per process\n\tsize_t n = N / omp_get_num_threads();\n\n\t// bins per process\n\tstd::array<size_t, 4> bins_per_thread{0,0,0,0};\n\n\t// compute bins per thread\n\t#pragma omp parallel for\n\tfor(size_t i = 0; i < n; i++)\n\t{\n\t\tdouble elem = x[omp_get_thread_num() * n + i];\n\t\tif(elem < 0.25)\n\t\t\tbins_per_thread[0]++;\n\t\telse if(elem < 0.5)\n\t\t\tbins_per_thread[1]++;\n\t\telse if(elem < 0.75)\n\t\t\tbins_per_thread[2]++;\n\t\telse\n\t\t\tbins_per_thread[3]++;\n\t}\n\n\t// bins per process\n\tbins[0] = bins_per_thread[0];\n\tbins[1] = bins_per_thread[1];\n\tbins[2] = bins_per_thread[2];\n\tbins[3] = bins_per_thread[3];\n\n\t// reduce bins\n\t#pragma omp parallel for\n\tfor(int i = 1; i < omp_get_num_threads(); i++)\n\t{\n\t\tbins[0] += bins_per_thread[i * 4];\n\t\tbins[1] += bins_per_thread[i * 4 + 1];\n\t\tbins[2] += bins_per_thread[i * 4 + 2];\n\t\tbins[3] += bins_per_thread[i * 4 + 3];\n\t}\n\n\t// sum bins to get bins on rank 0\n\tif(MPI_Get_rank(MPI_COMM_WORLD, 0) == 0)\n\t{\n\t\tfor(int i = 1; i < omp_get_num_threads(); i++)\n\t\t{\n\t\t\tMPI_Recv(&bins[0], 4, MPI_LONG_LONG_INT, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t}\n\t}\n\n\t// send bins from rank 0\n\tif(MPI_Get_rank(MPI_COMM_WORLD, 0)!= 0)\n\t{\n\t\tMPI_Send(&bins[0], 4, MPI_LONG_LONG_INT, 0, 1, MPI_COMM_WORLD);\n\t}\n\n\t// bins on rank 0\n\tMPI_Barrier(MPI_COMM_WORLD);\n\tif(MPI_Get_rank(MPI_COMM_WORLD, 0) == 0)\n\t{\n\t\tfor(int i = 1; i < omp_get_num_threads(); i++)\n\t\t{\n\t\t\tMPI_Recv(&bins[0], 4, MPI_LONG_LONG_INT, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t}\n\t}\n}",
            "int rank = 0;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint nprocs = 1;\n\tMPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n\tsize_t localSize = x.size() / nprocs;\n\tint remainder = x.size() % nprocs;\n\n\tstd::vector<double> localVector;\n\tlocalVector.resize(localSize + (rank < remainder? 1 : 0));\n\n\tfor (size_t i = 0; i < localSize; ++i) {\n\t\tlocalVector[i] = x[rank * localSize + i];\n\t}\n\tif (rank < remainder) {\n\t\tlocalVector[localSize] = x[rank * localSize + localSize];\n\t}\n\n\tbins[0] = 0;\n\tbins[1] = 0;\n\tbins[2] = 0;\n\tbins[3] = 0;\n\n#pragma omp parallel for\n\tfor (size_t i = 0; i < localVector.size(); ++i) {\n\t\tif (localVector[i] < 0.25) {\n\t\t\tbins[0]++;\n\t\t}\n\t\telse if (localVector[i] < 0.5) {\n\t\t\tbins[1]++;\n\t\t}\n\t\telse if (localVector[i] < 0.75) {\n\t\t\tbins[2]++;\n\t\t}\n\t\telse {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n\n\tstd::array<size_t, 4> sendBins = bins;\n\n\tMPI_Allreduce(MPI_IN_PLACE, &sendBins, 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n\tbins = sendBins;\n}",
            "int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    auto n = x.size();\n\n    if (world_rank == 0) {\n        size_t bins_total[4] = { 0 };\n        for (int i = 0; i < world_size; i++) {\n            size_t bins_part[4] = { 0 };\n            MPI_Recv(bins_part, 4, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 4; j++)\n                bins_total[j] += bins_part[j];\n        }\n        for (int j = 0; j < 4; j++)\n            bins[j] = bins_total[j];\n    } else {\n        size_t bins_part[4] = { 0 };\n        auto lo = (int) (n * world_rank / world_size);\n        auto hi = (int) (n * (world_rank + 1) / world_size);\n        for (auto i = lo; i < hi; i++) {\n            auto xi = x[i];\n            int j = 0;\n            if (xi < 0.25)\n                j = 0;\n            else if (xi >= 0.25 && xi < 0.5)\n                j = 1;\n            else if (xi >= 0.5 && xi < 0.75)\n                j = 2;\n            else\n                j = 3;\n            bins_part[j]++;\n        }\n        MPI_Send(bins_part, 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO\n}",
            "int num_procs, my_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    size_t n = x.size();\n    if (n == 0) {\n        return;\n    }\n    size_t block_size = n / num_procs;\n    std::vector<std::array<size_t, 4>> local_bins(block_size);\n    #pragma omp parallel for\n    for (size_t i = my_rank * block_size; i < my_rank * block_size + block_size; i++) {\n        if (i >= n) break;\n        double d = x[i];\n        size_t bin_index;\n        if (d >= 0 && d < 0.25) {\n            bin_index = 0;\n        } else if (d >= 0.25 && d < 0.5) {\n            bin_index = 1;\n        } else if (d >= 0.5 && d < 0.75) {\n            bin_index = 2;\n        } else {\n            bin_index = 3;\n        }\n        local_bins[i - my_rank * block_size][bin_index]++;\n    }\n    std::array<size_t, 4> sum_bins;\n    std::array<size_t, 4> temp_bins;\n    MPI_Reduce(&local_bins[0][0], &sum_bins[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (my_rank == 0) {\n        bins[0] = sum_bins[0];\n        bins[1] = sum_bins[1];\n        bins[2] = sum_bins[2];\n        bins[3] = sum_bins[3];\n        return;\n    }\n    MPI_Allreduce(&sum_bins[0], &temp_bins[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n    MPI_Reduce(&temp_bins[0], &bins[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "size_t n = x.size();\n  if (n == 0) return;\n\n  std::vector<int> binID(n);\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    binID[i] = int(floor((x[i] - 0.25) * 4));\n  }\n\n  // Count occurrences in bins.\n  std::vector<int> binsInt(4);\n  for (int i = 0; i < n; i++) {\n    binsInt[binID[i]] += 1;\n  }\n\n  // Sum of counts and find global count of each bin.\n  std::vector<int> globalBins(4);\n  MPI_Allreduce(binsInt.data(), globalBins.data(), 4, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n  // Convert global counts to size_t and store in bins.\n  bins = {globalBins[0], globalBins[1], globalBins[2], globalBins[3]};\n}",
            "int rank;\n  int size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // Step 1: count number of doubles in x that have a fractional part in [0, 0.25)\n  size_t numInBin0 = 0;\n  for (int i = 0; i < x.size(); ++i) {\n    if (x[i] - std::floor(x[i]) <= 0.25) numInBin0 += 1;\n  }\n\n  // Step 2: each rank computes its own bins, then adds the results\n  size_t numInBin1 = 0;\n  size_t numInBin2 = 0;\n  size_t numInBin3 = 0;\n  size_t total = 0;\n  for (int i = 0; i < x.size(); ++i) {\n    if (x[i] - std::floor(x[i]) > 0.25 && x[i] - std::floor(x[i]) <= 0.5) numInBin1 += 1;\n    if (x[i] - std::floor(x[i]) > 0.5 && x[i] - std::floor(x[i]) <= 0.75) numInBin2 += 1;\n    if (x[i] - std::floor(x[i]) > 0.75 && x[i] - std::floor(x[i]) <= 1) numInBin3 += 1;\n    total += 1;\n  }\n\n  // Step 3: add rank results and store the result in bins[0] if it's rank 0\n  if (rank == 0) {\n    bins[0] = numInBin0 + numInBin1 + numInBin2 + numInBin3;\n  }\n  MPI_Reduce(&numInBin0, &bins[0], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  MPI_Reduce(&numInBin1, &bins[1], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  MPI_Reduce(&numInBin2, &bins[2], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  MPI_Reduce(&numInBin3, &bins[3], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  MPI_Reduce(&total, &bins[4], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // Step 4: divide by total to normalize bins\n  for (int i = 0; i < bins.size(); ++i) {\n    if (i == 0) {\n      bins[i] = bins[i] / total;\n    } else {\n      bins[i] = bins[i] / bins[4];\n    }\n  }\n}",
            "// TODO\n}",
            "//TODO: implement\n}",
            "size_t n = x.size();\n    bins.fill(0);\n    if(n==0)\n        return;\n    size_t r = n%4;\n    if(r==0)\n        r=4;\n    size_t num_of_bins = n/r;\n    size_t num_of_ranks = 4;\n    std::vector<double> temp;\n    std::vector<double> send_data(num_of_bins);\n    std::vector<double> recv_data(num_of_bins);\n    int num_of_bins_per_rank = num_of_bins/num_of_ranks;\n    int num_of_bins_to_send = num_of_bins_per_rank;\n    int num_of_bins_to_recv = num_of_bins_per_rank;\n    int send_data_counter = 0;\n    int recv_data_counter = 0;\n    int send_rank;\n    int recv_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &send_rank);\n    MPI_Comm_rank(MPI_COMM_WORLD, &recv_rank);\n    if(send_rank==recv_rank){\n        temp = x;\n    }else{\n        MPI_Send(&x[0], n, MPI_DOUBLE, send_rank, 0, MPI_COMM_WORLD);\n        MPI_Recv(&recv_data[0], num_of_bins_to_recv, MPI_DOUBLE, recv_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    for(int i=0; i<r; i++){\n        if((i<num_of_bins_per_rank) && (i+num_of_bins_to_recv<=num_of_bins)){\n            if(send_rank==recv_rank){\n                temp.push_back(x[i]);\n            }else{\n                send_data[send_data_counter] = x[i];\n                send_data_counter++;\n            }\n        }\n        if((i+num_of_bins_to_send<=num_of_bins) && (i+num_of_bins_to_recv<=num_of_bins)){\n            if(send_rank==recv_rank){\n                temp.push_back(x[i+num_of_bins_to_send]);\n            }else{\n                send_data[send_data_counter] = x[i+num_of_bins_to_send];\n                send_data_counter++;\n            }\n        }\n    }\n    for(int i=0; i<num_of_bins_to_send; i++){\n        if(send_rank==recv_rank){\n            recv_data[recv_data_counter] = send_data[i];\n            recv_data_counter++;\n        }else{\n            MPI_Recv(&recv_data[0], num_of_bins_to_recv, MPI_DOUBLE, recv_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for(size_t i=0; i<recv_data.size(); i++){\n            if(recv_data[i]>=0.0 && recv_data[i]<0.25){\n                bins[0]++;\n            }else if(recv_data[i]>=0.25 && recv_data[i]<0.5){\n                bins[1]++;\n            }else if(recv_data[i]>=0.5 && recv_data[i]<0.75){\n                bins[2]++;\n            }else if(recv_data[i]>=0.75 && recv_data[i]<=1.0){\n                bins[3]++;\n            }\n        }\n    }\n    if(send_rank==rec",
            "//TODO\n\n}",
            "// TODO\n}",
            "const int n = x.size();\n    int nthreads = omp_get_max_threads();\n    std::array<size_t, 4> localBins;\n\n    for (int i = 0; i < 4; ++i) {\n        localBins[i] = 0;\n    }\n\n#pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        int threadNum = omp_get_thread_num();\n        double xi = x[i];\n        int j = 0;\n        if (xi < 0.25) {\n            j = 0;\n        } else if (xi >= 0.25 && xi < 0.5) {\n            j = 1;\n        } else if (xi >= 0.5 && xi < 0.75) {\n            j = 2;\n        } else if (xi >= 0.75 && xi <= 1) {\n            j = 3;\n        }\n        ++localBins[j];\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    size_t num_entries = x.size();\n    int block_size = num_entries / num_ranks;\n\n    // Partition the vector into equally sized chunks\n    std::vector<double> x_copy;\n    if (rank == 0) {\n        x_copy = std::vector<double>(x);\n    }\n    std::vector<double> x_chunk;\n    if (rank!= 0) {\n        x_chunk = std::vector<double>(x.begin() + rank * block_size, x.begin() + (rank + 1) * block_size);\n    }\n\n    // Determine the bins for each chunk\n    std::array<size_t, 4> bins_chunk{};\n    if (rank == 0) {\n        for (double entry : x_copy) {\n            if (entry < 0.25) {\n                bins_chunk[0] += 1;\n            } else if (entry < 0.5) {\n                bins_chunk[1] += 1;\n            } else if (entry < 0.75) {\n                bins_chunk[2] += 1;\n            } else {\n                bins_chunk[3] += 1;\n            }\n        }\n    }\n    MPI_Reduce(&bins_chunk[0], &bins[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // Reduce the chunk bins to the full vector bins\n    if (rank == 0) {\n        for (int i = 1; i < num_ranks; ++i) {\n            MPI_Recv(&bins_chunk[0], 4, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            bins[0] += bins_chunk[0];\n            bins[1] += bins_chunk[1];\n            bins[2] += bins_chunk[2];\n            bins[3] += bins_chunk[3];\n        }\n    } else {\n        MPI_Send(&bins_chunk[0], 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    size_t n = x.size();\n    // TODO: allocate bins\n    // TODO: create local array for counts\n    size_t start_idx = rank * n / size;\n    size_t end_idx = (rank + 1) * n / size;\n    for (size_t i = start_idx; i < end_idx; i++) {\n        // TODO: compute the number of elements in each interval\n        // TODO: store the counts in the local array\n    }\n    // TODO: reduce the local array to the global bins array\n    // TODO: deallocate the local array\n}",
            "int n_proc = omp_get_num_procs();\n\tint n_rank = omp_get_thread_num();\n\n\t// MPI_Gatherv not implemented\n\t//int n_rank_local = (x.size() / n_proc) + ((x.size() % n_proc) > n_rank);\n\tint n_rank_local = x.size() / n_proc;\n\tif(x.size() % n_proc > n_rank)\n\t\tn_rank_local++;\n\n\tint n_rank_local_offset = n_rank_local * n_rank;\n\tint n_rank_local_end = n_rank_local_offset + n_rank_local;\n\tint n_rank_local_max = x.size();\n\n\tint n_rank_local_start = n_rank_local_offset;\n\tif(n_rank_local_offset >= x.size())\n\t\tn_rank_local_start = n_rank_local_max;\n\n\tstd::vector<double> x_local(n_rank_local);\n\tfor(size_t i = n_rank_local_start; i < n_rank_local_end; i++) {\n\t\tx_local[i - n_rank_local_start] = x[i];\n\t}\n\t\n\tdouble min = x_local[0];\n\tdouble max = x_local[x_local.size() - 1];\n\n\t// MPI_Gatherv not implemented\n\t//MPI_Allreduce(&min, &min, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\t//MPI_Allreduce(&max, &max, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);\n\n\tdouble bin_size = (max - min) / 4;\n\n\tfor(size_t i = 0; i < 4; i++)\n\t\tbins[i] = 0;\n\n\t// MPI_Gatherv not implemented\n\t//MPI_Gatherv(&x_local[0], n_rank_local, MPI_DOUBLE, &bins[0], &n_rank_local, MPI_INT, 0, MPI_COMM_WORLD);\n\n\tfor(size_t i = 0; i < n_rank_local; i++) {\n\t\tint bin = 0;\n\t\tdouble val = x_local[i];\n\t\tif(val >= min + 0.25 * bin_size && val < min + 0.5 * bin_size)\n\t\t\tbin = 1;\n\t\telse if(val >= min + 0.5 * bin_size && val < min + 0.75 * bin_size)\n\t\t\tbin = 2;\n\t\telse if(val >= min + 0.75 * bin_size)\n\t\t\tbin = 3;\n\n\t\tbins[bin]++;\n\t}\n\n\t// MPI_Gatherv not implemented\n\t//if(n_rank == 0) {\n\t\t//for(size_t i = 0; i < 4; i++)\n\t\t//\tstd::cout << bins[i] << std::endl;\n\t//}\n}",
            "// TODO: Your code goes here\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t chunk_size = x.size() / size;\n  size_t chunk_rest = x.size() % size;\n  size_t start = chunk_size * rank;\n  size_t end = start + chunk_size;\n  if (rank < chunk_rest) {\n    end += 1;\n  }\n\n  std::vector<double> x_local(x.begin() + start, x.begin() + end);\n  bins = {0, 0, 0, 0};\n  #pragma omp parallel\n  {\n    size_t local_bins[4] = {0, 0, 0, 0};\n    #pragma omp for\n    for (size_t i = 0; i < x_local.size(); i++) {\n      if (x_local[i] >= 0. && x_local[i] < 0.25) {\n        local_bins[0] += 1;\n      }\n      else if (x_local[i] >= 0.25 && x_local[i] < 0.5) {\n        local_bins[1] += 1;\n      }\n      else if (x_local[i] >= 0.5 && x_local[i] < 0.75) {\n        local_bins[2] += 1;\n      }\n      else if (x_local[i] >= 0.75 && x_local[i] <= 1.) {\n        local_bins[3] += 1;\n      }\n      else {\n        throw \"Error! Number out of range\";\n      }\n    }\n\n    #pragma omp critical\n    {\n      #pragma omp simd\n      for (int i = 0; i < 4; i++) {\n        bins[i] += local_bins[i];\n      }\n    }\n  }\n}",
            "int myRank, numRanks, myCore;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\tomp_set_num_threads(numRanks);\n\tomp_set_dynamic(0);\n\t#pragma omp parallel\n\t{\n\t\tmyCore = omp_get_thread_num();\n\t}\n\n\tsize_t binsPerCore = (x.size() + numRanks - 1) / numRanks;\n\n\t#pragma omp parallel for\n\tfor (size_t i = myRank * binsPerCore; i < (myRank + 1) * binsPerCore; i++) {\n\t\tif (x[i] - floor(x[i]) < 0.25) {\n\t\t\t#pragma omp atomic\n\t\t\tbins[0]++;\n\t\t}\n\t\telse if (x[i] - floor(x[i]) >= 0.25 && x[i] - floor(x[i]) < 0.5) {\n\t\t\t#pragma omp atomic\n\t\t\tbins[1]++;\n\t\t}\n\t\telse if (x[i] - floor(x[i]) >= 0.5 && x[i] - floor(x[i]) < 0.75) {\n\t\t\t#pragma omp atomic\n\t\t\tbins[2]++;\n\t\t}\n\t\telse if (x[i] - floor(x[i]) >= 0.75) {\n\t\t\t#pragma omp atomic\n\t\t\tbins[3]++;\n\t\t}\n\t}\n\n\t//std::cout << myRank << \": \" << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << std::endl;\n\t//std::cout << \"binsPerCore = \" << binsPerCore << std::endl;\n\t//std::cout << \"myRank = \" << myRank << std::endl;\n\t//std::cout << \"numRanks = \" << numRanks << std::endl;\n\t//std::cout << \"x.size() = \" << x.size() << std::endl;\n\t//std::cout << \"bins.size() = \" << bins.size() << std::endl;\n\n\tMPI_Barrier(MPI_COMM_WORLD);\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < bins.size(); i++) {\n\t\tfor (int j = 1; j < numRanks; j++) {\n\t\t\t#pragma omp atomic\n\t\t\tbins[i] += bins[i];\n\t\t}\n\t\t#pragma omp barrier\n\t}\n\n\tif (myRank == 0) {\n\t\t#pragma omp parallel for\n\t\tfor (int i = 0; i < bins.size(); i++) {\n\t\t\t#pragma omp critical\n\t\t\tstd::cout << bins[i] << \", \";\n\t\t}\n\t\tstd::cout << std::endl;\n\t}\n\n\t//MPI_Barrier(MPI_COMM_WORLD);\n\t//std::cout << \"bins[0] = \" << bins[0] << \", bins[1] = \" << bins[1] << \", bins[2] = \" << bins[2] << \", bins[3] = \" << bins[3] << std::endl;\n\n\t//MPI_Barrier(MPI_COMM_WORLD);\n\n\t//MPI_Reduce(&bins, &bins, bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\t//if (myRank == 0) {\n\t//\tstd::cout << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << std::endl;\n\t//}\n\n\t//MPI_Reduce(&bins, &bins, 1, MPI_INT, MPI_",
            "// TODO: Your code here\n  int npes = omp_get_num_threads();\n  int mype = omp_get_thread_num();\n\n  size_t size = x.size();\n  size_t chunk = size / npes;\n\n  std::vector<size_t> local_bins = std::vector<size_t>(4);\n\n  if (mype == 0) {\n    size_t first = mype * chunk;\n    size_t last = first + chunk - 1;\n    if (mype == npes - 1) {\n      last = size - 1;\n    }\n    std::vector<double> x_local(x.begin() + first, x.begin() + last);\n    for (auto i = 0; i < x_local.size(); ++i) {\n      if (x_local[i] < 0.25) {\n        ++local_bins[0];\n      } else if (x_local[i] < 0.5) {\n        ++local_bins[1];\n      } else if (x_local[i] < 0.75) {\n        ++local_bins[2];\n      } else {\n        ++local_bins[3];\n      }\n    }\n  } else {\n    std::vector<double> x_local(x.begin() + mype * chunk, x.begin() + (mype + 1) * chunk);\n    for (auto i = 0; i < x_local.size(); ++i) {\n      if (x_local[i] < 0.25) {\n        ++local_bins[0];\n      } else if (x_local[i] < 0.5) {\n        ++local_bins[1];\n      } else if (x_local[i] < 0.75) {\n        ++local_bins[2];\n      } else {\n        ++local_bins[3];\n      }\n    }\n  }\n\n  MPI_Reduce(&local_bins[0], &bins[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  return;\n}",
            "size_t xsize = x.size();\n    size_t xsize_mpi_size = xsize/MPI_COMM_WORLD.size();\n    size_t xsize_mpi_rank = xsize/MPI_COMM_WORLD.size();\n\n    // MPI_COMM_WORLD.size() = 4\n    // xsize = 6\n    // xsize_mpi_rank = 1.5\n\n    std::vector<size_t> vec_bins(4,0);\n    double start, end;\n\n    if(MPI_COMM_WORLD.rank() == 0) {\n        start = 0;\n        end = 0.25;\n    }\n    if(MPI_COMM_WORLD.rank() == 1) {\n        start = 0.25;\n        end = 0.5;\n    }\n    if(MPI_COMM_WORLD.rank() == 2) {\n        start = 0.5;\n        end = 0.75;\n    }\n    if(MPI_COMM_WORLD.rank() == 3) {\n        start = 0.75;\n        end = 1;\n    }\n\n    #pragma omp parallel for\n    for(int i=0; i<xsize_mpi_size; i++) {\n        if(x[i] >= start && x[i] < end) {\n            #pragma omp atomic\n            vec_bins[MPI_COMM_WORLD.rank()]++;\n        }\n    }\n\n    MPI_Reduce(MPI_IN_PLACE, &vec_bins[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    bins = {vec_bins[0], vec_bins[1], vec_bins[2], vec_bins[3]};\n}",
            "assert(x.size() > 0);\n    assert(x.size() % omp_get_max_threads() == 0);\n\n    bins = { 0, 0, 0, 0 };\n\n    auto n = x.size();\n    auto chunk_size = n / omp_get_max_threads();\n    auto chunk_start = 0;\n\n#pragma omp parallel\n    {\n        size_t i, j, k;\n        MPI_Status status;\n\n#pragma omp for\n        for (i = chunk_start; i < n; i += chunk_size) {\n            if (x[i] < 0.25) bins[0]++;\n            else if (x[i] < 0.5) bins[1]++;\n            else if (x[i] < 0.75) bins[2]++;\n            else bins[3]++;\n        }\n\n#pragma omp single\n        MPI_Reduce(&bins[0], &bins[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        MPI_Status status;\n        int source;\n        double tmp;\n\n        // Get the total number of elements in x\n        size_t x_size;\n        MPI_Reduce(&x.size(), &x_size, 1, MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        assert(x_size % size == 0);\n        const size_t n = x_size / size;\n\n        // Get the number of elements in the first rank's part of x\n        size_t first_n;\n        MPI_Reduce(&n, &first_n, 1, MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        assert(first_n % size == 0);\n        const size_t first_n_per_rank = first_n / size;\n\n        // Get the number of elements in the last rank's part of x\n        size_t last_n;\n        MPI_Reduce(&n, &last_n, 1, MPI_LONG, MPI_SUM, size - 1, MPI_COMM_WORLD);\n        assert(last_n % size == 0);\n        const size_t last_n_per_rank = last_n / size;\n\n        // Allocate memory for the vector bins\n        bins = {};\n        bins.fill(0);\n        MPI_Gather(&bins[0], 4, MPI_LONG, &bins[0], 4, MPI_LONG, 0, MPI_COMM_WORLD);\n\n        // Initialize the counter\n        size_t counter = 0;\n        bins[0] = 0;\n        bins[1] = 0;\n        bins[2] = 0;\n        bins[3] = 0;\n\n        // Compute the counter in each rank\n        #pragma omp parallel for\n        for (int i = 1; i < size - 1; ++i) {\n            const size_t n_per_rank = n / size;\n            const size_t first_n_per_rank = first_n / size;\n            const size_t last_n_per_rank = last_n / size;\n\n            if (i == 1) {\n                // For the first rank, we need to count the first half of the \n                // elements that aren't in the first rank.\n                counter = 0;\n                for (size_t j = 1; j < first_n_per_rank; ++j) {\n                    tmp = x[j];\n                    if (tmp < 0.25) {\n                        counter += 1;\n                    }\n                }\n            } else if (i == size - 2) {\n                // For the last rank, we need to count the first half of the \n                // elements that aren't in the last rank.\n                counter = 0;\n                for (size_t j = first_n_per_rank + 1; j < last_n; ++j) {\n                    tmp = x[j];\n                    if (tmp < 0.25) {\n                        counter += 1;\n                    }\n                }\n            } else {\n                // For all the other ranks, we count the first half of the \n                // elements that aren't in the rank before or after it.\n                counter = 0;\n                for (size_t j = first_n_per_rank + 1; j < first_n_per_rank + n_per_rank; ++j) {\n                    tmp = x[j];\n                    if (tmp < 0.25) {\n                        counter += 1;\n                    }\n                }\n            }\n\n            #pragma omp critical\n            {\n                if (tmp < 0.25) {\n                    bins[0] += 1;\n                } else if (tmp < 0.5) {\n                    bins",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        int numberOfValues = x.size();\n        int chunkSize = numberOfValues / size;\n        int chunkRemainder = numberOfValues % size;\n\n        // The number of doubles in x that are less than 0.25.\n        int binOne = 0;\n        // The number of doubles in x that are less than 0.5.\n        int binTwo = 0;\n        // The number of doubles in x that are less than 0.75.\n        int binThree = 0;\n        // The number of doubles in x that are less than 1.\n        int binFour = 0;\n\n        // The number of doubles in x that are less than 0.25.\n        int binsOne[size];\n        // The number of doubles in x that are less than 0.5.\n        int binsTwo[size];\n        // The number of doubles in x that are less than 0.75.\n        int binsThree[size];\n        // The number of doubles in x that are less than 1.\n        int binsFour[size];\n\n        // Compute the fractional part of each double in x.\n        for (int i = 0; i < numberOfValues; i++) {\n            int index = i / chunkSize;\n            double fractionalPart = x[i] - floor(x[i]);\n\n            if (fractionalPart < 0.25) {\n                binOne++;\n                binsOne[index]++;\n            }\n            if (fractionalPart >= 0.25 && fractionalPart < 0.5) {\n                binTwo++;\n                binsTwo[index]++;\n            }\n            if (fractionalPart >= 0.5 && fractionalPart < 0.75) {\n                binThree++;\n                binsThree[index]++;\n            }\n            if (fractionalPart >= 0.75 && fractionalPart < 1) {\n                binFour++;\n                binsFour[index]++;\n            }\n        }\n\n        for (int i = 0; i < size; i++) {\n            if (i == 0) {\n                bins[0] = binsOne[i];\n                bins[1] = binsTwo[i];\n                bins[2] = binsThree[i];\n                bins[3] = binsFour[i];\n            }\n            else {\n                bins[0] += binsOne[i];\n                bins[1] += binsTwo[i];\n                bins[2] += binsThree[i];\n                bins[3] += binsFour[i];\n            }\n        }\n\n        // Print the bins if rank is 0.\n        if (rank == 0) {\n            std::cout << \"bins: \" << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << std::endl;\n        }\n    }\n\n    else {\n        // Compute the fractional part of each double in x.\n        for (int i = 0; i < x.size(); i++) {\n            double fractionalPart = x[i] - floor(x[i]);\n\n            if (fractionalPart < 0.25) {\n                bins[0]++;\n            }\n            if (fractionalPart >= 0.25 && fractionalPart < 0.5) {\n                bins[1]++;\n            }\n            if (fractionalPart >= 0.5 && fractionalPart < 0.75) {\n                bins[2]++;\n            }\n            if (fractionalPart >= 0.75 && fractionalPart < 1) {\n                bins[3]++;\n            }\n        }\n    }\n\n}",
            "int mpiRank;\n  int mpiSize;\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpiRank);\n  MPI_Comm_size(MPI_COMM_WORLD, &mpiSize);\n\n  int numBins = bins.size();\n\n  int n_per_rank = x.size() / mpiSize;\n\n  int remain = x.size() % mpiSize;\n  if (mpiRank < remain) {\n    n_per_rank++;\n  }\n\n  std::vector<double> x_local(n_per_rank);\n  MPI_Scatter(&x[mpiRank * n_per_rank], n_per_rank, MPI_DOUBLE, x_local.data(), n_per_rank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (int i = 0; i < n_per_rank; i++) {\n    if (x_local[i] < 0.25) {\n      bins[0]++;\n    }\n    if (x_local[i] >= 0.25 && x_local[i] < 0.5) {\n      bins[1]++;\n    }\n    if (x_local[i] >= 0.5 && x_local[i] < 0.75) {\n      bins[2]++;\n    }\n    if (x_local[i] >= 0.75 && x_local[i] < 1.0) {\n      bins[3]++;\n    }\n  }\n\n  // now all-reduce to sum the bins\n  int sum_bins[4];\n  MPI_Allreduce(&bins[0], &sum_bins[0], 4, MPI_UNSIGNED, MPI_SUM, MPI_COMM_WORLD);\n\n  for (int i = 0; i < numBins; i++) {\n    bins[i] = sum_bins[i];\n  }\n}",
            "/*\n  // Your code here\n  */\n}",
            "int rank = 0;\n    int size = 1;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<double> x_local(x);\n\n    // Parallel for\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] < 0.25) {\n            x_local[i] = 0;\n        }\n        else if (x[i] < 0.5) {\n            x_local[i] = 1;\n        }\n        else if (x[i] < 0.75) {\n            x_local[i] = 2;\n        }\n        else {\n            x_local[i] = 3;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<size_t> x_local_count(x_local.size());\n        #pragma omp parallel for schedule(static)\n        for (size_t i = 0; i < x_local.size(); i++) {\n            x_local_count[x_local[i]] += 1;\n        }\n        std::vector<size_t> x_count_global(4);\n        MPI_Reduce(&x_local_count[0], &x_count_global[0], x_local_count.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n        bins[0] = x_count_global[0];\n        bins[1] = x_count_global[1];\n        bins[2] = x_count_global[2];\n        bins[3] = x_count_global[3];\n    }\n    else {\n        MPI_Reduce(&x_local[0], NULL, x_local.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "size_t const size = x.size();\n    size_t const my_rank = MPI_Comm_rank(MPI_COMM_WORLD);\n\n    // Initialize the bins to zero\n    bins = std::array<size_t, 4>{0, 0, 0, 0};\n\n    // Compute the number of doubles in the vector x that have a fractional part \n    // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1) for each rank\n    // using OpenMP\n#pragma omp parallel for\n    for (size_t i = 0; i < size; ++i) {\n        auto const dbl = x[i];\n        auto const integral = static_cast<size_t>(std::floor(dbl));\n        auto const fraction = std::fmod(dbl, 1);\n        if (fraction <= 0.25) {\n            bins[0] += integral;\n        } else if (fraction <= 0.5) {\n            bins[1] += integral;\n        } else if (fraction <= 0.75) {\n            bins[2] += integral;\n        } else {\n            bins[3] += integral;\n        }\n    }\n\n    // Compute the number of doubles in the vector x that have a fractional part \n    // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1) in each rank.\n    std::array<size_t, 4> local_bins{0, 0, 0, 0};\n    std::array<size_t, 4> recv_bins{0, 0, 0, 0};\n    MPI_Allreduce(bins.data(), local_bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    // Print the number of doubles in the vector x that have a fractional part \n    // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1) on rank 0.\n    if (my_rank == 0) {\n        std::cout << \"[0, 0.25): \" << local_bins[0] << std::endl;\n        std::cout << \"[0.25, 0.5): \" << local_bins[1] << std::endl;\n        std::cout << \"[0.5, 0.75): \" << local_bins[2] << std::endl;\n        std::cout << \"[0.75, 1): \" << local_bins[3] << std::endl;\n    }\n\n    // Store the number of doubles in the vector x that have a fractional part \n    // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1) on rank 0.\n    if (my_rank == 0) {\n        recv_bins = local_bins;\n    }\n    MPI_Bcast(recv_bins.data(), 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    // Add the number of doubles in the vector x that have a fractional part \n    // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1) in each rank to get the final results.\n    for (size_t i = 0; i < 4; ++i) {\n        bins[i] += recv_bins[i];\n    }\n\n    if (my_rank == 0) {\n        std::cout << \"[0, 0.25): \" << bins[0] << std::endl;\n        std::cout << \"[0.25, 0.5): \" << bins[1] <<",
            "const size_t num_threads = omp_get_max_threads();\n  const size_t chunk_size = x.size() / num_threads;\n  const size_t rem = x.size() % num_threads;\n  size_t num_ranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  std::vector<size_t> counts(num_ranks);\n  std::vector<double> chunk_sizes(num_ranks);\n  #pragma omp parallel\n  {\n    int tid = omp_get_thread_num();\n    size_t begin = chunk_size * tid + std::min(rem, tid);\n    size_t end = chunk_size * (tid + 1) + std::min(rem, tid + 1);\n    std::vector<double> loc_bins(4);\n    for(size_t i = begin; i < end; ++i) {\n      int bucket = x[i] - floor(x[i]);\n      ++loc_bins[bucket];\n    }\n    MPI_Gather(&loc_bins[0], 4, MPI_UNSIGNED_LONG_LONG, &counts[0], 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  }\n\n  #pragma omp parallel\n  {\n    int tid = omp_get_thread_num();\n    size_t begin = chunk_size * tid + std::min(rem, tid);\n    size_t end = chunk_size * (tid + 1) + std::min(rem, tid + 1);\n    for(size_t i = begin; i < end; ++i) {\n      int bucket = x[i] - floor(x[i]);\n      ++bins[bucket];\n    }\n  }\n  MPI_Reduce(counts.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "double x_min = 100;\n\tdouble x_max = 0;\n\n\tsize_t n = x.size();\n\tsize_t n_chunk = n / omp_get_max_threads();\n\tsize_t n_rem = n % omp_get_max_threads();\n\tsize_t offset = 0;\n\tfor (int i = 0; i < omp_get_max_threads(); ++i) {\n\t\tsize_t chunk_size = n_chunk + (i < n_rem? 1 : 0);\n\t\t#pragma omp parallel for\n\t\tfor (size_t j = 0; j < chunk_size; ++j) {\n\t\t\tfor (size_t k = offset; k < offset + chunk_size; ++k) {\n\t\t\t\tif (x[k] < x_min) {\n\t\t\t\t\tx_min = x[k];\n\t\t\t\t}\n\t\t\t\tif (x[k] > x_max) {\n\t\t\t\t\tx_max = x[k];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\toffset += chunk_size;\n\t}\n\tdouble x_range = x_max - x_min;\n\tdouble bin_width = x_range / 4;\n\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < omp_get_max_threads(); ++i) {\n\t\tsize_t chunk_size = n_chunk + (i < n_rem? 1 : 0);\n\t\tsize_t offset = i * n_chunk;\n\t\tfor (size_t j = 0; j < chunk_size; ++j) {\n\t\t\tbins[0] += (x[offset + j] - x_min) / bin_width < 1? 1 : 0;\n\t\t\tbins[1] += (x[offset + j] - x_min) / bin_width < 2? 1 : 0;\n\t\t\tbins[2] += (x[offset + j] - x_min) / bin_width < 3? 1 : 0;\n\t\t\tbins[3] += (x[offset + j] - x_min) / bin_width < 4? 1 : 0;\n\t\t}\n\t}\n}",
            "// TODO\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Count the number of doubles in the vector x that have a fractional part \n    // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n    // Assume that MPI has already been initialized.\n    // Every rank has a complete copy of x. The result is stored in bins on rank 0.\n    // Examples:\n    //\n    // input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n    // output: [2, 1, 2, 2]\n    //\n    // input: [1.9, 0.2, 0.6, 10.1, 7.4]\n    // output: [2, 1, 1, 1]\n\n\n    double num;\n    double remainder;\n\n    bins = {0, 0, 0, 0};\n\n    // for (int i = 0; i < x.size(); i++)\n    // {\n    //     remainder = modf(x[i], &num);\n    //     if (remainder >= 0.0 && remainder < 0.25)\n    //         bins[0]++;\n    //     else if (remainder >= 0.25 && remainder < 0.5)\n    //         bins[1]++;\n    //     else if (remainder >= 0.5 && remainder < 0.75)\n    //         bins[2]++;\n    //     else\n    //         bins[3]++;\n    // }\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++)\n    {\n        remainder = modf(x[i], &num);\n        if (remainder >= 0.0 && remainder < 0.25)\n            bins[0]++;\n        else if (remainder >= 0.25 && remainder < 0.5)\n            bins[1]++;\n        else if (remainder >= 0.5 && remainder < 0.75)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(bins.data(), 4, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int i = 0; i < 4; i++) {\n                bins[i] += bins[i];\n            }\n        }\n    }\n    else {\n        MPI_Send(bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "/* NOTE: You can't use std::array as a return value because MPI_Gather \n              is not defined for std::array */\n    \n    // TODO: fill in the code\n    MPI_Barrier(MPI_COMM_WORLD);\n    \n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size_t n = x.size() / size;\n    \n    std::vector<size_t> counts(4);\n    std::vector<size_t> local_counts(4);\n    \n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            MPI_Status status;\n            MPI_Recv(&counts, 4, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n            bins[0] += counts[0];\n            bins[1] += counts[1];\n            bins[2] += counts[2];\n            bins[3] += counts[3];\n        }\n    }\n    else {\n        std::vector<double> local_x(n);\n        for (size_t i = 0; i < n; ++i) {\n            local_x[i] = x[i + rank * n];\n        }\n        for (size_t i = 0; i < n; ++i) {\n            if (local_x[i] < 1.0) {\n                if (local_x[i] < 0.25) {\n                    ++local_counts[0];\n                }\n                else if (local_x[i] < 0.5) {\n                    ++local_counts[1];\n                }\n                else if (local_x[i] < 0.75) {\n                    ++local_counts[2];\n                }\n                else {\n                    ++local_counts[3];\n                }\n            }\n        }\n        MPI_Send(&local_counts, 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n    \n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "size_t num_elements = x.size();\n    if (num_elements == 0) {\n        return;\n    }\n    size_t rank;\n    int num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    int num_threads = omp_get_max_threads();\n    size_t const num_per_thread = num_elements/num_threads;\n    std::array<size_t, 4> local_bins{0, 0, 0, 0};\n    #pragma omp parallel\n    {\n        size_t const tid = omp_get_thread_num();\n        size_t const start = tid * num_per_thread;\n        size_t const end = start + num_per_thread;\n        for (size_t i = start; i < end; i++) {\n            double v = x[i];\n            local_bins[(int)(v*4)]++;\n        }\n    }\n    int* bins_ptr = bins.data();\n    MPI_Reduce(local_bins.data(), bins_ptr, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    return;\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  // TODO: count the quartiles\n  //...\n  //...\n}",
            "// TODO: Fill in this function\n}",
            "int num_procs, rank, len;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<double> tmp_x(x);\n    std::sort(tmp_x.begin(), tmp_x.end());\n\n    double quartile = tmp_x.size() / 4;\n    size_t bin = 0;\n    for(size_t i = 0; i < tmp_x.size(); i++) {\n        if(i < quartile) {\n            bin = 0;\n        }\n        else if(i < 2 * quartile) {\n            bin = 1;\n        }\n        else if(i < 3 * quartile) {\n            bin = 2;\n        }\n        else {\n            bin = 3;\n        }\n        bins[bin]++;\n    }\n\n    MPI_Reduce(&bins, &bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if(rank == 0) {\n        for(size_t i = 0; i < 4; i++) {\n            std::cout << bins[i] << \" \";\n        }\n        std::cout << std::endl;\n    }\n}",
            "int rank;\n    int nproc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Status status;\n    std::vector<double> my_x(x);\n    std::sort(my_x.begin(), my_x.end());\n    int my_size = (int)my_x.size();\n    if (nproc!= my_size) {\n        if (rank == 0) {\n            std::cout << \"Number of processors must be equal to the vector size\" << std::endl;\n        }\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n    size_t bins_size = 4;\n    if (rank == 0) {\n        bins.fill(0);\n    }\n#pragma omp parallel for\n    for (int i = 0; i < my_size; i++) {\n        int rank_id = (int) my_x[i];\n        int rank_id_rounded = floor(rank_id);\n        if (rank_id_rounded == 0.0) {\n            if (rank == 0) {\n                bins[0]++;\n            }\n            rank_id_rounded = 0;\n        }\n        if (rank_id_rounded == 0.25) {\n            if (rank == 0) {\n                bins[1]++;\n            }\n            rank_id_rounded = 1;\n        }\n        if (rank_id_rounded == 0.5) {\n            if (rank == 0) {\n                bins[2]++;\n            }\n            rank_id_rounded = 2;\n        }\n        if (rank_id_rounded == 0.75) {\n            if (rank == 0) {\n                bins[3]++;\n            }\n            rank_id_rounded = 3;\n        }\n        if (rank_id_rounded > 3.0) {\n            if (rank == 0) {\n                bins[3]++;\n            }\n            rank_id_rounded = 3;\n        }\n        if (rank == rank_id_rounded) {\n            my_x[i] = rank_id_rounded;\n        }\n    }\n#pragma omp parallel for\n    for (int i = 0; i < my_size; i++) {\n        int rank_id = (int) my_x[i];\n        if (rank == rank_id) {\n            int bins_rank_id = rank_id;\n            MPI_Send(&bins_rank_id, 1, MPI_INT, 0, 1, MPI_COMM_WORLD);\n            MPI_Send(&my_x[i], 1, MPI_DOUBLE, 0, 1, MPI_COMM_WORLD);\n        } else if (rank == 0) {\n            int bins_rank_id;\n            double my_x_double;\n            MPI_Recv(&bins_rank_id, 1, MPI_INT, rank, 1, MPI_COMM_WORLD, &status);\n            MPI_Recv(&my_x_double, 1, MPI_DOUBLE, rank, 1, MPI_COMM_WORLD, &status);\n            bins[bins_rank_id]++;\n        }\n    }\n\n}",
            "int nbins = 4;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = (int) x.size();\n  int part_size = local_size/size;\n  int remainder = local_size%size;\n\n  std::vector<double> localsum(nbins);\n  #pragma omp parallel for\n  for(int i=0; i<part_size; i++) {\n    for(int j=0; j<nbins; j++) {\n      if(x[i*size+rank] <= 0.25) {\n        localsum[j]++;\n      }\n      else if(x[i*size+rank] <= 0.5) {\n        localsum[j]++;\n      }\n      else if(x[i*size+rank] <= 0.75) {\n        localsum[j]++;\n      }\n      else {\n        localsum[j]++;\n      }\n    }\n  }\n\n  std::vector<int> localsums(nbins);\n  std::vector<int> recvcounts(nbins);\n  std::vector<int> recvoffsets(nbins);\n\n  #pragma omp parallel for\n  for(int i=0; i<nbins; i++) {\n    localsums[i] = localsum[i];\n  }\n\n  if(remainder>0) {\n    #pragma omp parallel for\n    for(int i=0; i<nbins; i++) {\n      if(rank<remainder) {\n        localsums[i] = localsum[part_size*size + rank];\n      }\n    }\n  }\n\n  MPI_Allreduce(MPI_IN_PLACE, &localsums[0], nbins, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n  #pragma omp parallel for\n  for(int i=0; i<nbins; i++) {\n    recvcounts[i] = localsum[i];\n    recvoffsets[i] = localsum[i];\n  }\n\n  if(rank==0) {\n    for(int i=1; i<size; i++) {\n      MPI_Status status;\n      MPI_Recv(&localsums[0], nbins, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n      for(int j=0; j<nbins; j++) {\n        localsum[j] += localsums[j];\n        recvcounts[j] += localsums[j];\n        recvoffsets[j] += localsums[j];\n      }\n    }\n\n    bins[0] = localsum[0];\n    bins[1] = localsum[1];\n    bins[2] = localsum[2];\n    bins[3] = localsum[3];\n  }\n  else {\n    MPI_Send(&localsums[0], nbins, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n\n  std::vector<double> binsum(nbins);\n  #pragma omp parallel for\n  for(int i=0; i<nbins; i++) {\n    binsum[i] = (double)recvoffsets[i];\n  }\n\n  for(int i=0; i<nbins; i++) {\n    binsum[i] = (double)recvcounts[i]/(double)local_size;\n  }\n\n  #pragma omp parallel for\n  for(int i=0; i<nbins; i++) {\n    bins[i] = (double)binsum[i];\n  }\n\n  if(rank==0) {\n    std::cout << \"counts: \" << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << std::endl;\n  }\n}",
            "// TODO\n}",
            "// FIXME\n    const double PI = 3.14159265358979323846;\n    // FIXME\n    const int rank = 0;\n    const int nprocs = 1;\n    const int n = x.size();\n    double* x_ptr = x.data();\n    size_t n_bins = bins.size();\n\n    // FIXME\n    // Create a 2-D array to hold the quartile bins. \n    std::vector<double> qs(n_bins, 0.0);\n    // FIXME\n    for (size_t i = 0; i < n_bins; i++) {\n        qs[i] = (double)i/4.0 + 0.25;\n    }\n    // FIXME\n    double* qs_ptr = qs.data();\n\n    // FIXME\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        size_t qs_idx = 0;\n        // FIXME\n        while (x[i] > qs[qs_idx]) {\n            qs_idx++;\n        }\n        // FIXME\n        bins[qs_idx]++;\n    }\n    // FIXME\n    if (rank == 0) {\n        for (int i = 0; i < n_bins; i++) {\n            printf(\" %0.2lf\", qs[i]);\n        }\n        printf(\"\\n\");\n    }\n\n    // FIXME\n    return;\n}",
            "// TODO: implement the function\n    int N, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &N);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size_t num_procs = N;\n    size_t chunk = x.size() / num_procs;\n    size_t rem = x.size() % num_procs;\n    std::vector<size_t> local_bins(4, 0);\n    if(rank == 0){\n        for(int i = 0; i < num_procs; i++){\n            std::vector<double> subvector(x.begin() + chunk*i, chunk + x.begin() + chunk*i + rem);\n            std::vector<double> subvector_copy(subvector);\n            #pragma omp parallel for num_threads(4)\n            for(int j = 0; j < subvector.size(); j++){\n                if(subvector[j] < 0.25 && subvector[j] >= 0.0){\n                    local_bins[0]++;\n                }\n                else if(subvector[j] < 0.5 && subvector[j] >= 0.25){\n                    local_bins[1]++;\n                }\n                else if(subvector[j] < 0.75 && subvector[j] >= 0.5){\n                    local_bins[2]++;\n                }\n                else{\n                    local_bins[3]++;\n                }\n            }\n            for(int j = 0; j < 4; j++){\n                bins[j] += local_bins[j];\n            }\n        }\n    }\n    else{\n        #pragma omp parallel for num_threads(4)\n        for(int j = 0; j < x.size(); j++){\n            if(x[j] < 0.25 && x[j] >= 0.0){\n                local_bins[0]++;\n            }\n            else if(x[j] < 0.5 && x[j] >= 0.25){\n                local_bins[1]++;\n            }\n            else if(x[j] < 0.75 && x[j] >= 0.5){\n                local_bins[2]++;\n            }\n            else{\n                local_bins[3]++;\n            }\n        }\n    }\n}",
            "int rank, nproc;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  size_t block_size = x.size()/nproc;\n  size_t extra = x.size()%nproc;\n  size_t x_start = rank*block_size;\n  size_t x_end = x_start + block_size;\n  if(rank < extra)\n    x_end += 1;\n\n  size_t count_0 = 0, count_0_25 = 0, count_0_5 = 0, count_0_75 = 0;\n  for(size_t i = x_start; i < x_end; i++){\n    double val = x[i];\n    if (val > 0 && val < 0.25){\n      count_0_25++;\n    } else if (val > 0.25 && val < 0.5){\n      count_0_5++;\n    } else if (val > 0.5 && val < 0.75){\n      count_0_75++;\n    } else if (val > 0.75 && val < 1){\n      count_0++;\n    }\n  }\n\n  size_t counts[4] = {count_0, count_0_25, count_0_5, count_0_75};\n\n  if (rank == 0){\n    bins[0] = counts[0] + counts[1];\n    bins[1] = counts[1] + counts[2];\n    bins[2] = counts[2] + counts[3];\n    bins[3] = counts[3];\n  }\n\n  // use MPI to sum counts[0], counts[1], counts[2], and counts[3]\n  int disp[4];\n  disp[0] = 0;\n  disp[1] = counts[0];\n  disp[2] = disp[0] + counts[1];\n  disp[3] = disp[1] + counts[2];\n\n  int recvcounts[4] = {1, 1, 1, 1};\n  MPI_Datatype types[4] = {MPI_UNSIGNED_LONG, MPI_UNSIGNED_LONG, MPI_UNSIGNED_LONG, MPI_UNSIGNED_LONG};\n\n  std::array<size_t, 4> recvbuf;\n  MPI_Allgatherv(counts, 4, MPI_UNSIGNED_LONG, recvbuf.data(), recvcounts, disp, types, MPI_COMM_WORLD);\n\n  for (int i = 0; i < 4; i++)\n    bins[i] = recvbuf[i];\n}",
            "// YOUR CODE HERE\n  // --------------\n\n  return;\n\n  // END YOUR CODE HERE\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double l = 0, h = 1;\n    double chunk = (h - l) / size;\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    std::vector<size_t> counts(size);\n\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        double f = x[i] - floor(x[i]);\n        if (f < chunk / 4) {\n            local_bins[0]++;\n        } else if (f < 3 * chunk / 4) {\n            local_bins[1]++;\n        } else if (f < 2 * chunk) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    MPI_Gather(local_bins.data(), 4, MPI_LONG_LONG_INT, counts.data(), 4, MPI_LONG_LONG_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < size; i++) {\n            bins[0] += counts[i];\n        }\n        bins[1] = bins[0] / 4;\n        bins[2] = bins[1] + counts[0] / 4;\n        bins[3] = bins[2] + counts[0] / 4;\n    }\n}",
            "// FIXME: Implement me!\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        MPI_Bcast(x.data(), x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<double> x_temp(x);\n        MPI_Bcast(x_temp.data(), x_temp.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n\n    std::vector<double> x_local(x);\n\n    auto count = [](double x, double lower, double upper) {\n        if (x >= lower && x < upper) {\n            return 1;\n        } else {\n            return 0;\n        }\n    };\n\n    size_t local_bins[4] = {0, 0, 0, 0};\n    auto quartile_size = x.size() / 4;\n    for (int i = 0; i < quartile_size; ++i) {\n        local_bins[0] += count(x_local[i], 0.0, 0.25);\n        local_bins[1] += count(x_local[i], 0.25, 0.5);\n        local_bins[2] += count(x_local[i], 0.5, 0.75);\n        local_bins[3] += count(x_local[i], 0.75, 1.0);\n    }\n\n    MPI_Reduce(MPI_IN_PLACE, local_bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    std::array<size_t, 4> bins_temp;\n    if (rank == 0) {\n        bins = bins_temp;\n    }\n    MPI_Gather(local_bins, 4, MPI_INT, bins_temp.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int numRanks;\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n    double range = 0.25;\n    double rangeInv = 1 / range;\n\n    std::vector<double> localX;\n    localX.resize(x.size() / numRanks);\n    std::copy(x.begin(), x.begin() + (x.size() / numRanks), localX.begin());\n\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (int i = 0; i < localX.size(); i++) {\n        double v = localX[i];\n        if (v >= 0 && v < range) {\n            count++;\n        }\n        if (v >= range && v < range*2) {\n            count++;\n        }\n        if (v >= range*2 && v < range*3) {\n            count++;\n        }\n        if (v >= range*3 && v < 1.0) {\n            count++;\n        }\n    }\n\n    std::vector<size_t> temp;\n    temp.resize(4);\n    temp[0] = count;\n    temp[1] = 0;\n    temp[2] = 0;\n    temp[3] = 0;\n\n    MPI_Reduce(temp.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Implement this function\n}",
            "}",
            "int rank, nproc;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  // Initialize bins to zero\n  for(int i = 0; i < 4; i++) bins[i] = 0;\n\n  // Count the number of doubles in x that have a fractional part\n  // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n  #pragma omp parallel num_threads(nproc)\n  {\n    // Initialize local bin counts\n    std::array<size_t, 4> localBins{0, 0, 0, 0};\n\n    // Count the number of doubles in x that have a fractional part\n    // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n    #pragma omp for\n    for(size_t i = 0; i < x.size(); i++) {\n      // Get the fractional part of x[i]\n      double frac = std::modf(x[i], &x[i]);\n      // If the fractional part is between 0 and 0.25\n      if(frac >= 0 && frac < 0.25) {\n        localBins[0]++;\n      // If the fractional part is between 0.25 and 0.5\n      } else if(frac >= 0.25 && frac < 0.5) {\n        localBins[1]++;\n      // If the fractional part is between 0.5 and 0.75\n      } else if(frac >= 0.5 && frac < 0.75) {\n        localBins[2]++;\n      // If the fractional part is between 0.75 and 1\n      } else if(frac >= 0.75 && frac < 1) {\n        localBins[3]++;\n      }\n    }\n\n    // Add the local bin counts\n    for(int i = 0; i < 4; i++) {\n      #pragma omp atomic\n      bins[i] += localBins[i];\n    }\n  }\n}",
            "// TODO: implement\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int bins_per_rank = 4 / size;\n    int bins_left = 4 - (size - 1) * bins_per_rank;\n    if (rank == 0) {\n        std::vector<double> x_rank(x.begin() + rank * bins_per_rank, x.begin() + (rank + 1) * bins_per_rank);\n        std::sort(x_rank.begin(), x_rank.end());\n        for (int i = 0; i < bins_per_rank; ++i) {\n            double lo = x_rank[i];\n            double hi = lo + 0.25;\n            auto count = std::count_if(x.begin(), x.end(), [lo, hi](double val) {\n                return (val >= lo) && (val < hi);\n            });\n            bins[i] = count;\n        }\n        for (int i = 0; i < bins_left; ++i) {\n            double lo = x_rank[bins_per_rank - 1] + (i + 1) * 0.25;\n            double hi = lo + 0.25;\n            auto count = std::count_if(x.begin(), x.end(), [lo, hi](double val) {\n                return (val >= lo) && (val < hi);\n            });\n            bins[bins_per_rank + i] = count;\n        }\n    } else {\n        std::vector<double> x_rank(x.begin() + rank * bins_per_rank, x.begin() + (rank + 1) * bins_per_rank);\n        std::sort(x_rank.begin(), x_rank.end());\n        for (int i = 0; i < bins_per_rank; ++i) {\n            double lo = x_rank[i];\n            double hi = lo + 0.25;\n            auto count = std::count_if(x_rank.begin(), x_rank.end(), [lo, hi](double val) {\n                return (val >= lo) && (val < hi);\n            });\n            MPI_Send(&count, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n        }\n        for (int i = 0; i < bins_left; ++i) {\n            double lo = x_rank[bins_per_rank - 1] + (i + 1) * 0.25;\n            double hi = lo + 0.25;\n            auto count = std::count_if(x_rank.begin(), x_rank.end(), [lo, hi](double val) {\n                return (val >= lo) && (val < hi);\n            });\n            MPI_Send(&count, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<size_t> counts;\n        for (int i = 0; i < size; ++i) {\n            if (i == rank) {\n                for (int j = 0; j < bins_per_rank; ++j) {\n                    counts.push_back(bins[j]);\n                }\n                for (int j = 0; j < bins_left; ++j) {\n                    counts.push_back(bins[bins_per_rank + j]);\n                }\n            }\n            else {\n                size_t count;\n                MPI_Status status;\n                MPI_Recv(&count, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n                counts.push_back(count);\n            }\n        }\n        #pragma omp parallel for\n        for (int i = 0; i < bins_per_rank; ++i) {\n            bins[i] = 0;\n        }",
            "int nprocs, myrank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n    // your code here\n}",
            "size_t n = x.size();\n\n  // TODO: Fill in the code below, so that MPI ranks compute the number of doubles in their part of x, and then add the results\n  // to bins\n  int rank, num_ranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  std::vector<double> x_copy;\n  x_copy.resize(n);\n  if (rank == 0) {\n    x_copy = x;\n  } else {\n    x_copy.resize(n/num_ranks);\n    MPI_Scatter(x.data(), n/num_ranks, MPI_DOUBLE, x_copy.data(), n/num_ranks, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  }\n\n  // compute the fractional part\n  for (int i = 0; i < n; i++) {\n    x_copy[i] -= floor(x_copy[i]);\n  }\n\n  // use omp to parallelize the counting process\n  std::array<size_t, 4> bins_tmp;\n  bins_tmp.fill(0);\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    if (x_copy[i] < 0.25)\n      bins_tmp[0]++;\n    else if (x_copy[i] < 0.5)\n      bins_tmp[1]++;\n    else if (x_copy[i] < 0.75)\n      bins_tmp[2]++;\n    else \n      bins_tmp[3]++;\n  }\n  MPI_Reduce(MPI_IN_PLACE, bins_tmp.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  MPI_Bcast(bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "size_t size = x.size();\n    size_t N = size / omp_get_num_threads();\n    size_t thread_rank = omp_get_thread_num();\n\n    std::vector<double> x_temp(N);\n    for (size_t i = 0; i < N; ++i) {\n        x_temp[i] = x[i + N * thread_rank];\n    }\n\n    double const threshold_1 = 0.25;\n    double const threshold_2 = 0.5;\n    double const threshold_3 = 0.75;\n    double const threshold_4 = 1;\n\n    std::array<size_t, 4> temp_bins = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x_temp.size(); ++i) {\n        if (x_temp[i] < threshold_1) {\n            ++temp_bins[0];\n        }\n        else if (x_temp[i] >= threshold_1 && x_temp[i] < threshold_2) {\n            ++temp_bins[1];\n        }\n        else if (x_temp[i] >= threshold_2 && x_temp[i] < threshold_3) {\n            ++temp_bins[2];\n        }\n        else if (x_temp[i] >= threshold_3 && x_temp[i] < threshold_4) {\n            ++temp_bins[3];\n        }\n    }\n\n    size_t rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (rank == 0) {\n        for (size_t i = 0; i < 4; ++i) {\n            bins[i] = 0;\n            for (size_t j = 0; j < omp_get_num_threads(); ++j) {\n                bins[i] += temp_bins[i];\n            }\n        }\n    }\n\n}",
            "}",
            "auto size = x.size();\n    size_t nbins = 4;\n    // bins[i] is the number of doubles in the vector x that have a fractional part\n    // in [i/4, (i+1)/4) for i = 0, 1, 2, 3\n    bins.fill(0);\n    if (size <= 0)\n    {\n        return;\n    }\n    #pragma omp parallel \n    {\n        #pragma omp for\n        for (size_t i = 0; i < size; i++)\n        {\n            size_t bin = size_t(((x[i] - floor(x[i])) * 4.0));\n            bins[bin]++;\n        }\n    }\n    return;\n}",
            "// YOUR CODE HERE\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, nproc;\n    MPI_Comm_size(comm, &nproc);\n    MPI_Comm_rank(comm, &rank);\n\n    int my_bins[4];\n    for(int i = 0; i < 4; i++){\n        my_bins[i] = 0;\n    }\n\n    // Count the number of double in the vector x that have a fractional part \n    // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n    for(int i = 0; i < x.size(); i++){\n        if(x[i] >= 0.75 && x[i] < 1.0)\n            my_bins[3] += 1;\n        else if(x[i] >= 0.5 && x[i] < 0.75)\n            my_bins[2] += 1;\n        else if(x[i] >= 0.25 && x[i] < 0.5)\n            my_bins[1] += 1;\n        else\n            my_bins[0] += 1;\n    }\n    \n    if(rank == 0){\n        MPI_Reduce(MPI_IN_PLACE, my_bins, 4, MPI_INT, MPI_SUM, 0, comm);\n        for(int i = 0; i < 4; i++)\n            bins[i] = my_bins[i];\n    }else{\n        MPI_Reduce(my_bins, my_bins, 4, MPI_INT, MPI_SUM, 0, comm);\n    }\n\n}",
            "// TODO: implement\n}",
            "// You code here\n\n  // Hint: it may be useful to first compute the size of the input vector, \n  //       and then to use std::partition to split the input into quartiles.\n\n  // Hint: you may use std::partition_copy.\n\n  // Hint: you may also want to make use of MPI_Reduce to combine the\n  //       results of each rank.\n\n  // Hint: you may also want to make use of OpenMP\n\n  // Hint: if you use OpenMP, you may want to use #pragma omp parallel for reduction(+:...)\n}",
            "// TODO\n  // compute global size of x\n  int size_global = 0;\n  MPI_Reduce(&x.size(), &size_global, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // compute local size of x and offset\n  int size_local = x.size();\n  int rank = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  size_t offset = 0;\n  MPI_Exscan(&size_local, &offset, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n  // check if rank 0 is correct\n  if (rank == 0) {\n    if (size_global!= offset + x.size()) {\n      printf(\"MPI_Exscan failed\\n\");\n      exit(-1);\n    }\n  }\n\n  // allocate space\n  std::vector<double> x_local(size_local);\n\n  // copy to local x_local\n  for (int i = 0; i < size_local; i++) {\n    x_local[i] = x[i+offset];\n  }\n\n  #pragma omp parallel\n  {\n    double sum = 0.0;\n    #pragma omp for reduction(+: sum)\n    for (int i = 0; i < size_local; i++) {\n      sum += x_local[i];\n    }\n    double average = sum / size_local;\n\n    #pragma omp for\n    for (int i = 0; i < size_local; i++) {\n      if (x_local[i] < average) {\n        x_local[i] = 0.0;\n      } else {\n        x_local[i] = 1.0;\n      }\n    }\n  }\n\n  size_t bins_local[4] = {0, 0, 0, 0};\n\n  #pragma omp parallel\n  {\n    // count the number of fractions in 4 bins\n    #pragma omp for\n    for (int i = 0; i < size_local; i++) {\n      double fraction = fmod(x_local[i], 1.0);\n      if (fraction < 0.25) {\n        bins_local[0] += 1;\n      } else if (fraction < 0.5) {\n        bins_local[1] += 1;\n      } else if (fraction < 0.75) {\n        bins_local[2] += 1;\n      } else {\n        bins_local[3] += 1;\n      }\n    }\n  }\n\n  MPI_Allreduce(MPI_IN_PLACE, bins_local, 4, MPI_LONG_LONG_INT, MPI_SUM, MPI_COMM_WORLD);\n\n  bins[0] = bins_local[0];\n  bins[1] = bins_local[1];\n  bins[2] = bins_local[2];\n  bins[3] = bins_local[3];\n\n  // free space\n  // delete x_local;\n  // x_local = NULL;\n  // delete bins_local;\n  // bins_local = NULL;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    constexpr size_t n = 4;\n    std::array<size_t, 4> my_bins;\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++)\n        if (x[i] >= 0 && x[i] <= 0.25) my_bins[0]++;\n        else if (x[i] >= 0.25 && x[i] <= 0.5) my_bins[1]++;\n        else if (x[i] >= 0.5 && x[i] <= 0.75) my_bins[2]++;\n        else if (x[i] >= 0.75 && x[i] <= 1) my_bins[3]++;\n    MPI_Reduce(my_bins.data(), bins.data(), n, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "assert(!x.empty());\n\n    int rank = 0;\n    int mpi_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    int num_ranks = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    if (mpi_rank == 0) {\n        for (int i = 0; i < 4; ++i) {\n            bins[i] = 0;\n        }\n    }\n\n    // Split the vector into chunks and send chunks to other ranks\n    size_t size = x.size();\n    size_t chunk_size = size / num_ranks;\n    size_t last_chunk_size = size - chunk_size * (num_ranks - 1);\n    size_t num_chunks = (mpi_rank == 0? num_ranks : num_ranks - 1);\n    std::vector<std::vector<double>> x_chunks(num_chunks);\n    for (size_t i = 0; i < num_chunks; ++i) {\n        x_chunks[i].reserve(i == num_chunks - 1? last_chunk_size : chunk_size);\n    }\n    for (size_t i = 0; i < size; ++i) {\n        x_chunks[i % num_chunks].push_back(x[i]);\n    }\n\n    std::vector<std::array<size_t, 4>> bins_chunks(num_chunks);\n    for (size_t i = 0; i < num_chunks; ++i) {\n        countQuartiles(x_chunks[i], bins_chunks[i]);\n    }\n\n    // Receive bins from other ranks\n    std::vector<std::array<size_t, 4>> all_bins(num_ranks);\n    MPI_Allgather(bins_chunks.data(), sizeof(bins_chunks[0]), MPI_BYTE,\n                  all_bins.data(), sizeof(all_bins[0]), MPI_BYTE, MPI_COMM_WORLD);\n\n    if (mpi_rank == 0) {\n        for (int i = 1; i < num_ranks; ++i) {\n            bins[0] += all_bins[i][0];\n            bins[1] += all_bins[i][1];\n            bins[2] += all_bins[i][2];\n            bins[3] += all_bins[i][3];\n        }\n    }\n}",
            "size_t num_doubles = x.size();\n\tsize_t global_num_doubles = 0;\n\tMPI_Allreduce(&num_doubles, &global_num_doubles, 1, MPI_UNSIGNED, MPI_SUM, MPI_COMM_WORLD);\n\tbins = { 0, 0, 0, 0 };\n\tstd::vector<double> local_x(x.begin(), x.end());\n\tomp_set_num_threads(2);\n\tomp_set_dynamic(0);\n\tomp_set_nested(1);\n#pragma omp parallel\n\t{\n\t\t//bins = { 0, 0, 0, 0 };\n#pragma omp for\n\t\tfor (int i = 0; i < num_doubles; i++) {\n\t\t\tif (local_x[i] >= 0.0 && local_x[i] < 0.25) {\n\t\t\t\tbins[0]++;\n\t\t\t}\n\t\t\telse if (local_x[i] >= 0.25 && local_x[i] < 0.5) {\n\t\t\t\tbins[1]++;\n\t\t\t}\n\t\t\telse if (local_x[i] >= 0.5 && local_x[i] < 0.75) {\n\t\t\t\tbins[2]++;\n\t\t\t}\n\t\t\telse if (local_x[i] >= 0.75 && local_x[i] <= 1.0) {\n\t\t\t\tbins[3]++;\n\t\t\t}\n\t\t}\n#pragma omp barrier\n#pragma omp for\n\t\tfor (int i = 0; i < 4; i++) {\n\t\t\tbins[i] = global_num_doubles * bins[i] / num_doubles;\n\t\t}\n\t}\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n\tsize_t n = x.size();\n\n\tstd::vector<double> allx(n);\n\tMPI_Allgather(&x[0], n, MPI_DOUBLE, &allx[0], n, MPI_DOUBLE, comm);\n\n\tstd::vector<int> all_ranks(n);\n\tMPI_Allgather(&comm, 1, MPI_INT, &all_ranks[0], 1, MPI_INT, comm);\n\n\tsize_t offset = 0;\n\tint rank = 0;\n\tint nranks = 0;\n\tMPI_Comm_size(comm, &nranks);\n\tMPI_Comm_rank(comm, &rank);\n\n\tstd::vector<double> xranks(n);\n\tstd::vector<int> ranks(n);\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < n; i++) {\n\t\txranks[i] = allx[i + offset];\n\t\tranks[i] = all_ranks[i + offset];\n\t}\n\n\tstd::sort(xranks.begin(), xranks.end());\n\tstd::sort(ranks.begin(), ranks.end());\n\n\t// first find out the number of doubles with rank less than or equal to mine\n\tsize_t first_mine = std::upper_bound(ranks.begin(), ranks.end(), rank) - ranks.begin();\n\tsize_t n_mine = first_mine;\n\n\tfor (int i = 0; i < rank; i++) {\n\t\tn_mine += std::count(ranks.begin(), ranks.end(), i);\n\t}\n\n\tstd::cout << \"number of my doubles = \" << n_mine << std::endl;\n\n\t// find the quartiles\n\tsize_t n_q0 = n_mine / 4;\n\tsize_t n_q1 = n_mine / 2;\n\tsize_t n_q2 = 3 * n_mine / 4;\n\tsize_t n_q3 = n_mine;\n\tsize_t start = 0;\n\tsize_t end = 0;\n\tif (rank == 0) {\n\t\tbins[0] = 0;\n\t\tbins[1] = 0;\n\t\tbins[2] = 0;\n\t\tbins[3] = 0;\n\t}\n\telse {\n\t\tstart = n_q0;\n\t\tend = start + n_q1;\n\t\tfor (size_t i = start; i < end; i++) {\n\t\t\tbins[0] += 1;\n\t\t}\n\n\t\tstart = end;\n\t\tend = start + n_q2;\n\t\tfor (size_t i = start; i < end; i++) {\n\t\t\tbins[1] += 1;\n\t\t}\n\n\t\tstart = end;\n\t\tend = start + n_q3;\n\t\tfor (size_t i = start; i < end; i++) {\n\t\t\tbins[2] += 1;\n\t\t}\n\n\t\tstart = end;\n\t\tend = start + n_mine;\n\t\tfor (size_t i = start; i < end; i++) {\n\t\t\tbins[3] += 1;\n\t\t}\n\t}\n\n\t// now sum up the results from the other ranks\n\tsize_t total = n_mine;\n\tstd::vector<int> counts(4, 0);\n\tfor (int i = 0; i < nranks; i++) {\n\t\tif (i == rank) {\n\t\t\tcontinue;\n\t\t}\n\t\tMPI_Send(&bins[0], 4, MPI_INT, i, 0, comm);\n\t}\n\n\tfor (int i = 0; i < nranks; i++) {\n\t\tif (i == rank) {\n\t\t\tcontinue;\n\t\t}\n\t\tMPI_Recv(&counts[0], 4, MPI_INT, i, 0, comm, MPI_STATUS_IGNORE);",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t n = x.size();\n    size_t m = (n + size - 1) / size;\n    std::vector<double> subX(x.begin() + rank*m, std::min(x.begin() + (rank+1)*m, x.end()));\n\n    if (rank == 0) {\n        bins[0] = std::count_if(subX.begin(), subX.end(), [](double x){return x < 0.25;});\n        bins[1] = std::count_if(subX.begin(), subX.end(), [](double x){return x >= 0.25 && x < 0.5;});\n        bins[2] = std::count_if(subX.begin(), subX.end(), [](double x){return x >= 0.5 && x < 0.75;});\n        bins[3] = std::count_if(subX.begin(), subX.end(), [](double x){return x >= 0.75;});\n    } else {\n        MPI_Send(subX.data(), m, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n\n    // if (rank == 0) {\n    //     MPI_Request reqs[4];\n    //     size_t req = 0;\n    //     while (req < 4) {\n    //         MPI_Irecv(&bins[req], 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &reqs[req]);\n    //         req++;\n    //     }\n    //     MPI_Waitall(4, reqs, MPI_STATUSES_IGNORE);\n    // } else {\n    //     MPI_Send(&subX[0], m, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    // }\n    // MPI_Barrier(MPI_COMM_WORLD);\n\n    // if (rank == 0) {\n    //     for (int i = 1; i < size; i++) {\n    //         MPI_Recv(&bins[0], 4, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    //     }\n    // } else {\n    //     MPI_Send(&subX[0], m, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    // }\n\n    // #pragma omp parallel for\n    // for (size_t i = 0; i < m; i++) {\n    //     if (x[i + rank * m] < 0.25) {\n    //         bins[0]++;\n    //     } else if (x[i + rank * m] >= 0.25 && x[i + rank * m] < 0.5) {\n    //         bins[1]++;\n    //     } else if (x[i + rank * m] >= 0.5 && x[i + rank * m] < 0.75) {\n    //         bins[2]++;\n    //     } else {\n    //         bins[3]++;\n    //     }\n    // }\n\n    // MPI_Reduce(bins.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // MPI_Bcast(bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // MPI_Bcast(bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n    // MPI_Bcast(bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n    // MPI_Bcast",
            "size_t const N = x.size();\n\n  std::vector<size_t> counts(4);\n\n  #pragma omp parallel\n  {\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    size_t const block_size = N / 4;\n    size_t const local_start = rank * block_size;\n    size_t const local_end = local_start + block_size;\n\n    size_t local_counts[4] = {0, 0, 0, 0};\n    for(size_t i = local_start; i < local_end; i++) {\n      if (x[i] < 0.25) {\n        local_counts[0]++;\n      } else if (x[i] < 0.5) {\n        local_counts[1]++;\n      } else if (x[i] < 0.75) {\n        local_counts[2]++;\n      } else {\n        local_counts[3]++;\n      }\n    }\n    #pragma omp atomic\n    counts[0] += local_counts[0];\n    #pragma omp atomic\n    counts[1] += local_counts[1];\n    #pragma omp atomic\n    counts[2] += local_counts[2];\n    #pragma omp atomic\n    counts[3] += local_counts[3];\n  }\n  if (rank == 0) {\n    for(int i = 0; i < 4; i++) {\n      bins[i] = counts[i];\n    }\n  }\n\n}",
            "// TODO\n}",
            "//TODO: YOUR CODE GOES HERE\n\n\n\t//TODO: END\n\n}",
            "// Compute the number of doubles in each quartile in the vector x.\n   // The result is stored in `bins` in the following format:\n   // [0] 0.0 <= x[i] < 0.25\n   // [1] 0.25 <= x[i] < 0.5\n   // [2] 0.5 <= x[i] < 0.75\n   // [3] 0.75 <= x[i] < 1.0\n   // You may use the following functions to get the index of the first element,\n   // the index of the last element, and the size of a range.\n   // size_t first(std::vector<double> const& v, double val);\n   // size_t last(std::vector<double> const& v, double val);\n   // size_t size(std::vector<double> const& v, size_t start, size_t end);\n}",
            "// YOUR CODE GOES HERE\n    int rank, nproc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    size_t len = x.size();\n    size_t count = len/nproc;\n    size_t rem = len%nproc;\n\n    int i, j, k, idx;\n\n    std::vector<double> newx;\n    if(rank == 0)\n        newx.resize(len);\n\n    // Sending vector to each rank\n    #pragma omp parallel for\n    for(i=0; i<len; i++)\n    {\n        idx = i%nproc;\n        if(rank == idx)\n            newx[i] = x[i];\n    }\n\n    // Reduce the vector to one rank\n    MPI_Reduce(MPI_IN_PLACE, newx.data(), len, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // Counting the quartiles\n    if(rank == 0)\n    {\n        for(i=0; i<len; i++)\n        {\n            j = (int) (x[i]*4);\n            bins[j]++;\n        }\n    }\n\n    // MPI_Finalize();\n}",
            "// TODO\n}",
            "auto size = x.size();\n    size_t nbins = 4;\n    size_t rank = 0;\n    size_t nprocs = 1;\n    int mpi_thread_support = 0;\n    int mpi_thread_init_error = 0;\n\n    MPI_Initialized(&mpi_thread_support);\n    if (!mpi_thread_support) {\n        mpi_thread_init_error = MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &mpi_thread_support);\n        if (mpi_thread_init_error!= MPI_SUCCESS) {\n            printf(\"MPI_Init_thread error\");\n            exit(EXIT_FAILURE);\n        }\n    }\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (nprocs > 1) {\n        size_t n = size/nprocs;\n        size_t rest = size%nprocs;\n        size_t start = 0;\n        std::vector<double> x_local;\n        std::array<size_t, 4> bins_local;\n        std::vector<double> x_local_tmp;\n        x_local.resize(n + (rank < rest? 1 : 0));\n        bins_local.fill(0);\n        if (rank < rest) {\n            x_local.resize(n + 1);\n        }\n        if (rank == 0) {\n            bins.fill(0);\n        }\n\n        MPI_Scatter(x.data(), n + rest, MPI_DOUBLE, x_local.data(), n + (rank < rest? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        #pragma omp parallel for\n        for (size_t i = 0; i < x_local.size(); i++) {\n            double x_i = x_local[i];\n            if (x_i > 0 && x_i < 0.25) {\n                bins_local[0]++;\n            }\n            else if (x_i >= 0.25 && x_i < 0.5) {\n                bins_local[1]++;\n            }\n            else if (x_i >= 0.5 && x_i < 0.75) {\n                bins_local[2]++;\n            }\n            else if (x_i >= 0.75 && x_i < 1) {\n                bins_local[3]++;\n            }\n        }\n        MPI_Gather(bins_local.data(), 4, MPI_INT, bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n    else {\n        std::array<size_t, 4> bins_local;\n        bins_local.fill(0);\n        #pragma omp parallel for\n        for (size_t i = 0; i < x.size(); i++) {\n            double x_i = x[i];\n            if (x_i > 0 && x_i < 0.25) {\n                bins_local[0]++;\n            }\n            else if (x_i >= 0.25 && x_i < 0.5) {\n                bins_local[1]++;\n            }\n            else if (x_i >= 0.5 && x_i < 0.75) {\n                bins_local[2]++;\n            }\n            else if (x_i >= 0.75 && x_i < 1) {\n                bins_local[3]++;\n            }\n        }\n        MPI_Gather(bins_local.data(), 4, MPI_INT, bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO\n}",
            "int rank = 0, size = 1;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t n = x.size()/size;\n    size_t rest = x.size()%size;\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::vector<double> temp_x(n + rest);\n\n            MPI_Recv(&temp_x[0], n + rest, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n            for (size_t j = 0; j < n + rest; j++) {\n                if (x[j] < 0.25) {\n                    bins[0]++;\n                } else if (x[j] < 0.5) {\n                    bins[1]++;\n                } else if (x[j] < 0.75) {\n                    bins[2]++;\n                } else {\n                    bins[3]++;\n                }\n            }\n        }\n    } else {\n        std::vector<double> temp_x(n + rest);\n        MPI_Send(&x[rank * n], n + rest, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "std::array<size_t, 4> local_bins{0, 0, 0, 0};\n\n\t// find the indices of the first and last elements in each bin\n\tauto f = [](double x) -> size_t { return x >= 0.75? 3 : x >= 0.5? 2 : x >= 0.25? 1 : 0; };\n\tauto f2 = [](size_t i) -> size_t { return i * 0.25; };\n\n\tsize_t n = x.size();\n\n\t// find the indices of the first and last elements in each bin\n\tsize_t lo = f(x[0]);\n\tsize_t hi = f(x[n - 1]);\n\n\t// do the computation on the local data\n\tfor (size_t i = 0; i < n; ++i)\n\t{\n\t\tif (i <= lo)\n\t\t\t++local_bins[f(x[i])];\n\t\telse if (i >= hi)\n\t\t\t++local_bins[f(x[i - 1])];\n\t\telse\n\t\t{\n\t\t\t++local_bins[f(x[i])];\n\t\t\t++local_bins[f(x[i - 1])];\n\t\t}\n\t}\n\n\t// gather the results to rank 0\n\tstd::array<size_t, 4> bins_all;\n\tMPI_Gather(local_bins.data(), 4, MPI_INT, bins_all.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n\t// compute the total sum of the bins\n\tsize_t total = 0;\n\tfor (size_t i = 0; i < 4; ++i)\n\t\ttotal += bins_all[i];\n\n\t// compute the local sum of the bins\n\tsize_t sum = 0;\n\tfor (size_t i = 0; i < 4; ++i)\n\t\tsum += local_bins[i];\n\n\t// get the rank number\n\tint rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t// compute the bin width\n\tdouble width = 1.0 / 4.0;\n\n\t// compute the offset for the bin\n\tdouble offset = rank * width;\n\n\t// compute the bin edges\n\tstd::array<double, 4> edges = { 0, 0.25, 0.5, 0.75 };\n\n\t// compute the bin indices\n\tstd::array<size_t, 4> indices = { 0, 0, 0, 0 };\n\n\tfor (size_t i = 0; i < 4; ++i)\n\t{\n\t\twhile (offset < edges[i])\n\t\t{\n\t\t\t++indices[i];\n\t\t\toffset += 1.0 / total;\n\t\t}\n\t}\n\n\t// compute the contribution of the local bins to the global bins\n\tfor (size_t i = 0; i < 4; ++i)\n\t\tbins[i] = int(sum * (indices[i + 1] - indices[i]) / total + 0.5);\n}",
            "// TODO: implement me\n\n}",
            "/* \n       TODO: fill in the function body to count the number of doubles in the vector x that have a fractional part \n       in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n       HINT: use MPI_Reduce and MPI_Allreduce and OpenMP directives\n    */\n    bins[0] = 0; bins[1] = 0; bins[2] = 0; bins[3] = 0;\n\n#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i)\n        if (x[i] - floor(x[i]) <= 0.25)\n            ++bins[0];\n        else if (x[i] - floor(x[i]) > 0.25 && x[i] - floor(x[i]) <= 0.5)\n            ++bins[1];\n        else if (x[i] - floor(x[i]) > 0.5 && x[i] - floor(x[i]) <= 0.75)\n            ++bins[2];\n        else\n            ++bins[3];\n\n    MPI_Reduce(MPI_IN_PLACE, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n    //\n    // Parallelize using MPI and OpenMP.\n    //\n    // In particular:\n    // 1. Assign the number of elements to process to each MPI rank.\n    // 2. Split the input vector x into four subvectors.\n    //    (you can use x.data() + rank * size / nranks to get the address\n    //     of the first element in the rank-th subvector. The subvector\n    //     has size = size / nranks)\n    // 3. Use OpenMP to compute the quartile counts for each subvector.\n    // 4. Gather the counts into a single vector on rank 0.\n    // 5. Sort the vector on rank 0.\n\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Number of elements per rank\n    auto size = x.size();\n\n    // Number of elements per rank\n    size_t size_per_rank = size / nranks;\n\n    // Vector to store quartile counts on rank 0\n    std::vector<size_t> result;\n\n    // Assign the number of elements to process to each MPI rank\n    // TODO\n    //\n\n\n    // Split the input vector x into four subvectors\n    // TODO\n    //\n\n    // Use OpenMP to compute the quartile counts for each subvector\n    // TODO\n    //\n\n    // Gather the counts into a single vector on rank 0\n    // TODO\n    //\n\n    // Sort the vector on rank 0\n    // TODO\n    //\n\n    bins = {0, 0, 0, 0};\n}",
            "int my_rank;\n  int num_ranks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n  // Compute the length of the vector x owned by each rank\n  int my_length = x.size() / num_ranks;\n\n  // Get the index of the first element of x owned by this rank\n  int first_index = my_rank * my_length;\n\n  // Compute the index of the last element of x owned by this rank\n  int last_index = first_index + my_length - 1;\n\n  // Initialize the number of elements of x that fall into each bin\n  bins = {0, 0, 0, 0};\n\n  // Compute the number of elements of x that fall into each bin\n  #pragma omp parallel for\n  for (int i = first_index; i <= last_index; i++) {\n    double frac = x[i] - (int) x[i];\n\n    // Use modulus to find the bin\n    if (frac < 0.25) {\n      bins[0]++;\n    }\n    else if (frac >= 0.25 && frac < 0.5) {\n      bins[1]++;\n    }\n    else if (frac >= 0.5 && frac < 0.75) {\n      bins[2]++;\n    }\n    else if (frac >= 0.75) {\n      bins[3]++;\n    }\n  }\n}",
            "int n_bins = 4;\n\n    #pragma omp parallel\n    {\n        // allocate private arrays\n        std::array<size_t, 4> bins_private;\n        std::array<size_t, 4> bins_private_final;\n\n        // use private bins\n        #pragma omp for\n        for (int i = 0; i < n_bins; i++) {\n            bins_private[i] = 0;\n        }\n\n        // count in private bins\n        #pragma omp for\n        for (int i = 0; i < x.size(); i++) {\n            int bin = floor(x[i] * 4.0);\n            bins_private[bin]++;\n        }\n\n        // add up private bins to get global counts\n        #pragma omp critical\n        for (int i = 0; i < n_bins; i++) {\n            bins_private_final[i] = 0;\n        }\n        #pragma omp critical\n        for (int i = 0; i < n_bins; i++) {\n            bins_private_final[i] += bins_private[i];\n        }\n\n        // store global bins\n        #pragma omp critical\n        for (int i = 0; i < n_bins; i++) {\n            bins[i] += bins_private_final[i];\n        }\n    }\n\n}",
            "int nbins = bins.size();\n\n   // TODO: Your code here\n\n}",
            "size_t size = x.size();\n    size_t n = 4;\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int rank;\n    MPI_Comm_rank(comm, &rank);\n    double start = omp_get_wtime();\n    if (rank == 0) {\n        for (size_t i = 0; i < size; i++) {\n            if (x[i] < 0.25) {\n                bins[0]++;\n            } else if (x[i] < 0.5) {\n                bins[1]++;\n            } else if (x[i] < 0.75) {\n                bins[2]++;\n            } else if (x[i] < 1) {\n                bins[3]++;\n            }\n        }\n    } else {\n        std::vector<double> v;\n        MPI_Bcast(v.data(), size, MPI_DOUBLE, 0, comm);\n        for (size_t i = 0; i < size; i++) {\n            if (v[i] < 0.25) {\n                bins[0]++;\n            } else if (v[i] < 0.5) {\n                bins[1]++;\n            } else if (v[i] < 0.75) {\n                bins[2]++;\n            } else if (v[i] < 1) {\n                bins[3]++;\n            }\n        }\n    }\n    double end = omp_get_wtime();\n    if (rank == 0) {\n        printf(\"Rank 0 time: %f\\n\", end - start);\n    }\n}",
            "// TODO: Your code here\n\n}",
            "size_t size = x.size();\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  int rank = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  MPI_Datatype dt_double = MPI_DOUBLE;\n  MPI_Datatype dt_size_t = MPI_UNSIGNED_LONG_LONG;\n  MPI_Datatype dt_array_size_t = MPI_UNSIGNED_LONG_LONG;\n\n  double min_x = *std::min_element(x.begin(), x.end());\n  double max_x = *std::max_element(x.begin(), x.end());\n\n  double dx = (max_x - min_x) / 4;\n\n  double lower_x = min_x;\n  double upper_x = lower_x + dx;\n\n  std::array<size_t, 4> bins_local = {0, 0, 0, 0};\n\n  int num_threads = omp_get_max_threads();\n  int thread_id = omp_get_thread_num();\n\n  int chunk_size = size / num_threads;\n\n  int last_chunk_size = chunk_size + size % num_threads;\n\n  int chunk_start = thread_id * chunk_size;\n\n  if (thread_id == num_threads - 1)\n    chunk_start += last_chunk_size;\n\n  int chunk_end = chunk_start + chunk_size;\n\n  for (int i = chunk_start; i < chunk_end; ++i) {\n\n    if (x[i] < lower_x)\n      bins_local[0]++;\n    else if (x[i] < upper_x)\n      bins_local[1]++;\n    else if (x[i] < upper_x + dx)\n      bins_local[2]++;\n    else if (x[i] < max_x)\n      bins_local[3]++;\n    else\n      bins_local[0]++;\n\n  }\n\n  std::array<size_t, 4> bins_partial = bins_local;\n\n  MPI_Allreduce(&bins_partial, &bins, 4, dt_size_t, MPI_SUM, MPI_COMM_WORLD);\n\n}",
            "size_t const n = x.size();\n  std::vector<double> myX(n);\n  std::copy(x.begin(), x.end(), myX.begin());\n  std::array<size_t, 4> localBins;\n  std::array<size_t, 4> globalBins = {{0,0,0,0}};\n  MPI_Request req;\n  MPI_Iallreduce(&localBins, &globalBins, 1, MPI_ARRAY_INT, MPI_SUM, MPI_COMM_WORLD, &req);\n  MPI_Wait(&req, MPI_STATUS_IGNORE);\n\n  double size = (double)n;\n\n#pragma omp parallel for\n  for (int i = 0; i < 4; i++) {\n    size_t lower = (int)(i * (size/4));\n    size_t upper = (int)((i+1)*(size/4));\n    localBins[i] = (size_t)(std::count_if(myX.begin()+lower, myX.begin()+upper, [](double x){return x >= lower && x < upper;}));\n  }\n\n  bins = globalBins;\n}",
            "// TODO: add your solution here\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] >= 0.75) {\n      bins[3]++;\n    }\n    else if (x[i] >= 0.50) {\n      bins[2]++;\n    }\n    else if (x[i] >= 0.25) {\n      bins[1]++;\n    }\n    else {\n      bins[0]++;\n    }\n  }\n}",
            "#ifdef HAS_MPI\n    int rank, num_procs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n    std::array<size_t, 4> bins_partial;\n\n    // Get the number of doubles in each quartile.\n    #pragma omp parallel for\n    for (int i=0; i<x.size(); i++) {\n      if (x[i] <= 0.25) {\n        bins_partial[0]++;\n      }\n      else if (x[i] <= 0.5) {\n        bins_partial[1]++;\n      }\n      else if (x[i] <= 0.75) {\n        bins_partial[2]++;\n      }\n      else {\n        bins_partial[3]++;\n      }\n    }\n\n    // Add the partial counts.\n    std::vector<size_t> bins_local(4, 0);\n    bins_local[0] = bins_partial[0];\n    bins_local[1] = bins_partial[1];\n    bins_local[2] = bins_partial[2];\n    bins_local[3] = bins_partial[3];\n\n    std::vector<size_t> bins_global(4, 0);\n    if (rank == 0) {\n      for (int i=0; i<num_procs; i++) {\n        MPI_Recv(&bins_global[0], 4, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n      for (int i=0; i<4; i++) {\n        bins[i] = bins_local[i] + bins_global[i];\n      }\n    }\n    else {\n      MPI_Send(&bins_local[0], 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n  #else\n    #pragma omp parallel for\n    for (int i=0; i<x.size(); i++) {\n      if (x[i] <= 0.25) {\n        bins[0]++;\n      }\n      else if (x[i] <= 0.5) {\n        bins[1]++;\n      }\n      else if (x[i] <= 0.75) {\n        bins[2]++;\n      }\n      else {\n        bins[3]++;\n      }\n    }\n  #endif\n\n}",
            "// TODO: compute the bins in parallel\n\n\tbins[0] = 0;\n\tbins[1] = 0;\n\tbins[2] = 0;\n\tbins[3] = 0;\n\n\tint rank, numprocs;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\n\tint binCount = 0;\n\tfor (auto it = x.begin(); it!= x.end(); ++it) {\n\t\tif ((*it) < 0.25) {\n\t\t\tbinCount++;\n\t\t} else if ((*it) >= 0.25 && (*it) < 0.5) {\n\t\t\tbinCount++;\n\t\t} else if ((*it) >= 0.5 && (*it) < 0.75) {\n\t\t\tbinCount++;\n\t\t} else if ((*it) >= 0.75 && (*it) < 1.0) {\n\t\t\tbinCount++;\n\t\t}\n\t}\n\tMPI_Reduce(&binCount, &bins[0], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tbinCount = 0;\n\tfor (auto it = x.begin(); it!= x.end(); ++it) {\n\t\tif ((*it) >= 0.25 && (*it) < 0.5) {\n\t\t\tbinCount++;\n\t\t} else if ((*it) >= 0.5 && (*it) < 0.75) {\n\t\t\tbinCount++;\n\t\t} else if ((*it) >= 0.75 && (*it) < 1.0) {\n\t\t\tbinCount++;\n\t\t} else if ((*it) >= 1.0) {\n\t\t\tbinCount++;\n\t\t}\n\t}\n\tMPI_Reduce(&binCount, &bins[1], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tbinCount = 0;\n\tfor (auto it = x.begin(); it!= x.end(); ++it) {\n\t\tif ((*it) >= 0.5 && (*it) < 0.75) {\n\t\t\tbinCount++;\n\t\t} else if ((*it) >= 0.75 && (*it) < 1.0) {\n\t\t\tbinCount++;\n\t\t} else if ((*it) >= 1.0) {\n\t\t\tbinCount++;\n\t\t}\n\t}\n\tMPI_Reduce(&binCount, &bins[2], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tbinCount = 0;\n\tfor (auto it = x.begin(); it!= x.end(); ++it) {\n\t\tif ((*it) >= 0.75 && (*it) < 1.0) {\n\t\t\tbinCount++;\n\t\t} else if ((*it) >= 1.0) {\n\t\t\tbinCount++;\n\t\t}\n\t}\n\tMPI_Reduce(&binCount, &bins[3], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\t// TODO: use OpenMP to parallelize the for loops\n\t#pragma omp parallel for\n\tfor (auto it = x.begin(); it!= x.end(); ++it) {\n\t\tif ((*it) < 0.25) {\n\t\t\tbins[0]++;\n\t\t} else if ((*it) >= 0.25 && (*it) < 0.5) {\n\t\t\tbins[1]++;\n\t\t} else if ((*it) >= 0.5 && (*it) < 0.75) {\n\t\t\tbins[2]++;\n\t\t} else if ((*it) >= 0.75 && (*it) < 1.0) {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n\n\t// TODO: use OpenMP",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int chunkSize = x.size()/size;\n    int remainder = x.size()%size;\n\n    std::vector<double> x_rank(chunkSize+remainder);\n\n    for(int i=0;i<remainder;i++)\n        x_rank[i] = x[i+rank*chunkSize];\n\n    if(rank!=size-1)\n        MPI_Send(&x_rank[remainder], chunkSize, MPI_DOUBLE, rank+1, 0, MPI_COMM_WORLD);\n\n    std::vector<double> x_temp(chunkSize);\n\n    for(int i=0;i<size-1;i++){\n        MPI_Recv(&x_temp, chunkSize, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        for(int j=0;j<chunkSize;j++)\n            x_rank[j+remainder+i*chunkSize] = x_temp[j];\n    }\n\n    //std::cout << \"Vector with complete data on rank \" << rank << std::endl;\n    //for(int i=0;i<x_rank.size();i++)\n    //    std::cout << x_rank[i] << \" \";\n    //std::cout << std::endl;\n\n    size_t count1 = 0;\n    size_t count2 = 0;\n    size_t count3 = 0;\n    size_t count4 = 0;\n\n    double low, high;\n\n    for(int i=0;i<x_rank.size();i++){\n        if(x_rank[i]%1==0)\n            continue;\n        else if(x_rank[i]%1<0.25)\n            count1++;\n        else if(x_rank[i]%1<0.50)\n            count2++;\n        else if(x_rank[i]%1<0.75)\n            count3++;\n        else\n            count4++;\n    }\n\n    std::array<size_t, 4> bins_temp;\n    bins_temp[0] = count1;\n    bins_temp[1] = count2;\n    bins_temp[2] = count3;\n    bins_temp[3] = count4;\n\n    #pragma omp parallel shared(x_rank, bins_temp, bins)\n    {\n        #pragma omp for\n        for(int i=0;i<x_rank.size();i++){\n            if(x_rank[i]%1==0)\n                continue;\n            else if(x_rank[i]%1<0.25)\n                count1++;\n            else if(x_rank[i]%1<0.50)\n                count2++;\n            else if(x_rank[i]%1<0.75)\n                count3++;\n            else\n                count4++;\n        }\n\n        #pragma omp single\n        {\n            if(rank==0){\n                bins[0] = bins_temp[0];\n                bins[1] = bins_temp[1];\n                bins[2] = bins_temp[2];\n                bins[3] = bins_temp[3];\n            }\n        }\n    }\n\n}",
            "int mpi_size, mpi_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n   if(x.size()!= mpi_size) {\n      std::cout << \"ERROR: countQuartiles: The size of the input vector x does not match the MPI_COMM_SIZE.\" << std::endl;\n      MPI_Abort(MPI_COMM_WORLD, 1);\n   }\n   auto x_count = x.size();\n   if(x_count < 1) {\n      std::cout << \"ERROR: countQuartiles: The input vector x should contain at least 1 element.\" << std::endl;\n      MPI_Abort(MPI_COMM_WORLD, 1);\n   }\n   std::vector<double> temp_bin(x_count);\n\n#pragma omp parallel for\n   for(size_t i = 0; i < x_count; i++) {\n      temp_bin[i] = x[i];\n   }\n\n   std::vector<double> temp_bin_sorted(x_count);\n   std::copy(temp_bin.begin(), temp_bin.end(), temp_bin_sorted.begin());\n\n   std::sort(temp_bin_sorted.begin(), temp_bin_sorted.end());\n\n   bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n\n#pragma omp parallel for reduction(+:bins[0], bins[1], bins[2], bins[3])\n   for(size_t i = 0; i < x_count; i++) {\n      if(temp_bin_sorted[i] < 0.25) {\n         bins[0] += 1;\n      } else if(temp_bin_sorted[i] >= 0.25 && temp_bin_sorted[i] < 0.5) {\n         bins[1] += 1;\n      } else if(temp_bin_sorted[i] >= 0.5 && temp_bin_sorted[i] < 0.75) {\n         bins[2] += 1;\n      } else if(temp_bin_sorted[i] >= 0.75) {\n         bins[3] += 1;\n      }\n   }\n\n   MPI_Allreduce(MPI_IN_PLACE, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n   std::cout << \"Count of bins for x: \" << x_count << \": \" << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << std::endl;\n\n}",
            "int rank = 0;\n    int world_size = 1;\n    int my_rank = 0;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    int local_size = x.size();\n    int local_counts[4] = {0,0,0,0};\n\n    std::vector<double> local_x(x.begin(), x.end());\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < local_size; i++) {\n        if (local_x[i] < 0.25) {\n            local_counts[0]++;\n        } else if (local_x[i] >= 0.25 && local_x[i] < 0.5) {\n            local_counts[1]++;\n        } else if (local_x[i] >= 0.5 && local_x[i] < 0.75) {\n            local_counts[2]++;\n        } else if (local_x[i] >= 0.75 && local_x[i] <= 1) {\n            local_counts[3]++;\n        }\n    }\n\n    MPI_Allreduce(&local_counts, &bins, 4, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n}",
            "int rank = 0, commsize = 1;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &commsize);\n\n\tsize_t xsize = x.size(), local_size = xsize / commsize, remainder = xsize % commsize;\n\n\t// if we are not rank 0, send to rank 0\n\tif (rank!= 0) {\n\t\tMPI_Send(&x[rank * local_size], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n\t}\n\n\t// if we are rank 0, receive, sort, and compute\n\tif (rank == 0) {\n\t\tstd::vector<double> sortedx(x.begin(), x.end());\n\t\tstd::sort(sortedx.begin(), sortedx.end());\n\n\t\tbins[0] = 0;\n\t\tbins[1] = 0;\n\t\tbins[2] = 0;\n\t\tbins[3] = 0;\n\n\t\tsize_t i;\n\t\t#pragma omp parallel for\n\t\tfor (i = 0; i < sortedx.size(); i++) {\n\t\t\tif (sortedx[i] < 0.25) {\n\t\t\t\tbins[0]++;\n\t\t\t} else if (sortedx[i] < 0.5) {\n\t\t\t\tbins[1]++;\n\t\t\t} else if (sortedx[i] < 0.75) {\n\t\t\t\tbins[2]++;\n\t\t\t} else {\n\t\t\t\tbins[3]++;\n\t\t\t}\n\t\t}\n\n\t\t// send back to all ranks\n\t\t#pragma omp parallel for\n\t\tfor (i = 0; i < commsize; i++) {\n\t\t\tMPI_Send(&bins[0], 4, MPI_INT, i, 0, MPI_COMM_WORLD);\n\t\t}\n\t}\n\n\t// if we are not rank 0, receive from rank 0 and copy the results to the local vector\n\tif (rank!= 0) {\n\t\tMPI_Recv(&bins[0], 4, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t}\n}",
            "// TODO: Your code here\n}",
            "if (x.size() == 0) {\n        return;\n    }\n    double max = x[0];\n    double min = x[0];\n\n    for (double const& d : x) {\n        if (d > max) {\n            max = d;\n        }\n        if (d < min) {\n            min = d;\n        }\n    }\n\n    double step = (max - min) / 4;\n    for (int i = 0; i < 4; i++) {\n        bins[i] = 0;\n    }\n    for (double const& d : x) {\n        if (d <= min + 0.25 * step) {\n            bins[0]++;\n        } else if (d <= min + 0.5 * step) {\n            bins[1]++;\n        } else if (d <= min + 0.75 * step) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n\n}",
            "}",
            "// your code here\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int chunk = x.size() / size;\n  int rem = x.size() % size;\n\n  std::vector<std::array<size_t, 4>> tmp(chunk);\n  MPI_Gather(&x[0], chunk, MPI_DOUBLE, &tmp[0], chunk, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    size_t i = 0;\n    for (size_t p = 0; p < size; ++p) {\n      if (p < rem) {\n        i += chunk + 1;\n      } else {\n        i += chunk;\n      }\n      bins[0] += tmp[p][0];\n      bins[1] += tmp[p][1];\n      bins[2] += tmp[p][2];\n      bins[3] += tmp[p][3];\n    }\n  }\n}",
            "int rank, nprocs;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\tbins.fill(0);\n\tstd::vector<double> x_sub;\n\tx_sub.resize(x.size()/nprocs);\n\tfor (size_t i=0; i<x.size(); i++) {\n\t\tx_sub[i%(x.size()/nprocs)] += x[i];\n\t}\n\n\t#pragma omp parallel for\n\tfor (size_t i=0; i<x_sub.size(); i++) {\n\t\tint rank_x = i%nprocs;\n\t\tif (x_sub[i]<0.25) bins[0]++;\n\t\telse if (x_sub[i]<0.5) bins[1]++;\n\t\telse if (x_sub[i]<0.75) bins[2]++;\n\t\telse if (x_sub[i]<1.0) bins[3]++;\n\t}\n\tMPI_Reduce(MPI_IN_PLACE, bins.data(), 4, MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int num_procs, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int num_rows = x.size();\n  int rows_per_proc = num_rows / num_procs;\n  int remainder = num_rows % num_procs;\n  int start_row, end_row;\n  if (rank == 0) {\n    start_row = 0;\n    end_row = rows_per_proc;\n  }\n  else {\n    start_row = rank * rows_per_proc + remainder;\n    end_row = start_row + rows_per_proc;\n  }\n\n  if (start_row!= end_row) {\n    bins = std::array<size_t, 4>{0, 0, 0, 0};\n    for (int i = start_row; i < end_row; i++) {\n      if (x[i] >= 0 && x[i] < 0.25)\n        bins[0]++;\n      else if (x[i] >= 0.25 && x[i] < 0.5)\n        bins[1]++;\n      else if (x[i] >= 0.5 && x[i] < 0.75)\n        bins[2]++;\n      else if (x[i] >= 0.75 && x[i] < 1)\n        bins[3]++;\n    }\n  }\n  else {\n    bins = std::array<size_t, 4>{0, 0, 0, 0};\n  }\n\n  std::array<size_t, 4> bins_temp = bins;\n  MPI_Reduce(bins_temp.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int comm_sz = 0;\n    MPI_Comm_size(comm, &comm_sz);\n    int comm_rank = 0;\n    MPI_Comm_rank(comm, &comm_rank);\n\n    std::array<size_t, 4> local_bins;\n    // Fill local_bins with 0's\n    std::fill(local_bins.begin(), local_bins.end(), 0);\n\n    // Fill local_bins with values from x\n    for (int i = 0; i < x.size(); i++) {\n        double value = x[i];\n        if (value >= 0.75) {\n            local_bins[3]++;\n        } else if (value >= 0.5) {\n            local_bins[2]++;\n        } else if (value >= 0.25) {\n            local_bins[1]++;\n        } else {\n            local_bins[0]++;\n        }\n    }\n\n    size_t global_bins[4] = {0};\n    MPI_Allreduce(local_bins.data(), global_bins, 4, MPI_INT, MPI_SUM, comm);\n\n    // Store the results into bins\n    bins = global_bins;\n}",
            "bins.fill(0);\n  const size_t n = x.size();\n\n  std::vector<int> bins_local(4);\n  double p = 0;\n  for (size_t i = 0; i < n; ++i) {\n    p = x[i] % 1;\n    if (p < 0.25) {\n      ++bins_local[0];\n    } else if (p < 0.5) {\n      ++bins_local[1];\n    } else if (p < 0.75) {\n      ++bins_local[2];\n    } else {\n      ++bins_local[3];\n    }\n  }\n\n  // MPI\n  std::vector<int> bins_global(4);\n  MPI_Reduce(bins_local.data(), bins_global.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (mpi_rank == 0) {\n    bins = bins_global;\n  }\n\n  // OpenMP\n  int sum_local = 0;\n  #pragma omp parallel for reduction(+: sum_local)\n  for (size_t i = 0; i < 4; ++i) {\n    sum_local += bins_local[i];\n  }\n\n  int sum_global = 0;\n  #pragma omp parallel for reduction(+: sum_global)\n  for (size_t i = 0; i < 4; ++i) {\n    sum_global += bins[i];\n  }\n\n  printf(\"Rank %d: sum_local = %d, sum_global = %d\\n\", mpi_rank, sum_local, sum_global);\n}",
            "size_t local_size = x.size();\n\n\t// Check that we have 4 evenly divisible intervals\n\tassert(local_size % 4 == 0);\n\n\t// Calculate the size of each local interval\n\tsize_t local_interval_size = local_size / 4;\n\n\t// Allocate the array to store the counts in\n\tbins = {};\n\n#pragma omp parallel for\n\tfor (int i = 0; i < 4; i++) {\n\n\t\t// Local copy of the interval number\n\t\tint interval_no = i;\n\n\t\t// Get the interval start and end\n\t\tint interval_start = interval_no * local_interval_size;\n\t\tint interval_end = (interval_no + 1) * local_interval_size;\n\n\t\t// Initialize the count to 0\n\t\tsize_t count = 0;\n\n\t\tfor (int j = interval_start; j < interval_end; j++) {\n\n\t\t\t// Check if the fractional part is in the appropriate interval\n\t\t\tif (x[j] - (int)x[j] < 0.25)\n\t\t\t\tcount++;\n\n\t\t}\n\n\t\t// Store the count\n\t\tbins[i] = count;\n\n\t}\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int quotient, remainder;\n    size_t bin;\n    double a, b;\n    // int xsize = x.size();\n\n    // for (int i = 0; i < xsize; i++) {\n    //     a = x[i] * 4;\n    //     quotient = (int)a;\n    //     remainder = a - quotient;\n    //     bin = (int)remainder * 4;\n    //     if (bin == 0 && remainder > 0.25) {\n    //         bin++;\n    //     }\n    //     else if (bin == 1 && remainder < 0.5) {\n    //         bin--;\n    //     }\n    //     else if (bin == 2 && remainder < 0.75) {\n    //         bin--;\n    //     }\n    //     else if (bin == 3 && remainder > 0.75) {\n    //         bin--;\n    //     }\n    //     bins[bin]++;\n    // }\n    // if (rank == 0) {\n    //     std::cout << std::setprecision(1);\n    //     std::cout << \"rank \" << rank << \" has: \" << std::endl;\n    //     for (int i = 0; i < 4; i++) {\n    //         std::cout << \"bin \" << i << \": \" << bins[i] << std::endl;\n    //     }\n    // }\n\n    for (int i = 0; i < x.size(); i++) {\n        a = x[i];\n        b = a * 4;\n        quotient = (int)b;\n        remainder = b - quotient;\n        bin = (int)remainder * 4;\n        if (bin == 0 && remainder > 0.25) {\n            bin++;\n        }\n        else if (bin == 1 && remainder < 0.5) {\n            bin--;\n        }\n        else if (bin == 2 && remainder < 0.75) {\n            bin--;\n        }\n        else if (bin == 3 && remainder > 0.75) {\n            bin--;\n        }\n        bins[bin]++;\n    }\n\n    if (rank == 0) {\n        std::cout << std::setprecision(1);\n        std::cout << \"rank \" << rank << \" has: \" << std::endl;\n        for (int i = 0; i < 4; i++) {\n            std::cout << \"bin \" << i << \": \" << bins[i] << std::endl;\n        }\n    }\n}",
            "}",
            "// TODO: fill in the code\n}",
            "//...\n\n}",
            "// TODO\n\n  const int world_rank = 0;\n  const int world_size = 1;\n\n  int my_rank = 0;\n  int my_size = 1;\n\n  int bins_size = 4;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &my_size);\n\n  std::vector<double> local_x(x.begin() + my_rank * (x.size()/my_size), x.begin() + (my_rank+1) * (x.size()/my_size));\n\n  int size = static_cast<int>(local_x.size());\n\n  int q1 = static_cast<int>(size/4);\n  int q2 = static_cast<int>(size/2);\n  int q3 = static_cast<int>(3*(size/4));\n  int q4 = static_cast<int>(size);\n\n  int count1 = 0;\n  int count2 = 0;\n  int count3 = 0;\n  int count4 = 0;\n\n  for (int i = 0; i < size; i++) {\n    double temp = local_x[i];\n    if (temp >= 0 && temp <= 0.25) {\n      count1++;\n    } else if (temp > 0.25 && temp <= 0.5) {\n      count2++;\n    } else if (temp > 0.5 && temp <= 0.75) {\n      count3++;\n    } else if (temp > 0.75 && temp <= 1) {\n      count4++;\n    }\n  }\n\n  // calculate the sum of the local counts\n  int sum1 = 0, sum2 = 0, sum3 = 0, sum4 = 0;\n\n#pragma omp parallel\n  {\n    #pragma omp single\n    {\n    #pragma omp task\n    {\n      sum1 += count1;\n    }\n    #pragma omp task\n    {\n      sum2 += count2;\n    }\n    #pragma omp task\n    {\n      sum3 += count3;\n    }\n    #pragma omp task\n    {\n      sum4 += count4;\n    }\n    }\n  }\n\n  bins[0] = sum1;\n  bins[1] = sum2;\n  bins[2] = sum3;\n  bins[3] = sum4;\n\n  // calculate the sum of the local counts\n  int sum = 0;\n  int max_val = 0;\n\n  for (int i = 0; i < 4; i++) {\n    max_val = max_val > bins[i]? max_val : bins[i];\n  }\n\n  for (int i = 0; i < 4; i++) {\n    sum += bins[i];\n  }\n\n  // determine the local binsize and the global binsize\n  int local_binsize = static_cast<int>(max_val)/my_size;\n  int binsize = static_cast<int>(max_val)/world_size;\n\n  // calculate the sums of all bins\n  std::vector<int> temp(bins.size(), 0);\n  int sum_bins = sum;\n\n  MPI_Allgather(&sum, 1, MPI_INT, &temp[0], 1, MPI_INT, MPI_COMM_WORLD);\n  for (int i = 0; i < 4; i++) {\n    sum_bins += temp[i];\n  }\n\n  // calculate the offsets\n  std::vector<int> offsets(world_size);\n  offsets[0] = sum_bins - temp[0];\n  for (int i = 1; i < world_size; i++) {\n    offsets[i] = offsets[i-1] + temp[i];\n  }\n\n  for (int i = 0; i < world_size; i++) {\n    for (int j = 0; j < 4; j++) {\n      int temp = (offsets[i] +",
            "// compute the local counts in the local partition\n    std::array<size_t, 4> local_bins = {};\n    for (double v : x) {\n        int i = v * 4;\n        if (i > 1 && i < 4) {\n            local_bins[i - 1]++;\n        }\n    }\n\n    // collect the counts from all ranks\n    MPI_Allreduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    // fill in the remaining values from the last rank\n    if (rank == 0) {\n        if (bins[0] < 4) {\n            bins[0] += local_bins[0];\n        }\n        if (bins[1] < 4) {\n            bins[1] += local_bins[1];\n        }\n        if (bins[2] < 4) {\n            bins[2] += local_bins[2];\n        }\n        if (bins[3] < 4) {\n            bins[3] += local_bins[3];\n        }\n    }\n\n}",
            "// Your code goes here\n\n    // MPI\n    int rank;\n    int size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // OpenMP\n    int nthreads;\n    #pragma omp parallel\n    {\n        #pragma omp master\n        {\n            nthreads = omp_get_num_threads();\n        }\n    }\n\n    // Preprocessing\n    double lower = 0.0;\n    double upper = 1.0;\n    double range = upper - lower;\n    int num_bins = 4;\n    int bin_size = nthreads * x.size() / num_bins;\n    std::vector<double> bins_hist(num_bins);\n    std::fill(bins_hist.begin(), bins_hist.end(), 0.0);\n\n    // Processing\n    for (int i = 0; i < x.size(); i++) {\n        int bin = static_cast<int>(x[i] / range * num_bins);\n        bins_hist[bin] += 1.0;\n    }\n\n    // Reduction\n    for (int i = 0; i < num_bins; i++) {\n        MPI_Allreduce(&bins_hist[i], &bins[i], 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    }\n\n    // Postprocessing\n    bins[0] = static_cast<size_t>(bins[0] / bin_size);\n    bins[1] = static_cast<size_t>(bins[1] / bin_size);\n    bins[2] = static_cast<size_t>(bins[2] / bin_size);\n    bins[3] = static_cast<size_t>(bins[3] / bin_size);\n\n    if (rank == 0) {\n        std::cout << \"bins = { \" << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << \" }\" << std::endl;\n    }\n}",
            "assert(x.size() > 0);\n    //TODO: replace assert with mpi error\n    int num_procs = omp_get_num_procs();\n    int rank = omp_get_thread_num();\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int world_size;\n    MPI_Comm_size(comm, &world_size);\n    //TODO: replace assert with mpi error\n    assert(num_procs == world_size);\n    //TODO: replace assert with mpi error\n    assert(num_procs > 0);\n    if (rank == 0) {\n        //TODO: replace assert with mpi error\n        assert(bins.size() == 4);\n    }\n    size_t const num_of_elem = x.size();\n    size_t count = 0;\n    std::vector<double> tmp;\n    //TODO: parallelize\n    for (auto const& elem : x) {\n        if (elem > 0.75) {\n            count++;\n        }\n    }\n    tmp.resize(num_of_elem);\n    //TODO: parallelize\n    for (size_t i = 0; i < num_of_elem; ++i) {\n        if (x[i] > 0.75) {\n            tmp[i] = 1;\n        }\n        else {\n            tmp[i] = 0;\n        }\n    }\n    //TODO: parallelize\n    MPI_Reduce(&tmp[0], &bins[0], num_of_elem, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, comm);\n\n}",
            "int my_rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n    size_t my_size = x.size();\n\n    std::vector<double> x_sorted(my_size);\n    for (size_t i = 0; i < my_size; i++) {\n        x_sorted[i] = x[i];\n    }\n    std::sort(x_sorted.begin(), x_sorted.end());\n\n    MPI_Request request;\n    MPI_Status status;\n\n    double my_start = x_sorted[0];\n    double my_end = x_sorted[my_size - 1];\n\n    if (my_rank == 0) {\n        bins[0] = 0;\n        bins[1] = 0;\n        bins[2] = 0;\n        bins[3] = 0;\n    }\n\n    for (size_t i = 0; i < my_size; i++) {\n        if (x_sorted[i] >= my_start && x_sorted[i] < my_start + 0.25 * (my_end - my_start)) {\n            bins[0]++;\n        }\n        if (x_sorted[i] >= my_start + 0.25 * (my_end - my_start) && x_sorted[i] < my_start + 0.5 * (my_end - my_start)) {\n            bins[1]++;\n        }\n        if (x_sorted[i] >= my_start + 0.5 * (my_end - my_start) && x_sorted[i] < my_start + 0.75 * (my_end - my_start)) {\n            bins[2]++;\n        }\n        if (x_sorted[i] >= my_start + 0.75 * (my_end - my_start) && x_sorted[i] <= my_end) {\n            bins[3]++;\n        }\n    }\n\n    // Gather the number of doubles from each rank.\n    size_t bins_sum[4] = { 0, 0, 0, 0 };\n\n    if (my_rank == 0) {\n        MPI_Request request[4];\n        MPI_Status status[4];\n        for (int i = 1; i < 4; i++) {\n            MPI_Irecv(bins_sum + i, 1, MPI_UNSIGNED_LONG_LONG, i, i, MPI_COMM_WORLD, &request[i]);\n        }\n        for (int i = 1; i < 4; i++) {\n            MPI_Wait(&request[i], &status[i]);\n        }\n        for (int i = 1; i < 4; i++) {\n            bins[i] += bins_sum[i];\n        }\n    } else {\n        MPI_Request request;\n        MPI_Status status;\n        MPI_Send(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  double dx = 1.0 / size;\n  double p = dx;\n  size_t Nx = x.size();\n\n  // count the bins on this rank\n  for (size_t i = 0; i < Nx; i++) {\n    int idx = int(x[i] / p);\n    idx = std::min(idx, 3);\n    idx = std::max(idx, 0);\n    bins[idx]++;\n  }\n\n  // reduce the counts on the ranks\n  std::vector<size_t> bins_sum(4, 0);\n  MPI_Allreduce(bins.data(), bins_sum.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n  bins = bins_sum;\n\n#ifdef _OPENMP\n  // divide the bin count equally among the threads on this rank\n  int num_threads = omp_get_max_threads();\n  std::vector<size_t> bins_thread(num_threads, 0);\n  omp_set_num_threads(num_threads);\n  #pragma omp parallel\n  {\n    int thread_id = omp_get_thread_num();\n    size_t block_size = bins[0] / num_threads;\n    size_t extra_size = bins[0] - block_size * num_threads;\n    if (thread_id < extra_size) {\n      bins_thread[thread_id] = block_size + 1;\n    } else {\n      bins_thread[thread_id] = block_size;\n    }\n  }\n\n  // sum the bin counts on the threads\n  MPI_Allreduce(bins_thread.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n#endif\n}",
            "// TODO\n}",
            "int mpi_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n  // initialize the bins on rank 0\n  if (mpi_rank == 0) {\n    bins = {0, 0, 0, 0};\n  }\n  int mpi_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\n  int rank_num = x.size() / mpi_size;\n  int left = rank_num * mpi_rank;\n  int right = std::min(x.size() - 1, left + rank_num - 1);\n\n  double *temp = new double[rank_num];\n  int i;\n  int bin;\n\n#pragma omp parallel for private(i, bin) shared(left, right, x, rank_num, temp)\n  for (i = 0; i < rank_num; i++) {\n    bin = 0;\n    if (x[left + i] - floor(x[left + i]) < 0.25) {\n      bin = 0;\n    } else if (x[left + i] - floor(x[left + i]) >= 0.25 && x[left + i] - floor(x[left + i]) < 0.5) {\n      bin = 1;\n    } else if (x[left + i] - floor(x[left + i]) >= 0.5 && x[left + i] - floor(x[left + i]) < 0.75) {\n      bin = 2;\n    } else if (x[left + i] - floor(x[left + i]) >= 0.75 && x[left + i] - floor(x[left + i]) < 1) {\n      bin = 3;\n    }\n    temp[i] = bin;\n  }\n  MPI_Gather(temp, rank_num, MPI_DOUBLE, bins.data(), rank_num, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  delete[] temp;\n  return;\n}",
            "int rank = 0, size = 0;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint chunk = x.size() / size;\n\tint left = chunk * rank;\n\tint right = left + chunk;\n\tif (rank == size - 1)\n\t\tright = x.size();\n\n\tstd::vector<double> local;\n\tlocal.insert(local.begin(), x.begin() + left, x.begin() + right);\n\n\tomp_set_num_threads(omp_get_max_threads());\n\n#pragma omp parallel\n\t{\n\t\tint nthreads = omp_get_num_threads();\n\t\tint thread = omp_get_thread_num();\n\t\tint st = chunk * thread;\n\t\tint ed = st + chunk;\n\t\tif (thread == nthreads - 1)\n\t\t\ted = x.size();\n\n\t\tfor (int i = st; i < ed; ++i)\n\t\t{\n\t\t\tint index = local[i] >= 0.0 && local[i] <= 0.25? 0 : (local[i] > 0.25 && local[i] <= 0.5? 1 : (local[i] > 0.5 && local[i] <= 0.75? 2 : 3));\n\t\t\tbins[index]++;\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tint sum = 0;\n\t\tfor (auto i : bins)\n\t\t\tsum += i;\n\n\t\tMPI_Reduce(MPI_IN_PLACE, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\t\tfor (int i = 0; i < 4; ++i)\n\t\t\tbins[i] /= sum;\n\t}\n\telse\n\t\tMPI_Reduce(bins.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// Your code here\n  int nprocs;\n  int rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  size_t n = x.size();\n\n  std::vector<int> histo(16, 0);\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < n; i++) {\n    int bucket = (x[i] - floor(x[i]));\n    if (bucket < 0.25) {\n      histo[0]++;\n    } else if (bucket < 0.5) {\n      histo[1]++;\n    } else if (bucket < 0.75) {\n      histo[2]++;\n    } else {\n      histo[3]++;\n    }\n  }\n\n  if (rank == 0) {\n    int global_histo[16] = {0, 0, 0, 0};\n    for (int i = 0; i < nprocs; i++) {\n      MPI_Recv(&global_histo[0], 4, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    for (int i = 0; i < 4; i++) {\n      bins[i] = global_histo[i];\n    }\n  } else {\n    MPI_Send(&histo[0], 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "int numprocs, myrank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\tint my_bins[4] = {0};\n\tomp_set_num_threads(numprocs);\n\t#pragma omp parallel for reduction(+:my_bins[:4])\n\tfor (size_t i=0; i<x.size(); i++) {\n\t\tif (x[i] < 0.25) my_bins[0]++;\n\t\telse if (x[i] < 0.5) my_bins[1]++;\n\t\telse if (x[i] < 0.75) my_bins[2]++;\n\t\telse my_bins[3]++;\n\t}\n\t// my_bins[0] = 2, my_bins[1] = 1, my_bins[2] = 2, my_bins[3] = 2\n\t// MPI_Reduce(my_bins, bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\tif (myrank == 0) {\n\t\tbins[0] = 0;\n\t\tbins[1] = 0;\n\t\tbins[2] = 0;\n\t\tbins[3] = 0;\n\t}\n\tfor (int i=0; i<numprocs; i++) {\n\t\tif (i == myrank) continue;\n\t\tint other_bins[4] = {0};\n\t\tMPI_Status status;\n\t\tMPI_Recv(other_bins, 4, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n\t\tbins[0] += other_bins[0];\n\t\tbins[1] += other_bins[1];\n\t\tbins[2] += other_bins[2];\n\t\tbins[3] += other_bins[3];\n\t}\n\t// bins[0] = 2, bins[1] = 1, bins[2] = 2, bins[3] = 2\n\tMPI_Send(my_bins, 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n\t// MPI_Reduce(my_bins, bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\t// bins[0] = 2, bins[1] = 1, bins[2] = 2, bins[3] = 2\n}",
            "const int rank = omp_get_thread_num();\n    const int numRanks = omp_get_num_threads();\n    const size_t N = x.size();\n    const size_t chunkSize = N / numRanks;\n    const size_t rest = N % numRanks;\n    const size_t start = rank * chunkSize + (rank < rest? rank : rest);\n    const size_t end = start + chunkSize + (rank < rest? 1 : 0);\n    int isEnd = 0;\n    int isStart = 0;\n    if (rank == numRanks - 1) {\n        isEnd = 1;\n    }\n    if (rank == 0) {\n        isStart = 1;\n    }\n    std::array<size_t, 4> binsTmp = {0, 0, 0, 0};\n#pragma omp parallel for num_threads(numRanks) reduction(+:binsTmp[0]) reduction(+:binsTmp[1]) reduction(+:binsTmp[2]) reduction(+:binsTmp[3])\n    for (size_t i = start; i < end; i++) {\n        if (x[i] < 0.25) {\n            binsTmp[0]++;\n        } else if (x[i] < 0.5) {\n            binsTmp[1]++;\n        } else if (x[i] < 0.75) {\n            binsTmp[2]++;\n        } else if (x[i] < 1.0) {\n            binsTmp[3]++;\n        }\n    }\n\n#pragma omp parallel num_threads(4)\n    {\n#pragma omp for\n        for (int i = 0; i < 4; i++) {\n            bins[i] = 0;\n        }\n        if (isStart == 1) {\n            for (int i = 0; i < 4; i++) {\n                bins[i] = binsTmp[i];\n            }\n        }\n        if (isEnd == 1) {\n            for (int i = 0; i < 4; i++) {\n                bins[i] = bins[i] + binsTmp[i];\n            }\n        }\n    }\n}",
            "// Your code here\n  int rank;\n  int numprocs;\n  MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  if(rank == 0)\n  {\n    std::vector<double> x1(x);\n    std::vector<double> x2(x);\n    std::vector<double> x3(x);\n    std::vector<double> x4(x);\n\n    #pragma omp parallel\n    {\n      int thread_id = omp_get_thread_num();\n      std::vector<double> t_x1(x1);\n      std::vector<double> t_x2(x2);\n      std::vector<double> t_x3(x3);\n      std::vector<double> t_x4(x4);\n\n      #pragma omp for\n      for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] >= 0.0 && x[i] < 0.25)\n          t_x1[i] = 1;\n        else if (x[i] >= 0.25 && x[i] < 0.5)\n          t_x2[i] = 1;\n        else if (x[i] >= 0.5 && x[i] < 0.75)\n          t_x3[i] = 1;\n        else if (x[i] >= 0.75 && x[i] <= 1.0)\n          t_x4[i] = 1;\n      }\n      #pragma omp critical\n      {\n        bins[0] = 0;\n        bins[1] = 0;\n        bins[2] = 0;\n        bins[3] = 0;\n\n        for (int i = 0; i < t_x1.size(); i++) {\n          bins[0] += t_x1[i];\n        }\n\n        for (int i = 0; i < t_x2.size(); i++) {\n          bins[1] += t_x2[i];\n        }\n\n        for (int i = 0; i < t_x3.size(); i++) {\n          bins[2] += t_x3[i];\n        }\n\n        for (int i = 0; i < t_x4.size(); i++) {\n          bins[3] += t_x4[i];\n        }\n      }\n    }\n  }\n  else {\n    std::vector<double> x1(x);\n    std::vector<double> x2(x);\n    std::vector<double> x3(x);\n    std::vector<double> x4(x);\n\n    #pragma omp parallel\n    {\n      int thread_id = omp_get_thread_num();\n      std::vector<double> t_x1(x1);\n      std::vector<double> t_x2(x2);\n      std::vector<double> t_x3(x3);\n      std::vector<double> t_x4(x4);\n\n      #pragma omp for\n      for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] >= 0.0 && x[i] < 0.25)\n          t_x1[i] = 1;\n        else if (x[i] >= 0.25 && x[i] < 0.5)\n          t_x2[i] = 1;\n        else if (x[i] >= 0.5 && x[i] < 0.75)\n          t_x3[i] = 1;\n        else if (x[i] >= 0.75 && x[i] <= 1.0)\n          t_x4[i] = 1;\n      }\n      #pragma omp critical\n      {\n        bins[0] = 0;\n        bins[1] = 0;\n        bins[2] = 0;\n        bins[3] = 0;\n\n        for (",
            "size_t const comm_size = x.size();\n  size_t const comm_rank = omp_get_thread_num();\n\n  // Step 1: compute the number of elements per processor\n  std::vector<size_t> elemPerProc(comm_size);\n  int ierr = MPI_Allgather(&x.size(), 1, MPI_UNSIGNED, &elemPerProc[0], 1, MPI_UNSIGNED, MPI_COMM_WORLD);\n  assert(ierr == 0);\n\n  // Step 2: compute the offset per processor\n  std::vector<size_t> offset(comm_size + 1);\n  offset[0] = 0;\n  for (size_t i = 0; i < comm_size; ++i) {\n    offset[i + 1] = offset[i] + elemPerProc[i];\n  }\n\n  // Step 3: determine the number of elements to be processed by this processor\n  size_t const nElems = offset[comm_rank + 1] - offset[comm_rank];\n  std::vector<double> myElems(nElems);\n  MPI_Gather(&x[0] + offset[comm_rank], nElems, MPI_DOUBLE,\n             &myElems[0], nElems, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  // Step 4: compute the fractional part of the elements of myElems\n  #pragma omp parallel\n  {\n    size_t const myRank = omp_get_thread_num();\n\n    // Determine the element with the smallest fractional part\n    double minFraction = 1.0;\n    size_t minFractionIdx = 0;\n    for (size_t i = 0; i < myElems.size(); ++i) {\n      double fraction = myElems[i] - floor(myElems[i]);\n      if (fraction < minFraction) {\n        minFraction = fraction;\n        minFractionIdx = i;\n      }\n    }\n\n    // Count the number of elements with the corresponding fractional part\n    size_t bins_[4] = {0};\n    for (size_t i = 0; i < myElems.size(); ++i) {\n      double fraction = myElems[i] - floor(myElems[i]);\n      if (fraction == minFraction) {\n        if (fraction <= 0.25) {\n          ++bins_[0];\n        } else if (fraction <= 0.5) {\n          ++bins_[1];\n        } else if (fraction <= 0.75) {\n          ++bins_[2];\n        } else {\n          ++bins_[3];\n        }\n      }\n    }\n\n    // Write the results to the global bins\n    #pragma omp critical\n    {\n      bins[0] += bins_[0];\n      bins[1] += bins_[1];\n      bins[2] += bins_[2];\n      bins[3] += bins_[3];\n    }\n  }\n}",
            "size_t n = x.size();\n\n    // Initialize the bin values\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    // Compute the local number of elements in each bin\n    // Use an OpenMP parallel for loop\n    #pragma omp parallel for\n    for(size_t i = 0; i < n; i++) {\n        if (x[i] <= 0.25) {\n            bins[0] += 1;\n        } else if (x[i] <= 0.5) {\n            bins[1] += 1;\n        } else if (x[i] <= 0.75) {\n            bins[2] += 1;\n        } else {\n            bins[3] += 1;\n        }\n    }\n\n    // Gather the local bin values from all ranks to rank 0\n    MPI_Gather(&bins, 4, MPI_UNSIGNED_LONG_LONG, &bins, 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n    // Rank 0 has all the bin counts\n    if (rank == 0) {\n        // Check that the local counts are the same as the global sum\n        for (size_t i = 0; i < 4; i++) {\n            // Print the bin counts\n            std::cout << \"Bin \" << i << \" has \" << bins[i] << \" elements\" << std::endl;\n            assert(bins[i] == n);\n        }\n    }\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size_t n = x.size();\n    bins.fill(0);\n\n    #pragma omp parallel for\n    for (int i=0; i<n; i++) {\n        double frac_part = std::modf(x[i], &(x[i]));\n        switch(frac_part*4) {\n            case 0:\n                bins[0]++;\n                break;\n            case 0.25:\n                bins[1]++;\n                break;\n            case 0.5:\n                bins[2]++;\n                break;\n            case 0.75:\n                bins[3]++;\n                break;\n            case 1:\n                bins[0]++;\n                break;\n            default:\n                bins[0]++;\n        }\n    }\n\n    int nranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n    int remainder = (n/nranks)%4;\n    int i=0;\n    int count = 0;\n    while (i<n) {\n        if (remainder > 0) {\n            bins[count] += remainder;\n            remainder = 0;\n            count++;\n        }\n        bins[count] += nranks;\n        count++;\n        i+=nranks;\n    }\n    int total = bins[0] + bins[1] + bins[2] + bins[3];\n    MPI_Reduce(MPI_IN_PLACE, &total, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (int i=0; i<4; i++) {\n            std::cout << \"Rank 0, bin[\" << i << \"] = \" << bins[i] << std::endl;\n        }\n    }\n}",
            "int rank, nproc;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  // TODO: YOUR CODE HERE\n\n}",
            "// Fill in this function\n}",
            "// TODO: complete this function\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int remainder = size % 4;\n  size_t chunk_size = x.size() / size;\n  std::vector<double> x_new(x.begin() + rank * chunk_size, x.begin() + (rank + 1) * chunk_size);\n  int x_size = x_new.size();\n  if (rank == size - remainder) {\n    x_size = x_size + chunk_size - remainder * chunk_size;\n  }\n  std::vector<size_t> bin_count(4, 0);\n  if (rank == 0) {\n    bin_count[0] = x.size() - x_size;\n  }\n  if (x_size > 0) {\n    std::vector<double> x_tmp(x_size);\n    std::copy(x_new.begin(), x_new.end(), x_tmp.begin());\n    std::sort(x_tmp.begin(), x_tmp.end());\n    for (auto d : x_tmp) {\n      if (d < 0.25) {\n        bin_count[0]++;\n      } else if (d < 0.5) {\n        bin_count[1]++;\n      } else if (d < 0.75) {\n        bin_count[2]++;\n      } else {\n        bin_count[3]++;\n      }\n    }\n  }\n  std::vector<size_t> bins_total(4);\n  MPI_Reduce(bin_count.data(), bins_total.data(), 4, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    bins[0] = bins_total[0];\n    bins[1] = bins_total[1];\n    bins[2] = bins_total[2];\n    bins[3] = bins_total[3];\n  }\n}",
            "size_t N = x.size();\n    int Nx = N;\n    int my_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    double q_bins[4];\n    double * x_copy = new double[N];\n    for (int i = 0; i < N; i++) {\n        x_copy[i] = x[i];\n    }\n    MPI_Allgather(x_copy, N, MPI_DOUBLE, x_copy, N, MPI_DOUBLE, MPI_COMM_WORLD);\n    for (int i = 0; i < N; i++) {\n        if (x_copy[i] - std::floor(x_copy[i]) >= 0 && x_copy[i] - std::floor(x_copy[i]) < 0.25) {\n            q_bins[0]++;\n        }\n        if (x_copy[i] - std::floor(x_copy[i]) >= 0.25 && x_copy[i] - std::floor(x_copy[i]) < 0.5) {\n            q_bins[1]++;\n        }\n        if (x_copy[i] - std::floor(x_copy[i]) >= 0.5 && x_copy[i] - std::floor(x_copy[i]) < 0.75) {\n            q_bins[2]++;\n        }\n        if (x_copy[i] - std::floor(x_copy[i]) >= 0.75 && x_copy[i] - std::floor(x_copy[i]) < 1) {\n            q_bins[3]++;\n        }\n    }\n    std::array<size_t, 4> q_bins_all;\n    MPI_Allgather(q_bins, 4, MPI_UNSIGNED_LONG_LONG, q_bins_all.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_COMM_WORLD);\n    if (my_rank == 0) {\n        bins[0] = q_bins_all[0];\n        bins[1] = q_bins_all[1];\n        bins[2] = q_bins_all[2];\n        bins[3] = q_bins_all[3];\n    }\n    else {\n        bins[0] = 0;\n        bins[1] = 0;\n        bins[2] = 0;\n        bins[3] = 0;\n    }\n    return;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t n = x.size();\n    size_t block_size = n / size;\n    size_t block_remainder = n % size;\n    size_t block_start = rank * block_size + std::min(rank, block_remainder);\n    size_t block_end = block_start + block_size + (rank < block_remainder);\n\n    std::array<size_t, 4> local_bins;\n    for(size_t i = block_start; i < block_end; i++) {\n        double xi = x[i];\n        if(xi < 0.25) {\n            local_bins[0] += 1;\n        } else if(xi < 0.5) {\n            local_bins[1] += 1;\n        } else if(xi < 0.75) {\n            local_bins[2] += 1;\n        } else {\n            local_bins[3] += 1;\n        }\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, &local_bins, 4, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n    bins = local_bins;\n}",
            "const int N = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        std::array<size_t, 4> bins2{0, 0, 0, 0};\n\n        // Compute the local quartiles\n        for (int i = 0; i < N; i++) {\n            if (x[i] >= 0 && x[i] <= 0.25) {\n                bins2[0]++;\n            } else if (x[i] > 0.25 && x[i] <= 0.5) {\n                bins2[1]++;\n            } else if (x[i] > 0.5 && x[i] <= 0.75) {\n                bins2[2]++;\n            } else if (x[i] > 0.75) {\n                bins2[3]++;\n            } else {\n                // x[i] = 0\n                bins2[0]++;\n            }\n        }\n\n        // Count the number of doubles in each bin on all ranks\n        std::vector<size_t> allBins(4*size);\n#pragma omp parallel for\n        for (int i = 0; i < size; i++) {\n            allBins[i*4] = bins2[0];\n            allBins[i*4+1] = bins2[1];\n            allBins[i*4+2] = bins2[2];\n            allBins[i*4+3] = bins2[3];\n        }\n\n        // Gather the number of doubles in each bin on rank 0\n        MPI_Gather(allBins.data(), 4, MPI_UNSIGNED_LONG_LONG, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n        // Compute the global quartiles\n        if (rank == 0) {\n            size_t total = std::accumulate(bins.begin(), bins.end(), 0ull);\n            bins[0] = total*0.25;\n            bins[1] = total*0.50;\n            bins[2] = total*0.75;\n            bins[3] = total;\n        }\n    } else {\n        std::array<size_t, 4> bins2{0, 0, 0, 0};\n\n        // Compute the local quartiles\n        for (int i = 0; i < N; i++) {\n            if (x[i] >= 0 && x[i] <= 0.25) {\n                bins2[0]++;\n            } else if (x[i] > 0.25 && x[i] <= 0.5) {\n                bins2[1]++;\n            } else if (x[i] > 0.5 && x[i] <= 0.75) {\n                bins2[2]++;\n            } else if (x[i] > 0.75) {\n                bins2[3]++;\n            } else {\n                // x[i] = 0\n                bins2[0]++;\n            }\n        }\n\n        // Count the number of doubles in each bin on all ranks\n#pragma omp parallel for\n        for (int i = 0; i < 4; i++) {\n            bins[i] += bins2[i];\n        }\n    }\n}",
            "}",
            "int rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int chunk = x.size() / size;\n\n    if (rank!= 0) {\n        int remainder = x.size() - size * chunk;\n        chunk = chunk + (remainder >= 0? 1 : 0);\n    }\n\n    std::array<size_t, 4> localBins = { 0, 0, 0, 0 };\n    int start = rank * chunk;\n    int end = start + chunk - 1;\n    if (rank == size - 1) end = x.size() - 1;\n    if (chunk == 0) {\n        start = end = -1;\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i <= end; i++) {\n        double temp = x.at(i);\n        if (temp < 0.25) {\n            localBins[0] += 1;\n        } else if (temp >= 0.25 && temp < 0.5) {\n            localBins[1] += 1;\n        } else if (temp >= 0.5 && temp < 0.75) {\n            localBins[2] += 1;\n        } else {\n            localBins[3] += 1;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&bins, 4, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            bins[0] += localBins[0];\n            bins[1] += localBins[1];\n            bins[2] += localBins[2];\n            bins[3] += localBins[3];\n        }\n    } else {\n        MPI_Send(&localBins, 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "size_t total_count = x.size();\n  MPI_Comm comm = MPI_COMM_WORLD;\n  int rank;\n  int num_ranks;\n  MPI_Comm_rank(comm, &rank);\n  MPI_Comm_size(comm, &num_ranks);\n\n  size_t rank_count = total_count/num_ranks;\n\n  size_t local_start = rank*rank_count;\n  size_t local_end = local_start + rank_count;\n  if(rank == num_ranks-1) local_end = total_count;\n\n  std::array<size_t, 4> local_bins;\n\n#pragma omp parallel for\n  for(size_t i = local_start; i < local_end; i++){\n    double temp = x[i];\n    if(temp<0.25)\n      local_bins[0]++;\n    else if(temp<0.5)\n      local_bins[1]++;\n    else if(temp<0.75)\n      local_bins[2]++;\n    else\n      local_bins[3]++;\n  }\n\n  for(int i = 0; i<4; i++)\n    bins[i] = 0;\n\n  MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, comm);\n\n}",
            "size_t n_local = x.size();\n  std::vector<double> x_local(n_local);\n\n  MPI_Bcast(&n_local, 1, MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n  MPI_Bcast(&x[0], n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  // Parallelization using OpenMP\n  #pragma omp parallel for\n  for (size_t i = 0; i < n_local; i++) {\n    x_local[i] = x[i];\n  }\n\n  size_t min = 0, max = 0;\n  std::array<double, 4> quartiles{0, 0, 0, 0};\n\n  // Compute the quartiles with OpenMP, and store them in quartiles\n  #pragma omp parallel for reduction(min: min) reduction(max: max)\n  for (size_t i = 0; i < n_local; i++) {\n    min = std::min(min, x_local[i]);\n    max = std::max(max, x_local[i]);\n    if (x_local[i] >= 0.25 && x_local[i] < 0.5) {\n      quartiles[0]++;\n    }\n    else if (x_local[i] >= 0.5 && x_local[i] < 0.75) {\n      quartiles[1]++;\n    }\n    else if (x_local[i] >= 0.75 && x_local[i] <= 1.0) {\n      quartiles[2]++;\n    }\n    else {\n      quartiles[3]++;\n    }\n  }\n\n  // Compute the sum of quartiles for each rank\n  double quartiles_sum = 0;\n  for (size_t i = 0; i < 4; i++) {\n    quartiles_sum += quartiles[i];\n  }\n\n  // Compute the mean of the quartiles (per rank)\n  double mean = quartiles_sum / 4;\n\n  // Get the mean of the quartiles across all ranks\n  double mean_global = 0;\n  MPI_Allreduce(&mean, &mean_global, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  // Compute the difference between the mean of the quartiles and the mean of the values\n  double diff = mean_global - mean;\n\n  // Compute the std deviation of the quartiles (per rank)\n  double std_dev = diff / 4;\n\n  // Compute the std deviation across all ranks\n  double std_dev_global = 0;\n  MPI_Allreduce(&std_dev, &std_dev_global, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  // Compute the max_diff between the std deviation and the std deviation across all ranks\n  double max_diff = std::abs(std_dev_global - std_dev);\n\n  // Compute the bins with OpenMP\n  // The bins are stored in the array `bins`\n  #pragma omp parallel for\n  for (size_t i = 0; i < 4; i++) {\n    bins[i] = (size_t) (0.5 + quartiles[i] + max_diff);\n  }\n\n}",
            "// your code goes here\n  std::cout << \"Starting countQuartiles\" << std::endl;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int size = x.size();\n  std::vector<int> count_all;\n  int max_val = 0;\n\n  if (rank == 0) {\n    count_all = std::vector<int>(size);\n  }\n\n  std::vector<double> x_cpy = x;\n\n  MPI_Gather(&size, 1, MPI_INT, count_all.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    max_val = std::max_element(count_all.begin(), count_all.end());\n  }\n\n  MPI_Bcast(&max_val, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  // divide the vector x evenly among the ranks\n  int chunk_size = (max_val + num_procs - 1) / num_procs;\n\n  if (rank == 0) {\n    MPI_Scatter(count_all.data(), 1, MPI_INT, &chunk_size, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Scatter(count_all.data(), 1, MPI_INT, &chunk_size, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  }\n\n  int offset = 0;\n  for (int i = 0; i < rank; i++) {\n    offset += count_all[i];\n  }\n\n  std::vector<double> x_rank(chunk_size);\n  int x_size = x.size();\n  if (x_size > chunk_size) {\n    x_size = chunk_size;\n  }\n  MPI_Gather(&x_size, 1, MPI_INT, &chunk_size, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    MPI_Scatter(x_cpy.data() + offset, chunk_size, MPI_DOUBLE, x_rank.data(), chunk_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Scatter(x_cpy.data() + offset, chunk_size, MPI_DOUBLE, x_rank.data(), chunk_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  }\n\n  int num_threads = omp_get_max_threads();\n  std::vector<double> bins_local(num_threads, 0.0);\n\n#pragma omp parallel for shared(bins_local)\n  for (int i = 0; i < chunk_size; i++) {\n    double val = x_rank[i];\n    if (val < 0.25) {\n      bins_local[omp_get_thread_num()] += 1;\n    } else if (val < 0.5) {\n      bins_local[omp_get_thread_num()] += 1;\n    } else if (val < 0.75) {\n      bins_local[omp_get_thread_num()] += 1;\n    } else {\n      bins_local[omp_get_thread_num()] += 1;\n    }\n  }\n\n  // reduce the partial bins from each rank into a total bins\n  for (int i = 0; i < num_threads; i++) {\n    bins[i] = 0;\n  }\n  for (int i = 0; i < num_threads; i++) {\n    for (int j = 0; j < num_threads; j++) {\n      bins[i] += bins_local[j];\n    }\n  }\n\n  std::cout << \"Finished countQuart",
            "int rank, num_ranks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n   //TODO\n   std::vector<double> localx;\n   for(size_t i=0; i<x.size(); ++i) {\n      localx.push_back(x[i]);\n   }\n   std::vector<size_t> local_bins(4,0);\n   double x0 = 0.0, x1 = 0.25, x2 = 0.5, x3 = 0.75;\n   for(size_t i=0; i<localx.size(); ++i) {\n      if(localx[i]<x0) {\n         local_bins[0]++;\n      } else if(localx[i]<x1) {\n         local_bins[1]++;\n      } else if(localx[i]<x2) {\n         local_bins[2]++;\n      } else if(localx[i]<x3) {\n         local_bins[3]++;\n      } else {\n         local_bins[4]++;\n      }\n   }\n   std::vector<size_t> global_bins(4,0);\n   MPI_Allreduce(local_bins.data(), global_bins.data(), 5, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n   bins[0] = global_bins[0];\n   bins[1] = global_bins[1];\n   bins[2] = global_bins[2];\n   bins[3] = global_bins[3];\n   // std::cout << \"bins:\" << local_bins[0] << \" \" << local_bins[1] << \" \" << local_bins[2] << \" \" << local_bins[3] << std::endl;\n}",
            "size_t N = x.size();\n    // MPI_COMM_WORLD -> size\n    int rank, nranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n    int chunk = N / nranks;\n    int leftover = N % nranks;\n    int start = rank * chunk;\n    int end = start + chunk;\n    if (rank == nranks - 1) {\n        end = end + leftover;\n    }\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (int i = start; i < end; i++) {\n            if (x[i] <= 0.25) {\n                local_bins[0]++;\n            }\n            else if (x[i] <= 0.5) {\n                local_bins[1]++;\n            }\n            else if (x[i] <= 0.75) {\n                local_bins[2]++;\n            }\n            else {\n                local_bins[3]++;\n            }\n        }\n    }\n\n    MPI_Allreduce(&local_bins, &bins, 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n   const int num_procs = omp_get_num_procs();\n   const int my_rank = omp_get_thread_num();\n#pragma omp parallel for reduction(+:bins[0],bins[1],bins[2],bins[3])\n   for(int i=0;i<x.size();i++){\n      if(x[i]<0.25)\n         bins[0]+=1;\n      else if(x[i]>=0.25 && x[i]<0.5)\n         bins[1]+=1;\n      else if(x[i]>=0.5 && x[i]<0.75)\n         bins[2]+=1;\n      else\n         bins[3]+=1;\n   }\n   MPI_Reduce(bins.data(), bins.data(), 4, MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   return;\n}",
            "size_t N = x.size();\n\n    #pragma omp parallel\n    {\n        size_t myRank = omp_get_thread_num();\n        size_t numThreads = omp_get_num_threads();\n        size_t chunkSize = N/numThreads;\n        size_t i;\n        #pragma omp for\n        for (i=0; i<chunkSize; i++) {\n            int bin = (x[i]>0.75)?3:(x[i]>0.5)?2:(x[i]>0.25)?1:0;\n            bins[bin]++;\n        }\n    }\n\n    /* TODO: use MPI to compute the results across all threads */\n\n    /* TODO: gather the results from all threads to the rank 0 process */\n\n    /* TODO: sum the bin counts from all processes */\n\n    /* TODO: store the summed bin counts in bins */\n\n    /* TODO: free the memory */\n}",
            "// TODO: replace the following lines of code with your implementation\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int chunk = x.size() / size;\n  int start = rank * chunk;\n  int end = std::min(start + chunk, (int) x.size());\n  //std::cout << \"Rank \" << rank << \" has a chunk of \" << chunk << \" from \" << start << \" to \" << end << \"\\n\";\n\n  // compute the number of doubles in the current chunk that have a fractional part in each of the 4 intervals\n  bins[0] = std::count_if(x.begin() + start, x.begin() + end, [](double x){ return x - int(x) < 0.25; });\n  bins[1] = std::count_if(x.begin() + start, x.begin() + end, [](double x){ return x - int(x) >= 0.25 && x - int(x) < 0.5; });\n  bins[2] = std::count_if(x.begin() + start, x.begin() + end, [](double x){ return x - int(x) >= 0.5 && x - int(x) < 0.75; });\n  bins[3] = std::count_if(x.begin() + start, x.begin() + end, [](double x){ return x - int(x) >= 0.75; });\n\n  // gather the results\n  MPI_Allreduce(MPI_IN_PLACE, bins.data(), 4, MPI_UNSIGNED, MPI_SUM, MPI_COMM_WORLD);\n\n  //std::cout << \"Rank \" << rank << \" got result: \" << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << \"\\n\";\n}",
            "// Fill in the code here\n  int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  const size_t N = x.size();\n  // Divide the vector into chunks of work\n  int chunk = N / size;\n\n  // Create a vector of doubles, each thread will contribute to this vector\n  // the size of the chunk\n  std::vector<double> local_x(chunk);\n\n  // We want to store the counts of each thread in the bins array, but this\n  // array is on rank 0, so each thread needs to communicate the count for the\n  // bin it worked on to rank 0.\n  std::array<size_t, 4> bin_counts;\n  bin_counts.fill(0);\n\n  // Create a vector of MPI_Requests, each thread will need to create one\n  // request to keep track of the communication of the counts to rank 0\n  std::vector<MPI_Request> request(4);\n\n  // Iterate through the vector and count the quartiles\n  // Make sure you use a single thread on each rank for the loop\n  // OpenMP directives (i.e. omp for, omp single)\n\n  // Create a thread private array for the counts of the quartiles\n  std::array<size_t, 4> local_bins;\n  local_bins.fill(0);\n\n  int start = chunk * rank;\n  int end = start + chunk;\n  if (rank == size - 1) {\n    end = N;\n  }\n\n  for (int i = start; i < end; i++) {\n    if (x[i] < 0.25) {\n      local_bins[0]++;\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      local_bins[1]++;\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      local_bins[2]++;\n    } else if (x[i] >= 0.75 && x[i] < 1.0) {\n      local_bins[3]++;\n    }\n  }\n\n  // Add the counts for the quartiles that belong to the thread to the array that\n  // will be stored on rank 0\n  for (size_t i = 0; i < 4; i++) {\n    bin_counts[i] += local_bins[i];\n  }\n\n  // Now send the counts of the thread to rank 0\n  // Use a single thread on each rank to do this\n  // OpenMP directives\n  // Use the MPI_Send function, make sure to pass the correct arguments (see MPI documentation)\n  // The request in request[i] will be used by the rank 0 to wait for the communication\n  // Make sure to use the correct source rank\n  if (rank == 0) {\n    for (size_t i = 0; i < 4; i++) {\n      MPI_Send(&bin_counts[i], 1, MPI_UNSIGNED_LONG, i + 1, 0, MPI_COMM_WORLD);\n    }\n  } else {\n    MPI_Send(&bin_counts, 4, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n  // Now wait for the counts to come from the ranks and add them to bins\n  if (rank == 0) {\n    for (size_t i = 0; i < 4; i++) {\n      MPI_Recv(&bin_counts[i], 1, MPI_UNSIGNED_LONG, i + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      bins[i] += bin_counts[i];\n    }\n  }\n  // Wait for all threads to finish their computations on the vector before continuing\n  // OpenMP directives\n  MPI_Barrier(MPI_COMM_WORLD);",
            "int rank, num_processes;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &num_processes);\n\n\tint num_iterations = 4;\n\tint bins_per_process = bins.size() / num_iterations;\n\n\tstd::array<size_t, 4> bins_local = { 0,0,0,0 };\n\tfor (int i = 0; i < x.size(); ++i) {\n\t\tint indx = 0;\n\t\tif (x[i] >= 0.0 && x[i] < 0.25) indx = 0;\n\t\telse if (x[i] >= 0.25 && x[i] < 0.5) indx = 1;\n\t\telse if (x[i] >= 0.5 && x[i] < 0.75) indx = 2;\n\t\telse if (x[i] >= 0.75 && x[i] <= 1.0) indx = 3;\n\t\tbins_local[indx]++;\n\t}\n\tstd::array<size_t, 4> bins_all;\n\tstd::fill(bins_all.begin(), bins_all.end(), 0);\n\n#pragma omp parallel for\n\tfor (int i = 0; i < bins_per_process; ++i) {\n\t\tbins_all[rank * bins_per_process + i] = bins_local[i];\n\t}\n\n\tMPI_Allreduce(MPI_IN_PLACE, bins_all.data(), bins_all.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < num_iterations; ++i) {\n\t\t\tbins[i] = bins_all[i];\n\t\t}\n\t}\n}",
            "// TODO: implement\n\n}",
            "int rank, num_ranks;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n\tsize_t count, count_sum = 0;\n\tsize_t x_len = x.size();\n\n\tstd::vector<double> x_local;\n\tfor (size_t i = rank; i < x_len; i += num_ranks) {\n\t\tx_local.push_back(x[i]);\n\t}\n\n\tdouble lower_bound = 0;\n\tdouble upper_bound = 0.25;\n\tfor (size_t i = 0; i < x_local.size(); ++i) {\n\t\tif (x_local[i] >= lower_bound && x_local[i] < upper_bound) {\n\t\t\t++count;\n\t\t}\n\t}\n\tbins[0] = count;\n\tcount_sum += count;\n\n\tlower_bound = 0.25;\n\tupper_bound = 0.5;\n\tcount = 0;\n\tfor (size_t i = 0; i < x_local.size(); ++i) {\n\t\tif (x_local[i] >= lower_bound && x_local[i] < upper_bound) {\n\t\t\t++count;\n\t\t}\n\t}\n\tbins[1] = count;\n\tcount_sum += count;\n\n\tlower_bound = 0.5;\n\tupper_bound = 0.75;\n\tcount = 0;\n\tfor (size_t i = 0; i < x_local.size(); ++i) {\n\t\tif (x_local[i] >= lower_bound && x_local[i] < upper_bound) {\n\t\t\t++count;\n\t\t}\n\t}\n\tbins[2] = count;\n\tcount_sum += count;\n\n\tlower_bound = 0.75;\n\tupper_bound = 1;\n\tcount = 0;\n\tfor (size_t i = 0; i < x_local.size(); ++i) {\n\t\tif (x_local[i] >= lower_bound && x_local[i] < upper_bound) {\n\t\t\t++count;\n\t\t}\n\t}\n\tbins[3] = count;\n\tcount_sum += count;\n\n\tbins[0] = bins[0] * num_ranks;\n\tbins[1] = bins[1] * num_ranks;\n\tbins[2] = bins[2] * num_ranks;\n\tbins[3] = bins[3] * num_ranks;\n\n\tstd::vector<size_t> bins_local(4, 0);\n\tbins_local[0] = bins[0];\n\tbins_local[1] = bins[1];\n\tbins_local[2] = bins[2];\n\tbins_local[3] = bins[3];\n\n\tint root = 0;\n\tMPI_Reduce(bins_local.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, root, MPI_COMM_WORLD);\n\t// std::cout << \"after mpi reduce: \" << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << std::endl;\n\n\t// std::cout << \"before omp parallel: \" << count << std::endl;\n\tomp_set_num_threads(num_ranks);\n\t#pragma omp parallel\n\t{\n\t\t#pragma omp single\n\t\t{\n\t\t\t// std::cout << \"inside omp single: \" << count << std::endl;\n\t\t\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\t\t\tif (x[i] >= lower_bound && x[i] < upper_bound) {\n\t\t\t\t\t++count;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbins[0] = count;",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<double> work;\n    if (rank == 0) {\n        work = x;\n    }\n    MPI_Bcast(&work[0], x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    bins.fill(0);\n    #pragma omp parallel\n    {\n        std::array<size_t, 4> local_bins{0, 0, 0, 0};\n        for (auto x_ : work) {\n            if (x_ < 0.25) {\n                local_bins[0] += 1;\n            } else if (x_ < 0.5) {\n                local_bins[1] += 1;\n            } else if (x_ < 0.75) {\n                local_bins[2] += 1;\n            } else {\n                local_bins[3] += 1;\n            }\n        }\n        #pragma omp critical\n        for (int i = 0; i < 4; ++i) {\n            bins[i] += local_bins[i];\n        }\n    }\n}",
            "// TODO: write code here\n  // for(int i=0; i<x.size(); i++){\n  //   if(x[i]<1){\n  //     bins[0]++;\n  //   }else if(x[i]<2){\n  //     bins[1]++;\n  //   }else if(x[i]<3){\n  //     bins[2]++;\n  //   }else{\n  //     bins[3]++;\n  //   }\n  // }\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 1) {\n      bins[0]++;\n    } else if (x[i] >= 1 && x[i] < 2) {\n      bins[1]++;\n    } else if (x[i] >= 2 && x[i] < 3) {\n      bins[2]++;\n    } else if (x[i] >= 3) {\n      bins[3]++;\n    }\n  }\n}",
            "int const comm_size = MPI_Comm_size(MPI_COMM_WORLD);\n\tint const comm_rank = MPI_Comm_rank(MPI_COMM_WORLD);\n\t\n\tstd::vector<double> x_local(x.begin()+comm_rank*x.size()/comm_size, x.begin()+(comm_rank+1)*x.size()/comm_size);\n\t//std::cout << \"x_local[0] \" << x_local[0] << std::endl;\n\n\n\t\n\n\t//#pragma omp parallel for\n\tfor (int i=0; i<x_local.size(); i++) {\n\t\tif (x_local[i] >= 0.75) {\n\t\t\tx_local[i] = 3;\n\t\t}\n\t\telse if (x_local[i] >= 0.50) {\n\t\t\tx_local[i] = 2;\n\t\t}\n\t\telse if (x_local[i] >= 0.25) {\n\t\t\tx_local[i] = 1;\n\t\t}\n\t\telse {\n\t\t\tx_local[i] = 0;\n\t\t}\n\t}\n\n\n\n\tint count_sum = 0;\n\tint count_local = x_local[0];\n\tfor (int i=1; i<x_local.size(); i++) {\n\t\tcount_sum += count_local;\n\t\tcount_local = x_local[i];\n\t}\n\tcount_sum += count_local;\n\t//std::cout << \"count_sum \" << count_sum << std::endl;\n\t\n\tbins[0] = 0;\n\tbins[1] = 0;\n\tbins[2] = 0;\n\tbins[3] = 0;\n\n\n\tint count_sum_local[4];\n\tcount_sum_local[0] = 0;\n\tcount_sum_local[1] = 0;\n\tcount_sum_local[2] = 0;\n\tcount_sum_local[3] = 0;\n\n\t#pragma omp parallel for\n\tfor (int i=0; i<x_local.size(); i++) {\n\t\tcount_sum_local[x_local[i]]++;\n\t}\n\t\n\t//std::cout << \"count_sum_local[0] \" << count_sum_local[0] << std::endl;\n\t//std::cout << \"count_sum_local[1] \" << count_sum_local[1] << std::endl;\n\t//std::cout << \"count_sum_local[2] \" << count_sum_local[2] << std::endl;\n\t//std::cout << \"count_sum_local[3] \" << count_sum_local[3] << std::endl;\n\n\t\n\t//std::cout << \"count_sum_local[0] \" << count_sum_local[0] << std::endl;\n\t//std::cout << \"count_sum_local[1] \" << count_sum_local[1] << std::endl;\n\t//std::cout << \"count_sum_local[2] \" << count_sum_local[2] << std::endl;\n\t//std::cout << \"count_sum_local[3] \" << count_sum_local[3] << std::endl;\n\t\n\n\tint count_sum_recv[4];\n\n\tcount_sum_recv[0] = 0;\n\tcount_sum_recv[1] = 0;\n\tcount_sum_recv[2] = 0;\n\tcount_sum_recv[3] = 0;\n\n\tint count_sum_send[4];\n\n\tcount_sum_send[0] = 0;\n\tcount_sum_send[1] = 0;\n\tcount_sum_send[2] = 0;\n\tcount_sum_send[3] = 0;\n\n\tif (comm_rank>0) {\n\t\tMPI_Status status;\n\t\tMPI_Recv(&count_sum_recv, 4, MPI_INT, comm_rank-1, 0, MPI_",
            "int nb_ranks = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &nb_ranks);\n  int rank = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (nb_ranks == 1) {\n    for (int i = 0; i < 4; i++) {\n      bins[i] = 0;\n    }\n\n    for (int i = 0; i < x.size(); i++) {\n      if (x[i] < 0.25) {\n        bins[0]++;\n      }\n\n      else if (x[i] < 0.5) {\n        bins[1]++;\n      }\n\n      else if (x[i] < 0.75) {\n        bins[2]++;\n      }\n\n      else if (x[i] < 1) {\n        bins[3]++;\n      }\n    }\n  } else if (rank!= 0) {\n    std::array<size_t, 4> bins_part;\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n      if (x[i] < 0.25) {\n        bins_part[0]++;\n      }\n\n      else if (x[i] < 0.5) {\n        bins_part[1]++;\n      }\n\n      else if (x[i] < 0.75) {\n        bins_part[2]++;\n      }\n\n      else if (x[i] < 1) {\n        bins_part[3]++;\n      }\n    }\n\n    MPI_Status status;\n    MPI_Send(bins_part.data(), 4, MPI_LONG, 0, rank, MPI_COMM_WORLD);\n    MPI_Recv(bins.data(), 4, MPI_LONG, 0, rank, MPI_COMM_WORLD, &status);\n  } else {\n    std::array<size_t, 4> bins_part;\n    for (int i = 1; i < nb_ranks; i++) {\n      MPI_Status status;\n      MPI_Recv(bins_part.data(), 4, MPI_LONG, i, i, MPI_COMM_WORLD, &status);\n\n      #pragma omp parallel for\n      for (int j = 0; j < 4; j++) {\n        bins[j] += bins_part[j];\n      }\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n      if (x[i] < 0.25) {\n        bins[0]++;\n      }\n\n      else if (x[i] < 0.5) {\n        bins[1]++;\n      }\n\n      else if (x[i] < 0.75) {\n        bins[2]++;\n      }\n\n      else if (x[i] < 1) {\n        bins[3]++;\n      }\n    }\n  }\n}",
            "int nprocs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int nbins = 4;\n    int chunk = x.size()/nprocs;\n    int remainder = x.size()%nprocs;\n\n    std::vector<double> my_x;\n    if(rank < remainder) {\n        my_x = std::vector<double>(x.begin() + chunk*rank, x.begin() + chunk*(rank+1));\n    } else {\n        my_x = std::vector<double>(x.begin() + chunk*(rank+1) + remainder, x.begin() + chunk*(rank+1) + remainder + chunk);\n    }\n\n    std::vector<double> my_bins(4);\n\n    for(auto & val : my_x) {\n        int i = 0;\n        if(val < 0.25) {\n            i = 0;\n        } else if (val >= 0.25 && val < 0.5) {\n            i = 1;\n        } else if (val >= 0.5 && val < 0.75) {\n            i = 2;\n        } else if (val >= 0.75 && val <= 1.0) {\n            i = 3;\n        }\n        my_bins[i]++;\n    }\n\n    std::vector<double> total(nbins);\n#pragma omp parallel for\n    for(int i=0; i < nbins; ++i) {\n        total[i] = 0.0;\n    }\n\n    MPI_Reduce(my_bins.data(), total.data(), nbins, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if(rank == 0) {\n        bins = std::array<size_t, 4>(total.begin(), total.end());\n    }\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  const size_t nx = x.size();\n  const int nprocs = omp_get_num_procs();\n  if (nx == 0) {\n    return;\n  }\n  bins = {0, 0, 0, 0};\n  if (rank == 0) {\n    std::array<size_t, 4> bins_local = {0, 0, 0, 0};\n    for (size_t i = 0; i < nx; i++) {\n      if (x[i] < 0.25) {\n        bins_local[0]++;\n      } else if (x[i] >= 0.25 && x[i] < 0.5) {\n        bins_local[1]++;\n      } else if (x[i] >= 0.5 && x[i] < 0.75) {\n        bins_local[2]++;\n      } else if (x[i] >= 0.75 && x[i] < 1) {\n        bins_local[3]++;\n      }\n    }\n    for (int i = 1; i < nprocs; i++) {\n      std::array<size_t, 4> bins_tmp;\n      MPI_Recv(bins_tmp.data(), 4, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 4; j++) {\n        bins_local[j] += bins_tmp[j];\n      }\n    }\n    for (int j = 0; j < 4; j++) {\n      bins[j] = bins_local[j];\n    }\n  } else {\n    std::array<size_t, 4> bins_tmp = {0, 0, 0, 0};\n    for (size_t i = 0; i < nx; i++) {\n      if (x[i] < 0.25) {\n        bins_tmp[0]++;\n      } else if (x[i] >= 0.25 && x[i] < 0.5) {\n        bins_tmp[1]++;\n      } else if (x[i] >= 0.5 && x[i] < 0.75) {\n        bins_tmp[2]++;\n      } else if (x[i] >= 0.75 && x[i] < 1) {\n        bins_tmp[3]++;\n      }\n    }\n    MPI_Send(bins_tmp.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int mysize = x.size();\n    size_t mybins[4] = { 0, 0, 0, 0 };\n    for (int i = 0; i < mysize; ++i) {\n        double value = x[i];\n        double rem = std::fmod(value, 1.0);\n        double bin = (rem < 0.25)? 0 : (rem < 0.5)? 1 : (rem < 0.75)? 2 : 3;\n        mybins[bin]++;\n    }\n\n    std::array<size_t, 4> bins_sum{ 0, 0, 0, 0 };\n    MPI_Reduce(mybins, bins_sum.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (int i = 0; i < 4; ++i) {\n            bins[i] = bins_sum[i] / size;\n        }\n    }\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    size_t n = x.size();\n    size_t n_per_rank = n / size_t(MPI_COMM_WORLD.Size());\n\n    // First count the number of elements in each rank, this will allow us to\n    // distribute the work more evenly.\n    std::vector<size_t> counts(MPI_COMM_WORLD.Size());\n    std::vector<size_t> start_indices(MPI_COMM_WORLD.Size());\n    MPI_Gather(&n_per_rank, 1, MPI_UNSIGNED_LONG_LONG, &counts[0], 1, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        // Compute the starting indices for each rank.\n        size_t total = 0;\n        for (int i = 0; i < MPI_COMM_WORLD.Size(); i++) {\n            start_indices[i] = total;\n            total += counts[i];\n        }\n    }\n\n    std::vector<double> local_x(n_per_rank);\n    std::vector<size_t> local_bins(4);\n\n    // Gather local data on rank 0.\n    MPI_Gather(&x[0], n_per_rank, MPI_DOUBLE, &local_x[0], n_per_rank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        // Compute local histogram.\n        #pragma omp parallel for\n        for (int i = 0; i < n_per_rank; i++) {\n            double value = local_x[i];\n            double remainder = value - int(value);\n            if (remainder <= 0.25) {\n                local_bins[0]++;\n            } else if (remainder <= 0.5) {\n                local_bins[1]++;\n            } else if (remainder <= 0.75) {\n                local_bins[2]++;\n            } else {\n                local_bins[3]++;\n            }\n        }\n\n        // Compute global histogram.\n        std::vector<size_t> global_bins(4);\n        MPI_Reduce(&local_bins[0], &global_bins[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n        for (int i = 0; i < 4; i++) {\n            bins[i] += global_bins[i];\n        }\n    }\n}",
            "size_t const N = x.size();\n    size_t const P = omp_get_num_threads();\n\n    size_t const N2 = N / 2;\n    size_t const N3 = N2 / 2;\n\n    size_t *local_bins = new size_t[4];\n    local_bins[0] = 0;\n    local_bins[1] = 0;\n    local_bins[2] = 0;\n    local_bins[3] = 0;\n\n    for (size_t i = 0; i < N2; ++i) {\n        local_bins[(x[i] > N3) * 2 + (x[i] > N3 + 1)] += 1;\n    }\n\n    size_t *local_bins_all = new size_t[4 * P];\n    MPI_Allgather(local_bins, 4, MPI_UNSIGNED_LONG_LONG, local_bins_all, 4, MPI_UNSIGNED_LONG_LONG, MPI_COMM_WORLD);\n    for (size_t i = 0; i < 4; ++i) {\n        bins[i] = 0;\n    }\n    for (size_t i = 0; i < P; ++i) {\n        bins[0] += local_bins_all[4 * i];\n        bins[1] += local_bins_all[4 * i + 1];\n        bins[2] += local_bins_all[4 * i + 2];\n        bins[3] += local_bins_all[4 * i + 3];\n    }\n    delete[] local_bins_all;\n    delete[] local_bins;\n}",
            "// TODO: implement me\n}",
            "// TODO: implement\n\n    // get the number of processes\n    int nprocs = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    // get the current rank\n    int rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // calculate the size of the vector in each process\n    int num_local = x.size() / nprocs;\n\n    // vector to store the number of elements in each rank\n    std::vector<int> counts(nprocs, 0);\n\n    // get the total number of elements\n    int total = 0;\n\n    // get the index of the first element of the current rank\n    int start = rank * num_local;\n\n    // get the number of elements in the current rank\n    int size = num_local;\n\n    if (rank == nprocs - 1) {\n        // last rank\n        size = x.size() - start;\n    }\n\n    // set the index of the last element of the current rank\n    int end = start + size;\n\n    // add the number of elements in the current rank to the total\n    total += size;\n\n    // increment the counter in the 4 bins\n    for (int i = 0; i < size; i++) {\n        if (x[start + i] - (int) x[start + i] >= 0.25) {\n            if (x[start + i] - (int) x[start + i] < 0.5) {\n                counts[0]++;\n            } else if (x[start + i] - (int) x[start + i] < 0.75) {\n                counts[1]++;\n            } else {\n                counts[2]++;\n            }\n        } else {\n            counts[3]++;\n        }\n    }\n\n    // add up the number of elements in each bin\n    std::vector<int> partial(4, 0);\n    MPI_Allreduce(counts.data(), partial.data(), 4, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    // save the result in the 4 bins\n    bins[0] = partial[0];\n    bins[1] = partial[1];\n    bins[2] = partial[2];\n    bins[3] = partial[3];\n\n    // calculate the sum of the elements in each rank\n    std::vector<int> partial_sum(4, 0);\n    MPI_Reduce(counts.data(), partial_sum.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // divide by the total number of elements\n    if (rank == 0) {\n        bins[0] /= total;\n        bins[1] /= total;\n        bins[2] /= total;\n        bins[3] /= total;\n    }\n}",
            "int nproc, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<double> xcopy;\n    xcopy.assign(x.begin(), x.end());\n\n    std::sort(xcopy.begin(), xcopy.end());\n\n    int n = xcopy.size();\n    int nbins = n / nproc;\n\n    int r = n % nproc;\n    int b = r * nbins;\n\n    int a = 0;\n    int bb = bins[0];\n    int c = 0;\n    int d = 0;\n\n    if (rank == 0) {\n        for (int i = 0; i < nbins; i++) {\n\n            if (xcopy[a + i] < 0.25) {\n                bins[0]++;\n            } else if (xcopy[a + i] < 0.5) {\n                bins[1]++;\n            } else if (xcopy[a + i] < 0.75) {\n                bins[2]++;\n            } else if (xcopy[a + i] < 1) {\n                bins[3]++;\n            }\n        }\n    } else if (rank == nproc - 1) {\n\n        for (int i = 0; i < b; i++) {\n\n            if (xcopy[a + i] < 0.25) {\n                bins[0]++;\n            } else if (xcopy[a + i] < 0.5) {\n                bins[1]++;\n            } else if (xcopy[a + i] < 0.75) {\n                bins[2]++;\n            } else if (xcopy[a + i] < 1) {\n                bins[3]++;\n            }\n        }\n    } else if (rank >= 1 && rank < nproc - 1) {\n\n        for (int i = 0; i < nbins; i++) {\n\n            if (xcopy[a + i] < 0.25) {\n                bins[0]++;\n            } else if (xcopy[a + i] < 0.5) {\n                bins[1]++;\n            } else if (xcopy[a + i] < 0.75) {\n                bins[2]++;\n            } else if (xcopy[a + i] < 1) {\n                bins[3]++;\n            }\n        }\n    } else {\n\n        for (int i = 0; i < b; i++) {\n\n            if (xcopy[a + i] < 0.25) {\n                bins[0]++;\n            } else if (xcopy[a + i] < 0.5) {\n                bins[1]++;\n            } else if (xcopy[a + i] < 0.75) {\n                bins[2]++;\n            } else if (xcopy[a + i] < 1) {\n                bins[3]++;\n            }\n        }\n\n        for (int i = 0; i < r; i++) {\n\n            if (xcopy[a + i + b] < 0.25) {\n                bins[0]++;\n            } else if (xcopy[a + i + b] < 0.5) {\n                bins[1]++;\n            } else if (xcopy[a + i + b] < 0.75) {\n                bins[2]++;\n            } else if (xcopy[a + i + b] < 1) {\n                bins[3]++;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < nproc; i++) {\n            MPI_Recv(&bins, 4, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&bins, 4, MPI_INT, 0, 0, MPI_CO",
            "// TODO: Your code here\n}",
            "// TODO: Implement\n}",
            "int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    // determine the number of elements in each piece of the array\n    size_t n_elements = x.size() / world_size;\n    size_t n_left = x.size() - n_elements * (world_size - 1);\n\n    // the last process will have different numbers of elements\n    if (world_rank == world_size - 1) n_elements = n_left;\n\n    // determine the local offset\n    size_t local_offset = n_elements * world_rank;\n\n    // determine the local size\n    size_t local_size = n_elements;\n    if (world_rank == world_size - 1) local_size = n_left;\n\n    // create an array of local elements\n    std::vector<double> local_x(x.begin() + local_offset, x.begin() + local_offset + local_size);\n\n    // sort the elements in the local vector\n    std::sort(local_x.begin(), local_x.end());\n\n    size_t low_count = 0;\n    size_t mid_count = 0;\n    size_t high_count = 0;\n    size_t upper_count = 0;\n\n    for (double d : local_x) {\n        if (d < 0.25) {\n            low_count++;\n        } else if (d < 0.5) {\n            mid_count++;\n        } else if (d < 0.75) {\n            high_count++;\n        } else {\n            upper_count++;\n        }\n    }\n\n    bins[0] = low_count;\n    bins[1] = mid_count;\n    bins[2] = high_count;\n    bins[3] = upper_count;\n\n    if (world_rank == 0) {\n        for (int i = 1; i < world_size; i++) {\n            MPI_Recv(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "constexpr int NBINS = 4;\n    bins = {0,0,0,0};\n\n    int rank;\n    int nranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n    size_t n = x.size();\n    size_t nperrank = n/nranks;\n    size_t start = rank*nperrank;\n    size_t end = (rank+1)*nperrank;\n    if (rank == nranks-1) {\n        end = n;\n    }\n    std::array<size_t, NBINS> bins_local;\n    for (int i=0; i<NBINS; i++) {\n        bins_local[i] = 0;\n    }\n    for (int i=start; i<end; i++) {\n        int bin = 0;\n        if (x[i] >= 0.0 && x[i] < 0.25) {\n            bin = 0;\n        }\n        else if (x[i] >= 0.25 && x[i] < 0.5) {\n            bin = 1;\n        }\n        else if (x[i] >= 0.5 && x[i] < 0.75) {\n            bin = 2;\n        }\n        else {\n            bin = 3;\n        }\n        bins_local[bin] += 1;\n    }\n\n    std::array<size_t, NBINS> bins_reduced = bins_local;\n    MPI_Allreduce(MPI_IN_PLACE, bins_reduced.data(), NBINS, MPI_UNSIGNED, MPI_SUM, MPI_COMM_WORLD);\n\n    for (int i=0; i<NBINS; i++) {\n        bins[i] = bins_reduced[i];\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use Kokkos to count in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(Kokkos::View<const Point*> const& points, Kokkos::View<size_t[4]> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// Implement this function\n}",
            "constexpr int N = 4;\n  bins = 0;\n  Kokkos::parallel_for(N, [&] (int i) {\n    bins[i] = 0;\n  });\n  Kokkos::parallel_reduce(points.size(), KOKKOS_LAMBDA(const int i, size_t& count) {\n    auto p = points[i];\n    if (p.x > 0.0) {\n      if (p.y > 0.0) {\n        count++;\n      } else {\n        bins[1]++;\n      }\n    } else {\n      if (p.y > 0.0) {\n        bins[2]++;\n      } else {\n        bins[3]++;\n      }\n    }\n  }, bins[0]);\n}",
            "// Fill this in\n}",
            "}",
            "// TODO: complete this function\n    // hint:\n    // 1. The number of elements in the View `points` is called `points.size()`.\n    // 2. The View `bins` is an array of size 4, where the elements are initialized to zero.\n    // 3. The Views are of the same type (`Point`)\n    // 4. The Views are Kokkos::View\n    // 5. You can access the element at position i using the method `bins(i)`.\n\n    // TODO: complete this function\n}",
            "//...\n\n   Kokkos::deep_copy(bins, bins_kokkos);\n}",
            "// YOUR CODE GOES HERE\n}",
            "}",
            "// Start by setting all bins to zero\n    Kokkos::deep_copy(bins, 0);\n\n    // Fill the bins\n    //...\n}",
            "//...\n}",
            "// TODO: implement\n    Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace> policy(0, points.size());\n    Kokkos::parallel_for(\"Count points\", policy, KOKKOS_LAMBDA(const int i) {\n        Point p = points(i);\n        if(p.x > 0 && p.y > 0) {\n            bins(0) += 1;\n        } else if(p.x < 0 && p.y > 0) {\n            bins(1) += 1;\n        } else if(p.x < 0 && p.y < 0) {\n            bins(2) += 1;\n        } else {\n            bins(3) += 1;\n        }\n    });\n    Kokkos::fence();\n    //Kokkos::deep_copy(bins, bins_host);\n}",
            "// 1. Set bins to zero.\n    // 2. Loop over all points\n    //    a. for each point, determine which quadrant it is in\n    //    b. increment the count for the quadrant\n    // 3. The above loop can be parallelized.\n    //    You can use Kokkos::parallel_reduce for the above loop.\n}",
            "// TODO\n}",
            "// TODO\n}",
            "}",
            "// TODO\n  // 1. Loop over the input points\n  //    - Find the quadrant in which the point is located:\n  //      - x > 0 && y > 0: 0\n  //      - x > 0 && y < 0: 1\n  //      - x < 0 && y > 0: 2\n  //      - x < 0 && y < 0: 3\n  //    - Increment the count of the correct quadrant\n  // 2. Store the counts in the output view\n\n   Kokkos::parallel_for(\"countQuadrants\", points.extent(0), KOKKOS_LAMBDA (const int i) {\n    if(points(i).x > 0 && points(i).y > 0){\n      bins(0) += 1;\n    }\n    else if(points(i).x > 0 && points(i).y < 0){\n      bins(1) += 1;\n    }\n    else if(points(i).x < 0 && points(i).y > 0){\n      bins(2) += 1;\n    }\n    else if(points(i).x < 0 && points(i).y < 0){\n      bins(3) += 1;\n    }\n\n  });\n}",
            "// TODO: Your code goes here\n}",
            "// TODO: Implement this function.\n}",
            "int numQuadrants = 4;\n   Kokkos::parallel_for(numQuadrants, KOKKOS_LAMBDA(const int& i) {\n      double x = (i == 0)? -std::numeric_limits<double>::infinity()\n                          : (i == 1)? 0\n                                     : (i == 2)? std::numeric_limits<double>::infinity()\n                                                : 0;\n      double y = (i == 0)? std::numeric_limits<double>::infinity()\n                          : (i == 1)? 0\n                                     : (i == 2)? -std::numeric_limits<double>::infinity()\n                                                : 0;\n      int count = 0;\n      Kokkos::parallel_reduce(points.size(), KOKKOS_LAMBDA(const int& j, int& l) {\n         if (points(j).x > x && points(j).y > y)\n            ++l;\n      }, count);\n      bins(i) = count;\n   });\n}",
            "// TODO: implement this function\n}",
            "/* TODO: Add the Kokkos parallel_for loop here */\n\n    // TODO: Check if the number of points is divisible by 4\n\n    // TODO: Initialize bins to zero\n\n    // TODO: Declare a functor that has a member variable that contains the vector of points\n\n    // TODO: Declare a lambda function that returns whether a point is in the lower left quadrant or not\n\n    // TODO: Declare a Kokkos parallel_for loop that loops over the points and uses the lambda function to count the number of points in each quadrant\n\n    // TODO: Loop over the bins and print out the number of points in each quadrant\n\n}",
            "}",
            "// Implement this function\n\n  // Use the Kokkos::Experimental::reduce_each_view to sum the counts in each quadrant.\n  // Remember that a quadrant is defined by two Cartesian coordinates\n  //     x >= 0, y >= 0\n  //     x <  0, y <  0\n  //     x >= 0, y <  0\n  //     x <  0, y >= 0\n  // In the implementation, you will need to use Kokkos::Experimental::create_mirror_view to create a mirror view of bins.\n\n  // Hint: Use Kokkos::Impl::is_even_thread() to check if the thread is in an even or odd quadrant.\n  //       Use Kokkos::Impl::get_team_shift() to get the offset of the thread's quadrant.\n  //       Use Kokkos::Impl::get_thread_team_sub() to get the thread's id in the quadrant.\n\n  // Once you have the count for each quadrant, you can reduce over the threads in the team to get the total count\n}",
            "const int n = points.extent_int(0);\n\n    Kokkos::RangePolicy<Kokkos::Serial> policy(0, n);\n    Kokkos::parallel_for(\"CountQuadrants\", policy, KOKKOS_LAMBDA(int i) {\n        auto &p = points[i];\n        if (p.x >= 0 && p.y >= 0) bins(0)++;\n        else if (p.x <= 0 && p.y >= 0) bins(1)++;\n        else if (p.x <= 0 && p.y <= 0) bins(2)++;\n        else if (p.x >= 0 && p.y <= 0) bins(3)++;\n    });\n\n    Kokkos::deep_copy(bins, bins_host);\n}",
            "// Fill in code here.\n}",
            "Kokkos::parallel_for(\"CountQuadrants\", points.size(), KOKKOS_LAMBDA(int i) {\n      auto x = points(i).x;\n      auto y = points(i).y;\n      if (x >= 0.0 && y >= 0.0)\n         ++bins[0];\n      else if (x < 0.0 && y >= 0.0)\n         ++bins[1];\n      else if (x < 0.0 && y < 0.0)\n         ++bins[2];\n      else if (x >= 0.0 && y < 0.0)\n         ++bins[3];\n   });\n}",
            "}",
            "// TODO: your code here\n}",
            "// TODO: Implement me\n}",
            "// Implement this function\n   return;\n}",
            "}",
            "constexpr int N = 4;\n   Kokkos::parallel_for(Kokkos::RangePolicy<>(0, N),\n                        KOKKOS_LAMBDA(int i) { bins[i] = 0; });\n   // TODO: Your code here.\n\n\n   Kokkos::parallel_for(Kokkos::RangePolicy<>(0, points.size()),\n                        KOKKOS_LAMBDA(int i) {\n         // TODO: Your code here.\n         //printf(\"Point = %f %f\\n\",points[i].x,points[i].y);\n         //printf(\"countQuadrants point x,y = %f, %f\\n\",points[i].x,points[i].y);\n\n         int q = 0;\n         if (points[i].x > 0 && points[i].y > 0) {\n            q = 0;\n         }\n         else if (points[i].x > 0 && points[i].y < 0) {\n            q = 1;\n         }\n         else if (points[i].x < 0 && points[i].y > 0) {\n            q = 2;\n         }\n         else if (points[i].x < 0 && points[i].y < 0) {\n            q = 3;\n         }\n\n         Kokkos::atomic_increment(&bins(q));\n         //printf(\"countQuadrants bins = %d\\n\",bins(q));\n      });\n   //printf(\"points = %f %f\\n\",points[0].x,points[0].y);\n}",
            "// TODO\n}",
            "// TODO: Fill in the implementation\n    // Hint: Think about how you might implement this using OpenMP or CUDA\n\n    Kokkos::parallel_for(\"quadrant counter\", \n        Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static>>(0, points.size()),\n        KOKKOS_LAMBDA(const int i) {\n            if (points(i).x > 0.0 && points(i).y > 0.0) {\n                bins(0) += 1;\n            } else if (points(i).x < 0.0 && points(i).y > 0.0) {\n                bins(1) += 1;\n            } else if (points(i).x < 0.0 && points(i).y < 0.0) {\n                bins(2) += 1;\n            } else {\n                bins(3) += 1;\n            }\n        }\n    );\n\n    Kokkos::fence();\n}",
            "// TODO: Write your code here\n    auto count_quadrant = [](double x, double y) {\n        if (x >= 0 && y >= 0) return 0;\n        else if (x >= 0 && y <= 0) return 1;\n        else if (x <= 0 && y >= 0) return 2;\n        else return 3;\n    };\n    // TODO: Call the parallel_reduce function on points with the reduce functor count_quadrant\n    Kokkos::parallel_reduce(\"\", 0, 1, [&](size_t, size_t) {\n        size_t res = 0;\n        for (int i = 0; i < points.size(); ++i) {\n            res += count_quadrant(points(i).x, points(i).y);\n        }\n        bins(0) += res;\n        bins(1) += res;\n        bins(2) += res;\n        bins(3) += res;\n    });\n}",
            "auto quadrant = [](Point p) {\n        if (p.x > 0) {\n            if (p.y > 0) {\n                return 1;\n            } else {\n                return 4;\n            }\n        } else {\n            if (p.y > 0) {\n                return 2;\n            } else {\n                return 3;\n            }\n        }\n    };\n\n    //TODO\n    //count quadrants with Kokkos\n}",
            "// TODO: Your code goes here.\n    return;\n}",
            "// YOUR CODE HERE\n   Kokkos::parallel_for(points.size(),[&] (int i) {\n       if(points(i).x>0 && points(i).y>0){\n           bins(0)++;\n       } else if(points(i).x<0 && points(i).y>0){\n           bins(1)++;\n       } else if(points(i).x<0 && points(i).y<0){\n           bins(2)++;\n       } else if(points(i).x>0 && points(i).y<0){\n           bins(3)++;\n       }\n   });\n}",
            "// TODO:\n    // Your code here\n    return;\n}",
            "Kokkos::RangePolicy rp(0, bins.size());\n    Kokkos::parallel_for(rp, [=](int i){\n        for(int j=0; j<points.size(); j++){\n            if(points[j].x >= 0 && points[j].y >= 0){\n                bins(i)++;\n            }\n            else if(points[j].x < 0 && points[j].y >= 0){\n                bins(i)++;\n            }\n            else if(points[j].x >= 0 && points[j].y < 0){\n                bins(i)++;\n            }\n            else if(points[j].x < 0 && points[j].y < 0){\n                bins(i)++;\n            }\n        }\n    });\n    Kokkos::fence();\n}",
            "}",
            "// Your code here.\n}",
            "// TODO: Your code here\n}",
            "auto count = [&] (const Point& p, size_t i) {\n      bins[((p.x >= 0) * (p.y >= 0)) + 1]++;\n      return 0;\n   };\n   Kokkos::parallel_reduce(\"\", points.size(), count);\n}",
            "auto n_points = points.size();\n    const int num_threads = Kokkos::HPX.concurrency();\n    Kokkos::View<size_t*> count(Kokkos::view_alloc(Kokkos::WithoutInitializing, \"count\"), 4);\n    Kokkos::parallel_for(\"count-points\", num_threads, KOKKOS_LAMBDA(const int &i) {\n        const int tid = i;\n        size_t count_tid = 0;\n        for (size_t j = tid; j < n_points; j += num_threads) {\n            const double x = points(j).x;\n            const double y = points(j).y;\n            if (x > 0 && y > 0) {\n                count_tid++;\n            } else if (x < 0 && y > 0) {\n                count_tid++;\n            } else if (x < 0 && y < 0) {\n                count_tid++;\n            } else if (x > 0 && y < 0) {\n                count_tid++;\n            }\n        }\n        count(i) = count_tid;\n    });\n    Kokkos::deep_copy(bins, count);\n}",
            "}",
            "// Your code goes here\n   Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static> > range_policy(0, points.extent(0));\n   Kokkos::parallel_for(\"countQuadrants\", range_policy, KOKKOS_LAMBDA(const int i) {\n      Point pt = points(i);\n      if (pt.x >= 0)\n      {\n         if (pt.y >= 0)\n         {\n            bins(0) += 1;\n         }\n         else\n         {\n            bins(2) += 1;\n         }\n      }\n      else\n      {\n         if (pt.y >= 0)\n         {\n            bins(3) += 1;\n         }\n         else\n         {\n            bins(1) += 1;\n         }\n      }\n   });\n}",
            "// TODO: Fill in the blanks\n\n}",
            "//TODO: fill in your code here\n\n}",
            "// TODO\n   return;\n}",
            "// Your code here\n}",
            "//TODO: Your code here\n    const double x = points[0].x;\n    const double y = points[0].y;\n    const int quadrant_x = (x > 0)? 0 : ((x < 0)? 1 : 2);\n    const int quadrant_y = (y > 0)? 0 : ((y < 0)? 1 : 2);\n    for(size_t i = 0; i < points.extent(0); i++){\n      const int x = (points[i].x > 0)? 0 : ((points[i].x < 0)? 1 : 2);\n      const int y = (points[i].y > 0)? 0 : ((points[i].y < 0)? 1 : 2);\n      bins(x+quadrant_x, y+quadrant_y)++;\n    }\n}",
            "Kokkos::RangePolicy<> policy(0, points.size());\n\n    Kokkos::parallel_for(\"PointCounter\", policy, KOKKOS_LAMBDA (int i) {\n\n        auto p = points(i);\n        if (p.x < 0 && p.y < 0) {\n            bins[0]++;\n        }\n        else if (p.x < 0 && p.y >= 0) {\n            bins[1]++;\n        }\n        else if (p.x >= 0 && p.y < 0) {\n            bins[2]++;\n        }\n        else if (p.x >= 0 && p.y >= 0) {\n            bins[3]++;\n        }\n\n    });\n\n    //Kokkos::fence();\n\n    //if (Kokkos::impl::kk_unlikely(Kokkos::abort_on_error)) {\n    //    throw_runtime_exception(\"Error while executing PointCounter.\");\n    //}\n\n}",
            "// TODO: Implement this function\n}",
            "auto x_min = points.data()[0].x;\n   auto x_max = points.data()[0].x;\n   auto y_min = points.data()[0].y;\n   auto y_max = points.data()[0].y;\n   // find min max values for x and y\n   for (int i = 1; i < points.size(); i++) {\n      if (x_min > points.data()[i].x) x_min = points.data()[i].x;\n      if (x_max < points.data()[i].x) x_max = points.data()[i].x;\n      if (y_min > points.data()[i].y) y_min = points.data()[i].y;\n      if (y_max < points.data()[i].y) y_max = points.data()[i].y;\n   }\n   // find quadrant each point is in\n   for (int i = 0; i < points.size(); i++) {\n      if (points.data()[i].x >= 0 && points.data()[i].x <= x_max && points.data()[i].y >= 0 && points.data()[i].y <= y_max) {\n         // quadrant 1\n         bins[0]++;\n      } else if (points.data()[i].x < 0 && points.data()[i].x >= x_min && points.data()[i].y >= 0 && points.data()[i].y <= y_max) {\n         // quadrant 2\n         bins[1]++;\n      } else if (points.data()[i].x < 0 && points.data()[i].x >= x_min && points.data()[i].y < 0 && points.data()[i].y >= y_min) {\n         // quadrant 3\n         bins[2]++;\n      } else if (points.data()[i].x >= 0 && points.data()[i].x <= x_max && points.data()[i].y < 0 && points.data()[i].y >= y_min) {\n         // quadrant 4\n         bins[3]++;\n      }\n   }\n}",
            "int numThreads = 100; // number of threads in each team (per device)\n\n   Kokkos::parallel_for(numThreads, [&] (int i) {\n      if(points(i).x >= 0) {\n         if(points(i).y >= 0) {\n            bins(0) += 1;\n         } else {\n            bins(3) += 1;\n         }\n      } else {\n         if(points(i).y >= 0) {\n            bins(1) += 1;\n         } else {\n            bins(2) += 1;\n         }\n      }\n   });\n}",
            "}",
            "// Initialize `bins` to 0.\n    // This sets the `bins` to a zero-initialized vector of 4 `size_t`s.\n    // You may initialize `bins` here, or later in the function.\n    // bins = 0;\n\n    // Loop over all points\n    // TODO: Count the points in each quadrant using Kokkos\n    // TODO: Store the results in the view `bins`\n    //\n    // Example:\n    //    if (points[i].x > 0) {\n    //       if (points[i].y > 0) {\n    //           bins[0] += 1;\n    //       }\n    //       else {\n    //           bins[1] += 1;\n    //       }\n    //   }\n    //   else {\n    //       if (points[i].y > 0) {\n    //           bins[2] += 1;\n    //       }\n    //       else {\n    //           bins[3] += 1;\n    //       }\n    //   }\n    Kokkos::parallel_for(\"countQuadrants\", Kokkos::RangePolicy<Kokkos::Serial>(0, points.extent(0)), [&](int i){\n        if (points[i].x > 0) {\n           if (points[i].y > 0) {\n               bins[0] += 1;\n           }\n           else {\n               bins[1] += 1;\n           }\n       }\n       else {\n           if (points[i].y > 0) {\n               bins[2] += 1;\n           }\n           else {\n               bins[3] += 1;\n           }\n       }\n    });\n\n    //\n    // End of solution\n    //\n}",
            "const size_t N = points.size();\n\n    // TODO\n\n    // TODO\n}",
            "// TODO\n}",
            "auto x_points = points; //create view to work on points, can be done any way\n    bins[0] = 0; bins[1] = 0; bins[2] = 0; bins[3] = 0;\n    //auto n_points = x_points.size();\n    //auto points_host = Kokkos::create_mirror_view(x_points);\n    //Kokkos::deep_copy(points_host,x_points);\n    for(auto i=0; i<x_points.size(); i++){\n        auto point_host = x_points[i];\n        double x = point_host.x;\n        double y = point_host.y;\n        if(x >= 0 && y >= 0){\n            bins[0]++;\n        }\n        else if(x <= 0 && y >= 0){\n            bins[1]++;\n        }\n        else if(x <= 0 && y <= 0){\n            bins[2]++;\n        }\n        else{\n            bins[3]++;\n        }\n    }\n    Kokkos::deep_copy(x_points,points);\n}",
            "// Your code here\n}",
            "// Your code here\n}",
            "}",
            "// TODO: fill in the Kokkos function\n    // Hint: the Kokkos lambda expression is `KOKKOS_LAMBDA(const int& i) {... }\n}",
            "// TODO: Your code here.\n    // Hint: This may be easier if you convert the Point objects into a Kokkos view.\n    //       You may find a function in Kokkos called Kokkos::make_pair_view\n}",
            "// Your code goes here\n}",
            "}",
            "Kokkos::parallel_for(points.size(), KOKKOS_LAMBDA(int i) {\n        if (points(i).x > 0) {\n            if (points(i).y > 0) {\n                ++bins(0);\n            } else {\n                ++bins(1);\n            }\n        } else {\n            if (points(i).y > 0) {\n                ++bins(2);\n            } else {\n                ++bins(3);\n            }\n        }\n    });\n}",
            "const auto size = points.extent(0);\n    Kokkos::parallel_for(\"count_quadrants\", size, KOKKOS_LAMBDA(const int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        if (x >= 0 && y >= 0) {\n            ++bins(0);\n        } else if (x < 0 && y >= 0) {\n            ++bins(1);\n        } else if (x < 0 && y < 0) {\n            ++bins(2);\n        } else {\n            ++bins(3);\n        }\n    });\n}",
            "// YOUR CODE HERE\n  // TODO: Implement Kokkos parallel for loop to count points in each quadrant\n  // Hint: Use Kokkos::Experimental::HPX to run in parallel\n  // Hint: Use Kokkos::Experimental::HPX::ParallelReduce to return partial results\n  // Hint: Use Kokkos::Experimental::HPX::ParallelReduce to sum bins[0:3]\n  // Hint: The reduce operator is in the namespace Kokkos::Experimental::HPX::Functor\n  // Hint: The reduction tag is in the namespace Kokkos::Experimental::HPX::Tag\n  Kokkos::Experimental::HPX::ParallelReduce<Kokkos::Experimental::HPX::Tag::reduce<void, Kokkos::Experimental::HPX::Functor::sum<size_t>, size_t>>(bins, points.size(), 0, [=](const int& i, size_t &total) {\n    if (points[i].x < 0) {\n      if (points[i].y < 0) {\n        total++;\n      }\n    } else {\n      if (points[i].y > 0) {\n        total++;\n      }\n    }\n  });\n}",
            "/* YOUR CODE HERE */\n    return;\n}",
            "Kokkos::parallel_for(\"Count Quadrants\", Kokkos::RangePolicy<>(0, points.size()), KOKKOS_LAMBDA(int i) {\n        Point &point = points(i);\n        if (point.x >= 0 && point.y >= 0) {\n            ++bins(0);\n        }\n        if (point.x < 0 && point.y < 0) {\n            ++bins(3);\n        }\n        if (point.x > 0 && point.y > 0) {\n            ++bins(1);\n        }\n        if (point.x < 0 && point.y > 0) {\n            ++bins(2);\n        }\n    });\n}",
            "constexpr auto N = 4;\n   // TODO: count number of points in each quadrant\n   size_t bins_host[4] = {0};\n   for (auto p: points) {\n      if (p.x >= 0 && p.y >= 0)\n         bins_host[0]++;\n      else if (p.x < 0 && p.y >= 0)\n         bins_host[1]++;\n      else if (p.x < 0 && p.y < 0)\n         bins_host[2]++;\n      else\n         bins_host[3]++;\n   }\n\n   // Transfer to device\n   Kokkos::deep_copy(bins, bins_host);\n}",
            "//... Your code here...\n}",
            "for(auto i = 0; i < points.size(); i++) {\n        if (points(i).x >= 0 && points(i).y >= 0)\n            bins(0) += 1;\n        else if (points(i).x >= 0 && points(i).y < 0)\n            bins(1) += 1;\n        else if (points(i).x < 0 && points(i).y >= 0)\n            bins(2) += 1;\n        else if (points(i).x < 0 && points(i).y < 0)\n            bins(3) += 1;\n    }\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement this function\n}",
            "const int n = points.size();\n    constexpr int num_bins = 4;\n\n    Kokkos::parallel_for(\"countQuadrants\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, n),\n            [&](int i) {\n                // TODO: Implement\n                int x = points(i).x;\n                int y = points(i).y;\n                if (x > 0 && y > 0) {\n                    bins(0)++;\n                } else if (x < 0 && y > 0) {\n                    bins(1)++;\n                } else if (x < 0 && y < 0) {\n                    bins(2)++;\n                } else if (x > 0 && y < 0) {\n                    bins(3)++;\n                }\n            });\n}",
            "//... your code here...\n}",
            "auto quadrant = Kokkos::Experimental::create_mirror_view(bins);\n    auto points_kokkos = Kokkos::create_mirror_view(points);\n    for (int i = 0; i < points.size(); i++) {\n        points_kokkos[i].x = points(i).x;\n        points_kokkos[i].y = points(i).y;\n    }\n\n    Kokkos::parallel_for(\"countQuadrant\", Kokkos::RangePolicy<Kokkos::IndexType>(0, points.size()),\n                         [&](Kokkos::IndexType i) {\n        if (points_kokkos[i].x >= 0 && points_kokkos[i].y >= 0) {\n            quadrant[0]++;\n        } else if (points_kokkos[i].x < 0 && points_kokkos[i].y >= 0) {\n            quadrant[1]++;\n        } else if (points_kokkos[i].x < 0 && points_kokkos[i].y < 0) {\n            quadrant[2]++;\n        } else if (points_kokkos[i].x >= 0 && points_kokkos[i].y < 0) {\n            quadrant[3]++;\n        }\n    });\n    Kokkos::deep_copy(bins, quadrant);\n}",
            "/* TODO: complete this function */\n}",
            "}",
            "// TODO: Implement me\n}",
            "int n = points.size();\n   Kokkos::RangePolicy<> policy(0, n);\n   Kokkos::parallel_for(\"CountQuadrants\", policy, [=](int i) {\n      Point p = points(i);\n      double qx = p.x / 4.0;\n      double qy = p.y / 2.0;\n      int x = (qx > 0.5)? 2 : (qx < -0.5)? 0 : 1;\n      int y = (qy > 0.5)? 1 : (qy < -0.5)? 0 : 0;\n      bins(x + 2 * y) += 1;\n   });\n}",
            "// Your code here\n}",
            "// TODO\n  // hint: use a parallel_for with an index range\n  // hint: create a view of the points as a 2d View, so that you can use `points(i,j)`\n  // hint: the quadrants are numbered as follows:\n  //       0: [x0, +inf), [y0, +inf)\n  //       1: (-inf, x1], [y0, +inf)\n  //       2: (-inf, x1], (-inf, y1]\n  //       3: [x0, +inf), (-inf, y1]\n  //       where x0 and y0 are the minimum values of x and y respectively.\n}",
            "// TODO: write your solution here\n}",
            "// TODO: Implement\n}",
            "// TODO: complete this function\n   return;\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "auto q = [](const Point p) {\n      if (p.x > 0.0 && p.y > 0.0)\n         return 0;\n      else if (p.x < 0.0 && p.y > 0.0)\n         return 1;\n      else if (p.x < 0.0 && p.y < 0.0)\n         return 2;\n      else\n         return 3;\n   };\n\n   auto count = [](const size_t a, const size_t b) { return a+b; };\n   auto reduce = [](const size_t a, const size_t b) { return count(a,b); };\n\n   Kokkos::parallel_scan(points.extent(0),\n      KOKKOS_LAMBDA (const size_t& i, size_t& value, const bool final) {\n         if (final)\n            value = q(points(i));\n      }, bins, reduce);\n}",
            "// TODO\n\n}",
            "// Implementation goes here.\n}",
            "}",
            "// TODO: your code here\n}",
            "}",
            "const int n = points.extent(0);\n    Kokkos::parallel_for(\"countQuadrants\", n, [&](int i) {\n        size_t& bin = bins(points(i).x >= 0? 0 : 1);\n        bin++;\n        bin = points(i).y >= 0? bin + 1 : bin + 2;\n    });\n}",
            "// TODO: implement me\n    // You need to first count the number of points in each quadrant.\n    // For example:\n    // 1.5 in quadrant 1, 0.1 in quadrant 2\n    //...\n    // 3 in quadrant 3, -7 in quadrant 1\n    //\n    // Then you need to use a parallel reduction to sum up the counts in each quadrant\n    // so that the output is [3, 1, 0, 2]\n    //\n    // Hint: you can do a parallel reduction with Kokkos::sum and Kokkos::TeamPolicy\n\n    // 1. Initialize the count of each quadrant.\n    // 2. Loop over all points to count in quadrants\n    // 3. Sum up the counts\n\n    // The following code is just to test your result.\n    int testData[6] = {1,2,3,1,2,3};\n    Kokkos::View<const int*> testPoints(\"TestPoints\", testData, 6);\n    Kokkos::View<int[4]> testBins(\"TestBins\");\n    Kokkos::deep_copy(testBins, testPoints);\n    // Run the test\n    countQuadrants(points, testBins);\n    // Check the result\n    int testResult[4] = {3, 1, 0, 2};\n    for(int i = 0; i < 4; i++){\n        if(testBins(i)!= testResult[i]){\n            std::cout << \"Test failed\" << std::endl;\n            break;\n        }\n    }\n\n}",
            "// TODO\n}",
            "}",
            "}",
            "//...\n}",
            "//...\n}",
            "// TODO: Your code here\n}",
            "const int numPoints = points.size();\n   const size_t numBins = bins.size();\n\n   // TODO: compute the number of points in each quadrant in parallel using Kokkos\n   for (int p = 0; p < numPoints; ++p) {\n      Point point = points[p];\n      if (point.x > 0 && point.y > 0) {\n         bins[0] += 1;\n      } else if (point.x < 0 && point.y > 0) {\n         bins[1] += 1;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2] += 1;\n      } else {\n         bins[3] += 1;\n      }\n   }\n}",
            "auto quadrant = [](double x, double y) {\n        if (x < 0) {\n            if (y > 0) {\n                return 0;\n            } else {\n                return 3;\n            }\n        } else {\n            if (y > 0) {\n                return 1;\n            } else {\n                return 2;\n            }\n        }\n    };\n\n    //...\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<>(0, points.extent(0)),\n                         [&](const int i) {\n                             bins(points(i).x >= 0? 0 : 1)++;\n                             bins(points(i).y >= 0? 2 : 3)++;\n                         });\n}",
            "// TODO: implement this function\n\n}",
            "}",
            "}",
            "}",
            "// TODO\n}",
            "int num_points = points.size();\n    int num_bins = bins.size();\n    for (int i=0; i<num_points; i++) {\n        Point point = points[i];\n        if (point.x > 0 && point.y > 0) {\n            bins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            bins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            bins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace> range(0, points.extent(0));\n\n    Kokkos::parallel_for(\"countQuadrants\", range, KOKKOS_LAMBDA(const size_t& i) {\n        const double x = points[i].x;\n        const double y = points[i].y;\n\n        if (x >= 0 && y >= 0) {\n            bins(0)++;\n        } else if (x < 0 && y >= 0) {\n            bins(1)++;\n        } else if (x < 0 && y < 0) {\n            bins(2)++;\n        } else {\n            bins(3)++;\n        }\n    });\n}",
            "}",
            "//...\n}",
            "}",
            "// TODO\n}",
            "// Fill this in!\n}",
            "}",
            "// TODO: fill in the body\n}",
            "Kokkos::RangePolicy<Kokkos::Serial> policy(0, points.size());\n    Kokkos::parallel_for(policy, [&] (int i) {\n        if (points[i].x < 0) {\n            if (points[i].y < 0) {\n                bins[0]++;\n            } else {\n                bins[1]++;\n            }\n        } else {\n            if (points[i].y < 0) {\n                bins[2]++;\n            } else {\n                bins[3]++;\n            }\n        }\n    });\n}",
            "// TODO\n}",
            "using execution_space = Kokkos::DefaultExecutionSpace;\n\n    // Your code here\n    // Remember to define the execution space (execution_space)\n    // Remember to fill in the bins vector with the correct values\n\n}",
            "}",
            "using Point = Point;\n   using Scalar = double;\n   using Quadrant = size_t;\n\n   using PointsView = Kokkos::View<Point*, Kokkos::HostSpace, Kokkos::MemoryTraits<Kokkos::Unmanaged>>;\n   using BinsView = Kokkos::View<Scalar*, Kokkos::HostSpace, Kokkos::MemoryTraits<Kokkos::Unmanaged>>;\n\n   auto pointsView = PointsView(points.data(), points.size());\n   auto binsView = BinsView(bins.data(), 4);\n\n   auto quadrant = [](Point p) {\n      if (p.x > 0.0 && p.y > 0.0) return Quadrant(0);\n      if (p.x < 0.0 && p.y > 0.0) return Quadrant(1);\n      if (p.x < 0.0 && p.y < 0.0) return Quadrant(2);\n      if (p.x > 0.0 && p.y < 0.0) return Quadrant(3);\n   };\n\n   Kokkos::parallel_reduce(\"count_quadrants\", pointsView.size(), KOKKOS_LAMBDA(const int i, size_t& count) {\n      auto p = pointsView[i];\n      count += 1;\n      if (quadrant(p) < 4) binsView[quadrant(p)] += 1;\n   }, Kokkos::Sum<size_t>(binsView));\n}",
            "// TODO: Your code here.\n}",
            "// TODO: Your code here\n}",
            "// TODO: implement me\n}",
            "Kokkos::parallel_for(points.size(), KOKKOS_LAMBDA(const int i) {\n    const Point& point = points(i);\n    if (point.x > 0) {\n      if (point.y > 0) {\n        bins(0)++;\n      } else {\n        bins(3)++;\n      }\n    } else {\n      if (point.y > 0) {\n        bins(1)++;\n      } else {\n        bins(2)++;\n      }\n    }\n  });\n}",
            "// TODO\n}",
            "}",
            "// TODO: implement\n}",
            "//...\n}",
            "// TODO: Implement this function\n}",
            "//TODO: implement this\n}",
            "const size_t numPoints = points.size();\n    Kokkos::RangePolicy policy(0, numPoints);\n\n    // TODO: implement parallel for loop\n\n    // TODO: fill in the correct values in the bins\n\n    Kokkos::deep_copy(bins, counts);\n}",
            "using execution_space = typename Point::execution_space;\n    size_t n = points.size();\n    // TODO 1: Allocate bins\n\n    // TODO 2: Compute the quadrant index for each point. The quadrant index is an integer in [0, 4)\n\n    // TODO 3: Increment the count for each quadrant index\n\n    // TODO 4: Print the counts\n}",
            "// TODO\n}",
            "// TODO\n   // For each point, count into the appropriate bin. \n}",
            "const auto count_points = KOKKOS_LAMBDA (size_t i) {\n    const Point p = points[i];\n    if (p.x >= 0) {\n      if (p.y >= 0) {\n        bins(0)++;\n      }\n      else {\n        bins(3)++;\n      }\n    }\n    else {\n      if (p.y >= 0) {\n        bins(1)++;\n      }\n      else {\n        bins(2)++;\n      }\n    }\n  };\n\n  Kokkos::parallel_for(points.size(), count_points);\n  Kokkos::fence();\n}",
            "// TODO:\n\n}",
            "}",
            "int num_points = points.size();\n\n  // TODO: Implement this function!\n}",
            "}",
            "auto teamPolicy = Kokkos::TeamPolicy<>(points.size(), 1024);\n    Kokkos::parallel_for(teamPolicy, [&](const Kokkos::TeamThreadRange<int> &teamRange) {\n        const int teamId = teamRange.team_rank();\n        const int teamSize = teamRange.team_size();\n\n        for (int i = teamId; i < points.size(); i += teamSize) {\n            int index = 0;\n\n            if (points[i].x >= 0) {\n                index += 1;\n            }\n\n            if (points[i].x >= 0) {\n                index += 2;\n            }\n\n            Kokkos::atomic_add(&bins(index), 1);\n        }\n    });\n}",
            "size_t totalPoints = points.size();\n    bins = 0;\n    Kokkos::parallel_for(\"countQuadrants\", totalPoints, KOKKOS_LAMBDA(int i) {\n        const double x = points(i).x;\n        const double y = points(i).y;\n        if(x > 0 && y > 0)\n            bins[0]++;\n        else if(x > 0 && y < 0)\n            bins[3]++;\n        else if(x < 0 && y < 0)\n            bins[2]++;\n        else\n            bins[1]++;\n    });\n}",
            "size_t N = points.size();\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(size_t i) {\n        const auto& p = points[i];\n        if (p.x > 0) {\n            if (p.y > 0) {\n                bins(0)++;\n            } else {\n                bins(3)++;\n            }\n        } else {\n            if (p.y > 0) {\n                bins(1)++;\n            } else {\n                bins(2)++;\n            }\n        }\n    });\n}",
            "// TODO: Implement\n    return;\n}",
            "Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static> > policy(0, points.size());\n   // TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO: implement this function\n    // Note: do not allocate memory for bins or make copies of points\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace> rangePolicy(0, points.extent(0));\n    Kokkos::parallel_for(rangePolicy, [=](const int idx) {\n        if (points(idx).x >= 0 && points(idx).y >= 0) {\n            bins[0] += 1;\n        } else if (points(idx).x < 0 && points(idx).y >= 0) {\n            bins[1] += 1;\n        } else if (points(idx).x < 0 && points(idx).y < 0) {\n            bins[2] += 1;\n        } else if (points(idx).x >= 0 && points(idx).y < 0) {\n            bins[3] += 1;\n        }\n    });\n}",
            "// TODO: Your code here\n   // HINT: consider using a parallel_for to loop over the points\n   // HINT: consider using a if statement to test which quadrant a point belongs in\n   // HINT: consider using a reduction to sum the points in each quadrant\n\n\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO: implement this function\n}",
            "Kokkos::RangePolicy<Kokkos::Serial> policy(0, points.size());\n    Kokkos::parallel_for(policy, [=](int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        size_t& bin = bins[0];\n        if (x > 0) {\n            bin += 1;\n        } else {\n            bin = 0;\n        }\n        if (y > 0) {\n            bin = 1;\n        } else {\n            bin += 0;\n        }\n    });\n}",
            "// TODO: Implement this!\n}",
            "// TODO\n}",
            "//...\n   // TODO\n   //...\n}",
            "// YOUR CODE HERE\n}",
            "// Write your code here\n}",
            "}",
            "}",
            "// TODO\n}",
            "}",
            "// TODO\n}",
            "}",
            "}",
            "Kokkos::View<Point*, Kokkos::HostSpace> host_points = Kokkos::create_mirror_view(points);\n  Kokkos::deep_copy(host_points, points);\n\n  // initialize the array bins to zero\n  bins = 0;\n\n  for (int i = 0; i < host_points.extent(0); i++)\n  {\n    if (host_points[i].x >= 0.0 && host_points[i].y >= 0.0)\n    {\n      bins(0)++;\n    }\n    else if (host_points[i].x < 0.0 && host_points[i].y < 0.0)\n    {\n      bins(3)++;\n    }\n    else if (host_points[i].x >= 0.0 && host_points[i].y < 0.0)\n    {\n      bins(2)++;\n    }\n    else if (host_points[i].x < 0.0 && host_points[i].y >= 0.0)\n    {\n      bins(1)++;\n    }\n  }\n\n}",
            "Kokkos::parallel_for(\"quadrantCounter\", points.size(), KOKKOS_LAMBDA(const size_t idx) {\n    const auto p = points(idx);\n    if(p.x >= 0 && p.y >= 0) {\n      bins(0)++;\n    } else if(p.x < 0 && p.y >= 0) {\n      bins(1)++;\n    } else if(p.x < 0 && p.y < 0) {\n      bins(2)++;\n    } else if(p.x >= 0 && p.y < 0) {\n      bins(3)++;\n    }\n  });\n}",
            "// TODO: Your code goes here.\n}",
            "auto countQuadrant = [](const Point& p) {\n    const double x = p.x;\n    const double y = p.y;\n    if (x >= 0 && y >= 0) {\n      return 0;\n    }\n    else if (x < 0 && y >= 0) {\n      return 1;\n    }\n    else if (x < 0 && y < 0) {\n      return 2;\n    }\n    else {\n      return 3;\n    }\n  };\n  // TODO: Your code here\n  bins = Kokkos::create_mirror_view(bins);\n  Kokkos::parallel_for(Kokkos::RangePolicy<>(0, points.size()), KOKKOS_LAMBDA(int i) {\n    bins(countQuadrant(points(i))) += 1;\n  });\n}",
            "// TODO\n    auto quadrant_idx = [](double x, double y) -> size_t {\n        if (x > 0 && y > 0) { return 0; }\n        if (x > 0 && y < 0) { return 1; }\n        if (x < 0 && y > 0) { return 2; }\n        return 3;\n    };\n\n    const int count = points.size();\n\n    Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace> policy(0, count);\n    Kokkos::parallel_for(\n        \"count_quadrants\",\n        policy,\n        KOKKOS_LAMBDA(size_t idx) {\n            const double x = points(idx).x;\n            const double y = points(idx).y;\n            const size_t idx = quadrant_idx(x, y);\n            Kokkos::atomic_fetch_add(&bins(idx), 1);\n        });\n}",
            "constexpr double QUAD_WIDTH = 1.0;\n\n  auto numPoints = points.extent_int(0);\n  Kokkos::parallel_for(\"count_quadrants\",\n                       Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, numPoints),\n                       [=] KOKKOS_INLINE_FUNCTION(const int i) {\n                         const double x = points(i).x;\n                         const double y = points(i).y;\n                         int quadrant = 0;\n                         if (x > 0) quadrant += 1;\n                         if (y > 0) quadrant += 2;\n                         bins(quadrant)++;\n                       });\n}",
            "// TODO: Fill in this function.\n    // Hint: For each `Point` in `points`, determine the index of the quadrant\n    //       (0, 1, 2, or 3) the `Point` belongs in.\n}",
            "// TODO\n}",
            "// Your code here\n}",
            "// Your code goes here\n}",
            "using view_t = Kokkos::View<Point*, Kokkos::HostSpace>;\n    auto view = view_t(points.data(), points.extent(0));\n    //... implement this function\n}",
            "Kokkos::parallel_for(\"countQuadrants\", Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static>>(0, points.extent(0)),\n    KOKKOS_LAMBDA (const int i) {\n        size_t idx = 0;\n        if(points[i].x > 0) idx += 1;\n        if(points[i].y > 0) idx += 2;\n        bins(idx)++;\n    });\n}",
            "/* TODO: Implement this function */\n}",
            "for (int i = 0; i < points.size(); i++) {\n    if (points[i].x > 0.0) {\n      if (points[i].y > 0.0) {\n        bins[0]++;\n      }\n      else {\n        bins[1]++;\n      }\n    }\n    else {\n      if (points[i].y > 0.0) {\n        bins[2]++;\n      }\n      else {\n        bins[3]++;\n      }\n    }\n  }\n}",
            "// TODO: implement this function\n}",
            "// TODO implement the counting function\n\n  auto num = points.size();\n\n  auto quadrant_functor = [=] (const int &i, size_t &quadrant) {\n    auto point = points(i);\n    if (point.x > 0 && point.y > 0)\n      quadrant = 0;\n    else if (point.x < 0 && point.y > 0)\n      quadrant = 1;\n    else if (point.x < 0 && point.y < 0)\n      quadrant = 2;\n    else if (point.x > 0 && point.y < 0)\n      quadrant = 3;\n  };\n\n  Kokkos::parallel_for(\"quadrant_functor\", Kokkos::RangePolicy<Kokkos::Serial>(0, num), quadrant_functor);\n\n  auto add_functor = [=] (const int &i, const int &j, size_t &a, const size_t &b) {\n    a = a + b;\n  };\n\n  Kokkos::parallel_reduce(\"add_functor\", Kokkos::RangePolicy<Kokkos::Serial>(0, 4), add_functor, bins(0), bins(1), bins(2), bins(3));\n}",
            "//TODO: Implement this function.\n\n\n}",
            "// TODO: Your code here\n}",
            "}",
            "const size_t num_points = points.size();\n\n   // TODO: fill this in\n   for (int i = 0; i < 4; i++) {\n      bins(i) = 0;\n   }\n\n   // This is the Kokkos execution space.\n   // If you use an execution space other than this, you may need to change the\n   // Kokkos::parallel_for() calls below.\n   using execution_space = Kokkos::DefaultExecutionSpace;\n\n   // Kokkos parallel_for loop to iterate over the points\n   Kokkos::parallel_for(num_points, KOKKOS_LAMBDA(int j) {\n      int i = 0;\n      if (points(j).x > 0) {\n         i++;\n      }\n      if (points(j).y > 0) {\n         i += 2;\n      }\n      bins(i)++;\n   });\n   // Synchronize to make sure all of the above work is done before the next line executes.\n   Kokkos::fence();\n}",
            "}",
            "//...\n}",
            "auto count = [](const Point& pt) {\n        return pt.x >= 0? (pt.y >= 0? 0 : 1) : (pt.y >= 0? 2 : 3);\n    };\n    auto sum = Kokkos::sum(points, count);\n    bins[0] = sum(0);\n    bins[1] = sum(1);\n    bins[2] = sum(2);\n    bins[3] = sum(3);\n}",
            "}",
            "// TODO: Implement me\n}",
            "}",
            "// TODO:\n   // write code to count in parallel using Kokkos\n}",
            "auto countQuadrant = [&](Point p, size_t index) {\n    if (p.x >= 0 && p.y >= 0)\n      bins[index] += 1;\n  };\n\n  Kokkos::parallel_for(\"quadrants\", points.size(), countQuadrant);\n\n  Kokkos::deep_copy(bins, bins);\n}",
            "const size_t n = points.size();\n    Kokkos::parallel_for(\"countQuadrants\", n, KOKKOS_LAMBDA (int i) {\n        const Point &p = points[i];\n        if (p.x > 0) {\n            if (p.y > 0) {\n                bins(0)++;\n            }\n            else {\n                bins(2)++;\n            }\n        }\n        else {\n            if (p.y > 0) {\n                bins(1)++;\n            }\n            else {\n                bins(3)++;\n            }\n        }\n    });\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use OpenMP to count in parallel.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "//TODO\n}",
            "#pragma omp parallel\n  {\n#pragma omp for\n    for (size_t i = 0; i < points.size(); i++) {\n      if (points[i].x >= 0) {\n        if (points[i].y >= 0) {\n          bins[0]++;\n        }\n        else {\n          bins[3]++;\n        }\n      }\n      else {\n        if (points[i].y >= 0) {\n          bins[1]++;\n        }\n        else {\n          bins[2]++;\n        }\n      }\n    }\n  }\n}",
            "// TODO:\n}",
            "#pragma omp parallel for\n   for(int i = 0; i < 4; i++)\n   {\n      size_t counter = 0;\n      for(auto& p: points) {\n         if(p.x > 0 && p.y > 0) {\n            if(i == 0)\n               counter++;\n         }\n         else if(p.x > 0 && p.y < 0) {\n            if(i == 1)\n               counter++;\n         }\n         else if(p.x < 0 && p.y < 0) {\n            if(i == 2)\n               counter++;\n         }\n         else if(p.x < 0 && p.y > 0) {\n            if(i == 3)\n               counter++;\n         }\n      }\n      bins[i] = counter;\n   }\n}",
            "// parallel for\n#pragma omp parallel for num_threads(omp_get_max_threads())\n   for (size_t i = 0; i < points.size(); ++i) {\n      if (points[i].x > 0.0) {\n         if (points[i].y > 0.0) {\n            ++bins[0];\n         } else {\n            ++bins[3];\n         }\n      } else {\n         if (points[i].y > 0.0) {\n            ++bins[1];\n         } else {\n            ++bins[2];\n         }\n      }\n   }\n}",
            "#pragma omp parallel\n\t{\n\t\t//TODO\n\t}\n}",
            "size_t nbins = 0;\n#pragma omp parallel for reduction(+:nbins)\n    for (int i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            nbins++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            nbins++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            nbins++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            nbins++;\n        }\n    }\n\n    bins[0] = nbins;\n    nbins = 0;\n#pragma omp parallel for reduction(+:nbins)\n    for (int i = 0; i < points.size(); ++i) {\n        if (points[i].x == 0 && points[i].y > 0) {\n            nbins++;\n        }\n    }\n\n    bins[1] = nbins;\n    nbins = 0;\n#pragma omp parallel for reduction(+:nbins)\n    for (int i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y == 0) {\n            nbins++;\n        }\n    }\n\n    bins[2] = nbins;\n    nbins = 0;\n#pragma omp parallel for reduction(+:nbins)\n    for (int i = 0; i < points.size(); ++i) {\n        if (points[i].x < 0 && points[i].y == 0) {\n            nbins++;\n        }\n    }\n\n    bins[3] = nbins;\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for nowait\n        for (int i = 0; i < points.size(); ++i) {\n            if (points[i].x > 0.0 && points[i].y > 0.0) {\n                ++bins[0];\n            } else if (points[i].x < 0.0 && points[i].y > 0.0) {\n                ++bins[1];\n            } else if (points[i].x < 0.0 && points[i].y < 0.0) {\n                ++bins[2];\n            } else {\n                ++bins[3];\n            }\n        }\n    }\n}",
            "size_t len = points.size();\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < len; i++) {\n        Point point = points.at(i);\n\n        if (point.x > 0) {\n            if (point.y > 0) {\n                bins[0]++;\n            }\n            else {\n                bins[3]++;\n            }\n        }\n        else {\n            if (point.y > 0) {\n                bins[1]++;\n            }\n            else {\n                bins[2]++;\n            }\n        }\n    }\n}",
            "bins.fill(0);\n\n    // #pragma omp parallel for\n    for (auto &point : points) {\n        if (point.x > 0 && point.y > 0)\n            bins[0]++;\n        else if (point.x < 0 && point.y > 0)\n            bins[1]++;\n        else if (point.x < 0 && point.y < 0)\n            bins[2]++;\n        else if (point.x > 0 && point.y < 0)\n            bins[3]++;\n    }\n\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            bins[0]++;\n        }\n        else if (points[i].x < 0 && points[i].y > 0) {\n            bins[1]++;\n        }\n        else if (points[i].x < 0 && points[i].y < 0) {\n            bins[2]++;\n        }\n        else if (points[i].x > 0 && points[i].y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "size_t size = points.size();\n\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for(size_t i=0; i<size; i++) {\n      if (points[i].x >= 0) {\n        if (points[i].y >= 0) {\n          bins[0]++;\n        }\n        else {\n          bins[3]++;\n        }\n      }\n      else {\n        if (points[i].y >= 0) {\n          bins[1]++;\n        }\n        else {\n          bins[2]++;\n        }\n      }\n    }\n  }\n}",
            "#pragma omp parallel\n    {\n        int i = omp_get_thread_num();\n        int num_thread = omp_get_num_threads();\n        std::array<size_t, 4> bins_t = {0,0,0,0};\n        #pragma omp for\n        for (int j = 0; j < points.size(); j++){\n            if (points[j].x>0){\n                if (points[j].y>0){\n                    bins_t[0]++;\n                }\n                else if (points[j].y<0){\n                    bins_t[3]++;\n                }\n            }\n            else if (points[j].x<0){\n                if (points[j].y>0){\n                    bins_t[1]++;\n                }\n                else if (points[j].y<0){\n                    bins_t[2]++;\n                }\n            }\n        }\n        #pragma omp critical\n        {\n            for (int k = 0; k < bins_t.size(); k++){\n                bins[k]+=bins_t[k];\n            }\n        }\n    }\n}",
            "// Your code here\n}",
            "constexpr int nthreads = 4;\n   int ix[nthreads];\n   int iy[nthreads];\n   int iz[nthreads];\n\n   #pragma omp parallel for private(iz)\n   for (int i = 0; i < nthreads; i++) {\n      iz[i] = i;\n   }\n\n   #pragma omp parallel for private(ix, iy)\n   for (int i = 0; i < nthreads; i++) {\n      ix[i] = i;\n      iy[i] = i;\n   }\n\n   #pragma omp parallel for reduction(+:bins[0])\n   for (int i = 0; i < nthreads; i++) {\n      if (points[i].x > 0) {\n         if (points[i].y > 0) {\n            bins[iz[i]] += 1;\n         }\n         else {\n            bins[ix[i]] += 1;\n         }\n      }\n      else {\n         if (points[i].y > 0) {\n            bins[iy[i]] += 1;\n         }\n         else {\n            bins[i] += 1;\n         }\n      }\n   }\n}",
            "int nThreads = omp_get_max_threads();\n   size_t chunkSize = points.size() / nThreads;\n   std::vector<size_t> counts(4, 0);\n\n   #pragma omp parallel for reduction(+:counts[0], counts[1], counts[2], counts[3])\n   for(size_t i = 0; i < points.size(); i += chunkSize) {\n      size_t k = 0;\n      for(size_t j = i; j < i + chunkSize && j < points.size(); j++) {\n         Point p = points[j];\n         if(p.x >= 0 && p.y >= 0)\n            k = 0;\n         else if(p.x < 0 && p.y >= 0)\n            k = 1;\n         else if(p.x < 0 && p.y < 0)\n            k = 2;\n         else\n            k = 3;\n\n         counts[k]++;\n      }\n   }\n\n   bins[0] = counts[0];\n   bins[1] = counts[1];\n   bins[2] = counts[2];\n   bins[3] = counts[3];\n}",
            "// Your code here\n#pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        if (points[i].x >= 0 && points[i].x > 0 && points[i].y >= 0 && points[i].y > 0) {\n            bins[0]++;\n        }\n        else if (points[i].x >= 0 && points[i].x > 0 && points[i].y <= 0 && points[i].y < 0) {\n            bins[1]++;\n        }\n        else if (points[i].x <= 0 && points[i].x < 0 && points[i].y >= 0 && points[i].y > 0) {\n            bins[2]++;\n        }\n        else {\n            bins[3]++;\n        }\n    }\n}",
            "//TODO: replace this code with OpenMP code\n    //auto lambda = [](Point p) {\n    //    if (p.x > 0 && p.y > 0) {\n    //        return 0;\n    //    }\n    //    else if (p.x < 0 && p.y > 0) {\n    //        return 1;\n    //    }\n    //    else if (p.x < 0 && p.y < 0) {\n    //        return 2;\n    //    }\n    //    else {\n    //        return 3;\n    //    }\n    //};\n    //for (int i = 0; i < points.size(); ++i) {\n    //    bins[lambda(points[i])] += 1;\n    //}\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            bins[0] += 1;\n        }\n        else if (points[i].x < 0 && points[i].y > 0) {\n            bins[1] += 1;\n        }\n        else if (points[i].x < 0 && points[i].y < 0) {\n            bins[2] += 1;\n        }\n        else {\n            bins[3] += 1;\n        }\n    }\n}",
            "// TODO: Fill this function in.\n}",
            "// TODO: YOUR CODE HERE\n  #pragma omp parallel num_threads(2) shared(points, bins)\n  {\n    #pragma omp for\n    for (int i = 0; i < points.size(); i++) {\n      if (points[i].x > 0 && points[i].y > 0)\n        bins[0]++;\n      if (points[i].x < 0 && points[i].y > 0)\n        bins[1]++;\n      if (points[i].x < 0 && points[i].y < 0)\n        bins[2]++;\n      if (points[i].x > 0 && points[i].y < 0)\n        bins[3]++;\n    }\n  }\n}",
            "#pragma omp parallel for schedule(static)\n    for (int i = 0; i < points.size(); i++) {\n        if (points[i].x >= 0) {\n            if (points[i].y >= 0) {\n                bins[0]++;\n            } else {\n                bins[1]++;\n            }\n        } else {\n            if (points[i].y >= 0) {\n                bins[2]++;\n            } else {\n                bins[3]++;\n            }\n        }\n    }\n}",
            "int n_thr = 4;\n    std::array<size_t, 4> bins_tmp;\n    // #pragma omp parallel for num_threads(n_thr)\n    #pragma omp parallel for\n    for(size_t i = 0; i < points.size(); i++) {\n        Point p = points[i];\n        if (p.x > 0 && p.y > 0) bins_tmp[0] += 1;\n        else if (p.x < 0 && p.y > 0) bins_tmp[1] += 1;\n        else if (p.x < 0 && p.y < 0) bins_tmp[2] += 1;\n        else if (p.x > 0 && p.y < 0) bins_tmp[3] += 1;\n    }\n    #pragma omp critical\n    {\n        for (size_t i = 0; i < 4; i++) {\n            bins[i] += bins_tmp[i];\n        }\n    }\n}",
            "// TODO: Your code here\n  \n  \n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); i++)\n  {\n    if (points[i].x > 0 && points[i].y > 0)\n    {\n      bins[0]++;\n    }\n    else if (points[i].x > 0 && points[i].y < 0)\n    {\n      bins[1]++;\n    }\n    else if (points[i].x < 0 && points[i].y < 0)\n    {\n      bins[2]++;\n    }\n    else if (points[i].x < 0 && points[i].y > 0)\n    {\n      bins[3]++;\n    }\n    \n  }\n  \n}",
            "// TODO: Your code here\n    omp_set_num_threads(4);\n    #pragma omp parallel\n    {\n        int rank = omp_get_thread_num();\n        int num_threads = omp_get_num_threads();\n        std::vector<Point> subset;\n        for (int i = 0; i < points.size(); i++)\n        {\n            if (rank == (points[i].x > 0 && points[i].y > 0))\n                subset.push_back(points[i]);\n        }\n        int subset_size = subset.size();\n        for (int i = 0; i < subset_size; i++)\n        {\n            if (subset[i].x > 0)\n                bins[0]++;\n            else\n                bins[1]++;\n            if (subset[i].y > 0)\n                bins[2]++;\n            else\n                bins[3]++;\n        }\n    }\n}",
            "// Your code here\n  \n  size_t n = points.size();\n  int nthreads;\n  omp_set_num_threads(nthreads);\n\n  for (size_t i = 0; i < points.size(); i++){\n    if (points[i].x >= 0 && points[i].y > 0){\n      bins[0] += 1;\n    }\n    else if (points[i].x <= 0 && points[i].y > 0){\n      bins[1] += 1;\n    }\n    else if (points[i].x < 0 && points[i].y <= 0){\n      bins[2] += 1;\n    }\n    else if (points[i].x > 0 && points[i].y <= 0){\n      bins[3] += 1;\n    }\n  }\n}",
            "#pragma omp parallel\n    {\n        const auto bins_l = omp_get_num_threads();\n        std::vector<size_t> bins_v(bins_l);\n        for (size_t i = 0; i < points.size(); i++) {\n            auto& p = points[i];\n            if (p.x >= 0) {\n                if (p.y >= 0) {\n                    bins_v[omp_get_thread_num()]++;\n                } else {\n                    bins_v[bins_l - 1 - omp_get_thread_num()]++;\n                }\n            } else {\n                if (p.y >= 0) {\n                    bins_v[bins_l - 1 - omp_get_thread_num()]++;\n                } else {\n                    bins_v[omp_get_thread_num()]++;\n                }\n            }\n        }\n        for (size_t i = 0; i < bins_l; i++) {\n            bins[i] += bins_v[i];\n        }\n    }\n}",
            "// your code here\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); i++) {\n        if(points[i].x >= 0 && points[i].y >= 0){\n            bins[0]++;\n        }\n        else if(points[i].x < 0 && points[i].y >= 0){\n            bins[1]++;\n        }\n        else if(points[i].x < 0 && points[i].y < 0){\n            bins[2]++;\n        }\n        else if(points[i].x >= 0 && points[i].y < 0){\n            bins[3]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n   for(size_t i = 0; i < points.size(); ++i) {\n      if (points[i].x >= 0) {\n         if (points[i].y >= 0)\n            bins[0]++;\n         else\n            bins[2]++;\n      }\n      else {\n         if (points[i].y >= 0)\n            bins[1]++;\n         else\n            bins[3]++;\n      }\n   }\n}",
            "const double xMin = -10;\n    const double xMax = 10;\n    const double yMin = -10;\n    const double yMax = 10;\n\n    bins = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i)\n    {\n        double x = points[i].x;\n        double y = points[i].y;\n\n        if (x > xMax || x < xMin || y > yMax || y < yMin)\n        {\n            continue;\n        }\n\n        if (x > 0 && y > 0)\n        {\n            bins[0]++;\n        }\n        else if (x > 0 && y <= 0)\n        {\n            bins[1]++;\n        }\n        else if (x <= 0 && y > 0)\n        {\n            bins[2]++;\n        }\n        else if (x <= 0 && y <= 0)\n        {\n            bins[3]++;\n        }\n    }\n}",
            "int nThreads = omp_get_max_threads();\n    int nChunks = nThreads + points.size();\n    int chunkSize = points.size() / nChunks;\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        int chunk = tid;\n        if (tid >= nChunks) {\n            chunk = nChunks - 1;\n        }\n\n        int begin = chunk * chunkSize;\n        int end = std::min((chunk + 1) * chunkSize, points.size());\n\n        std::vector<Point> points_copy;\n\n        for (int i = begin; i < end; ++i) {\n            points_copy.push_back(points[i]);\n        }\n\n        for (size_t i = 0; i < points_copy.size(); ++i) {\n            double x = points_copy[i].x;\n            double y = points_copy[i].y;\n            if (x >= 0) {\n                if (y >= 0) {\n                    bins[0] += 1;\n                } else {\n                    bins[3] += 1;\n                }\n            } else {\n                if (y >= 0) {\n                    bins[1] += 1;\n                } else {\n                    bins[2] += 1;\n                }\n            }\n        }\n    }\n}",
            "}",
            "const int n = points.size();\n#pragma omp parallel for\n\tfor(int i = 0; i < n; i++){\n\t\tif(points[i].x > 0 && points[i].y > 0){\n\t\t\tbins[0] += 1;\n\t\t} else if(points[i].x < 0 && points[i].y > 0){\n\t\t\tbins[1] += 1;\n\t\t} else if(points[i].x < 0 && points[i].y < 0){\n\t\t\tbins[2] += 1;\n\t\t} else {\n\t\t\tbins[3] += 1;\n\t\t}\n\t}\n}",
            "// YOUR CODE HERE\n}",
            "omp_set_num_threads(4);\n\n#pragma omp parallel for\n    for (int i = 0; i < 4; ++i) {\n        bins[i] = 0;\n        Point p;\n        for (int j = 0; j < points.size(); j++) {\n            p = points[j];\n            if ((i == 0 && p.x >= 0 && p.y >= 0) || (i == 1 && p.x < 0 && p.y >= 0) || (i == 2 && p.x < 0 && p.y < 0) || (i == 3 && p.x >= 0 && p.y < 0)) {\n                ++bins[i];\n            }\n        }\n    }\n}",
            "#pragma omp parallel\n   {\n      #pragma omp for schedule(static)\n      for (size_t i = 0; i < points.size(); i++) {\n         if (points[i].x > 0 && points[i].y > 0) {\n            bins[0]++;\n         } else if (points[i].x < 0 && points[i].y > 0) {\n            bins[1]++;\n         } else if (points[i].x < 0 && points[i].y < 0) {\n            bins[2]++;\n         } else {\n            bins[3]++;\n         }\n      }\n   }\n}",
            "size_t N = points.size();\n\n    for (size_t i = 0; i < N; ++i) {\n        if (points[i].x > 0) {\n            if (points[i].y > 0) {\n                bins[0]++;\n            } else {\n                bins[3]++;\n            }\n        } else {\n            if (points[i].y > 0) {\n                bins[1]++;\n            } else {\n                bins[2]++;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tPoint p = points[i];\n\n\t\tdouble x = p.x;\n\t\tdouble y = p.y;\n\n\t\tif (x >= 0 && y >= 0) {\n\t\t\tbins[0]++;\n\t\t} else if (x >= 0 && y <= 0) {\n\t\t\tbins[1]++;\n\t\t} else if (x <= 0 && y <= 0) {\n\t\t\tbins[2]++;\n\t\t} else if (x <= 0 && y >= 0) {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n\n\n}",
            "//std::array<size_t, 4> bins;\n\t#pragma omp parallel\n\t{\n\t\tint x_bin = 0, y_bin = 0;\n\t\t#pragma omp for\n\t\tfor(int i = 0; i < points.size(); i++) {\n\t\t\tif(points[i].x > 0) {\n\t\t\t\tx_bin = 1;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tx_bin = 0;\n\t\t\t}\n\n\t\t\tif(points[i].y > 0) {\n\t\t\t\ty_bin = 2;\n\t\t\t}\n\t\t\telse {\n\t\t\t\ty_bin = 3;\n\t\t\t}\n\t\t\tbins[x_bin]++;\n\t\t\tbins[y_bin]++;\n\t\t}\n\t}\n}",
            "// TODO: Implement\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i)\n    {\n        if (points[i].x >= 0 && points[i].y >= 0)\n            bins[0]++;\n        else if (points[i].x < 0 && points[i].y < 0)\n            bins[3]++;\n        else if (points[i].x < 0 && points[i].y >= 0)\n            bins[1]++;\n        else if (points[i].x >= 0 && points[i].y < 0)\n            bins[2]++;\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); i++) {\n        if (points[i].x >= 0 && points[i].y >= 0) {\n            bins[0]++;\n        }\n        else if (points[i].x < 0 && points[i].y >= 0) {\n            bins[1]++;\n        }\n        else if (points[i].x < 0 && points[i].y < 0) {\n            bins[2]++;\n        }\n        else if (points[i].x >= 0 && points[i].y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "#pragma omp parallel\n   {\n      int thread_num = omp_get_thread_num();\n      int num_threads = omp_get_num_threads();\n\n      int start = thread_num * points.size() / num_threads;\n      int end = (thread_num + 1) * points.size() / num_threads;\n\n      for(int i = start; i < end; i++) {\n         Point point = points[i];\n\n         if(point.x >= 0 && point.y >= 0) bins[0] += 1;\n         else if(point.x < 0 && point.y >= 0) bins[1] += 1;\n         else if(point.x < 0 && point.y < 0) bins[2] += 1;\n         else bins[3] += 1;\n      }\n   }\n}",
            "#pragma omp parallel\n   {\n      int num_threads = omp_get_num_threads();\n      int thread_num = omp_get_thread_num();\n      int chunk_size = points.size() / num_threads;\n      int begin = chunk_size * thread_num;\n      int end = begin + chunk_size;\n      if (thread_num == num_threads - 1) end = points.size();\n      std::array<size_t, 4> local_bins = { 0, 0, 0, 0 };\n      for (int i = begin; i < end; ++i) {\n         auto point = points[i];\n         if (point.x > 0 && point.y > 0) ++local_bins[0];\n         if (point.x < 0 && point.y > 0) ++local_bins[1];\n         if (point.x < 0 && point.y < 0) ++local_bins[2];\n         if (point.x > 0 && point.y < 0) ++local_bins[3];\n      }\n\n      #pragma omp critical\n      {\n         for (size_t i = 0; i < local_bins.size(); ++i)\n            bins[i] += local_bins[i];\n      }\n   }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); i++) {\n        auto& point = points[i];\n\n        if (point.x > 0) {\n            if (point.y > 0)\n                bins[0]++;\n            else\n                bins[3]++;\n        } else {\n            if (point.y > 0)\n                bins[1]++;\n            else\n                bins[2]++;\n        }\n    }\n}",
            "for(size_t i = 0; i < 4; ++i) {\n    bins[i] = 0;\n  }\n\n  #pragma omp parallel for\n  for(int i = 0; i < points.size(); ++i) {\n    if(points[i].x >= 0 && points[i].y >= 0) {\n      ++bins[0];\n    }\n    else if(points[i].x < 0 && points[i].y >= 0) {\n      ++bins[1];\n    }\n    else if(points[i].x < 0 && points[i].y < 0) {\n      ++bins[2];\n    }\n    else if(points[i].x >= 0 && points[i].y < 0) {\n      ++bins[3];\n    }\n  }\n}",
            "std::array<size_t, 4> temp;\n\ttemp.fill(0);\n#pragma omp parallel for\n\tfor (size_t i = 0; i < points.size(); i++)\n\t{\n\t\tdouble x = points[i].x;\n\t\tdouble y = points[i].y;\n\n\t\tif (x >= 0 && y >= 0)\n\t\t{\n\t\t\ttemp[0]++;\n\t\t}\n\t\telse if (x <= 0 && y >= 0)\n\t\t{\n\t\t\ttemp[1]++;\n\t\t}\n\t\telse if (x <= 0 && y <= 0)\n\t\t{\n\t\t\ttemp[2]++;\n\t\t}\n\t\telse\n\t\t{\n\t\t\ttemp[3]++;\n\t\t}\n\t}\n\tbins = temp;\n}",
            "#pragma omp parallel\n    {\n        int thread_id = omp_get_thread_num();\n        int num_threads = omp_get_num_threads();\n\n        // split the work among the threads by chunking the range of x values into num_threads chunks\n        // each thread is assigned a different chunk of x values to consider\n        int chunk_size = points.size()/num_threads;\n        int thread_start = chunk_size*thread_id;\n        int thread_end = std::min(chunk_size*(thread_id+1), points.size());\n\n        // For each thread, iterate through the points in the given chunk of x values\n        // and increment the corresponding counter in bins\n        //\n        // HINT: Use std::lower_bound to find the start of each chunk of x values to increment\n        //       the counter for that chunk.\n        //\n        // HINT: Increment the counter using std::distance().\n        //       e.g. std::distance(std::lower_bound(points.begin(), points.end(), Point{x=-3}), points.end())\n        //\n        // HINT: Use an OpenMP for loop to do this for every thread.\n\n\n\n\n\n\n\n\n\n\n\n\n    }\n}",
            "size_t n = points.size();\n   bins = {0, 0, 0, 0};\n   for (size_t i = 0; i < n; i++) {\n      double x = points[i].x, y = points[i].y;\n      if (x > 0 && y > 0) {\n         bins[0]++;\n      }\n      else if (x < 0 && y > 0) {\n         bins[1]++;\n      }\n      else if (x < 0 && y < 0) {\n         bins[2]++;\n      }\n      else if (x > 0 && y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 4; i++)\n        for (size_t j = 0; j < points.size(); j++)\n            if (points[j].x > 0 && points[j].y > 0)\n                if (i == 0) bins[i] += 1;\n            else if (points[j].x < 0 && points[j].y > 0)\n                if (i == 1) bins[i] += 1;\n            else if (points[j].x < 0 && points[j].y < 0)\n                if (i == 2) bins[i] += 1;\n            else if (points[j].x > 0 && points[j].y < 0)\n                if (i == 3) bins[i] += 1;\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++)\n   {\n      auto &p = points[i];\n      size_t b;\n      if (p.x > 0) {\n         if (p.y > 0) b = 0;\n         else b = 3;\n      } else {\n         if (p.y > 0) b = 1;\n         else b = 2;\n      }\n      #pragma omp atomic\n      bins[b]++;\n   }\n}",
            "bins.fill(0);\n\n    omp_set_num_threads(4);\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        Point pt = points[i];\n        if (pt.x > 0 && pt.y > 0)\n            bins[0]++;\n        else if (pt.x < 0 && pt.y > 0)\n            bins[1]++;\n        else if (pt.x < 0 && pt.y < 0)\n            bins[2]++;\n        else if (pt.x > 0 && pt.y < 0)\n            bins[3]++;\n    }\n\n}",
            "double minX = 0;\n   double maxX = 10;\n   double minY = -10;\n   double maxY = 10;\n   const size_t n = 4;\n   std::array<std::vector<Point>, n> points_quadrant;\n   std::array<size_t, n> count;\n   std::array<double, n> q_min = { minX, minY, maxX, maxY };\n   std::array<double, n> q_max = { maxX, maxY, minX, minY };\n\n   // Partition points into quadrants\n   for(size_t i=0; i<points.size(); i++) {\n      size_t q = 0;\n      for(size_t j=0; j<n; j++) {\n         if(points[i].x > q_min[j] && points[i].x < q_max[j]) {\n            if(points[i].y > q_min[j+n] && points[i].y < q_max[j+n]) {\n               q = j;\n               break;\n            }\n         }\n      }\n      points_quadrant[q].push_back(points[i]);\n   }\n\n   // Count number of points in quadrants\n   for(size_t i=0; i<n; i++) {\n      size_t n_thread = omp_get_max_threads();\n      size_t thread_id = omp_get_thread_num();\n      #pragma omp parallel for schedule(static) shared(points_quadrant, count)\n      for(size_t j=0; j<n; j++) {\n         for(size_t k=0; k<points_quadrant[j].size(); k+=n_thread) {\n            if(k+thread_id < points_quadrant[j].size()) {\n               count[j] += 1;\n            }\n         }\n      }\n   }\n\n   // Store counts in bins\n   for(size_t i=0; i<4; i++) {\n      bins[i] = count[i];\n   }\n}",
            "#pragma omp parallel for num_threads(4)\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            bins[0]++;\n        }\n        else if (points[i].x < 0 && points[i].y > 0) {\n            bins[1]++;\n        }\n        else if (points[i].x < 0 && points[i].y < 0) {\n            bins[2]++;\n        }\n        else if (points[i].x > 0 && points[i].y < 0) {\n            bins[3]++;\n        }\n        else {\n            bins[4]++;\n        }\n    }\n}",
            "// TODO: Fill in this function\n#pragma omp parallel\n\tfor (int i = 0; i < points.size(); i++)\n\t{\n\t\tPoint point = points[i];\n\t\tif (point.x >= 0 && point.y >= 0)\n\t\t{\n\t\t\tbins[0]++;\n\t\t}\n\t\telse if (point.x < 0 && point.y >= 0)\n\t\t{\n\t\t\tbins[1]++;\n\t\t}\n\t\telse if (point.x < 0 && point.y < 0)\n\t\t{\n\t\t\tbins[2]++;\n\t\t}\n\t\telse if (point.x >= 0 && point.y < 0)\n\t\t{\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "int N = points.size();\n    #pragma omp parallel for\n    for (int i = 0; i < N; i++)\n    {\n        Point p = points[i];\n        if(p.x >= 0 && p.y >= 0)\n            bins[0]++;\n        else if(p.x < 0 && p.y >= 0)\n            bins[1]++;\n        else if(p.x < 0 && p.y < 0)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "bins.fill(0);\n    for (auto &point : points) {\n        if (point.x >= 0 && point.y >= 0)\n            bins[0]++;\n        if (point.x < 0 && point.y >= 0)\n            bins[1]++;\n        if (point.x < 0 && point.y < 0)\n            bins[2]++;\n        if (point.x >= 0 && point.y < 0)\n            bins[3]++;\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); i++){\n        bins[0] += points[i].x > 0 && points[i].y > 0;\n        bins[1] += points[i].x < 0 && points[i].y > 0;\n        bins[2] += points[i].x < 0 && points[i].y < 0;\n        bins[3] += points[i].x > 0 && points[i].y < 0;\n    }\n}",
            "#pragma omp parallel\n   {\n#pragma omp single\n      {\n#pragma omp taskgroup\n         {\n            for (auto& p : points) {\n               if (p.x > 0 && p.y > 0)\n                  ++bins[0];\n               else if (p.x < 0 && p.y > 0)\n                  ++bins[1];\n               else if (p.x < 0 && p.y < 0)\n                  ++bins[2];\n               else if (p.x > 0 && p.y < 0)\n                  ++bins[3];\n            }\n         }\n      }\n   }\n}",
            "//TODO\n\n}",
            "#pragma omp parallel for\n   for (int i = 0; i < 4; ++i) {\n      int x = 0;\n      for (size_t j = 0; j < points.size(); ++j) {\n         if (points[j].x >= 0)\n            ++x;\n      }\n      bins[i] = x;\n   }\n}",
            "#pragma omp parallel\n   {\n      int myId = omp_get_thread_num();\n      std::array<size_t, 4> threadBins;\n      #pragma omp for schedule(static)\n      for (size_t i = 0; i < points.size(); i++) {\n         double x = points[i].x;\n         double y = points[i].y;\n         if (x >= 0 && y >= 0) {\n            threadBins[0]++;\n         }\n         else if (x < 0 && y >= 0) {\n            threadBins[1]++;\n         }\n         else if (x < 0 && y < 0) {\n            threadBins[2]++;\n         }\n         else if (x >= 0 && y < 0) {\n            threadBins[3]++;\n         }\n      }\n      #pragma omp critical\n      {\n         for (int i = 0; i < threadBins.size(); i++) {\n            bins[i] += threadBins[i];\n         }\n      }\n   }\n}",
            "// Your code here\n}",
            "// TODO: Your code here.\n    size_t len = points.size();\n    if(len == 0) return;\n    #pragma omp parallel for num_threads(4)\n    for(int i = 0; i < len; i++) {\n        Point pt = points[i];\n        if(pt.x >= 0) {\n            if(pt.y >= 0) {\n                bins[0]++;\n            } else {\n                bins[3]++;\n            }\n        } else {\n            if(pt.y >= 0) {\n                bins[1]++;\n            } else {\n                bins[2]++;\n            }\n        }\n    }\n}",
            "// TODO\n\n}",
            "#pragma omp parallel\n{\n\tint id = omp_get_thread_num();\n\tint nthreads = omp_get_num_threads();\n\tint q = id / (nthreads / 4);\n\n\tint start, end;\n\tif(id == 0)\n\t\tstart = 0;\n\telse if(id == 1)\n\t\tstart = nthreads / 4;\n\telse if(id == 2)\n\t\tstart = (nthreads/4) * 2;\n\telse if(id == 3)\n\t\tstart = (nthreads/4) * 3;\n\n\tif(id == 0)\n\t\tend = nthreads / 4;\n\telse if(id == 1)\n\t\tend = (nthreads/4) * 2;\n\telse if(id == 2)\n\t\tend = (nthreads/4) * 3;\n\telse if(id == 3)\n\t\tend = nthreads;\n\n\tfor(int i = start; i < end; i++)\n\t{\n\t\tdouble x = points[i].x;\n\t\tdouble y = points[i].y;\n\n\t\tif(x >= 0 && y >= 0)\n\t\t\tbins[q]++;\n\t\telse if(x < 0 && y >= 0)\n\t\t\tbins[q+1]++;\n\t\telse if(x < 0 && y < 0)\n\t\t\tbins[q+2]++;\n\t\telse if(x >= 0 && y < 0)\n\t\t\tbins[q+3]++;\n\t}\n}\n}",
            "// Your code here\n\tdouble max_x = -1.0;\n\tdouble max_y = -1.0;\n\tint max_x_count = 0;\n\tint max_y_count = 0;\n\n\t#pragma omp parallel\n\t{\n\t\tint bin = omp_get_thread_num();\n\n\t\t#pragma omp for\n\t\tfor (int i = 0; i < points.size(); i++) {\n\t\t\tif (points[i].x > max_x) {\n\t\t\t\tmax_x = points[i].x;\n\t\t\t\tmax_x_count = 1;\n\t\t\t}\n\t\t\telse if (points[i].x == max_x) {\n\t\t\t\tmax_x_count++;\n\t\t\t}\n\n\t\t\tif (points[i].y > max_y) {\n\t\t\t\tmax_y = points[i].y;\n\t\t\t\tmax_y_count = 1;\n\t\t\t}\n\t\t\telse if (points[i].y == max_y) {\n\t\t\t\tmax_y_count++;\n\t\t\t}\n\t\t}\n\n\t\tif (bin == 0) {\n\t\t\tbins[0] = max_x_count;\n\t\t\tbins[1] = max_y_count;\n\t\t\tbins[2] = points.size() - (max_x_count + max_y_count);\n\t\t\tbins[3] = points.size() - max_x_count - max_y_count;\n\t\t}\n\n\t\t#pragma omp barrier\n\n\t\tif (bin == 1) {\n\t\t\tbins[2] = bins[2] + max_x_count;\n\t\t\tbins[3] = bins[3] + max_y_count;\n\t\t}\n\t}\n}",
            "// TODO: Add your code here\n    #pragma omp parallel\n    {\n        size_t id = omp_get_thread_num();\n        size_t nthreads = omp_get_num_threads();\n        int minX = 0;\n        int minY = 0;\n        int maxX = 0;\n        int maxY = 0;\n        size_t pointsCount = points.size();\n        int count = 0;\n        if(id == 0){\n            minX = maxX = points[0].x;\n            minY = maxY = points[0].y;\n        }\n        #pragma omp barrier\n\n        #pragma omp for schedule(static)\n        for(size_t i = 0; i < pointsCount; ++i){\n            if(points[i].x < minX) minX = points[i].x;\n            if(points[i].y < minY) minY = points[i].y;\n            if(points[i].x > maxX) maxX = points[i].x;\n            if(points[i].y > maxY) maxY = points[i].y;\n        }\n        #pragma omp barrier\n        if(id == 0){\n            double x = maxX, y = maxY;\n            maxX = x / nthreads;\n            maxY = y / nthreads;\n        }\n        #pragma omp barrier\n\n        #pragma omp for schedule(static)\n        for(size_t i = 0; i < pointsCount; ++i){\n            if(points[i].x >= minX && points[i].x <= maxX){\n                if(points[i].y >= minY && points[i].y <= maxY){\n                    count++;\n                }\n            }\n        }\n        #pragma omp barrier\n\n        #pragma omp critical\n        {\n            bins[id] = count;\n            count = 0;\n        }\n    }\n}",
            "// Your code goes here\n}",
            "#pragma omp parallel\n    {\n        int i;\n        #pragma omp for\n        for(i=0; i<points.size(); i++)\n        {\n            if(points[i].x>=0 && points[i].y>=0)\n                bins[0]++;\n            if(points[i].x<0 && points[i].y>=0)\n                bins[1]++;\n            if(points[i].x<0 && points[i].y<0)\n                bins[2]++;\n            if(points[i].x>=0 && points[i].y<0)\n                bins[3]++;\n        }\n    }\n}",
            "// TODO\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); i++){\n        double x = points.at(i).x;\n        double y = points.at(i).y;\n        if (x > 0 && y > 0){\n            bins[0]++;\n        }\n        else if (x > 0 && y < 0){\n            bins[1]++;\n        }\n        else if (x < 0 && y > 0){\n            bins[2]++;\n        }\n        else{\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: Your code here\n}",
            "size_t num_threads = omp_get_max_threads();\n\n    // TODO: Count points in each quadrant\n    size_t len = points.size();\n    #pragma omp parallel for num_threads(num_threads) shared(len, bins)\n    for (size_t i = 0; i < len; ++i) {\n        Point p = points[i];\n        if (p.x > 0 && p.y > 0) {\n            ++bins[0];\n        } else if (p.x > 0 && p.y < 0) {\n            ++bins[2];\n        } else if (p.x < 0 && p.y < 0) {\n            ++bins[3];\n        } else if (p.x < 0 && p.y > 0) {\n            ++bins[1];\n        } else {\n            ++bins[0];\n        }\n    }\n}",
            "bins = {0, 0, 0, 0};\n\n#pragma omp parallel for\n  for (size_t i = 0; i < points.size(); i++) {\n    if (points[i].x >= 0 && points[i].y >= 0) {\n      bins[0]++;\n    } else if (points[i].x < 0 && points[i].y >= 0) {\n      bins[1]++;\n    } else if (points[i].x < 0 && points[i].y < 0) {\n      bins[2]++;\n    } else if (points[i].x >= 0 && points[i].y < 0) {\n      bins[3]++;\n    }\n  }\n}",
            "// TODO: Your code here\n}",
            "// Your code here\n    size_t n = points.size();\n    for (size_t i = 0; i < n; i++) {\n        double x = points[i].x;\n        double y = points[i].y;\n        if (x >= 0 && y >= 0) bins[0]++;\n        else if (x < 0 && y >= 0) bins[1]++;\n        else if (x < 0 && y < 0) bins[2]++;\n        else if (x >= 0 && y < 0) bins[3]++;\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i=0; i<points.size(); ++i) {\n        if(points[i].x >= 0 && points[i].y >= 0)\n            ++bins[0];\n        else if(points[i].x < 0 && points[i].y >= 0)\n            ++bins[1];\n        else if(points[i].x < 0 && points[i].y < 0)\n            ++bins[2];\n        else\n            ++bins[3];\n    }\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      double x = points[i].x, y = points[i].y;\n      if (x >= 0 && y >= 0) bins[0]++;\n      else if (x < 0 && y >= 0) bins[1]++;\n      else if (x < 0 && y < 0) bins[2]++;\n      else bins[3]++;\n   }\n}",
            "bins.fill(0);\n   #pragma omp parallel for\n   for(auto const& point: points) {\n      if(point.x < 0.0) {\n         if(point.y < 0.0) {\n            bins[0]++;\n         }\n         else {\n            bins[1]++;\n         }\n      }\n      else {\n         if(point.y < 0.0) {\n            bins[2]++;\n         }\n         else {\n            bins[3]++;\n         }\n      }\n   }\n}",
            "auto x_quadrant = [](Point p) {\n        if (p.x < 0) {\n            return 0;\n        } else if (p.x > 0) {\n            return 1;\n        } else {\n            return 2;\n        }\n    };\n\n    auto y_quadrant = [](Point p) {\n        if (p.y < 0) {\n            return 0;\n        } else if (p.y > 0) {\n            return 1;\n        } else {\n            return 2;\n        }\n    };\n\n    // Parallel for\n#pragma omp parallel for\n    for (int i = 0; i < (int)points.size(); i++) {\n        bins[x_quadrant(points[i]) * 2 + y_quadrant(points[i])]++;\n    }\n}",
            "// TODO: count points in each quadrant\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); i++)\n    {\n        if (points[i].x >= 0 && points[i].y >= 0) {\n            bins[0] += 1;\n        }\n        else if (points[i].x >= 0 && points[i].y < 0) {\n            bins[1] += 1;\n        }\n        else if (points[i].x < 0 && points[i].y >= 0) {\n            bins[2] += 1;\n        }\n        else if (points[i].x < 0 && points[i].y < 0) {\n            bins[3] += 1;\n        }\n\n    }\n\n}",
            "#pragma omp parallel for\n  for(int i = 0; i < points.size(); i++) {\n    if(points[i].x >= 0.0) {\n      if(points[i].y >= 0.0) {\n        bins[0] += 1;\n      }\n      else {\n        bins[3] += 1;\n      }\n    }\n    else {\n      if(points[i].y >= 0.0) {\n        bins[1] += 1;\n      }\n      else {\n        bins[2] += 1;\n      }\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0)\n            bins[0]++;\n        if (points[i].x < 0)\n            bins[1]++;\n        if (points[i].y > 0)\n            bins[2]++;\n        if (points[i].y < 0)\n            bins[3]++;\n    }\n}",
            "//TODO\n\n    // Create a vector of bins to store the counts\n    std::vector<size_t> bin_counts(bins.size());\n\n    // #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); i++) {\n        if (points[i].x >= 0 && points[i].y >= 0) {\n            bin_counts[0]++;\n        }\n        if (points[i].x < 0 && points[i].y >= 0) {\n            bin_counts[1]++;\n        }\n        if (points[i].x < 0 && points[i].y < 0) {\n            bin_counts[2]++;\n        }\n        if (points[i].x >= 0 && points[i].y < 0) {\n            bin_counts[3]++;\n        }\n    }\n\n    // Update the bins in the output array\n    for (size_t i = 0; i < bins.size(); i++) {\n        bins[i] = bin_counts[i];\n    }\n\n    // std::cout << \"bin_counts:\\n\";\n    // for (size_t i = 0; i < bin_counts.size(); i++) {\n    //     std::cout << bin_counts[i] << \" \";\n    // }\n    // std::cout << \"\\n\";\n}",
            "int n = points.size();\n   size_t chunkSize = n/omp_get_max_threads();\n   #pragma omp parallel for schedule(static, chunkSize)\n   for (int i = 0; i < n; ++i) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x > 0 && y > 0)\n         bins[0]++;\n      if (x < 0 && y > 0)\n         bins[1]++;\n      if (x < 0 && y < 0)\n         bins[2]++;\n      if (x > 0 && y < 0)\n         bins[3]++;\n   }\n}",
            "// TODO: Your code goes here\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i = 0; i < points.size(); ++i)\n        {\n            if (points[i].x >= 0 && points[i].x <= 2 && points[i].y >= 0 && points[i].y <= 2)\n            {\n                ++bins[0];\n            }\n            else if (points[i].x >= 0 && points[i].x <= 2 && points[i].y <= 0 && points[i].y >= -2)\n            {\n                ++bins[1];\n            }\n            else if (points[i].x <= 0 && points[i].x >= -2 && points[i].y <= 0 && points[i].y >= -2)\n            {\n                ++bins[2];\n            }\n            else if (points[i].x <= 0 && points[i].x >= -2 && points[i].y >= 0 && points[i].y <= 2)\n            {\n                ++bins[3];\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i=0; i<points.size(); i++) {\n        Point p = points[i];\n        if (p.x >= 0.0 && p.y >= 0.0) {\n            bins[0] += 1;\n        } else if (p.x >= 0.0 && p.y < 0.0) {\n            bins[1] += 1;\n        } else if (p.x < 0.0 && p.y < 0.0) {\n            bins[2] += 1;\n        } else if (p.x < 0.0 && p.y >= 0.0) {\n            bins[3] += 1;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 4; i++) {\n        for (int j = 0; j < points.size(); j++) {\n            if ((points.at(j).x > 0 && points.at(j).y > 0) || (points.at(j).x < 0 && points.at(j).y < 0)) {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "bins.fill(0);\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); i++) {\n        if (points[i].x > 0 && points[i].y > 0) bins[0]++;\n        else if (points[i].x < 0 && points[i].y > 0) bins[1]++;\n        else if (points[i].x < 0 && points[i].y < 0) bins[2]++;\n        else if (points[i].x > 0 && points[i].y < 0) bins[3]++;\n    }\n}",
            "#pragma omp parallel\n   {\n      int thread_id = omp_get_thread_num();\n      int thread_num = omp_get_num_threads();\n      // Calculate the beginning and end of the interval for each thread\n      size_t begin = points.size() / thread_num * thread_id;\n      size_t end = (points.size() / thread_num * (thread_id + 1) < points.size())? points.size() / thread_num * (thread_id + 1) : points.size();\n\n      // Go through all points in this interval, and calculate the quadrant\n      for (int i = begin; i < end; i++) {\n         if (points[i].x > 0) {\n            if (points[i].y > 0) {\n               bins[0]++;\n            } else {\n               bins[3]++;\n            }\n         } else {\n            if (points[i].y > 0) {\n               bins[1]++;\n            } else {\n               bins[2]++;\n            }\n         }\n      }\n   }\n}",
            "bins.fill(0);\n\n    for(auto const& p : points) {\n        auto const x = p.x;\n        auto const y = p.y;\n        auto quadrant = 0;\n        if (x > 0.0) quadrant |= 0x1;\n        if (y > 0.0) quadrant |= 0x2;\n        bins[quadrant] += 1;\n    }\n}",
            "// Write your code here\n}",
            "#pragma omp parallel\n    for(auto &p : points)\n#pragma omp for\n        for(int i = 0; i < bins.size(); ++i)\n            if(p.x >= 0 && p.y >= 0)\n                ++bins[i];\n            else if(p.x < 0 && p.y >= 0)\n                ++bins[1];\n            else if(p.x >= 0 && p.y < 0)\n                ++bins[2];\n            else\n                ++bins[3];\n}",
            "// TODO: insert your solution here\n\n    // count the number of points in each quadrant\n    for (auto const & point: points)\n    {\n        double x = point.x;\n        double y = point.y;\n        if (x > 0 && y > 0)\n            bins[0]++;\n        else if (x < 0 && y > 0)\n            bins[1]++;\n        else if (x < 0 && y < 0)\n            bins[2]++;\n        else if (x > 0 && y < 0)\n            bins[3]++;\n    }\n    return;\n}",
            "#pragma omp parallel\n    #pragma omp single\n    {\n        for(int i=0; i<points.size(); i++)\n        {\n            #pragma omp task\n            if(points[i].x >= 0 && points[i].y >= 0)\n            {\n                bins[0]++;\n            }\n            else if(points[i].x < 0 && points[i].y >= 0)\n            {\n                bins[1]++;\n            }\n            else if(points[i].x < 0 && points[i].y < 0)\n            {\n                bins[2]++;\n            }\n            else if(points[i].x >= 0 && points[i].y < 0)\n            {\n                bins[3]++;\n            }\n        }\n    }\n}",
            "// Fill in this code\n\n\n\n\n\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); i++) {\n        if (points[i].x > 0 && points[i].y > 0)\n            bins[0]++;\n        else if (points[i].x < 0 && points[i].y > 0)\n            bins[1]++;\n        else if (points[i].x < 0 && points[i].y < 0)\n            bins[2]++;\n        else if (points[i].x > 0 && points[i].y < 0)\n            bins[3]++;\n    }\n}",
            "// Fill in your solution here.\n}",
            "bins.fill(0);\n    #pragma omp parallel for\n    for(const auto &point : points){\n        if(point.x > 0 && point.y > 0){\n            bins[0]++;\n        }\n        else if(point.x < 0 && point.y > 0){\n            bins[1]++;\n        }\n        else if(point.x < 0 && point.y < 0){\n            bins[2]++;\n        }\n        else if(point.x > 0 && point.y < 0){\n            bins[3]++;\n        }\n        else{\n            bins[4]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        if (points[i].x >= 0 && points[i].y >= 0)\n            ++bins[0];\n        else if (points[i].x < 0 && points[i].y >= 0)\n            ++bins[1];\n        else if (points[i].x < 0 && points[i].y < 0)\n            ++bins[2];\n        else\n            ++bins[3];\n    }\n}",
            "bins = {};\n\n    #pragma omp parallel for\n    for (auto i = 0u; i < points.size(); i++) {\n        if (points[i].x >= 0)\n            if (points[i].y >= 0)\n                bins[0]++;\n            else\n                bins[3]++;\n        else\n            if (points[i].y >= 0)\n                bins[1]++;\n            else\n                bins[2]++;\n    }\n}",
            "size_t const size = points.size();\n   bins = {0, 0, 0, 0};\n   // #pragma omp parallel for\n   for (size_t i = 0; i < size; i++) {\n      double const x = points[i].x;\n      double const y = points[i].y;\n      if (x > 0 && y > 0) {\n         bins[0] += 1;\n      } else if (x < 0 && y > 0) {\n         bins[1] += 1;\n      } else if (x < 0 && y < 0) {\n         bins[2] += 1;\n      } else if (x > 0 && y < 0) {\n         bins[3] += 1;\n      }\n   }\n   std::cout << \"output: \" << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << std::endl;\n}",
            "size_t num_points = points.size();\n\n    // #pragma omp parallel for\n    #pragma omp parallel for schedule(dynamic, 1)\n    for(size_t i = 0; i < num_points; i++) {\n        if(points[i].x > 0.0 && points[i].y > 0.0) {\n            bins[0]++;\n        } else if(points[i].x < 0.0 && points[i].y > 0.0) {\n            bins[1]++;\n        } else if(points[i].x < 0.0 && points[i].y < 0.0) {\n            bins[2]++;\n        } else if(points[i].x > 0.0 && points[i].y < 0.0) {\n            bins[3]++;\n        }\n    }\n}",
            "for (size_t i = 0; i < 4; i++) bins[i] = 0;\n\n   for (auto p: points) {\n      if (p.x >= 0) {\n         if (p.y >= 0) bins[0] += 1;\n         else bins[3] += 1;\n      }\n      else {\n         if (p.y >= 0) bins[1] += 1;\n         else bins[2] += 1;\n      }\n   }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i)\n    {\n        if (points[i].x > 0 && points[i].y > 0)\n            bins[0]++;\n        else if (points[i].x < 0 && points[i].y > 0)\n            bins[1]++;\n        else if (points[i].x < 0 && points[i].y < 0)\n            bins[2]++;\n        else if (points[i].x > 0 && points[i].y < 0)\n            bins[3]++;\n    }\n}",
            "bins = std::array<size_t, 4>{0};\n    int n = points.size();\n    #pragma omp parallel for\n    for(int i = 0; i < n; i++) {\n        if (points[i].x >= 0 && points[i].y >= 0)\n            bins[0] += 1;\n        if (points[i].x < 0 && points[i].y < 0)\n            bins[3] += 1;\n        if (points[i].x < 0 && points[i].y >= 0)\n            bins[1] += 1;\n        if (points[i].x >= 0 && points[i].y < 0)\n            bins[2] += 1;\n    }\n\n}",
            "#pragma omp parallel\n   {\n      double minX = points[0].x;\n      double maxX = points[0].x;\n      double minY = points[0].y;\n      double maxY = points[0].y;\n\n      #pragma omp for\n      for (size_t i = 1; i < points.size(); i++) {\n         minX = min(minX, points[i].x);\n         maxX = max(maxX, points[i].x);\n         minY = min(minY, points[i].y);\n         maxY = max(maxY, points[i].y);\n      }\n\n      double w = maxX - minX;\n      double h = maxY - minY;\n\n      #pragma omp single\n      {\n         double l = max(w, h);\n         double step = l / 4;\n\n         #pragma omp for\n         for (size_t i = 0; i < points.size(); i++) {\n            int x = floor((points[i].x - minX) / step);\n            int y = floor((points[i].y - minY) / step);\n\n            if (x < 0 || y < 0 || x >= 4 || y >= 4)\n               continue;\n\n            #pragma omp atomic\n            bins[y * 4 + x]++;\n         }\n      }\n   }\n}",
            "#pragma omp parallel for\n    for (auto i = 0; i < points.size(); i++) {\n        if (points[i].x >= 0 && points[i].y >= 0) {\n            bins[0]++;\n        }\n        if (points[i].x < 0 && points[i].y >= 0) {\n            bins[1]++;\n        }\n        if (points[i].x < 0 && points[i].y < 0) {\n            bins[2]++;\n        }\n        if (points[i].x >= 0 && points[i].y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < points.size(); i++){\n        double x = points[i].x;\n        double y = points[i].y;\n\n        if(x > 0 && y > 0){\n            bins[0]++;\n        }\n        else if(x < 0 && y > 0){\n            bins[1]++;\n        }\n        else if(x < 0 && y < 0){\n            bins[2]++;\n        }\n        else if(x > 0 && y < 0){\n            bins[3]++;\n        }\n        else{\n            bins[4]++;\n        }\n    }\n}",
            "bins.fill(0);\n    for (auto const& p : points) {\n        if (p.x > 0) {\n            if (p.y > 0) {\n                bins[0]++;\n            } else {\n                bins[3]++;\n            }\n        } else {\n            if (p.y > 0) {\n                bins[1]++;\n            } else {\n                bins[2]++;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < points.size(); i++) {\n        if (points[i].x >= 0) {\n            if (points[i].y >= 0) {\n                bins[0]++;\n            } else {\n                bins[1]++;\n            }\n        } else {\n            if (points[i].y >= 0) {\n                bins[2]++;\n            } else {\n                bins[3]++;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for(auto& point : points){\n        if (point.x >= 0 && point.y >= 0){\n            bins[0]++;\n        } else if (point.x < 0 && point.y < 0){\n            bins[3]++;\n        } else if (point.x < 0 && point.y >= 0){\n            bins[2]++;\n        } else {\n            bins[1]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i=0; i<points.size(); i++) {\n        if (points[i].x > 0) {\n            if (points[i].y > 0) {\n                bins[0]++;\n            } else {\n                bins[3]++;\n            }\n        } else {\n            if (points[i].y > 0) {\n                bins[1]++;\n            } else {\n                bins[2]++;\n            }\n        }\n    }\n}",
            "}",
            "for (auto& point : points) {\n        if (point.x > 0)\n            if (point.y > 0)\n                bins[0] += 1;\n            else\n                bins[2] += 1;\n        else\n            if (point.y > 0)\n                bins[1] += 1;\n            else\n                bins[3] += 1;\n    }\n}",
            "for (size_t i = 0; i < points.size(); i++) {\n        if (points[i].x >= 0 && points[i].y >= 0) {\n            bins[0] += 1;\n        } else if (points[i].x >= 0 && points[i].y < 0) {\n            bins[1] += 1;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            bins[2] += 1;\n        } else if (points[i].x < 0 && points[i].y >= 0) {\n            bins[3] += 1;\n        }\n    }\n\n    omp_set_num_threads(4);\n\n#pragma omp parallel for\n    for (int i = 0; i < 4; i++) {\n        bins[i] = 0;\n        for (size_t j = 0; j < points.size(); j++) {\n            if (i == 0) {\n                if (points[j].x >= 0 && points[j].y >= 0) {\n                    bins[i] += 1;\n                }\n            } else if (i == 1) {\n                if (points[j].x >= 0 && points[j].y < 0) {\n                    bins[i] += 1;\n                }\n            } else if (i == 2) {\n                if (points[j].x < 0 && points[j].y < 0) {\n                    bins[i] += 1;\n                }\n            } else if (i == 3) {\n                if (points[j].x < 0 && points[j].y >= 0) {\n                    bins[i] += 1;\n                }\n            }\n        }\n    }\n}",
            "for(size_t i = 0; i < points.size(); i++){\n    bins[0] += (points[i].x >= 0) && (points[i].y >= 0);\n    bins[1] += (points[i].x < 0) && (points[i].y >= 0);\n    bins[2] += (points[i].x < 0) && (points[i].y < 0);\n    bins[3] += (points[i].x >= 0) && (points[i].y < 0);\n  }\n\n\n}",
            "bins.fill(0);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        auto point = points[i];\n        if (point.x > 0 && point.y > 0)\n            bins[0] += 1;\n        if (point.x < 0 && point.y > 0)\n            bins[1] += 1;\n        if (point.x < 0 && point.y < 0)\n            bins[2] += 1;\n        if (point.x > 0 && point.y < 0)\n            bins[3] += 1;\n    }\n}",
            "#pragma omp parallel\n{\n   #pragma omp single\n{\n\tstd::cout<<\"This is my single\\n\";\n}\n   #pragma omp sections\n   {\n\t   #pragma omp section\n\t{\n\t\tstd::cout<<\"This is my section\\n\";\n\t}\n\t#pragma omp section\n\t{\n\t\tstd::cout<<\"This is my section\\n\";\n\t}\n\t#pragma omp section\n\t{\n\t\tstd::cout<<\"This is my section\\n\";\n\t}\n\t#pragma omp section\n\t{\n\t\tstd::cout<<\"This is my section\\n\";\n\t}\n   }\n}\n}",
            "// TODO: Implement this function\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    double x = points[i].x;\n    double y = points[i].y;\n    if (x >= 0 && y >= 0) {\n      bins[0]++;\n    } else if (x < 0 && y >= 0) {\n      bins[1]++;\n    } else if (x < 0 && y < 0) {\n      bins[2]++;\n    } else if (x >= 0 && y < 0) {\n      bins[3]++;\n    }\n  }\n}",
            "// TODO\n}",
            "bins = {};\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (auto p: points)\n        {\n            int x_sign = p.x >= 0? 0 : 1;\n            int y_sign = p.y >= 0? 0 : 1;\n            bins[x_sign * 2 + y_sign] += 1;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for(size_t i = 0; i < points.size(); i++) {\n        double x = points[i].x;\n        double y = points[i].y;\n        if(x > 0) {\n            if(y > 0)\n                bins[0]++;\n            else\n                bins[3]++;\n        } else {\n            if(y > 0)\n                bins[1]++;\n            else\n                bins[2]++;\n        }\n    }\n}",
            "#pragma omp parallel for num_threads(omp_get_max_threads())\n    for (int i = 0; i < points.size(); i++) {\n        if (points[i].x >= 0 && points[i].y >= 0) {\n            bins[0] += 1;\n        } else if (points[i].x < 0 && points[i].y >= 0) {\n            bins[1] += 1;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            bins[2] += 1;\n        } else if (points[i].x >= 0 && points[i].y < 0) {\n            bins[3] += 1;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i)\n    {\n        if (points[i].x > 0)\n            ++bins[0];\n        else\n            ++bins[1];\n\n        if (points[i].y > 0)\n            ++bins[2];\n        else\n            ++bins[3];\n    }\n}",
            "for (size_t i = 0; i < points.size(); i++) {\n        if (points[i].x >= 0.0 && points[i].y >= 0.0) {\n            bins[0] += 1;\n        } else if (points[i].x < 0.0 && points[i].y >= 0.0) {\n            bins[1] += 1;\n        } else if (points[i].x < 0.0 && points[i].y < 0.0) {\n            bins[2] += 1;\n        } else if (points[i].x >= 0.0 && points[i].y < 0.0) {\n            bins[3] += 1;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < points.size(); i++) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            bins[0]++;\n        }\n        if (points[i].x < 0 && points[i].y > 0) {\n            bins[1]++;\n        }\n        if (points[i].x < 0 && points[i].y < 0) {\n            bins[2]++;\n        }\n        if (points[i].x > 0 && points[i].y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "size_t npoints = points.size();\n\t\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < npoints; i++) {\n\t\tif (points[i].x >= 0 && points[i].y >= 0) {\n\t\t\tbins[0]++;\n\t\t} else if (points[i].x < 0 && points[i].y >= 0) {\n\t\t\tbins[1]++;\n\t\t} else if (points[i].x < 0 && points[i].y < 0) {\n\t\t\tbins[2]++;\n\t\t} else if (points[i].x >= 0 && points[i].y < 0) {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "int i,j;\n    int x,y;\n\n    for (i = 0; i < 4; i++) {\n        bins[i] = 0;\n    }\n\n    #pragma omp parallel for\n    for (i = 0; i < (int) points.size(); i++) {\n        x = (points[i].x > 0)? 1 : 0;\n        y = (points[i].y > 0)? 1 : 0;\n        j = x + 2*y;\n        bins[j]++;\n    }\n}",
            "#pragma omp parallel for\n    for(int i=0; i<points.size(); i++) {\n        if(points.at(i).x >= 0 && points.at(i).y >= 0) {\n            bins.at(0) += 1;\n        } else if(points.at(i).x < 0 && points.at(i).y < 0) {\n            bins.at(1) += 1;\n        } else if(points.at(i).x < 0 && points.at(i).y >= 0) {\n            bins.at(2) += 1;\n        } else {\n            bins.at(3) += 1;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for(int i = 0; i < points.size(); ++i) {\n        Point p = points[i];\n        if (p.x > 0 && p.y > 0) {\n            ++bins[0];\n        }\n        else if (p.x < 0 && p.y > 0) {\n            ++bins[1];\n        }\n        else if (p.x < 0 && p.y < 0) {\n            ++bins[2];\n        }\n        else {\n            ++bins[3];\n        }\n    }\n}",
            "}",
            "size_t n = points.size();\n\tint i;\n#pragma omp parallel for private(i)\n\tfor (i = 0; i < n; i++) {\n\t\tPoint p = points[i];\n\t\tif (p.x > 0 && p.y > 0) bins[0]++;\n\t\telse if (p.x > 0 && p.y < 0) bins[1]++;\n\t\telse if (p.x < 0 && p.y < 0) bins[2]++;\n\t\telse bins[3]++;\n\t}\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); i++) {\n        if (points[i].x > 0 && points[i].y > 0)\n            bins[0] += 1;\n        else if (points[i].x < 0 && points[i].y > 0)\n            bins[1] += 1;\n        else if (points[i].x < 0 && points[i].y < 0)\n            bins[2] += 1;\n        else if (points[i].x > 0 && points[i].y < 0)\n            bins[3] += 1;\n    }\n}",
            "size_t threadNum = omp_get_max_threads();\n\tstd::array<std::array<size_t, 4>, threadNum> threadBins;\n\n\t#pragma omp parallel\n\t{\n\t\tint id = omp_get_thread_num();\n\t\tthreadBins[id][0] = 0;\n\t\tthreadBins[id][1] = 0;\n\t\tthreadBins[id][2] = 0;\n\t\tthreadBins[id][3] = 0;\n\n\t\t#pragma omp for nowait\n\t\tfor (int i = 0; i < points.size(); i++) {\n\t\t\tPoint p = points.at(i);\n\t\t\tif (p.x > 0.0 && p.y > 0.0) threadBins[id][0] += 1;\n\t\t\telse if (p.x < 0.0 && p.y > 0.0) threadBins[id][1] += 1;\n\t\t\telse if (p.x < 0.0 && p.y < 0.0) threadBins[id][2] += 1;\n\t\t\telse if (p.x > 0.0 && p.y < 0.0) threadBins[id][3] += 1;\n\t\t}\n\t}\n\n\tbins[0] = 0;\n\tbins[1] = 0;\n\tbins[2] = 0;\n\tbins[3] = 0;\n\n\t#pragma omp parallel for reduction(+:bins[0])\n\tfor (int i = 0; i < threadNum; i++) {\n\t\tbins[0] += threadBins[i][0];\n\t}\n\t#pragma omp parallel for reduction(+:bins[1])\n\tfor (int i = 0; i < threadNum; i++) {\n\t\tbins[1] += threadBins[i][1];\n\t}\n\t#pragma omp parallel for reduction(+:bins[2])\n\tfor (int i = 0; i < threadNum; i++) {\n\t\tbins[2] += threadBins[i][2];\n\t}\n\t#pragma omp parallel for reduction(+:bins[3])\n\tfor (int i = 0; i < threadNum; i++) {\n\t\tbins[3] += threadBins[i][3];\n\t}\n\n\n}",
            "for (int i = 0; i < bins.size(); i++) {\n      bins[i] = 0;\n   }\n#pragma omp parallel\n   {\n#pragma omp for\n      for (int i = 0; i < points.size(); i++) {\n         if (points[i].x > 0 && points[i].y > 0) {\n            bins[0] += 1;\n         } else if (points[i].x < 0 && points[i].y > 0) {\n            bins[1] += 1;\n         } else if (points[i].x < 0 && points[i].y < 0) {\n            bins[2] += 1;\n         } else if (points[i].x > 0 && points[i].y < 0) {\n            bins[3] += 1;\n         } else {\n            bins[0] += 1;\n         }\n      }\n   }\n}",
            "#pragma omp parallel for\n    for(int i = 0; i < points.size(); i++){\n        if(points[i].x > 0 && points[i].y > 0){\n            bins[0]++;\n        }else if(points[i].x < 0 && points[i].y > 0){\n            bins[1]++;\n        }else if(points[i].x < 0 && points[i].y < 0){\n            bins[2]++;\n        }else{\n            bins[3]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n   for (int i = 0; i < points.size(); ++i) {\n      if (points[i].x < 0 && points[i].y < 0)\n         ++bins[0];\n      else if (points[i].x >= 0 && points[i].y < 0)\n         ++bins[1];\n      else if (points[i].x >= 0 && points[i].y >= 0)\n         ++bins[2];\n      else\n         ++bins[3];\n   }\n}",
            "bins = {0, 0, 0, 0};\n    size_t count = 0;\n    #pragma omp parallel for\n    for(size_t i = 0; i < points.size(); i++) {\n        if(points[i].x >= 0) {\n            if(points[i].y >= 0) {\n                bins[0] += 1;\n            } else {\n                bins[3] += 1;\n            }\n        } else {\n            if(points[i].y >= 0) {\n                bins[1] += 1;\n            } else {\n                bins[2] += 1;\n            }\n        }\n        count += 1;\n    }\n\n    double const count_d = count;\n    double const bins_d[4] = {bins[0], bins[1], bins[2], bins[3]};\n\n    for(size_t i = 0; i < bins.size(); i++) {\n        bins[i] = bins_d[i] * 1.0 / count_d;\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        double x = points.at(i).x;\n        double y = points.at(i).y;\n        if (x >= 0) {\n            if (y >= 0) {\n                bins.at(0) += 1;\n            }\n            else {\n                bins.at(3) += 1;\n            }\n        }\n        else {\n            if (y >= 0) {\n                bins.at(1) += 1;\n            }\n            else {\n                bins.at(2) += 1;\n            }\n        }\n    }\n}",
            "#pragma omp parallel\n   {\n      #pragma omp for\n      for(size_t i = 0; i < points.size(); ++i)\n      {\n         size_t id = 0;\n         if(points[i].x > 0)\n            id += 1;\n         if(points[i].y > 0)\n            id += 2;\n         if(points[i].x == 0)\n            id += 4;\n         if(points[i].y == 0)\n            id += 8;\n         #pragma omp atomic\n         ++bins[id];\n      }\n   }\n}",
            "#pragma omp parallel for num_threads(2)\n    for (size_t i = 0; i < points.size(); i++) {\n        int x = points[i].x;\n        int y = points[i].y;\n        if (x > 0 && y > 0) {\n            bins[0] += 1;\n        } else if (x < 0 && y > 0) {\n            bins[1] += 1;\n        } else if (x < 0 && y < 0) {\n            bins[2] += 1;\n        } else if (x > 0 && y < 0) {\n            bins[3] += 1;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); i++) {\n        if (points[i].x < 0 && points[i].y < 0)\n            bins[0]++;\n        else if (points[i].x < 0 && points[i].y > 0)\n            bins[1]++;\n        else if (points[i].x > 0 && points[i].y > 0)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "int const n = points.size();\n    int nthreads = omp_get_max_threads();\n    int tid = omp_get_thread_num();\n    int nthreads_active = omp_get_num_threads();\n    // std::cout << \"active threads: \" << nthreads_active << std::endl;\n\n    int bin;\n    size_t start_index = tid * n / nthreads;\n    size_t end_index = (tid + 1) * n / nthreads;\n    for (size_t i = start_index; i < end_index; i++) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            bin = 0;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            bin = 1;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            bin = 2;\n        } else {\n            bin = 3;\n        }\n        bins[bin] += 1;\n    }\n}",
            "// TODO: Your code here\n}",
            "for (int i = 0; i < 4; i++) bins[i] = 0;\n\n   #pragma omp parallel for reduction(+:bins)\n   for (size_t i = 0; i < points.size(); i++) {\n\n      int quadrant = 0;\n      if (points[i].x > 0 && points[i].y > 0) quadrant = 1;\n      else if (points[i].x < 0 && points[i].y > 0) quadrant = 2;\n      else if (points[i].x < 0 && points[i].y < 0) quadrant = 3;\n\n      #pragma omp atomic\n      bins[quadrant]++;\n   }\n}",
            "// TODO: Your code here\n    //std::array<size_t, 4> bins = {};\n\n    //std::array<size_t, 4> bins = {0, 0, 0, 0};\n    #pragma omp parallel for\n    for(size_t i = 0; i < points.size(); i++) {\n        if(points[i].x > 0 && points[i].y > 0) {\n            bins[0]++;\n        } else if(points[i].x < 0 && points[i].y > 0) {\n            bins[1]++;\n        } else if(points[i].x < 0 && points[i].y < 0) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < points.size(); i++){\n    if (points[i].x > 0 && points[i].y > 0){\n      bins[0]++;\n    }else if (points[i].x < 0 && points[i].y < 0){\n      bins[1]++;\n    }else if (points[i].x < 0 && points[i].y > 0){\n      bins[2]++;\n    }else if (points[i].x > 0 && points[i].y < 0){\n      bins[3]++;\n    }\n  }\n}",
            "#pragma omp parallel\n  {\n    #pragma omp for\n    for (int i = 0; i < points.size(); i++) {\n      Point point = points[i];\n      if (point.x >= 0 && point.y >= 0) {\n        bins[0] += 1;\n      } else if (point.x < 0 && point.y < 0) {\n        bins[1] += 1;\n      } else if (point.x >= 0 && point.y < 0) {\n        bins[2] += 1;\n      } else if (point.x < 0 && point.y >= 0) {\n        bins[3] += 1;\n      }\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < points.size(); i++) {\n        if (points[i].x < 0) {\n            if (points[i].y < 0) {\n                #pragma omp atomic\n                bins[0]++;\n            } else {\n                #pragma omp atomic\n                bins[1]++;\n            }\n        } else {\n            if (points[i].y < 0) {\n                #pragma omp atomic\n                bins[2]++;\n            } else {\n                #pragma omp atomic\n                bins[3]++;\n            }\n        }\n    }\n}",
            "// Add your code here\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); i++) {\n        if (points[i].x > 0 && points[i].y > 0)\n            bins[0]++;\n        else if (points[i].x > 0 && points[i].y < 0)\n            bins[1]++;\n        else if (points[i].x < 0 && points[i].y < 0)\n            bins[2]++;\n        else if (points[i].x < 0 && points[i].y > 0)\n            bins[3]++;\n    }\n}",
            "#pragma omp parallel for\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tif (points[i].x > 0 && points[i].y > 0) {\n\t\t\tbins[0]++;\n\t\t} else if (points[i].x < 0 && points[i].y > 0) {\n\t\t\tbins[1]++;\n\t\t} else if (points[i].x < 0 && points[i].y < 0) {\n\t\t\tbins[2]++;\n\t\t} else {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "// your code here\n    #pragma omp parallel for schedule(guided)\n    for (int i = 0; i < points.size(); i++) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            bins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            bins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n   for(size_t i = 0; i < points.size(); i++) {\n      if(points[i].x > 0 && points[i].y > 0)\n         bins[0]++;\n      else if(points[i].x > 0 && points[i].y < 0)\n         bins[1]++;\n      else if(points[i].x < 0 && points[i].y < 0)\n         bins[2]++;\n      else\n         bins[3]++;\n   }\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for\n        for(size_t i = 0; i < points.size(); ++i) {\n            if (points.at(i).x >= 0 && points.at(i).y >= 0) {\n                bins[0]++;\n            } else if (points.at(i).x >= 0 && points.at(i).y < 0) {\n                bins[1]++;\n            } else if (points.at(i).x < 0 && points.at(i).y < 0) {\n                bins[2]++;\n            } else if (points.at(i).x < 0 && points.at(i).y >= 0) {\n                bins[3]++;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < points.size(); ++i) {\n    if (points[i].x > 0.0 && points[i].y > 0.0)\n      bins[0] += 1;\n    else if (points[i].x < 0.0 && points[i].y > 0.0)\n      bins[1] += 1;\n    else if (points[i].x < 0.0 && points[i].y < 0.0)\n      bins[2] += 1;\n    else if (points[i].x > 0.0 && points[i].y < 0.0)\n      bins[3] += 1;\n  }\n}",
            "bins.fill(0);\n#pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        if (points[i].x >= 0 && points[i].y >= 0) {\n            bins[0]++;\n        } else if (points[i].x < 0 && points[i].y >= 0) {\n            bins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < points.size(); i++) {\n    if (points[i].x > 0 && points[i].y > 0) {\n      bins[0]++;\n    }\n    else if (points[i].x < 0 && points[i].y > 0) {\n      bins[1]++;\n    }\n    else if (points[i].x < 0 && points[i].y < 0) {\n      bins[2]++;\n    }\n    else if (points[i].x > 0 && points[i].y < 0) {\n      bins[3]++;\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); i++) {\n        if (points[i].x > 0) {\n            if (points[i].y > 0) {\n                bins[0]++;\n            } else {\n                bins[3]++;\n            }\n        } else {\n            if (points[i].y > 0) {\n                bins[1]++;\n            } else {\n                bins[2]++;\n            }\n        }\n    }\n}",
            "//...\n}",
            "#pragma omp parallel for\n    for (int i=0; i < (int)points.size(); i++) {\n        if (points[i].x >= 0 && points[i].y >= 0) {\n            bins[0] += 1;\n        } else if (points[i].x < 0 && points[i].y >= 0) {\n            bins[1] += 1;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            bins[2] += 1;\n        } else if (points[i].x >= 0 && points[i].y < 0) {\n            bins[3] += 1;\n        }\n    }\n}",
            "bins.fill(0);\n#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x >= 0 && points[i].y >= 0) {\n            bins[0] += 1;\n        } else if (points[i].x < 0 && points[i].y >= 0) {\n            bins[1] += 1;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            bins[2] += 1;\n        } else {\n            bins[3] += 1;\n        }\n    }\n}",
            "for (size_t i = 0; i < bins.size(); ++i) {\n    bins[i] = 0;\n  }\n\n#pragma omp parallel\n  {\n#pragma omp for schedule(static, 1)\n    for (size_t i = 0; i < points.size(); ++i) {\n      Point const& p = points[i];\n      if (p.x > 0) {\n        if (p.y > 0) {\n          bins[0]++;\n        }\n        else {\n          bins[3]++;\n        }\n      }\n      else {\n        if (p.y > 0) {\n          bins[1]++;\n        }\n        else {\n          bins[2]++;\n        }\n      }\n    }\n  }\n\n  return;\n}",
            "size_t n = points.size();\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; ++i) {\n        const Point& p = points[i];\n        if (p.x >= 0 && p.y >= 0) bins[0]++;\n        else if (p.x >= 0 && p.y < 0) bins[1]++;\n        else if (p.x < 0 && p.y < 0) bins[2]++;\n        else if (p.x < 0 && p.y >= 0) bins[3]++;\n    }\n}",
            "for (auto &p : points) {\n      // Compute quadrant\n      size_t quadrant = 0;\n      if (p.x >= 0)\n         quadrant |= 1;\n      if (p.y >= 0)\n         quadrant |= 2;\n\n      // Compute bin index\n      size_t idx = quadrant;\n\n      #pragma omp atomic\n      bins[idx]++;\n   }\n}",
            "for (auto point: points) {\n        if (point.x < 0.0 && point.y < 0.0) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (point.x < 0.0 && point.y > 0.0) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (point.x > 0.0 && point.y > 0.0) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         bins[0]++;\n      } else if (points[i].x > 0 && points[i].y < 0) {\n         bins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         bins[2]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         bins[3]++;\n      }\n   }\n}",
            "int num_points = points.size();\n    int num_threads = omp_get_max_threads();\n    int block_size = num_points/num_threads;\n\n    omp_set_num_threads(num_threads);\n\n#pragma omp parallel\n    {\n        int thread_num = omp_get_thread_num();\n        int thread_size = thread_num < num_threads - 1? block_size : num_points - block_size * (num_threads - 1);\n\n        int start = thread_num * block_size;\n        int end = start + thread_size;\n\n        for (int i = start; i < end; i++) {\n            if (points[i].x > 0 && points[i].y > 0) {\n                bins[0]++;\n            } else if (points[i].x < 0 && points[i].y > 0) {\n                bins[1]++;\n            } else if (points[i].x < 0 && points[i].y < 0) {\n                bins[2]++;\n            } else if (points[i].x > 0 && points[i].y < 0) {\n                bins[3]++;\n            }\n        }\n    }\n}",
            "// TODO: implement this function\n   #pragma omp parallel\n   {\n      #pragma omp for\n      for(size_t i = 0; i < points.size(); i++) {\n         if(points[i].x >= 0 && points[i].y >= 0) {\n            bins[0]++;\n         }\n         else if(points[i].x < 0 && points[i].y < 0) {\n            bins[3]++;\n         }\n         else if(points[i].x < 0) {\n            bins[2]++;\n         }\n         else {\n            bins[1]++;\n         }\n      }\n   }\n}",
            "#pragma omp parallel\n   {\n      int omp_thread_num = omp_get_thread_num();\n      int omp_num_threads = omp_get_num_threads();\n\n      size_t num_points = points.size();\n      size_t points_per_thread = num_points / omp_num_threads;\n      size_t extra_points = num_points % omp_num_threads;\n\n      size_t start_point = omp_thread_num * points_per_thread;\n      size_t end_point = start_point + points_per_thread;\n\n      if (omp_thread_num < extra_points) {\n         end_point++;\n      }\n\n      for (size_t i = start_point; i < end_point; i++) {\n         Point p = points[i];\n         if (p.x > 0) {\n            if (p.y > 0) {\n               bins[0]++;\n            }\n            else {\n               bins[3]++;\n            }\n         }\n         else {\n            if (p.y > 0) {\n               bins[1]++;\n            }\n            else {\n               bins[2]++;\n            }\n         }\n      }\n   }\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for schedule(static)\n        for (int i = 0; i < points.size(); i++) {\n            if (points[i].x > 0)\n                if (points[i].y > 0)\n                    bins[0]++;\n                else\n                    bins[3]++;\n            else\n                if (points[i].y > 0)\n                    bins[1]++;\n                else\n                    bins[2]++;\n        }\n    }\n}",
            "// TODO\n}",
            "// 1. Loop over all points\n    // 2. Classify each point into one of the four quadrants\n    // 3. Add 1 to the correct bin\n\n    bins = {0,0,0,0};\n\n    // 1. Loop over all points\n#pragma omp parallel for\n    for (int i = 0; i < points.size(); i++)\n    {\n        // 2. Classify each point into one of the four quadrants\n        if (points[i].x > 0 && points[i].y > 0)\n        {\n            bins[0]++;\n        }\n        if (points[i].x > 0 && points[i].y < 0)\n        {\n            bins[1]++;\n        }\n        if (points[i].x < 0 && points[i].y < 0)\n        {\n            bins[2]++;\n        }\n        if (points[i].x < 0 && points[i].y > 0)\n        {\n            bins[3]++;\n        }\n    }\n\n    // 3. Add 1 to the correct bin\n\n}",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < points.size(); i++) {\n\t\tint quadrant = 0;\n\t\tif (points[i].x > 0 && points[i].y > 0) quadrant = 0;\n\t\telse if (points[i].x < 0 && points[i].y > 0) quadrant = 1;\n\t\telse if (points[i].x < 0 && points[i].y < 0) quadrant = 2;\n\t\telse if (points[i].x > 0 && points[i].y < 0) quadrant = 3;\n\t\t\n\t\tswitch (quadrant) {\n\t\t\tcase 0:\n\t\t\t\tbins[0]++;\n\t\t\t\tbreak;\n\t\t\tcase 1:\n\t\t\t\tbins[1]++;\n\t\t\t\tbreak;\n\t\t\tcase 2:\n\t\t\t\tbins[2]++;\n\t\t\t\tbreak;\n\t\t\tcase 3:\n\t\t\t\tbins[3]++;\n\t\t\t\tbreak;\n\t\t}\n\t}\n}",
            "// TODO: fill in this function\n   // You can use #pragma omp parallel for\n   bins.fill(0);\n   for (size_t i = 0; i < points.size(); i++){\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x > 0.0 && y > 0.0) {\n         bins[0]++;\n      }\n      if (x < 0.0 && y > 0.0) {\n         bins[1]++;\n      }\n      if (x < 0.0 && y < 0.0) {\n         bins[2]++;\n      }\n      if (x > 0.0 && y < 0.0) {\n         bins[3]++;\n      }\n   }\n}",
            "omp_set_num_threads(2);\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      if (points[i].x >= 0) {\n         if (points[i].y >= 0) {\n            bins[0]++;\n         } else {\n            bins[3]++;\n         }\n      } else {\n         if (points[i].y >= 0) {\n            bins[1]++;\n         } else {\n            bins[2]++;\n         }\n      }\n   }\n}",
            "int  nthreads = 0;\n    #pragma omp parallel \n    {\n        #pragma omp single\n        nthreads = omp_get_num_threads();\n    }\n    std::cout << \"Num of threads: \" << nthreads << std::endl;\n    int quadrant;\n    int i;\n    #pragma omp parallel for default(none) shared(points,bins) private(i, quadrant) schedule(dynamic)\n    for(i = 0; i < points.size(); i++)\n    {\n        quadrant = 0;\n        if (points.at(i).x >= 0 && points.at(i).y >= 0) quadrant = 0;\n        if (points.at(i).x < 0 && points.at(i).y >= 0) quadrant = 1;\n        if (points.at(i).x < 0 && points.at(i).y < 0) quadrant = 2;\n        if (points.at(i).x >= 0 && points.at(i).y < 0) quadrant = 3;\n        // std::cout << \"Quadrant: \" << quadrant << std::endl;\n        bins.at(quadrant)++;\n    }\n}",
            "int numThreads = omp_get_num_threads();\n    int threadID = omp_get_thread_num();\n\n    size_t i = 0;\n    size_t index;\n\n    for (auto const& point : points) {\n\n        if (point.x < 0.0 && point.y < 0.0) {\n            index = 0;\n        }\n        else if (point.x >= 0.0 && point.y < 0.0) {\n            index = 1;\n        }\n        else if (point.x >= 0.0 && point.y >= 0.0) {\n            index = 2;\n        }\n        else {\n            index = 3;\n        }\n\n        #pragma omp atomic\n        bins[index]++;\n\n        i++;\n    }\n}",
            "#pragma omp parallel\n    {\n        int nthreads = omp_get_num_threads();\n        int thread_id = omp_get_thread_num();\n\n        int chunk_size = points.size() / nthreads;\n        int start_index = chunk_size * thread_id;\n\n        for (int i = 0; i < chunk_size; i++) {\n            int index = start_index + i;\n            if (points[index].x > 0 && points[index].y > 0)\n                bins[0]++;\n            else if (points[index].x < 0 && points[index].y > 0)\n                bins[1]++;\n            else if (points[index].x < 0 && points[index].y < 0)\n                bins[2]++;\n            else if (points[index].x > 0 && points[index].y < 0)\n                bins[3]++;\n            else\n                bins[4]++;\n        }\n    }\n\n}",
            "// Your code here\n}",
            "// TODO\n}",
            "#pragma omp parallel for\n    for(size_t i = 0; i < points.size(); ++i){\n        if (points[i].x > 0.0 && points[i].y > 0.0) {\n            bins[0] += 1;\n        }\n        else if (points[i].x < 0.0 && points[i].y > 0.0) {\n            bins[1] += 1;\n        }\n        else if (points[i].x < 0.0 && points[i].y < 0.0) {\n            bins[2] += 1;\n        }\n        else if (points[i].x > 0.0 && points[i].y < 0.0) {\n            bins[3] += 1;\n        }\n    }\n}",
            "int x;\n    int y;\n\n    #pragma omp parallel for\n    for (int i = 0; i < (int) points.size(); i++){\n        if(points.at(i).x >= 0 && points.at(i).x <= 2)\n            x = 0;\n        else if (points.at(i).x >= 2 && points.at(i).x <= 4)\n            x = 1;\n        else if (points.at(i).x >= 4 && points.at(i).x <= 6)\n            x = 2;\n        else\n            x = 3;\n\n        if(points.at(i).y >= 0 && points.at(i).y <= 2)\n            y = 0;\n        else if (points.at(i).y >= 2 && points.at(i).y <= 4)\n            y = 1;\n        else if (points.at(i).y >= 4 && points.at(i).y <= 6)\n            y = 2;\n        else\n            y = 3;\n\n        bins[x]++;\n        bins[y]++;\n    }\n}",
            "#pragma omp parallel for\n    for(auto const& pt: points){\n        size_t i=0;\n        if(pt.x>0 && pt.y>0){i=0;}\n        else if(pt.x<0 && pt.y>0){i=1;}\n        else if(pt.x<0 && pt.y<0){i=2;}\n        else if(pt.x>0 && pt.y<0){i=3;}\n        bins[i]++;\n    }\n}",
            "// TODO\n}",
            "for(auto i = 0; i<points.size(); i++){\n      if(points[i].x >= 0 && points[i].y >= 0){\n         bins[0]++;\n      } else if(points[i].x >= 0 && points[i].y < 0){\n         bins[1]++;\n      } else if(points[i].x < 0 && points[i].y >= 0){\n         bins[2]++;\n      } else{\n         bins[3]++;\n      }\n   }\n   return;\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < points.size(); i++){\n      if(points[i].x > 0 && points[i].y > 0) bins[0]++;\n      else if(points[i].x < 0 && points[i].y > 0) bins[1]++;\n      else if(points[i].x < 0 && points[i].y < 0) bins[2]++;\n      else if(points[i].x > 0 && points[i].y < 0) bins[3]++;\n    }\n}",
            "// Your code here\n#pragma omp parallel for\n   for (int i = 0; i < 4; i++){\n    for (int j = 0; j < points.size(); j++){\n      if(points[j].x > 0 && points[j].y > 0){\n        if(i == 0){\n          bins[0]++;\n        }\n      }else if(points[j].x > 0 && points[j].y < 0){\n        if(i == 1){\n          bins[1]++;\n        }\n      }else if(points[j].x < 0 && points[j].y < 0){\n        if(i == 2){\n          bins[2]++;\n        }\n      }else if(points[j].x < 0 && points[j].y > 0){\n        if(i == 3){\n          bins[3]++;\n        }\n      }\n    }\n  }\n}",
            "#pragma omp parallel for num_threads(6)\n    for(size_t i = 0; i < points.size(); i++) {\n        if (points[i].x >= 0.0) {\n            if (points[i].y >= 0.0) {\n                bins[0]++;\n            }\n            else {\n                bins[3]++;\n            }\n        }\n        else {\n            if (points[i].y >= 0.0) {\n                bins[1]++;\n            }\n            else {\n                bins[2]++;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    if (points[i].x >= 0 && points[i].y >= 0) {\n      bins[0]++;\n    }\n    if (points[i].x >= 0 && points[i].y < 0) {\n      bins[1]++;\n    }\n    if (points[i].x < 0 && points[i].y < 0) {\n      bins[2]++;\n    }\n    if (points[i].x < 0 && points[i].y >= 0) {\n      bins[3]++;\n    }\n  }\n}",
            "//TODO: Your code here\n\n}",
            "for (size_t i = 0; i < 4; i++)\n        bins[i] = 0;\n\n    omp_set_num_threads(omp_get_max_threads());\n#pragma omp parallel for\n    for (int i = 0; i < points.size(); i++)\n    {\n        int indx = 0;\n        if (points[i].x >= 0 && points[i].y >= 0)\n            indx = 0;\n        else if (points[i].x >= 0 && points[i].y < 0)\n            indx = 1;\n        else if (points[i].x < 0 && points[i].y >= 0)\n            indx = 2;\n        else if (points[i].x < 0 && points[i].y < 0)\n            indx = 3;\n\n        bins[indx]++;\n    }\n}",
            "for(auto p: points) {\n        if(p.x >= 0 && p.y >= 0) {\n            ++bins[0];\n        }\n        else if(p.x < 0 && p.y >= 0) {\n            ++bins[1];\n        }\n        else if(p.x < 0 && p.y < 0) {\n            ++bins[2];\n        }\n        else if(p.x >= 0 && p.y < 0) {\n            ++bins[3];\n        }\n    }\n}",
            "// TODO\n}",
            "std::array<size_t, 4> counts;\n   // 1. count in each thread (use a for loop)\n   // 2. join the counts\n   // 3. parallel\n#pragma omp parallel for\n   for(size_t i = 0; i < points.size(); i++){\n       int count = 0;\n       if(points[i].x > 0 && points[i].y > 0){\n           count = 1;\n       }\n       else if(points[i].x > 0 && points[i].y < 0){\n           count = 2;\n       }\n       else if(points[i].x < 0 && points[i].y > 0){\n           count = 3;\n       }\n       else if(points[i].x < 0 && points[i].y < 0){\n           count = 4;\n       }\n       counts[count]++;\n   }\n   for(int i = 0; i < 4; i++){\n       bins[i] += counts[i];\n   }\n}",
            "#pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        #pragma omp for schedule(static)\n        for(size_t i = 0; i < points.size(); i++) {\n            if(points[i].x > 0 && points[i].y > 0) {\n                bins[0] += 1;\n            } else if (points[i].x < 0 && points[i].y > 0) {\n                bins[1] += 1;\n            } else if (points[i].x < 0 && points[i].y < 0) {\n                bins[2] += 1;\n            } else if (points[i].x > 0 && points[i].y < 0) {\n                bins[3] += 1;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); i++){\n        double x = points[i].x;\n        double y = points[i].y;\n        if (x < 0){\n            if (y < 0){\n                bins[0]++;\n            }\n            else{\n                bins[2]++;\n            }\n        }\n        else {\n            if (y < 0){\n                bins[1]++;\n            }\n            else{\n                bins[3]++;\n            }\n        }\n    }\n}",
            "omp_set_num_threads(4);\n    #pragma omp parallel\n    {\n        const size_t my_thread_num = omp_get_thread_num();\n        const size_t my_num_threads = omp_get_num_threads();\n        auto const size = points.size();\n        double xmin = 0, xmax = 0, ymin = 0, ymax = 0;\n        double x = 0, y = 0;\n        size_t q = 0;\n        for (size_t i = 0; i < size; ++i) {\n            x = points[i].x;\n            y = points[i].y;\n            if (x < xmin) xmin = x;\n            if (x > xmax) xmax = x;\n            if (y < ymin) ymin = y;\n            if (y > ymax) ymax = y;\n        }\n        auto const step = (xmax - xmin) / my_num_threads;\n        auto const step2 = (ymax - ymin) / my_num_threads;\n        xmin = xmin + my_thread_num * step;\n        xmax = xmin + step;\n        ymin = ymin + my_thread_num * step2;\n        ymax = ymin + step2;\n        for (size_t i = 0; i < size; ++i) {\n            x = points[i].x;\n            y = points[i].y;\n            if (x < xmin && y < ymin)\n                q = 0;\n            else if (x >= xmin && y >= ymin && x < xmax && y < ymax)\n                q = 1;\n            else if (x >= xmin && y < ymin)\n                q = 2;\n            else\n                q = 3;\n            bins[q] += 1;\n        }\n    }\n}",
            "//TODO\n    // parallelization\n    int n = omp_get_max_threads();\n\n    #pragma omp parallel for num_threads(n)\n    for (int i = 0; i < points.size(); i++) {\n        Point p = points[i];\n        if (p.x > 0) {\n            if (p.y > 0) {\n                bins[0]++;\n            } else {\n                bins[3]++;\n            }\n        } else {\n            if (p.y > 0) {\n                bins[1]++;\n            } else {\n                bins[2]++;\n            }\n        }\n    }\n\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "serial",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "for (const auto &p: points) {\n    size_t index = 0;\n    if (p.x >= 0) {\n      index += 1;\n    }\n    if (p.y >= 0) {\n      index += 2;\n    }\n    bins[index]++;\n  }\n}",
            "for (auto const& point : points) {\n        if (point.x >= 0) {\n            if (point.y >= 0) {\n                bins[0]++;\n            } else {\n                bins[1]++;\n            }\n        } else {\n            if (point.y >= 0) {\n                bins[2]++;\n            } else {\n                bins[3]++;\n            }\n        }\n    }\n}",
            "std::array<size_t, 4> points_per_quadrant;\n   for (size_t i = 0; i < points_per_quadrant.size(); ++i) {\n      points_per_quadrant[i] = 0;\n   }\n   for (const auto &point : points) {\n      if (point.x >= 0) {\n         if (point.y >= 0) {\n            points_per_quadrant[0]++;\n         } else {\n            points_per_quadrant[3]++;\n         }\n      } else {\n         if (point.y >= 0) {\n            points_per_quadrant[1]++;\n         } else {\n            points_per_quadrant[2]++;\n         }\n      }\n   }\n   bins = points_per_quadrant;\n}",
            "// your code here\n}",
            "for (auto const& p: points) {\n      if (p.x > 0.0 && p.y > 0.0) {\n         ++bins[0];\n      } else if (p.x > 0.0 && p.y < 0.0) {\n         ++bins[1];\n      } else if (p.x < 0.0 && p.y < 0.0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "bins.fill(0);\n\n  for (const auto& point : points) {\n    if (point.x > 0 && point.y > 0) {\n      bins[0]++;\n    } else if (point.x < 0 && point.y > 0) {\n      bins[1]++;\n    } else if (point.x < 0 && point.y < 0) {\n      bins[2]++;\n    } else if (point.x > 0 && point.y < 0) {\n      bins[3]++;\n    }\n  }\n}",
            "// Your code goes here\n    size_t count = 0;\n    for(auto const& point : points){\n        if(point.x > 0 && point.y > 0){\n            bins[0]++;\n        }else if(point.x < 0 && point.y > 0){\n            bins[1]++;\n        }else if(point.x < 0 && point.y < 0){\n            bins[2]++;\n        }else if(point.x > 0 && point.y < 0){\n            bins[3]++;\n        }\n    }\n}",
            "for (auto const& p : points) {\n      auto i = std::max(0.0, std::min(1.0, std::floor(p.x)));\n      auto j = std::max(0.0, std::min(1.0, std::floor(p.y)));\n      if (i >= 0 && j >= 0) {\n         bins[0]++;\n      } else if (i < 0 && j >= 0) {\n         bins[1]++;\n      } else if (i >= 0 && j < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "for (Point p : points) {\n        size_t index;\n        if (p.x > 0 && p.y > 0) {\n            index = 0;\n        } else if (p.x < 0 && p.y > 0) {\n            index = 1;\n        } else if (p.x < 0 && p.y < 0) {\n            index = 2;\n        } else if (p.x > 0 && p.y < 0) {\n            index = 3;\n        }\n        bins[index]++;\n    }\n}",
            "for (auto point : points) {\n      if (point.x > 0 && point.y > 0)\n         bins[0]++;\n      else if (point.x < 0 && point.y > 0)\n         bins[1]++;\n      else if (point.x < 0 && point.y < 0)\n         bins[2]++;\n      else if (point.x > 0 && point.y < 0)\n         bins[3]++;\n   }\n}",
            "for(auto const& point : points)\n    {\n        if (point.x > 0 && point.y > 0)\n            bins[0]++;\n        else if (point.x < 0 && point.y > 0)\n            bins[1]++;\n        else if (point.x < 0 && point.y < 0)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "for (auto &p : points) {\n        bins[p.x >= 0.0? 0 : 1] += p.x >= 0.0? p.y >= 0.0 : 1;\n    }\n}",
            "for (auto p : points) {\n        if (p.x > 0 && p.y > 0)\n            ++bins[0];\n        else if (p.x > 0 && p.y < 0)\n            ++bins[1];\n        else if (p.x < 0 && p.y < 0)\n            ++bins[2];\n        else\n            ++bins[3];\n    }\n}",
            "/* Your code goes here */\n}",
            "for (auto const& p : points) {\n    if (p.x > 0) {\n      if (p.y > 0) {\n        bins[0] += 1;\n      } else {\n        bins[3] += 1;\n      }\n    } else {\n      if (p.y > 0) {\n        bins[1] += 1;\n      } else {\n        bins[2] += 1;\n      }\n    }\n  }\n}",
            "for (auto const& p : points) {\n      if (p.x < 0 && p.y < 0) bins[0]++;\n      else if (p.x < 0 && p.y > 0) bins[1]++;\n      else if (p.x > 0 && p.y > 0) bins[2]++;\n      else bins[3]++;\n   }\n}",
            "bins.fill(0);\n    for (auto& p : points) {\n        if (p.x >= 0) {\n            if (p.y >= 0) bins[0]++;\n            else bins[3]++;\n        }\n        else {\n            if (p.y >= 0) bins[1]++;\n            else bins[2]++;\n        }\n    }\n}",
            "// Your code here\n}",
            "for (auto const& point : points) {\n        if (point.x >= 0 && point.y >= 0) {\n            bins[0]++;\n        } else if (point.x < 0 && point.y >= 0) {\n            bins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            bins[2]++;\n        } else if (point.x >= 0 && point.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "// TODO\n}",
            "for(const auto& p : points) {\n      size_t bin = 0;\n      if (p.x >= 0) {\n         bin += 1;\n         if (p.y >= 0) {\n            bin += 1;\n         }\n      }\n      bins[bin]++;\n   }\n}",
            "bins.fill(0);\n    for (auto point : points) {\n        if (point.x > 0) {\n            if (point.y > 0)\n                bins[0]++;\n            else\n                bins[3]++;\n        }\n        else {\n            if (point.y > 0)\n                bins[1]++;\n            else\n                bins[2]++;\n        }\n    }\n}",
            "for (auto& point: points) {\n      if (point.x > 0) {\n         if (point.y > 0)\n            bins[0]++;\n         else\n            bins[3]++;\n      } else {\n         if (point.y > 0)\n            bins[2]++;\n         else\n            bins[1]++;\n      }\n   }\n}",
            "// Your code here\n}",
            "for(auto const& p : points) {\n      int x = static_cast<int>(p.x);\n      int y = static_cast<int>(p.y);\n      if(x > 0) {\n         if(y > 0) {\n            ++bins[0];\n         } else {\n            ++bins[3];\n         }\n      } else {\n         if(y > 0) {\n            ++bins[2];\n         } else {\n            ++bins[1];\n         }\n      }\n   }\n}",
            "}",
            "for (auto const& p : points) {\n        auto& bin = bins[p.x >= 0? (p.y >= 0? 0 : 2) : (p.y >= 0? 1 : 3)];\n        ++bin;\n    }\n}",
            "for (auto const& point : points) {\n        if (point.x >= 0 && point.y >= 0) {\n            ++bins[0];\n        }\n        else if (point.x < 0 && point.y < 0) {\n            ++bins[3];\n        }\n        else if (point.x < 0 && point.y >= 0) {\n            ++bins[1];\n        }\n        else if (point.x >= 0 && point.y < 0) {\n            ++bins[2];\n        }\n    }\n}",
            "auto f = [](auto &x, auto const& y) { return (x.x < 0? 0 : 1) + (x.y < 0? 0 : 2); };\n    std::for_each(std::execution::par, std::begin(points), std::end(points),\n                  [&](auto const& p) {\n                      bins.at(f(p, p))++;\n                  });\n}",
            "for (auto const& pt : points) {\n      bins[std::min(pt.x >= 0, pt.y >= 0)]++;\n   }\n}",
            "for (const auto& p : points) {\n    size_t b = 0;\n    if (p.x >= 0) {\n      b = 1;\n      if (p.y >= 0)\n        b = 2;\n    }\n    bins[b]++;\n  }\n}",
            "std::array<double, 4> x_bins = {0, 1, 3, 5};\n    std::array<double, 4> y_bins = {0, 1, -3, -7};\n\n    bins.fill(0);\n\n    for (auto const& point : points) {\n        if (point.x >= x_bins[0] && point.x < x_bins[1]) {\n            if (point.y >= y_bins[0] && point.y < y_bins[1]) {\n                ++bins[0];\n            } else if (point.y >= y_bins[1] && point.y < y_bins[2]) {\n                ++bins[1];\n            } else if (point.y >= y_bins[2] && point.y < y_bins[3]) {\n                ++bins[2];\n            } else if (point.y >= y_bins[3] && point.y <= y_bins[4]) {\n                ++bins[3];\n            } else {\n                std::cerr << \"Invalid point.x or point.y\\n\";\n            }\n        } else if (point.x >= x_bins[1] && point.x < x_bins[2]) {\n            if (point.y >= y_bins[0] && point.y < y_bins[1]) {\n                ++bins[1];\n            } else if (point.y >= y_bins[1] && point.y < y_bins[2]) {\n                ++bins[2];\n            } else if (point.y >= y_bins[2] && point.y < y_bins[3]) {\n                ++bins[3];\n            } else if (point.y >= y_bins[3] && point.y <= y_bins[4]) {\n                ++bins[0];\n            } else {\n                std::cerr << \"Invalid point.x or point.y\\n\";\n            }\n        } else if (point.x >= x_bins[2] && point.x < x_bins[3]) {\n            if (point.y >= y_bins[0] && point.y < y_bins[1]) {\n                ++bins[2];\n            } else if (point.y >= y_bins[1] && point.y < y_bins[2]) {\n                ++bins[3];\n            } else if (point.y >= y_bins[2] && point.y < y_bins[3]) {\n                ++bins[0];\n            } else if (point.y >= y_bins[3] && point.y <= y_bins[4]) {\n                ++bins[1];\n            } else {\n                std::cerr << \"Invalid point.x or point.y\\n\";\n            }\n        } else if (point.x >= x_bins[3] && point.x <= x_bins[4]) {\n            if (point.y >= y_bins[0] && point.y < y_bins[1]) {\n                ++bins[3];\n            } else if (point.y >= y_bins[1] && point.y < y_bins[2]) {\n                ++bins[0];\n            } else if (point.y >= y_bins[2] && point.y < y_bins[3]) {\n                ++bins[1];\n            } else if (point.y >= y_bins[3] && point.y <= y_bins[4]) {\n                ++bins[2];\n            } else {\n                std::cerr << \"Invalid point.x or point.y\\n\";\n            }\n        } else {\n            std::cerr << \"Invalid point.x\\n\";\n        }\n    }\n}",
            "// TODO: implement this\n}",
            "bins.fill(0);\n    for (auto const& p : points) {\n        if (p.x > 0.0 && p.y > 0.0)\n            ++bins[0];\n        else if (p.x > 0.0 && p.y < 0.0)\n            ++bins[1];\n        else if (p.x < 0.0 && p.y < 0.0)\n            ++bins[2];\n        else if (p.x < 0.0 && p.y > 0.0)\n            ++bins[3];\n    }\n}",
            "// insert your code here\n}",
            "for (const auto& p : points) {\n        bins[p.x > 0? 0 : 1]++;\n        bins[p.y > 0? 0 : 1]++;\n    }\n    for (auto& n : bins) {\n        n /= 2;\n    }\n}",
            "bins = {};\n    for (const auto &point : points) {\n        bins[static_cast<size_t>((point.x >= 0 && point.y >= 0)? 0 : (point.x < 0 && point.y < 0)? 3 : (point.x < 0 && point.y >= 0)? 1 : 2)]++;\n    }\n}",
            "for (auto& p : points) {\n      if (p.x >= 0 && p.y >= 0)\n         bins[0]++;\n      if (p.x >= 0 && p.y < 0)\n         bins[1]++;\n      if (p.x < 0 && p.y < 0)\n         bins[2]++;\n      if (p.x < 0 && p.y >= 0)\n         bins[3]++;\n   }\n}",
            "// insert your code here\n}",
            "for(size_t i=0;i<points.size();i++) {\n        if(points[i].x>=0 && points[i].y>=0)\n            bins[0]++;\n        else if(points[i].x>=0 && points[i].y<0)\n            bins[1]++;\n        else if(points[i].x<0 && points[i].y<0)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "for(auto const& p : points) {\n        if(p.x > 0.0 && p.y > 0.0) {\n            bins[0]++;\n        }\n        else if(p.x < 0.0 && p.y > 0.0) {\n            bins[1]++;\n        }\n        else if(p.x < 0.0 && p.y < 0.0) {\n            bins[2]++;\n        }\n        else if(p.x > 0.0 && p.y < 0.0) {\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: implement me!\n}",
            "// Your code here\n\n    std::vector<Point>::const_iterator point;\n    for (point = points.begin(); point!= points.end(); point++)\n    {\n        if (point->x >= 0 && point->y >= 0)\n        {\n            bins[0] += 1;\n        }\n        else if (point->x < 0 && point->y >= 0)\n        {\n            bins[1] += 1;\n        }\n        else if (point->x < 0 && point->y < 0)\n        {\n            bins[2] += 1;\n        }\n        else\n        {\n            bins[3] += 1;\n        }\n    }\n}",
            "// Your code here\n   bins.fill(0);\n   for (auto &p : points) {\n      if (p.x > 0 && p.y > 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y > 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else if (p.x > 0 && p.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "// TODO: your code goes here\n}",
            "for (auto const& point: points) {\n        auto quadrant = 1;\n        if (point.x < 0) {\n            quadrant *= -1;\n        }\n        if (point.y < 0) {\n            quadrant *= -1;\n        }\n        quadrant--;\n        bins[quadrant]++;\n    }\n}",
            "}",
            "for (auto p : points) {\n    if (p.x >= 0) {\n      if (p.y >= 0) {\n        bins[0] += 1;\n      }\n      else {\n        bins[2] += 1;\n      }\n    }\n    else {\n      if (p.y >= 0) {\n        bins[1] += 1;\n      }\n      else {\n        bins[3] += 1;\n      }\n    }\n  }\n}",
            "for (Point const& point : points) {\n        size_t index = 0;\n        if (point.x >= 0) {\n            index++;\n        }\n        if (point.y >= 0) {\n            index += 2;\n        }\n        bins[index]++;\n    }\n}",
            "for(auto const& point: points) {\n      if (point.x >= 0 && point.y >= 0)\n         ++bins[0];\n      else if (point.x < 0 && point.y >= 0)\n         ++bins[1];\n      else if (point.x < 0 && point.y < 0)\n         ++bins[2];\n      else if (point.x >= 0 && point.y < 0)\n         ++bins[3];\n   }\n}",
            "bins.fill(0);\n    for (auto const& point : points) {\n        size_t i = (point.x < 0)? 1 : 0;\n        i += (point.y < 0)? 2 : 0;\n        bins[i]++;\n    }\n}",
            "// Initialize bins\n   for (size_t i = 0; i < bins.size(); ++i) bins[i] = 0;\n\n   // Count in quadrants\n   for (auto& p : points)\n      bins[((p.x > 0.0 && p.y > 0.0) || (p.x <= 0.0 && p.y <= 0.0))? 0 :\n            (p.x > 0.0 && p.y <= 0.0)? 1 :\n            (p.x <= 0.0 && p.y > 0.0)? 2 : 3]++;\n}",
            "for(auto pt : points) {\n        size_t bin = 0;\n        if(pt.x > 0) bin += 1;\n        if(pt.y > 0) bin += 2;\n        bins[bin]++;\n    }\n}",
            "}",
            "//... your code...\n}",
            "bins = { 0, 0, 0, 0 };\n   for(auto const& p : points) {\n      if(p.x >= 0 && p.y >= 0) {\n         ++bins[0];\n      } else if(p.x < 0 && p.y >= 0) {\n         ++bins[1];\n      } else if(p.x < 0 && p.y < 0) {\n         ++bins[2];\n      } else if(p.x >= 0 && p.y < 0) {\n         ++bins[3];\n      }\n   }\n}",
            "bins.fill(0);\n\n    for (Point p : points) {\n        if (p.x >= 0 && p.y >= 0) {\n            ++bins[0];\n        } else if (p.x < 0 && p.y >= 0) {\n            ++bins[1];\n        } else if (p.x < 0 && p.y < 0) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "std::array<size_t, 4> counts = {0,0,0,0};\n    for (auto &p : points) {\n        if (p.x > 0.0 && p.y > 0.0) counts[0]++;\n        else if (p.x < 0.0 && p.y > 0.0) counts[1]++;\n        else if (p.x < 0.0 && p.y < 0.0) counts[2]++;\n        else if (p.x > 0.0 && p.y < 0.0) counts[3]++;\n    }\n    bins = counts;\n}",
            "for (Point const& point: points) {\n        if (point.x >= 0 && point.y >= 0) {\n            bins[0]++;\n        } else if (point.x >= 0 && point.y < 0) {\n            bins[1]++;\n        } else if (point.x < 0 && point.y >= 0) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "bins.fill(0);\n\n   for (auto &point: points) {\n      if (point.x >= 0 && point.y >= 0) {\n         ++bins[0];\n      }\n      else if (point.x < 0 && point.y >= 0) {\n         ++bins[1];\n      }\n      else if (point.x < 0 && point.y < 0) {\n         ++bins[2];\n      }\n      else if (point.x >= 0 && point.y < 0) {\n         ++bins[3];\n      }\n   }\n}",
            "bins.fill(0);\n   for (auto p : points) {\n      if (p.x >= 0 && p.y >= 0) {\n         ++bins[0];\n      } else if (p.x < 0 && p.y >= 0) {\n         ++bins[1];\n      } else if (p.x < 0 && p.y < 0) {\n         ++bins[2];\n      } else if (p.x >= 0 && p.y < 0) {\n         ++bins[3];\n      } else {\n         assert(false);\n      }\n   }\n}",
            "int quadrant[6] = {0, 0, 0, 0, 0, 0};\n    for(int i = 0; i < points.size(); ++i) {\n        if (points[i].x >= 0 && points[i].y >= 0)\n            quadrant[0]++;\n        else if (points[i].x < 0 && points[i].y >= 0)\n            quadrant[1]++;\n        else if (points[i].x < 0 && points[i].y < 0)\n            quadrant[2]++;\n        else if (points[i].x >= 0 && points[i].y < 0)\n            quadrant[3]++;\n        else if (points[i].x == 0 && points[i].y == 0)\n            quadrant[4]++;\n        else if (points[i].x == 0 && points[i].y == -0)\n            quadrant[5]++;\n    }\n    bins = {{quadrant[0], quadrant[1], quadrant[2], quadrant[3]}};\n}",
            "std::map<Point, size_t> quadrants;\n\n   for (auto p : points) {\n      quadrants[p]++;\n   }\n\n   bins.fill(0);\n   for (auto kv : quadrants) {\n      bins[getQuadrant(kv.first)] += kv.second;\n   }\n}",
            "bins.fill(0);\n\n   // TODO: Your code here\n   for (auto const& p: points) {\n      if (p.x > 0 && p.y > 0) bins[0]++;\n      else if (p.x > 0 && p.y < 0) bins[1]++;\n      else if (p.x < 0 && p.y < 0) bins[2]++;\n      else if (p.x < 0 && p.y > 0) bins[3]++;\n   }\n}",
            "size_t N = points.size();\n   for(size_t i = 0; i < N; ++i) {\n      size_t j = (points[i].x >= 0.0? 0 : 1);\n      j += (points[i].y >= 0.0? 0 : 2);\n      ++bins[j];\n   }\n}",
            "for (auto& point : points) {\n        auto quadrant = 0;\n        if (point.x >= 0) {\n            quadrant += 1;\n            if (point.y >= 0) {\n                quadrant += 2;\n            }\n        }\n        bins[quadrant - 1] += 1;\n    }\n}",
            "bins.fill(0);\n    for (auto const& p: points) {\n        if (p.x > 0) {\n            if (p.y > 0)\n                ++bins[0];\n            else\n                ++bins[3];\n        }\n        else {\n            if (p.y > 0)\n                ++bins[1];\n            else\n                ++bins[2];\n        }\n    }\n}",
            "bins.fill(0);\n   for (auto const& point : points) {\n      size_t i = 0;\n      if (point.x >= 0) {\n         ++bins[i++];\n      }\n      if (point.y >= 0) {\n         ++bins[i++];\n      }\n   }\n}",
            "for (Point const& point : points) {\n        bins[0] += point.x > 0 && point.y > 0;\n        bins[1] += point.x < 0 && point.y > 0;\n        bins[2] += point.x < 0 && point.y < 0;\n        bins[3] += point.x > 0 && point.y < 0;\n    }\n}",
            "for (auto const& p : points) {\n        int x = 0, y = 0;\n        if (p.x > 0) x = 1;\n        else if (p.x < 0) x = -1;\n        if (p.y > 0) y = 1;\n        else if (p.y < 0) y = -1;\n        bins[x + y + 1] += 1;\n    }\n}",
            "for (auto& point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         ++bins[0];\n      } else if (point.x < 0 && point.y >= 0) {\n         ++bins[1];\n      } else if (point.x < 0 && point.y < 0) {\n         ++bins[2];\n      } else if (point.x >= 0 && point.y < 0) {\n         ++bins[3];\n      }\n   }\n}",
            "bins.fill(0);\n\n   for(auto p : points)\n      if(p.x >= 0)\n         if(p.y >= 0)\n            bins[0] += 1;\n         else\n            bins[3] += 1;\n      else\n         if(p.y >= 0)\n            bins[1] += 1;\n         else\n            bins[2] += 1;\n}",
            "for(auto const& p : points) {\n        if(p.x >= 0) {\n            if(p.y >= 0) {\n                bins[0]++;\n            }\n            else {\n                bins[3]++;\n            }\n        }\n        else {\n            if(p.y >= 0) {\n                bins[1]++;\n            }\n            else {\n                bins[2]++;\n            }\n        }\n    }\n}",
            "std::array<size_t, 4> c = {0,0,0,0};\n   for(auto& p : points) {\n      if(p.x > 0)\n         if(p.y > 0)\n            ++c[0];\n         else\n            ++c[1];\n      else\n         if(p.y > 0)\n            ++c[2];\n         else\n            ++c[3];\n   }\n   bins = c;\n}",
            "// TODO: Your code goes here\n   for (auto i : points) {\n      if (i.x > 0) {\n         if (i.y > 0) {\n            bins[0] += 1;\n         }\n         else if (i.y < 0) {\n            bins[3] += 1;\n         }\n         else {\n            bins[2] += 1;\n         }\n      }\n      else if (i.x < 0) {\n         if (i.y > 0) {\n            bins[1] += 1;\n         }\n         else if (i.y < 0) {\n            bins[2] += 1;\n         }\n         else {\n            bins[3] += 1;\n         }\n      }\n   }\n}",
            "for (const Point &p: points) {\n    if (p.x > 0 && p.y > 0)\n      bins[0] += 1;\n    else if (p.x > 0 && p.y < 0)\n      bins[1] += 1;\n    else if (p.x < 0 && p.y < 0)\n      bins[2] += 1;\n    else if (p.x < 0 && p.y > 0)\n      bins[3] += 1;\n  }\n}",
            "bins = {};\n   for (auto const& p : points) {\n      size_t index = 0;\n      if (p.x > 0) {\n         if (p.y > 0) {\n            index = 0;\n         } else {\n            index = 3;\n         }\n      } else {\n         if (p.y > 0) {\n            index = 1;\n         } else {\n            index = 2;\n         }\n      }\n      ++bins[index];\n   }\n}",
            "for(const auto& p : points) {\n      if (p.x >= 0.0 && p.y >= 0.0) {\n         bins[0]++;\n      }\n      else if (p.x <= 0.0 && p.y >= 0.0) {\n         bins[1]++;\n      }\n      else if (p.x <= 0.0 && p.y <= 0.0) {\n         bins[2]++;\n      }\n      else if (p.x >= 0.0 && p.y <= 0.0) {\n         bins[3]++;\n      }\n   }\n}",
            "// Your code here\n}",
            "for (auto p : points) {\n        size_t quadrant = 0;\n        if (p.x > 0 && p.y > 0) {\n            quadrant = 1;\n        } else if (p.x < 0 && p.y > 0) {\n            quadrant = 2;\n        } else if (p.x < 0 && p.y < 0) {\n            quadrant = 3;\n        } else {\n            quadrant = 0;\n        }\n        bins[quadrant]++;\n    }\n}",
            "for(auto& pt : points) {\n        if(pt.x > 0 && pt.y > 0) {\n            bins[0]++;\n        } else if (pt.x > 0 && pt.y < 0) {\n            bins[1]++;\n        } else if (pt.x < 0 && pt.y > 0) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "for(auto& point : points) {\n      if(point.x >= 0.0 && point.y >= 0.0) {\n         bins[0]++;\n      }\n      if(point.x <= 0.0 && point.y >= 0.0) {\n         bins[1]++;\n      }\n      if(point.x <= 0.0 && point.y <= 0.0) {\n         bins[2]++;\n      }\n      if(point.x >= 0.0 && point.y <= 0.0) {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& point: points) {\n      auto x = point.x;\n      auto y = point.y;\n      if (x >= 0 && y >= 0) {\n         bins[0] += 1;\n      }\n      else if (x < 0 && y >= 0) {\n         bins[1] += 1;\n      }\n      else if (x < 0 && y < 0) {\n         bins[2] += 1;\n      }\n      else if (x >= 0 && y < 0) {\n         bins[3] += 1;\n      }\n   }\n}",
            "for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            ++bins[0];\n        }\n        else if (point.x > 0 && point.y < 0) {\n            ++bins[1];\n        }\n        else if (point.x < 0 && point.y < 0) {\n            ++bins[2];\n        }\n        else if (point.x < 0 && point.y > 0) {\n            ++bins[3];\n        }\n    }\n}",
            "// Your code here\n}",
            "std::array<std::vector<Point>, 4> quadrants{};\n   for (auto const& p : points) {\n      if (p.x >= 0.0) {\n         if (p.y >= 0.0)\n            quadrants[0].push_back(p);\n         else\n            quadrants[3].push_back(p);\n      } else {\n         if (p.y >= 0.0)\n            quadrants[1].push_back(p);\n         else\n            quadrants[2].push_back(p);\n      }\n   }\n\n   for (int i = 0; i < 4; ++i)\n      bins[i] = quadrants[i].size();\n}",
            "bins.fill(0);\n   for (auto const& p: points) {\n      if (p.x >= 0 && p.y >= 0)\n         ++bins[0];\n      else if (p.x >= 0 && p.y < 0)\n         ++bins[1];\n      else if (p.x < 0 && p.y >= 0)\n         ++bins[2];\n      else if (p.x < 0 && p.y < 0)\n         ++bins[3];\n   }\n}",
            "bins.fill(0);\n  for (auto const& point : points) {\n    if (point.x >= 0.0) {\n      if (point.y >= 0.0) {\n        bins[0]++;\n      } else {\n        bins[3]++;\n      }\n    } else {\n      if (point.y >= 0.0) {\n        bins[1]++;\n      } else {\n        bins[2]++;\n      }\n    }\n  }\n}",
            "bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n  for (size_t i = 0; i < points.size(); i++) {\n\n    if (points[i].x > 0 && points[i].y > 0)\n      bins[0]++;\n\n    if (points[i].x < 0 && points[i].y > 0)\n      bins[1]++;\n\n    if (points[i].x < 0 && points[i].y < 0)\n      bins[2]++;\n\n    if (points[i].x > 0 && points[i].y < 0)\n      bins[3]++;\n\n  }\n}",
            "// your code here...\n}",
            "}",
            "bins.fill(0);\n    for(auto const& point: points) {\n        auto quadrant = std::array<int, 2>{ \n            (point.x >= 0) - (point.x < 0),\n            (point.y >= 0) - (point.y < 0)\n        };\n        bins[quadrant[0] + quadrant[1] * 2]++;\n    }\n}",
            "for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0)\n            bins[0]++;\n        else if (point.x < 0 && point.y > 0)\n            bins[1]++;\n        else if (point.x < 0 && point.y < 0)\n            bins[2]++;\n        else if (point.x > 0 && point.y < 0)\n            bins[3]++;\n    }\n}",
            "for (auto const& p : points) {\n        if (p.x >= 0) {\n            if (p.y >= 0) {\n                bins[0]++;\n            }\n            else {\n                bins[2]++;\n            }\n        }\n        else {\n            if (p.y >= 0) {\n                bins[1]++;\n            }\n            else {\n                bins[3]++;\n            }\n        }\n    }\n}",
            "std::sort(points.begin(), points.end(),\n      [](const Point& lhs, const Point& rhs) { return lhs.x < rhs.x; });\n   //...\n}",
            "bins.fill(0);\n\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            ++bins[0];\n        } else if (point.x > 0 && point.y < 0) {\n            ++bins[1];\n        } else if (point.x < 0 && point.y < 0) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "}",
            "bins.fill(0);\n\n   for (size_t i = 0; i < points.size(); i++) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         bins[0]++;\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         bins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         bins[2]++;\n      } else if (points[i].x >= 0 && points[i].y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "std::array<size_t, 4> result;\n    result.fill(0);\n\n    for (auto const& point : points)\n    {\n        auto x = static_cast<size_t>(std::floor(point.x / 2));\n        auto y = static_cast<size_t>(std::floor(point.y / 2));\n        result[x + 2*y]++;\n    }\n\n    std::copy(result.begin(), result.end(), bins.begin());\n}",
            "/* your code here */\n}",
            "for (auto const& p : points) {\n      if (p.x > 0 && p.y > 0)\n         bins[0]++;\n      else if (p.x > 0 && p.y < 0)\n         bins[3]++;\n      else if (p.x < 0 && p.y > 0)\n         bins[1]++;\n      else\n         bins[2]++;\n   }\n}",
            "for (size_t i = 0; i < points.size(); i++) {\n        if (points[i].x < 0) {\n            if (points[i].y > 0) {\n                bins[1]++;\n            }\n            else {\n                bins[0]++;\n            }\n        }\n        else {\n            if (points[i].y > 0) {\n                bins[2]++;\n            }\n            else {\n                bins[3]++;\n            }\n        }\n    }\n}",
            "bins.fill(0);\n    for (auto const& point: points) {\n        auto index = (point.x > 0.0) + (point.y > 0.0);\n        ++bins[index];\n    }\n}",
            "bins = { 0, 0, 0, 0 };\n   for(auto& p: points) {\n      if(p.x > 0) {\n         if(p.y > 0) {\n            bins[0]++;\n         } else {\n            bins[3]++;\n         }\n      } else {\n         if(p.y > 0) {\n            bins[1]++;\n         } else {\n            bins[2]++;\n         }\n      }\n   }\n}",
            "bins = {0,0,0,0};\n  for (auto &p:points){\n    if (p.x > 0.0 && p.y > 0.0){\n      bins[0]++;\n    } else if (p.x < 0.0 && p.y > 0.0){\n      bins[1]++;\n    } else if (p.x < 0.0 && p.y < 0.0){\n      bins[2]++;\n    } else if (p.x > 0.0 && p.y < 0.0){\n      bins[3]++;\n    }\n  }\n}",
            "// TODO: implement\n}",
            "for (auto p : points)\n        if (p.x > 0) bins[0]++;\n        else if (p.x < 0) bins[1]++;\n        else if (p.y > 0) bins[2]++;\n        else bins[3]++;\n}",
            "// write your code here\n}",
            "for (auto point : points) {\n      if (point.x >= 0 && point.y >= 0) bins[0]++;\n      else if (point.x < 0 && point.y >= 0) bins[1]++;\n      else if (point.x < 0 && point.y < 0) bins[2]++;\n      else bins[3]++;\n   }\n}",
            "for (auto & p : points) {\n        bins[p.x >= 0 && p.y >= 0? 0 : p.x < 0 && p.y >= 0? 1 : p.x < 0 && p.y < 0? 2 : 3]++;\n    }\n}",
            "for (size_t i = 0; i < points.size(); i++)\n    {\n        if (points[i].x >= 0.0 && points[i].x <= 1.0 && points[i].y >= 0.0 && points[i].y <= 1.0)\n            bins[0]++;\n        else if (points[i].x >= 0.0 && points[i].x <= 1.0 && points[i].y <= -1.0)\n            bins[1]++;\n        else if (points[i].x <= 0.0 && points[i].x >= -1.0 && points[i].y <= 0.0)\n            bins[2]++;\n        else if (points[i].x <= 0.0 && points[i].x >= -1.0 && points[i].y >= 0.0 && points[i].y <= 1.0)\n            bins[3]++;\n    }\n}",
            "for (const auto& point : points) {\n    if (point.x >= 0) {\n      if (point.y >= 0) {\n        ++bins[0];\n      } else {\n        ++bins[3];\n      }\n    } else {\n      if (point.y >= 0) {\n        ++bins[1];\n      } else {\n        ++bins[2];\n      }\n    }\n  }\n}",
            "bins.fill(0);\n   for (auto p : points) {\n      if (p.x > 0 && p.y > 0) {\n         ++bins[0];\n      } else if (p.x > 0 && p.y < 0) {\n         ++bins[1];\n      } else if (p.x < 0 && p.y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "for (const auto& point : points) {\n    if (point.x > 0 && point.y > 0)\n      bins[0] += 1;\n    else if (point.x > 0 && point.y < 0)\n      bins[1] += 1;\n    else if (point.x < 0 && point.y < 0)\n      bins[2] += 1;\n    else if (point.x < 0 && point.y > 0)\n      bins[3] += 1;\n  }\n}",
            "for (auto const& p : points) {\n        if (p.x >= 0 && p.y >= 0) {\n            ++bins[0];\n        } else if (p.x < 0 && p.y >= 0) {\n            ++bins[1];\n        } else if (p.x < 0 && p.y < 0) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "for (auto const &p: points) {\n        auto const& x = p.x;\n        auto const& y = p.y;\n        if (x > 0 && y > 0) {\n            bins[0] += 1;\n        } else if (x < 0 && y > 0) {\n            bins[1] += 1;\n        } else if (x < 0 && y < 0) {\n            bins[2] += 1;\n        } else if (x > 0 && y < 0) {\n            bins[3] += 1;\n        } else {\n            bins[4] += 1;\n        }\n    }\n}",
            "// TODO: Your code goes here\n}",
            "for (auto const& p : points) {\n      if (p.x >= 0 && p.y >= 0) {\n         ++bins[0];\n      } else if (p.x < 0 && p.y >= 0) {\n         ++bins[1];\n      } else if (p.x < 0 && p.y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "size_t total = points.size();\n    for (auto& p : points) {\n        if (p.x >= 0.0 && p.y >= 0.0) {\n            bins[0]++;\n        }\n        else if (p.x < 0.0 && p.y < 0.0) {\n            bins[3]++;\n        }\n        else if (p.x >= 0.0 && p.y < 0.0) {\n            bins[1]++;\n        }\n        else {\n            bins[2]++;\n        }\n    }\n\n    for (auto& v : bins) {\n        std::cout << v << \" \";\n    }\n    std::cout << std::endl;\n}",
            "bins = {0, 0, 0, 0};\n    for(auto const& p : points) {\n        if(p.x >= 0 && p.y >= 0) {\n            bins[0]++;\n        } else if(p.x >= 0 && p.y < 0) {\n            bins[1]++;\n        } else if(p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "for (auto &point : points) {\n        if (point.x > 0.0 && point.y > 0.0) {\n            bins[0] += 1;\n        } else if (point.x < 0.0 && point.y > 0.0) {\n            bins[1] += 1;\n        } else if (point.x < 0.0 && point.y < 0.0) {\n            bins[2] += 1;\n        } else if (point.x > 0.0 && point.y < 0.0) {\n            bins[3] += 1;\n        }\n    }\n}",
            "for (auto const& p : points) {\n        if (p.x > 0 && p.y > 0)\n            bins[0]++;\n        else if (p.x < 0 && p.y > 0)\n            bins[1]++;\n        else if (p.x < 0 && p.y < 0)\n            bins[2]++;\n        else if (p.x > 0 && p.y < 0)\n            bins[3]++;\n    }\n}",
            "// TODO: Implement this function.\n\n}",
            "std::array<size_t, 4> counters = {0, 0, 0, 0};\n\n   for (Point point : points) {\n      if (point.x >= 0) {\n         counters[0]++;\n      } else {\n         counters[1]++;\n      }\n\n      if (point.y >= 0) {\n         counters[2]++;\n      } else {\n         counters[3]++;\n      }\n   }\n\n   bins = counters;\n}",
            "bins.fill(0);\n    for (auto p: points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        }\n        else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        }\n        else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        }\n        else {\n            bins[3]++;\n        }\n    }\n}",
            "// TODO\n}",
            "// TODO: fill in code\n}",
            "bins = { 0, 0, 0, 0 };\n    for(auto const& p : points) {\n        if(p.x < 0 && p.y > 0) {\n            ++bins[0];\n        } else if(p.x < 0 && p.y < 0) {\n            ++bins[1];\n        } else if(p.x > 0 && p.y < 0) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "// TODO\n}",
            "for (auto const& point : points) {\n        int quadrant = 0;\n        if (point.x >= 0) quadrant += 1;\n        if (point.y >= 0) quadrant += 2;\n        bins[quadrant]++;\n    }\n}",
            "for(const auto& p: points) {\n      if(p.x > 0) {\n         if(p.y > 0)\n            ++bins[0];\n         else\n            ++bins[2];\n      } else {\n         if(p.y > 0)\n            ++bins[1];\n         else\n            ++bins[3];\n      }\n   }\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: Your code here\n}",
            "for (auto const& point: points) {\n    auto bin = getQuadrant(point);\n    bins[bin]++;\n  }\n}",
            "size_t n = 0;\n    for (auto const& p : points) {\n        if (p.x < 0) {\n            n++;\n            if (p.y < 0) {\n                bins[0]++;\n            } else {\n                bins[1]++;\n            }\n        } else {\n            n++;\n            if (p.y < 0) {\n                bins[2]++;\n            } else {\n                bins[3]++;\n            }\n        }\n    }\n    bins[0] += n - bins[0] - bins[1] - bins[2] - bins[3];\n}",
            "for (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else if (point.x > 0 && point.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "}",
            "bins.fill(0);\n  for (auto const& p : points) {\n    if (p.x > 0) {\n      if (p.y > 0) bins[0]++;\n      else bins[3]++;\n    }\n    else {\n      if (p.y > 0) bins[1]++;\n      else bins[2]++;\n    }\n  }\n}",
            "// TODO: implement here\n}",
            "for (auto p : points) {\n    if (p.x > 0) {\n      bins[0]++;\n    } else {\n      bins[1]++;\n    }\n    if (p.y > 0) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "// YOUR CODE HERE\n   for (Point const& p : points)\n   {\n      if (p.x > 0.0 && p.y > 0.0) {\n         bins[0]++;\n      }\n      else if (p.x < 0.0 && p.y > 0.0) {\n         bins[1]++;\n      }\n      else if (p.x < 0.0 && p.y < 0.0) {\n         bins[2]++;\n      }\n      else {\n         bins[3]++;\n      }\n   }\n}",
            "bins.fill(0);\n   for (auto const& point : points) {\n      if (point.x > 0)\n         if (point.y > 0)\n            bins[0] += 1;\n         else\n            bins[1] += 1;\n      else\n         if (point.y > 0)\n            bins[2] += 1;\n         else\n            bins[3] += 1;\n   }\n}",
            "// TODO: your code here\n}",
            "bins = {0,0,0,0};\n    for (size_t i = 0; i < points.size(); i++)\n    {\n        if (points[i].x >= 0.0 && points[i].y >= 0.0)\n        {\n            bins[0]++;\n        }\n        else if (points[i].x < 0.0 && points[i].y >= 0.0)\n        {\n            bins[1]++;\n        }\n        else if (points[i].x < 0.0 && points[i].y < 0.0)\n        {\n            bins[2]++;\n        }\n        else\n        {\n            bins[3]++;\n        }\n    }\n}",
            "for (const Point &point : points) {\n        if (point.x > 0 && point.y > 0) {\n            bins[0]++;\n        } else if (point.x > 0 && point.y < 0) {\n            bins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            bins[2]++;\n        } else if (point.x < 0 && point.y > 0) {\n            bins[3]++;\n        }\n    }\n}",
            "for (auto const& point : points) {\n      size_t bin = 0;\n      if (point.x > 0) {\n         bin += 1;\n         if (point.y > 0)\n            bin += 2;\n      }\n      bins[bin]++;\n   }\n}",
            "bins.fill(0);\n   for (auto &p : points) {\n      if (p.x > 0) {\n         if (p.y > 0) {\n            bins[0]++;\n         } else {\n            bins[1]++;\n         }\n      } else {\n         if (p.y > 0) {\n            bins[2]++;\n         } else {\n            bins[3]++;\n         }\n      }\n   }\n}",
            "for (const auto& point : points) {\n        if (point.x >= 0.0 && point.y >= 0.0)\n            bins[0]++;\n        else if (point.x < 0.0 && point.y >= 0.0)\n            bins[1]++;\n        else if (point.x < 0.0 && point.y < 0.0)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "bins = std::array<size_t, 4>();\n   for(auto &p: points) {\n      if(p.x >= 0) {\n         if(p.y >= 0) {\n            ++bins[0];\n         }\n         else {\n            ++bins[2];\n         }\n      }\n      else {\n         if(p.y >= 0) {\n            ++bins[1];\n         }\n         else {\n            ++bins[3];\n         }\n      }\n   }\n}",
            "bins.fill(0);\n    for (auto const& p : points) {\n        auto q = 0;\n        if (p.x >= 0 && p.y >= 0) {\n            q = 0;\n        } else if (p.x < 0 && p.y >= 0) {\n            q = 1;\n        } else if (p.x < 0 && p.y < 0) {\n            q = 2;\n        } else if (p.x >= 0 && p.y < 0) {\n            q = 3;\n        }\n        ++bins[q];\n    }\n}",
            "}",
            "bins.fill(0);\n  for (const auto& point : points) {\n    if (point.x > 0 && point.y > 0) {\n      bins[0]++;\n    } else if (point.x < 0 && point.y > 0) {\n      bins[1]++;\n    } else if (point.x < 0 && point.y < 0) {\n      bins[2]++;\n    } else if (point.x > 0 && point.y < 0) {\n      bins[3]++;\n    }\n  }\n}",
            "for (auto const& point : points) {\n        if (point.x >= 0.0 && point.y >= 0.0) {\n            bins[0]++;\n        }\n        if (point.x <= 0.0 && point.y >= 0.0) {\n            bins[1]++;\n        }\n        if (point.x <= 0.0 && point.y <= 0.0) {\n            bins[2]++;\n        }\n        if (point.x >= 0.0 && point.y <= 0.0) {\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: Your code goes here\n   bins.fill(0);\n   for (auto const& p : points) {\n      if (p.x > 0 && p.y > 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y > 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else if (p.x > 0 && p.y < 0) {\n         bins[3]++;\n      } else {\n         bins[4]++;\n      }\n   }\n}",
            "bins.fill(0);\n   for (auto const& p : points) {\n      int quadrant = 0;\n      if (p.x > 0) quadrant += 1;\n      if (p.y > 0) quadrant += 2;\n      bins[quadrant]++;\n   }\n}",
            "for(auto const& point: points) {\n      if (point.x < 0) {\n         if (point.y < 0) {\n            bins[0]++;\n         } else {\n            bins[1]++;\n         }\n      } else {\n         if (point.y < 0) {\n            bins[2]++;\n         } else {\n            bins[3]++;\n         }\n      }\n   }\n}",
            "for (auto const& p : points) {\n        size_t i;\n        if (p.x > 0) {\n            if (p.y > 0)\n                i = 0;\n            else\n                i = 3;\n        }\n        else {\n            if (p.y > 0)\n                i = 1;\n            else\n                i = 2;\n        }\n        ++bins[i];\n    }\n}",
            "bins.fill(0);\n\n   for (auto& point : points) {\n      if (point.x >= 0.0 && point.y >= 0.0) {\n         bins[0]++;\n      } else if (point.x < 0.0 && point.y >= 0.0) {\n         bins[1]++;\n      } else if (point.x < 0.0 && point.y < 0.0) {\n         bins[2]++;\n      } else if (point.x >= 0.0 && point.y < 0.0) {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& p : points) {\n        if (p.x > 0 && p.y > 0) bins[0]++;\n        else if (p.x > 0 && p.y < 0) bins[3]++;\n        else if (p.x < 0 && p.y < 0) bins[2]++;\n        else if (p.x < 0 && p.y > 0) bins[1]++;\n    }\n}",
            "bins.fill(0);\n   for (auto const& point : points) {\n      bins[((point.x < 0) << 1) | (point.y < 0)] += 1;\n   }\n}",
            "// your code here\n}",
            "for(const auto& point: points){\n        if(point.x >= 0 && point.y >= 0){\n            bins[0]++;\n        } else if(point.x >= 0 && point.y < 0){\n            bins[1]++;\n        } else if(point.x < 0 && point.y < 0){\n            bins[2]++;\n        } else if(point.x < 0 && point.y >= 0){\n            bins[3]++;\n        }\n    }\n}",
            "for (Point const& point: points) {\n        size_t index = 0;\n        if (point.x > 0) {\n            index += 1;\n            if (point.y > 0) {\n                index += 1;\n            }\n        } else {\n            if (point.y < 0) {\n                index += 2;\n            }\n        }\n        bins[index]++;\n    }\n}",
            "// TODO: Your code here\n}",
            "bins = {0, 0, 0, 0};\n    for (auto p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        }\n        if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        }\n        if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        }\n        if (p.x > 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "}",
            "for (auto const &p : points) {\n      bins[getQuadrant(p)]++;\n   }\n}",
            "bins.fill(0);\n    for (auto const& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            bins[0] += 1;\n        } else if (point.x < 0 && point.y > 0) {\n            bins[1] += 1;\n        } else if (point.x < 0 && point.y < 0) {\n            bins[2] += 1;\n        } else if (point.x > 0 && point.y < 0) {\n            bins[3] += 1;\n        }\n    }\n}",
            "for (auto &pt : points) {\n        auto &bin = (pt.x > 0)!= (pt.y > 0)? bins[0] : bins[1];\n        bin += pt.x > 0;\n        bin += pt.y > 0;\n    }\n}",
            "bins = {0,0,0,0};\n    for (auto p : points) {\n        if (p.x >= 0 && p.y >= 0) {\n            bins[0] += 1;\n        } else if (p.x < 0 && p.y >= 0) {\n            bins[1] += 1;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2] += 1;\n        } else {\n            bins[3] += 1;\n        }\n    }\n}",
            "for (auto &p : points) {\n    if (p.x >= 0) {\n      if (p.y >= 0) {\n        bins[0] += 1;\n      } else {\n        bins[3] += 1;\n      }\n    } else {\n      if (p.y >= 0) {\n        bins[1] += 1;\n      } else {\n        bins[2] += 1;\n      }\n    }\n  }\n}",
            "//TODO: Fill bins\n}",
            "for (auto const& p : points) {\n      if (p.x > 0 && p.y > 0) {\n         ++bins[0];\n      } else if (p.x < 0 && p.y > 0) {\n         ++bins[1];\n      } else if (p.x < 0 && p.y < 0) {\n         ++bins[2];\n      } else if (p.x > 0 && p.y < 0) {\n         ++bins[3];\n      }\n   }\n}",
            "std::sort(points.begin(), points.end());\n   std::partition_point(points.begin(), points.end(), [](Point const& p){\n      return p.x > 0;\n   });\n   bins.fill(0);\n   for(auto const& p : points) {\n      if(p.y > 0)\n         bins[0]++;\n      else if(p.y < 0)\n         bins[2]++;\n      else\n         bins[1]++;\n   }\n   bins[3] = points.size() - bins[0] - bins[1] - bins[2];\n}",
            "for(int i = 0; i < points.size(); ++i){\n\n    if(points[i].x > 0 && points[i].y > 0)\n      bins[0]++;\n    else if(points[i].x < 0 && points[i].y > 0)\n      bins[1]++;\n    else if(points[i].x < 0 && points[i].y < 0)\n      bins[2]++;\n    else if(points[i].x > 0 && points[i].y < 0)\n      bins[3]++;\n\n  }\n\n}",
            "for (auto &p : points) {\n       if (p.x >= 0.0) {\n           if (p.y >= 0.0) {\n               bins[0]++;\n           } else {\n               bins[3]++;\n           }\n       } else {\n           if (p.y >= 0.0) {\n               bins[1]++;\n           } else {\n               bins[2]++;\n           }\n       }\n   }\n}",
            "for (auto const& point : points) {\n        if (point.x > 0.0) {\n            if (point.y > 0.0) {\n                bins[0]++;\n            } else {\n                bins[3]++;\n            }\n        } else {\n            if (point.y > 0.0) {\n                bins[1]++;\n            } else {\n                bins[2]++;\n            }\n        }\n    }\n}",
            "size_t quadrant = 0;\n   for (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         ++bins[quadrant++];\n      } else if (point.x < 0 && point.y > 0) {\n         ++bins[quadrant++];\n      } else if (point.x < 0 && point.y < 0) {\n         ++bins[quadrant++];\n      } else if (point.x > 0 && point.y < 0) {\n         ++bins[quadrant++];\n      }\n   }\n}",
            "//TODO\n}",
            "for (auto p : points) {\n        if (p.x > 0.0 && p.y > 0.0) {\n            ++bins[0];\n        }\n        else if (p.x > 0.0 && p.y <= 0.0) {\n            ++bins[1];\n        }\n        else if (p.x <= 0.0 && p.y <= 0.0) {\n            ++bins[2];\n        }\n        else {\n            ++bins[3];\n        }\n    }\n}",
            "bins.fill(0);\n    for (auto const& point : points) {\n        auto x = point.x;\n        auto y = point.y;\n        if (x > 0 && y > 0) {\n            ++bins[0];\n        }\n        if (x < 0 && y > 0) {\n            ++bins[1];\n        }\n        if (x < 0 && y < 0) {\n            ++bins[2];\n        }\n        if (x > 0 && y < 0) {\n            ++bins[3];\n        }\n    }\n}",
            "int i = 0;\n   for (auto p : points) {\n      double x = p.x, y = p.y;\n      if (x > 0 && y > 0)\n         ++bins[i++];\n      else if (x < 0 && y > 0)\n         ++bins[i++];\n      else if (x < 0 && y < 0)\n         ++bins[i++];\n      else if (x > 0 && y < 0)\n         ++bins[i++];\n      else\n         i = 0;\n   }\n}",
            "for (auto const& p : points)\n    {\n        if (p.x > 0)\n        {\n            if (p.y > 0)\n                bins[0]++;\n            else\n                bins[3]++;\n        }\n        else\n        {\n            if (p.y > 0)\n                bins[1]++;\n            else\n                bins[2]++;\n        }\n    }\n}",
            "bins = {0, 0, 0, 0};\n   for(auto& point : points)\n   {\n      if(point.x >= 0.0 && point.y >= 0.0) bins[0]++;\n      else if(point.x < 0.0 && point.y >= 0.0) bins[1]++;\n      else if(point.x < 0.0 && point.y < 0.0) bins[2]++;\n      else bins[3]++;\n   }\n}",
            "// YOUR CODE HERE\n}",
            "std::array<double, 4> x{0.0, 0.0, 0.0, 0.0};\n    std::array<double, 4> y{0.0, 0.0, 0.0, 0.0};\n    for(auto const& p : points) {\n        if (p.x >= 0 && p.y >= 0) {\n            x[0]++;\n            y[0]++;\n        }\n        else if (p.x < 0 && p.y >= 0) {\n            x[1]++;\n            y[1]++;\n        }\n        else if (p.x < 0 && p.y < 0) {\n            x[2]++;\n            y[2]++;\n        }\n        else {\n            x[3]++;\n            y[3]++;\n        }\n    }\n    for(size_t i = 0; i < 4; i++) {\n        bins[i] = x[i] * y[i];\n    }\n}",
            "bins.fill(0);\n    for (auto& p : points) {\n        if (p.x >= 0) {\n            if (p.y >= 0) {\n                bins[0]++;\n            } else {\n                bins[1]++;\n            }\n        } else {\n            if (p.y >= 0) {\n                bins[2]++;\n            } else {\n                bins[3]++;\n            }\n        }\n    }\n}",
            "for (const auto& point : points) {\n        if (point.x >= 0) {\n            if (point.y >= 0) {\n                ++bins[0];\n            } else {\n                ++bins[3];\n            }\n        } else {\n            if (point.y >= 0) {\n                ++bins[2];\n            } else {\n                ++bins[1];\n            }\n        }\n    }\n}",
            "for(auto const& p : points) {\n        if(p.x >= 0 && p.y >= 0) {\n            bins[0]++;\n        } else if(p.x <= 0 && p.y <= 0) {\n            bins[1]++;\n        } else if(p.x <= 0 && p.y >= 0) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "for (auto point : points) {\n    if (point.x > 0 && point.y > 0)\n      bins[0] += 1;\n    else if (point.x > 0 && point.y < 0)\n      bins[1] += 1;\n    else if (point.x < 0 && point.y > 0)\n      bins[2] += 1;\n    else\n      bins[3] += 1;\n  }\n}",
            "auto quadrant = [](Point const& p) {\n        if (p.x > 0 && p.y > 0) {\n            return 0;\n        } else if (p.x < 0 && p.y > 0) {\n            return 1;\n        } else if (p.x < 0 && p.y < 0) {\n            return 2;\n        } else if (p.x > 0 && p.y < 0) {\n            return 3;\n        } else {\n            return 4;\n        }\n    };\n\n    for (auto const& p : points) {\n        bins[quadrant(p)]++;\n    }\n}",
            "bins[0] = 0; bins[1] = 0; bins[2] = 0; bins[3] = 0;\n    for (auto const& p : points)\n    {\n        size_t q = 0;\n        if (p.x > 0) ++q;\n        if (p.y > 0) ++q;\n        ++bins[q];\n    }\n}",
            "std::array<std::vector<Point>, 4> quadrants;\n   for (const auto& p: points) {\n      size_t quadrant = (p.x > 0) + (p.y > 0) * 2;\n      quadrants[quadrant].emplace_back(p);\n   }\n   bins.fill(0);\n   for (const auto& q: quadrants) {\n      bins[countQuadrant(q)]++;\n   }\n}",
            "for (Point p : points) {\n        size_t bin = 0;\n        if (p.x >= 0) {\n            bin |= 1;\n        }\n        if (p.y >= 0) {\n            bin |= 2;\n        }\n        bins[bin]++;\n    }\n}",
            "// your code here\n}",
            "for (auto const& p : points) {\n        if (p.x >= 0 && p.y >= 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y >= 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x >= 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "bins.fill(0);\n\n    for (auto p: points) {\n        int i = (p.x >= 0) + (p.y >= 0);\n        if (i >= 0 && i <= 3)\n            bins[i]++;\n    }\n}",
            "auto quadrant = [](Point const& p) -> size_t\n   {\n      return p.x >= 0.0?\n         p.y >= 0.0? 0 : 1\n         : p.y >= 0.0? 3 : 2;\n   };\n\n   auto count = [](size_t & c, Point const& p) -> void\n   {\n      auto q = quadrant(p);\n      ++c[q];\n   };\n\n   for (auto const& p : points) {\n      count(bins[0], p);\n   }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (auto const& p : points) {\n        if (p.x >= 0 && p.y >= 0) {\n            ++bins[0];\n        }\n        else if (p.x < 0 && p.y >= 0) {\n            ++bins[1];\n        }\n        else if (p.x < 0 && p.y < 0) {\n            ++bins[2];\n        }\n        else if (p.x >= 0 && p.y < 0) {\n            ++bins[3];\n        }\n    }\n}",
            "bins.fill(0);\n    for (auto const& p : points)\n    {\n        if (p.x > 0 && p.y > 0)\n            ++bins[0];\n        else if (p.x > 0 && p.y < 0)\n            ++bins[1];\n        else if (p.x < 0 && p.y < 0)\n            ++bins[2];\n        else\n            ++bins[3];\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "hip",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use AMD HIP to count in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\n__global__ void countQuadrants(const Point *points, size_t N, size_t[4] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n   if (i >= N)\n      return;\n\n   double x = points[i].x;\n   double y = points[i].y;\n\n   size_t quadrant;\n   if (x >= 0.0) {\n      if (y >= 0.0)\n         quadrant = 0;\n      else\n         quadrant = 3;\n   } else {\n      if (y >= 0.0)\n         quadrant = 1;\n      else\n         quadrant = 2;\n   }\n\n   atomicAdd(&bins[quadrant], 1);\n}",
            "size_t xIdx = threadIdx.x;\n   size_t yIdx = threadIdx.y;\n\n   // AMD HIP has no support for `blockDim.x*blockDim.y`\n   size_t idx = xIdx + yIdx*blockDim.x;\n\n   // Each thread counts the number of elements that are greater than the quadrant that it belongs to.\n   // Only one thread in each quadrant will do the actual counting.\n   // The other threads will increment the number of elements that are less than the quadrant that they\n   // belong to.\n   for (size_t i = idx; i < N; i+= blockDim.x*blockDim.y) {\n      Point p = points[i];\n      if (p.x >= 0 && p.y >= 0) {\n         bins[0]++;\n      }\n      else if (p.x < 0 && p.y < 0) {\n         bins[3]++;\n      }\n      else if (p.x >= 0 && p.y < 0) {\n         bins[2]++;\n      }\n      else {\n         bins[1]++;\n      }\n   }\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n   if (tid < N) {\n      size_t x = points[tid].x;\n      size_t y = points[tid].y;\n      int quadrant = 0;\n      if (x >= 0) {\n         quadrant += 1;\n      }\n      if (y >= 0) {\n         quadrant += 2;\n      }\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "int t_idx = threadIdx.x + blockDim.x * blockIdx.x;\n   if (t_idx >= N) return;\n   const Point point = points[t_idx];\n\n   size_t &bin = bins[(point.x >= 0) + 2 * (point.y >= 0)];\n   atomicAdd(&bin, 1);\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        double x = points[i].x, y = points[i].y;\n        if (x > 0.0 && y > 0.0)\n            atomicAdd(&bins[0], 1);\n        else if (x < 0.0 && y > 0.0)\n            atomicAdd(&bins[1], 1);\n        else if (x < 0.0 && y < 0.0)\n            atomicAdd(&bins[2], 1);\n        else if (x > 0.0 && y < 0.0)\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n   if (i < N) {\n      double x = points[i].x;\n      double y = points[i].y;\n      int bin = 0;\n      if (x > 0 && y > 0) {\n         bin = 1;\n      } else if (x < 0 && y < 0) {\n         bin = 2;\n      } else if (x > 0 && y < 0) {\n         bin = 3;\n      }\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "// TODO: Implement this function\n}",
            "size_t myIdx = threadIdx.x + blockDim.x * blockIdx.x;\n   if (myIdx < N) {\n      size_t quadrant = 0;\n      if (points[myIdx].x > 0) {\n         quadrant++;\n      }\n      if (points[myIdx].y > 0) {\n         quadrant += 2;\n      }\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "// TODO: count the points in each quadrant in parallel\n\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n   if (i >= N)\n      return;\n\n   const Point& p = points[i];\n   size_t quadrant = 0;\n   if (p.x > 0)\n      quadrant |= 1;\n   if (p.y > 0)\n      quadrant |= 2;\n\n   atomicAdd(&bins[quadrant], 1);\n}",
            "size_t x = threadIdx.x + blockDim.x * blockIdx.x;\n   size_t y = threadIdx.y + blockDim.y * blockIdx.y;\n\n   if (x >= N || y >= N) {\n      return;\n   }\n\n   Point p = points[x + y * N];\n\n   if (p.x > 0 && p.y > 0) {\n      atomicAdd(&bins[0], 1);\n   } else if (p.x > 0 && p.y < 0) {\n      atomicAdd(&bins[1], 1);\n   } else if (p.x < 0 && p.y < 0) {\n      atomicAdd(&bins[2], 1);\n   } else if (p.x < 0 && p.y > 0) {\n      atomicAdd(&bins[3], 1);\n   }\n}",
            "// TODO: implement using atomicAdd and threadIdx.x\n}",
            "// TODO: Implement your code here\n}",
            "//TODO\n}",
            "// TODO: Implement\n}",
            "// TODO\n}",
            "int i = threadIdx.x;\n   if (i < N) {\n      Point point = points[i];\n      bins[point.x >= 0? (point.y >= 0? 0 : 3) : (point.y >= 0? 1 : 2)]++;\n   }\n}",
            "// TODO\n}",
            "int i = threadIdx.x;\n   if (i >= N) return;\n\n   //... your code here\n}",
            "int tid = threadIdx.x + blockIdx.x*blockDim.x;\n   if (tid < N) {\n      Point pt = points[tid];\n      if (pt.x > 0 && pt.y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (pt.x < 0 && pt.y > 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (pt.x < 0 && pt.y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else if (pt.x > 0 && pt.y < 0) {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// Your code here\n}",
            "int count_quadrant[4] = { 0, 0, 0, 0 };\n    int i;\n    for (i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        Point p = points[i];\n        int index = p.x >= 0? 0 : 1;\n        index += p.y >= 0? 0 : 2;\n        atomicAdd(&count_quadrant[index], 1);\n    }\n    // copy to output memory\n    for (i = threadIdx.x; i < 4; i += blockDim.x) {\n        bins[i] = count_quadrant[i];\n    }\n}",
            "// TODO: Implement the kernel here.\n   // HINT: each thread should increment the bin for its point.\n\n   // compute the quadrant for the current thread, store it in `quadrant`\n   // HINT: the quadrant index is the same as the sign of x and y\n   int x = round(points[blockDim.x * blockIdx.x + threadIdx.x].x);\n   int y = round(points[blockDim.x * blockIdx.x + threadIdx.x].y);\n   int quadrant = (x > 0) - (x < 0) - (y > 0) + (y < 0);\n\n   // increment the bin with the computed quadrant\n   atomicAdd(bins + quadrant, 1);\n}",
            "for (int i = 0; i < N; i++) {\n    int xBin = points[i].x >= 0? 0 : 1;\n    int yBin = points[i].y >= 0? 0 : 1;\n    atomicAdd(bins + xBin, 1);\n    atomicAdd(bins + yBin + 2, 1);\n  }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n   if (i < N) {\n      int bin = 0;\n      if (points[i].x >= 0) {\n         if (points[i].y >= 0) {\n            bin = 0;\n         } else {\n            bin = 3;\n         }\n      } else {\n         if (points[i].y >= 0) {\n            bin = 1;\n         } else {\n            bin = 2;\n         }\n      }\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "// TODO: Launch at least N threads\n   // TODO: Count the number of points in each quadrant\n   // TODO: Store the counts in `bins`\n}",
            "__shared__ __device__ size_t bin[4];\n   bin[threadIdx.x] = 0;\n   for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n      double x = points[i].x;\n      double y = points[i].y;\n      size_t which = (x > 0) + (x < 0) * 2 + (y > 0) * 4;\n      atomicAdd(&bin[which], 1);\n   }\n   __syncthreads();\n   for (int i = 1; i < 4; i *= 2) {\n      size_t sum = bin[i];\n      for (int j = i >> 1; j < i; j--) {\n         bin[j] += bin[i + j];\n      }\n   }\n   if (threadIdx.x == 0) {\n      bins[0] = bin[0];\n      bins[1] = bin[1];\n      bins[2] = bin[2];\n      bins[3] = bin[3];\n   }\n}",
            "/* Initialize the bins to 0. */\n    for (size_t i = 0; i < 4; ++i) {\n        bins[i] = 0;\n    }\n    const size_t startIdx = (blockIdx.x * blockDim.x + threadIdx.x) * N / blockDim.x;\n    const size_t endIdx = min((blockIdx.x + 1) * blockDim.x, N);\n    for (size_t i = startIdx; i < endIdx; ++i) {\n        Point point = points[i];\n        if (point.x >= 0 && point.y >= 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (point.x >= 0 && point.y < 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (point.x < 0 && point.y >= 0) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n\n   for (int i = 0; i < N; i++) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x >= 0 && y >= 0)\n         atomicAdd(&bins[0], 1);\n      else if (x < 0 && y >= 0)\n         atomicAdd(&bins[1], 1);\n      else if (x < 0 && y < 0)\n         atomicAdd(&bins[2], 1);\n      else\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "// TODO...\n}",
            "__shared__ size_t s_bins[4];\n\n   const size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n   const size_t stride = blockDim.x * gridDim.x;\n   for(size_t i = threadIdx.x; i < N; i += stride) {\n      size_t index = 0;\n      if(points[i].x > 0 && points[i].y > 0) index = 0;\n      else if(points[i].x < 0 && points[i].y > 0) index = 1;\n      else if(points[i].x < 0 && points[i].y < 0) index = 2;\n      else if(points[i].x > 0 && points[i].y < 0) index = 3;\n      atomicAdd(&s_bins[index], 1);\n   }\n\n   __syncthreads();\n\n   for(size_t i = threadIdx.x; i < 4; i += blockDim.x) {\n      atomicAdd(&bins[i], s_bins[i]);\n   }\n}",
            "// TODO\n}",
            "//...\n}",
            "__shared__ size_t blockBins[4];\n    if(threadIdx.x == 0) {\n        for(int i = 0; i < 4; ++i) blockBins[i] = 0;\n    }\n    __syncthreads();\n\n    const int threadIdx_x = threadIdx.x;\n    const int threadIdx_y = threadIdx.y;\n    const int blockSize_x = blockDim.x;\n    const int blockSize_y = blockDim.y;\n    for(size_t i = threadIdx_x + threadIdx_y * blockSize_x; i < N; i += blockSize_x * blockSize_y) {\n        const Point p = points[i];\n        const double x = p.x;\n        const double y = p.y;\n        int quad = 0;\n        if(x >= 0) {\n            if(y >= 0) {\n                quad = 0;\n            } else {\n                quad = 3;\n            }\n        } else {\n            if(y >= 0) {\n                quad = 1;\n            } else {\n                quad = 2;\n            }\n        }\n        atomicAdd(&blockBins[quad], 1);\n    }\n    __syncthreads();\n\n    const int blockIdx_x = blockIdx.x;\n    const int blockIdx_y = blockIdx.y;\n    const int blockSize_x_ = gridDim.x * blockSize_x;\n    const int blockSize_y_ = gridDim.y * blockSize_y;\n    for(int i = threadIdx_x + threadIdx_y * blockSize_x; i < 4; i += blockSize_x_ * blockSize_y_) {\n        atomicAdd(&bins[i], blockBins[i]);\n    }\n}",
            "/* Your code here */\n}",
            "int id = threadIdx.x + blockIdx.x * blockDim.x;\n    if(id < N) {\n        int x = (points[id].x < 0)? -1 : 0;\n        int y = (points[id].y < 0)? -1 : 0;\n        atomicAdd(&bins[x + y * 2], 1);\n    }\n}",
            "for (int i = 0; i < N; i++) {\n        const Point p = points[i];\n        const double qx = (p.x >= 0.0)? 0.0 : 1.0;\n        const double qy = (p.y >= 0.0)? 0.0 : 1.0;\n        atomicAdd(&bins[(int)qx + (int)qy * 2], 1);\n    }\n}",
            "size_t t = threadIdx.x + blockIdx.x*blockDim.x;\n   // TODO: add code here\n}",
            "// TODO: implement using AMD HIP\n   // Hint:\n   // 1) define a thread function for counting a quadrant. \n   // 2) launch as many threads as there are elements in `points`\n   // 3) each thread should count one quadrant and store the count in `bins`.\n   // 4) each thread should only count the quadrant that the element in `points` belongs to.\n   // 5) use AMD HIP to parallelize the for loop.\n   // Hint 2:\n   // The x coordinate is positive if it is in the first or third quadrant, 0 if it is in the second quadrant,\n   // and negative if it is in the fourth quadrant.\n   // The y coordinate is positive if it is in the first or fourth quadrant, 0 if it is in the second quadrant,\n   // and negative if it is in the third quadrant.\n\n}",
            "// Your code goes here\n}",
            "// TODO: implement\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if(i >= N)\n        return;\n\n    int quadrant = 0;\n    if(points[i].x > 0.0) {\n        if(points[i].y > 0.0)\n            quadrant = 1;\n        else\n            quadrant = 4;\n    } else {\n        if(points[i].y > 0.0)\n            quadrant = 2;\n        else\n            quadrant = 3;\n    }\n    bins[quadrant] += 1;\n}",
            "size_t id = blockDim.x * blockIdx.x + threadIdx.x;\n\n   if (id < N) {\n      double x = points[id].x;\n      double y = points[id].y;\n\n      if (x > 0) {\n         if (y > 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (y > 0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "int id = threadIdx.x;\n   int stride = blockDim.x;\n\n   // Initialize the histogram\n   for (int i = 0; i < 4; i++) {\n      bins[i] = 0;\n   }\n\n   // Count the number of points in each quadrant\n   for (int i = 0; i < N; i += stride) {\n      int quadrant = (points[i].x >= 0.0)? (points[i].y >= 0.0)? 0 : 3 : (points[i].y >= 0.0)? 1 : 2;\n      bins[quadrant]++;\n   }\n}",
            "// TODO: Implement me\n}",
            "// TODO\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n   size_t i;\n   if (tid < N) {\n      if (points[tid].x < 0) {\n         if (points[tid].y > 0) {\n            bins[0]++;\n         } else {\n            bins[3]++;\n         }\n      } else {\n         if (points[tid].y > 0) {\n            bins[1]++;\n         } else {\n            bins[2]++;\n         }\n      }\n   }\n}",
            "unsigned int tid = threadIdx.x + blockDim.x * blockIdx.x;\n    if (tid < N) {\n        int i = floor(points[tid].x / 4);\n        int j = floor(points[tid].y / 4);\n        int k = i + j;\n        atomicAdd(&bins[k], 1);\n    }\n}",
            "int index = threadIdx.x;\n    size_t n = 0;\n    if (index < N) {\n        double x = points[index].x;\n        double y = points[index].y;\n        n = x >= 0? (y >= 0? 0 : 3) : (y >= 0? 1 : 2);\n    }\n    __syncthreads();\n    if (index < 4) {\n        atomicAdd(&bins[index], n);\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        int x = (points[i].x >= 0)? 1 : 0;\n        int y = (points[i].y >= 0)? 1 : 0;\n        atomicAdd(&bins[x + 2 * y], 1);\n    }\n}",
            "/* Your code here */\n   bins[0] = 0; bins[1] = 0; bins[2] = 0; bins[3] = 0;\n   for (int i = 0; i < N; i++) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         bins[0]++;\n      } else if (points[i].x > 0 && points[i].y < 0) {\n         bins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         bins[2]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         bins[3]++;\n      }\n   }\n}",
            "/* TODO implement */\n}",
            "/* TODO: YOUR CODE GOES HERE */\n    //printf(\"hello from thread %u\\n\", blockIdx.x * blockDim.x + threadIdx.x);\n    //int tid = threadIdx.x;\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    for (int i = 0; i < N; i++)\n    {\n        if (points[i].x > 0 && points[i].y > 0) {\n            atomicAdd(&bins[0], 1);\n        }\n        else if (points[i].x < 0 && points[i].y > 0) {\n            atomicAdd(&bins[1], 1);\n        }\n        else if (points[i].x < 0 && points[i].y < 0) {\n            atomicAdd(&bins[2], 1);\n        }\n        else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "//...\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n\n   //...\n}",
            "}",
            "// TODO: implement this function\n}",
            "int nthreads = blockDim.x * gridDim.x;\n   int tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n   if (tid >= N) {\n      return;\n   }\n\n   int quadrant = 0;\n   if (points[tid].x >= 0.0) {\n      quadrant++;\n   }\n   if (points[tid].y >= 0.0) {\n      quadrant += 2;\n   }\n\n   atomicAdd(&bins[quadrant], 1);\n}",
            "__shared__ int s_bins[4];\n  if (threadIdx.x == 0) {\n    s_bins[0] = 0;\n    s_bins[1] = 0;\n    s_bins[2] = 0;\n    s_bins[3] = 0;\n  }\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    // TODO: Implement this\n  }\n  __syncthreads();\n  if (threadIdx.x == 0) {\n    atomicAdd(&bins[0], s_bins[0]);\n    atomicAdd(&bins[1], s_bins[1]);\n    atomicAdd(&bins[2], s_bins[2]);\n    atomicAdd(&bins[3], s_bins[3]);\n  }\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n   int y = blockIdx.y * blockDim.y + threadIdx.y;\n   int offset = x + y * blockDim.x * gridDim.x;\n\n   if (offset < N) {\n      Point p = points[offset];\n\n      if (p.x < 0) {\n         if (p.y < 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (p.y < 0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n    if (tid < N) {\n        Point point = points[tid];\n        if (point.x >= 0.0 && point.y >= 0.0) {\n            atomicAdd(&bins[0], 1);\n        } else if (point.x < 0.0 && point.y >= 0.0) {\n            atomicAdd(&bins[1], 1);\n        } else if (point.x < 0.0 && point.y < 0.0) {\n            atomicAdd(&bins[2], 1);\n        } else if (point.x >= 0.0 && point.y < 0.0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "unsigned int idx = threadIdx.x + blockIdx.x * blockDim.x;\n   if (idx >= N)\n      return;\n\n   // TODO: Implement the function\n\n}",
            "size_t idx = threadIdx.x + blockDim.x * blockIdx.x;\n   if (idx >= N) return;\n\n   double x = points[idx].x;\n   double y = points[idx].y;\n\n   if (x >= 0 && y >= 0) bins[0]++;\n   else if (x < 0 && y >= 0) bins[1]++;\n   else if (x < 0 && y < 0) bins[2]++;\n   else if (x >= 0 && y < 0) bins[3]++;\n}",
            "int quad = 0;\n   bins[quad] += 0;\n\n   for (size_t i = 0; i < N; i++) {\n      if (points[i].x >= 0.0) {\n         quad = 1;\n         bins[quad]++;\n      } else {\n         quad = 2;\n         bins[quad]++;\n      }\n\n      if (points[i].y >= 0.0) {\n         quad = 3;\n         bins[quad]++;\n      } else {\n         quad = 0;\n         bins[quad]++;\n      }\n   }\n}",
            "const int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    const int x = int(points[i].x);\n    const int y = int(points[i].y);\n    if (x > 0) {\n      if (y > 0)\n        atomicAdd(&bins[0], 1);\n      else if (y < 0)\n        atomicAdd(&bins[1], 1);\n      else\n        atomicAdd(&bins[2], 1);\n    } else if (x < 0) {\n      if (y > 0)\n        atomicAdd(&bins[3], 1);\n      else if (y < 0)\n        atomicAdd(&bins[2], 1);\n      else\n        atomicAdd(&bins[1], 1);\n    }\n  }\n}",
            "__shared__ size_t shared_bins[4];\n    size_t *local_bins = &shared_bins[threadIdx.x];\n    local_bins[threadIdx.x] = 0;\n    __syncthreads();\n\n    for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n        double x = points[i].x;\n        double y = points[i].y;\n        int quadrant = (x > 0) + (y > 0) * 2;\n        if (quadrant == 0) {\n            local_bins[threadIdx.x]++;\n        }\n    }\n    __syncthreads();\n\n    //  Reduction\n    if (threadIdx.x < 2) {\n        local_bins[threadIdx.x] += local_bins[threadIdx.x + 2];\n    }\n    __syncthreads();\n\n    if (threadIdx.x == 0) {\n        bins[0] = local_bins[0];\n        bins[1] = local_bins[1];\n        bins[2] = local_bins[2];\n        bins[3] = local_bins[3];\n    }\n}",
            "unsigned int tid = threadIdx.x;\n   if (tid >= N)\n      return;\n\n   Point p = points[tid];\n   int x = (p.x > 0)? 1 : (p.x == 0? 0 : -1);\n   int y = (p.y > 0)? 1 : (p.y == 0? 0 : -1);\n\n   bins[x + 1]++;\n   bins[y + 4]++;\n}",
            "// TODO: your code here\n}",
            "// TODO: your code here\n}",
            "// get thread id\n   int tid = blockDim.x*blockIdx.x + threadIdx.x;\n\n   // loop over points\n   for (size_t i = 0; i < N; i++) {\n\n      // get point\n      Point p = points[i];\n\n      // determine quadrant\n      size_t quadrant = 0;\n      if (p.x >= 0) {\n         quadrant += 1;\n         if (p.y >= 0) {\n            quadrant += 2;\n         }\n      }\n\n      // count point\n      if (quadrant < 4) {\n         atomicAdd(&bins[quadrant], 1);\n      }\n\n      // check for early termination\n      if (tid >= N)\n         return;\n   }\n}",
            "// TODO: Your code here\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n    if (tid >= N) {\n        return;\n    }\n\n    bins[0] += points[tid].x > 0 && points[tid].y > 0; // count all points in 1st quadrant\n    bins[1] += points[tid].x < 0 && points[tid].y > 0; // count all points in 2nd quadrant\n    bins[2] += points[tid].x < 0 && points[tid].y < 0; // count all points in 3rd quadrant\n    bins[3] += points[tid].x > 0 && points[tid].y < 0; // count all points in 4th quadrant\n}",
            "const size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        double x = points[tid].x;\n        double y = points[tid].y;\n        bins[0] += x > 0 && y > 0;\n        bins[1] += x <= 0 && y > 0;\n        bins[2] += x < 0 && y <= 0;\n        bins[3] += x > 0 && y <= 0;\n    }\n}",
            "int idx = threadIdx.x + blockDim.x * blockIdx.x;\n  if (idx < N) {\n    // Your code here\n  }\n}",
            "int bin = 0;\n   if (points[threadIdx.x].x >= 0) {\n      bin += 1;\n   }\n   if (points[threadIdx.x].y >= 0) {\n      bin += 2;\n   }\n   bins[threadIdx.x] = bin;\n}",
            "// TODO\n   const int tid = blockDim.x * blockIdx.x + threadIdx.x;\n   const int threads = blockDim.x * gridDim.x;\n   for (size_t i = tid; i < N; i += threads) {\n      if (points[i].x < 0) {\n         if (points[i].y > 0) {\n            atomicAdd(&bins[1], 1);\n         }\n         else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n      else {\n         if (points[i].y > 0) {\n            atomicAdd(&bins[0], 1);\n         }\n         else {\n            atomicAdd(&bins[2], 1);\n         }\n      }\n   }\n}",
            "size_t tid = threadIdx.x;\n   size_t start = tid;\n   size_t stride = blockDim.x;\n\n   for (size_t i = 0; i < N; i += stride) {\n      double x = points[i].x;\n      double y = points[i].y;\n\n      if (x >= 0) {\n         if (y >= 0)\n            atomicAdd(&bins[0], 1);\n         else\n            atomicAdd(&bins[1], 1);\n      } else {\n         if (y >= 0)\n            atomicAdd(&bins[2], 1);\n         else\n            atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// TODO\n}",
            "const size_t THREADS_PER_BLOCK = 256;\n\n   size_t count = 0;\n   for (size_t i = 0; i < N; i++) {\n      Point p = points[i];\n      if (p.x > 0) {\n         if (p.y > 0)\n            count++;\n         else\n            count++;\n      } else {\n         if (p.y > 0)\n            count++;\n         else\n            count++;\n      }\n   }\n\n   bins[0] = count;\n}",
            "// allocate shared memory for the quad counts\n   extern __shared__ int quadCount[];\n   // each thread will initialize its own count to 0\n   quadCount[threadIdx.x] = 0;\n   // this thread will wait until all other threads have completed their initialization\n   __syncthreads();\n   for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n      // get the thread's quadrant\n      Point p = points[i];\n      int q = p.x < 0? 2 : 0;\n      q += p.y < 0? 1 : 0;\n      quadCount[q]++;\n   }\n   // this thread will wait until all other threads have completed their counting\n   __syncthreads();\n   // this thread will store the count in the corresponding bin\n   bins[threadIdx.x] = quadCount[threadIdx.x];\n}",
            "// TODO: count the number of points in each quadrant\n}",
            "// TODO: your code here\n}",
            "// Your code here\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N) {\n      // TODO: compute the quadrant and increment the counter\n      // use HIP atomics to update the counter\n      if (points[idx].x < 0) {\n         if (points[idx].y < 0) {\n            atomicAdd(&bins[0], 1);\n         }\n         else {\n            atomicAdd(&bins[1], 1);\n         }\n      }\n      else {\n         if (points[idx].y < 0) {\n            atomicAdd(&bins[2], 1);\n         }\n         else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n   if (i < N) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x > 0) {\n         if (y > 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      } else {\n         if (y > 0) {\n            atomicAdd(&bins[1], 1);\n         } else {\n            atomicAdd(&bins[2], 1);\n         }\n      }\n   }\n}",
            "const size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n    if (i >= N) return;\n\n    bins[quadrant(points[i])]++;\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n   if (tid < N) {\n      if (points[tid].x >= 0 && points[tid].y >= 0)\n         atomicAdd(&bins[0], 1);\n      else if (points[tid].x < 0 && points[tid].y < 0)\n         atomicAdd(&bins[3], 1);\n      else if (points[tid].x < 0 && points[tid].y >= 0)\n         atomicAdd(&bins[2], 1);\n      else if (points[tid].x >= 0 && points[tid].y < 0)\n         atomicAdd(&bins[1], 1);\n   }\n}",
            "// TODO: implement\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      if (points[i].x >= 0) {\n         if (points[i].y >= 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (points[i].y >= 0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "// TODO: your code goes here\n}",
            "// TODO\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= N)\n      return;\n\n   if (points[i].x > 0 && points[i].y > 0)\n      bins[0]++;\n   else if (points[i].x < 0 && points[i].y > 0)\n      bins[1]++;\n   else if (points[i].x < 0 && points[i].y < 0)\n      bins[2]++;\n   else if (points[i].x > 0 && points[i].y < 0)\n      bins[3]++;\n}",
            "unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if(idx >= N) {\n        return;\n    }\n    int x = points[idx].x;\n    int y = points[idx].y;\n    if(x >= 0 && y >= 0) {\n        atomicAdd(&bins[0], 1);\n    }\n    if(x < 0 && y >= 0) {\n        atomicAdd(&bins[1], 1);\n    }\n    if(x < 0 && y < 0) {\n        atomicAdd(&bins[2], 1);\n    }\n    if(x >= 0 && y < 0) {\n        atomicAdd(&bins[3], 1);\n    }\n}",
            "// TODO: implement this function using HIP AMD's built-in atomic operations\n  int gridSize = 1;\n  int blockSize = N;\n\n  size_t tId = blockIdx.x * blockSize + threadIdx.x;\n  if (tId < N)\n  {\n    if (points[tId].x > 0.0 && points[tId].y > 0.0)\n    {\n      atomicAdd(&bins[0], 1);\n    }\n    else if (points[tId].x < 0.0 && points[tId].y > 0.0)\n    {\n      atomicAdd(&bins[1], 1);\n    }\n    else if (points[tId].x < 0.0 && points[tId].y < 0.0)\n    {\n      atomicAdd(&bins[2], 1);\n    }\n    else if (points[tId].x > 0.0 && points[tId].y < 0.0)\n    {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "// TODO: Your code here\n}",
            "int idx = threadIdx.x;\n    bins[idx] = 0;\n    for (int i = idx; i < N; i += blockDim.x) {\n        if (points[i].x >= 0 && points[i].y >= 0) bins[idx]++;\n        else if (points[i].x < 0 && points[i].y >= 0) bins[idx]++;\n        else if (points[i].x < 0 && points[i].y < 0) bins[idx]++;\n        else if (points[i].x >= 0 && points[i].y < 0) bins[idx]++;\n    }\n}",
            "}",
            "// HIP's threadIdx.x, blockIdx.x, blockDim.x are of type `unsigned int`.\n   // C++'s uint32_t does not match.\n   // Use `auto` to get the right type.\n   auto tid = threadIdx.x;\n   auto blockIdx = blockIdx.x;\n   auto blockDim = blockDim.x;\n   // Use `constexpr` to mark the size.\n   constexpr int numQuadrants = 4;\n\n   // Reduction in 4 parts, 1 per quadrant.\n   // Note: reduction within a block must use shared memory.\n   __shared__ size_t counts[numQuadrants];\n\n   // Each thread processes 4 points (for 4 quadrants).\n   for (size_t i = tid; i < N; i += blockDim) {\n      // Access the point.\n      Point point = points[i];\n      // Each thread in a block has to load it's own point.\n      // The quadrant index is:\n      // 0 - points.x > 0 and points.y > 0\n      // 1 - points.x < 0 and points.y > 0\n      // 2 - points.x < 0 and points.y < 0\n      // 3 - points.x > 0 and points.y < 0\n      // Use the `min` function for the quadrant condition.\n      int quadrant = (point.x >= 0) * (point.y >= 0);\n      // Add the count for the quadrant.\n      // Use `atomicAdd` to do the reduction.\n      atomicAdd(&counts[quadrant], 1);\n   }\n\n   // Reduce per quadrant.\n   __syncthreads();\n   if (tid < 2) {\n      // Each warp is responsible for 2 elements.\n      // Use the `min` function to reduce 2 elements to 1.\n      counts[tid] += counts[tid + 2];\n   }\n   __syncthreads();\n   if (tid == 0) {\n      // Each block is responsible for 1 element.\n      // Use the `min` function to reduce 1 element to 1.\n      bins[blockIdx] = counts[0];\n   }\n}",
            "// your code here\n}",
            "const int tid = threadIdx.x;\n\n   // TODO: insert your code here\n   // Hint: use at least 4 threads for each quadrant\n}",
            "size_t n = blockIdx.x * blockDim.x + threadIdx.x;\n   if (n >= N) {\n      return;\n   }\n   Point p = points[n];\n   if (p.x < 0) {\n      if (p.y < 0) {\n         atomicAdd(&bins[0], 1);\n      }\n      else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n   else {\n      if (p.y < 0) {\n         atomicAdd(&bins[1], 1);\n      }\n      else {\n         atomicAdd(&bins[2], 1);\n      }\n   }\n}",
            "int tx = threadIdx.x;\n   int bx = blockIdx.x;\n\n   size_t stride = blockDim.x * gridDim.x;\n   size_t i = bx * blockDim.x + tx;\n\n   if (i >= N) {\n      return;\n   }\n\n   const Point *p = &points[i];\n   int x = (p->x > 0)? 1 : 0;\n   int y = (p->y > 0)? 1 : 0;\n   bins[y*2 + x]++;\n}",
            "// TODO: count points in quadrants and write to bins\n    // you can use the function getQuadrant() to compute quadrant of a point\n}",
            "// TODO: implement\n}",
            "size_t threadId = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (threadId >= N) {\n    return;\n  }\n\n  double x = points[threadId].x;\n  double y = points[threadId].y;\n\n  if (x > 0.0 && y > 0.0) {\n    atomicAdd(&bins[0], 1);\n  } else if (x < 0.0 && y > 0.0) {\n    atomicAdd(&bins[1], 1);\n  } else if (x < 0.0 && y < 0.0) {\n    atomicAdd(&bins[2], 1);\n  } else if (x > 0.0 && y < 0.0) {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "// TODO: launch a kernel here\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (tid < N) {\n        double x = points[tid].x;\n        double y = points[tid].y;\n\n        bins[x > 0 && y > 0]++;\n        bins[x < 0 && y > 0]++;\n        bins[x < 0 && y < 0]++;\n        bins[x > 0 && y < 0]++;\n    }\n}",
            "const size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i >= N) return;\n    const Point p = points[i];\n    if (p.x > 0.0) {\n        if (p.y > 0.0) {\n            atomicAdd(&bins[0], 1);\n        } else {\n            atomicAdd(&bins[2], 1);\n        }\n    } else {\n        if (p.y > 0.0) {\n            atomicAdd(&bins[1], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "bins[0] = bins[1] = bins[2] = bins[3] = 0;\n   for(size_t i = 0; i < N; ++i) {\n      if(points[i].x > 0 && points[i].y > 0) {\n         bins[0]++;\n      } else if(points[i].x < 0 && points[i].y > 0) {\n         bins[1]++;\n      } else if(points[i].x < 0 && points[i].y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "const size_t threadId = blockIdx.x * blockDim.x + threadIdx.x;\n   const size_t Nthreads = blockDim.x * gridDim.x;\n   for (size_t i = threadId; i < N; i += Nthreads) {\n      const Point &p = points[i];\n      size_t bin = 0;\n      if (p.x > 0.0) {\n         bin += 1;\n         if (p.y > 0.0) {\n            bin += 2;\n         }\n      } else {\n         bin += 3;\n         if (p.y > 0.0) {\n            bin += 1;\n         }\n      }\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "/* Compute the quadrant id for a given point. */\n    auto quadrant = [&](size_t i) {\n        return (points[i].x >= 0 && points[i].y >= 0)? 0 :\n               (points[i].x >= 0 && points[i].y < 0)? 1 :\n               (points[i].x < 0 && points[i].y < 0)? 2 : 3;\n    };\n\n    /* Count the number of points in a quadrant. */\n    auto countQuadrant = [&](size_t i, size_t j) {\n        for (; i < N; i += blockDim.x * gridDim.x) {\n            if (j == quadrant(i)) {\n                atomicAdd(&bins[j], 1);\n            }\n        }\n    };\n\n    // The thread number in the block.\n    const size_t tid = threadIdx.x;\n    // The block number.\n    const size_t bid = blockIdx.x;\n    // The number of blocks.\n    const size_t nblocks = gridDim.x;\n    // The number of items in each block.\n    const size_t nblock = N / nblocks;\n\n    // The first element of the block.\n    const size_t i0 = bid * nblock;\n    // The end of the block (excluded).\n    const size_t iend = min(i0 + nblock, N);\n    // The end of the last block.\n    const size_t iendlast = min(N, i0 + nblock);\n\n    // The number of points to process.\n    size_t Nq = iend - i0;\n    // The last block has to process the remaining points.\n    if (i0 + nblock > N) {\n        Nq = iendlast - i0;\n    }\n\n    // Launch the threads.\n    countQuadrant(i0 + tid, bid);\n    __syncthreads();\n    // Merge the partial results.\n    for (int i = nblocks / 2; i > 0; i /= 2) {\n        if (bid < i) {\n            countQuadrant(i0 + tid, bid + i);\n            __syncthreads();\n        }\n    }\n    if (bid == 0) {\n        bins[0] += bins[2];\n        bins[1] += bins[3];\n        bins[2] = 0;\n        bins[3] = 0;\n    }\n}",
            "__shared__ size_t binCounts[4];\n\n  // TODO: count the number of points in the quadrants\n  for (int i = 0; i < 4; i++) {\n    binCounts[i] = 0;\n  }\n\n  for (int i = 0; i < N; i++) {\n    Point p = points[i];\n    if (p.x >= 0 && p.x < 0.5) {\n      if (p.y >= 0 && p.y < 0.5) {\n        atomicAdd(&binCounts[0], 1);\n      } else if (p.y >= 0.5) {\n        atomicAdd(&binCounts[1], 1);\n      }\n    } else if (p.x < 0.5) {\n      if (p.y >= 0 && p.y < 0.5) {\n        atomicAdd(&binCounts[2], 1);\n      } else if (p.y >= 0.5) {\n        atomicAdd(&binCounts[3], 1);\n      }\n    } else {\n      if (p.y >= 0 && p.y < 0.5) {\n        atomicAdd(&binCounts[4], 1);\n      } else if (p.y >= 0.5) {\n        atomicAdd(&binCounts[5], 1);\n      }\n    }\n  }\n\n  __syncthreads();\n  // TODO: copy the bin counts into the output array\n  // Store the counts in `bins`.\n  // TODO: write the bin counts in shared memory `binCounts` into the output array `bins`.\n  for (int i = 0; i < 4; i++) {\n    bins[i] = binCounts[i];\n  }\n\n  //__syncthreads();\n  // TODO: copy the bin counts into the output array\n  // Store the counts in `bins`.\n  // TODO: write the bin counts in shared memory `binCounts` into the output array `bins`.\n  for (int i = 0; i < 4; i++) {\n    bins[i] = binCounts[i];\n  }\n}",
            "size_t idx = threadIdx.x;\n    if (idx >= N) {\n        return;\n    }\n    //TODO: Implement\n}",
            "// Write your code here\n}",
            "size_t threadIdx = blockIdx.x * blockDim.x + threadIdx.x;\n\n   if (threadIdx >= N)\n      return;\n\n   Point p = points[threadIdx];\n   if (p.x > 0.0 && p.y > 0.0)\n      atomicAdd(&bins[0], 1);\n   else if (p.x < 0.0 && p.y > 0.0)\n      atomicAdd(&bins[1], 1);\n   else if (p.x < 0.0 && p.y < 0.0)\n      atomicAdd(&bins[2], 1);\n   else\n      atomicAdd(&bins[3], 1);\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n    while (tid < N) {\n        double x = points[tid].x;\n        double y = points[tid].y;\n        int bin;\n        if (x >= 0 && y >= 0) {\n            bin = 0;\n        } else if (x <= 0 && y <= 0) {\n            bin = 1;\n        } else if (x < 0 && y > 0) {\n            bin = 2;\n        } else {\n            bin = 3;\n        }\n        atomicAdd(&bins[bin], 1);\n        tid += blockDim.x * gridDim.x;\n    }\n}",
            "...\n}",
            "size_t gtid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n   if(gtid >= N)\n       return;\n\n   auto x = points[gtid].x;\n   auto y = points[gtid].y;\n\n   bins[0] += x >= 0 && y >= 0;\n   bins[1] += x <= 0 && y >= 0;\n   bins[2] += x <= 0 && y <= 0;\n   bins[3] += x >= 0 && y <= 0;\n}",
            "// Initialize the bins to 0\n   for (int i = 0; i < 4; i++)\n      bins[i] = 0;\n\n   // Each thread performs one bin-count\n   int x = points[threadIdx.x].x;\n   int y = points[threadIdx.x].y;\n\n   if (x > 0 && y > 0)\n      atomicAdd(&bins[0], 1);\n   else if (x < 0 && y > 0)\n      atomicAdd(&bins[1], 1);\n   else if (x < 0 && y < 0)\n      atomicAdd(&bins[2], 1);\n   else if (x > 0 && y < 0)\n      atomicAdd(&bins[3], 1);\n}",
            "bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n\n   // TODO: add kernel code\n}",
            "for (int i = 0; i < N; i++) {\n      int idx = 0;\n      if (points[i].x >= 0) {\n         idx += 1;\n      }\n      if (points[i].y >= 0) {\n         idx += 2;\n      }\n      atomicAdd(&bins[idx], 1);\n   }\n}",
            "__shared__ size_t shared_bins[4];\n\n    // count the number of points in each quadrant.\n    if (threadIdx.x < 4) {\n        shared_bins[threadIdx.x] = 0;\n    }\n\n    __syncthreads();\n\n    size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        double x = points[i].x;\n        double y = points[i].y;\n        int quadrant = 0;\n        if (y >= 0) {\n            if (x > 0) {\n                quadrant = 1;\n            } else {\n                quadrant = 2;\n            }\n        } else {\n            quadrant = 3;\n        }\n        atomicAdd(&shared_bins[quadrant], 1);\n    }\n\n    __syncthreads();\n    for (int i = 0; i < 4; i++) {\n        atomicAdd(&bins[i], shared_bins[i]);\n    }\n\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n   if (i >= N) return;\n   Point p = points[i];\n   size_t quadrant = 0;\n   if (p.x >= 0) {\n      if (p.y >= 0) quadrant = 0;\n      else quadrant = 3;\n   } else {\n      if (p.y >= 0) quadrant = 1;\n      else quadrant = 2;\n   }\n   atomicAdd(&bins[quadrant], 1);\n}",
            "for (size_t i = 0; i < N; i++) {\n        double x = points[i].x;\n        double y = points[i].y;\n\n        if (x > 0) {\n            if (y > 0) {\n                atomicAdd(&bins[0], 1);\n            } else {\n                atomicAdd(&bins[1], 1);\n            }\n        } else {\n            if (y > 0) {\n                atomicAdd(&bins[2], 1);\n            } else {\n                atomicAdd(&bins[3], 1);\n            }\n        }\n    }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n   if (index < N) {\n      // TODO\n   }\n}",
            "size_t q = 0;\n   for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n      Point p = points[i];\n      if (p.x < 0)\n         q = 1;\n      else if (p.y < 0)\n         q = 2;\n      else if (p.x > 0)\n         q = 3;\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "// TODO\n}",
            "size_t index = threadIdx.x + blockDim.x * blockIdx.x;\n  if (index < N) {\n    bins[(int) ((points + index)->x / 4)]++;\n  }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n    for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n        Point p = points[i];\n        bins[0] += p.x > 0 && p.y > 0;\n        bins[1] += p.x < 0 && p.y > 0;\n        bins[2] += p.x < 0 && p.y < 0;\n        bins[3] += p.x > 0 && p.y < 0;\n    }\n}",
            "size_t idx = threadIdx.x + blockDim.x * blockIdx.x;\n    for (size_t i = idx; i < N; i += blockDim.x * gridDim.x) {\n        double x = points[i].x;\n        double y = points[i].y;\n        if (x > 0) {\n            if (y > 0) {\n                atomicAdd(&bins[0], 1);\n            } else {\n                atomicAdd(&bins[3], 1);\n            }\n        } else {\n            if (y > 0) {\n                atomicAdd(&bins[2], 1);\n            } else {\n                atomicAdd(&bins[1], 1);\n            }\n        }\n    }\n}",
            "// TODO\n}",
            "size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n   if (i >= N)\n      return;\n\n   double x = points[i].x;\n   double y = points[i].y;\n   size_t quadrant = 0;\n   if (x >= 0.0) {\n      if (y >= 0.0) {\n         quadrant = 0;\n      }\n      else {\n         quadrant = 3;\n      }\n   }\n   else {\n      if (y >= 0.0) {\n         quadrant = 1;\n      }\n      else {\n         quadrant = 2;\n      }\n   }\n   atomicAdd(&bins[quadrant], 1);\n}",
            "// Your code here\n}",
            "int tid = threadIdx.x;\n   int bid = blockIdx.x;\n   int bdim = blockDim.x;\n\n   int x, y;\n   int quadrant;\n   for (size_t i = bid * bdim + tid; i < N; i += bdim * gridDim.x) {\n      x = (int) round(points[i].x);\n      y = (int) round(points[i].y);\n      if (x >= 0) {\n         if (y >= 0)\n            quadrant = 0;\n         else\n            quadrant = 3;\n      }\n      else {\n         if (y >= 0)\n            quadrant = 1;\n         else\n            quadrant = 2;\n      }\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "// TODO: allocate storage for the bins\n\n    // TODO: for each element of points, determine which quadrant it belongs to\n\n    // TODO: write the result into the bins\n\n}",
            "// Insert your code here\n}",
            "int i = threadIdx.x;\n   int stride = blockDim.x;\n\n   for (; i < N; i += stride) {\n      const Point &p = points[i];\n\n      if (p.x > 0.0 && p.y > 0.0) {\n         atomicAdd(&bins[0], 1);\n      } else if (p.x < 0.0 && p.y > 0.0) {\n         atomicAdd(&bins[1], 1);\n      } else if (p.x < 0.0 && p.y < 0.0) {\n         atomicAdd(&bins[2], 1);\n      } else if (p.x > 0.0 && p.y < 0.0) {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// HIP Note:\n   // - launch with at least N threads\n   // - use a thread index from [0:N)\n   // - use bins[0:4) to store counts\n   // - use atomicAdd() to increment counts\n\n   // HIP Note:\n   // - use atomicAdd() to increment counts\n   // - atomicAdd() is a function that returns the previous value before increment\n   // - atomicAdd() is only guaranteed to work on a single float value or double value\n   // - atomicAdd() is not guaranteed to work on int values\n   // - atomicAdd() is only guaranteed to work on addresses aligned to 4 bytes\n   // - atomicAdd() works on both host and device\n   // - atomicAdd() is a __device__ function\n   // - atomicAdd() only works on double values and float values\n   // - atomicAdd() is not guaranteed to work on integers\n\n   // HIP Note:\n   // - all threads in a warp execute the same instructions together\n   // - a warp is 32 threads\n   // - all threads in a warp execute the same instructions together\n   // - warps are 32 threads\n   // - warp divergence\n   // - one thread diverges from the warp\n   // - thread divergence\n   // - warp divergence\n   // - a warp diverges from the block\n   // - a block diverges from the grid\n   // - a thread diverges from the block\n   // - thread divergence\n   // - thread divergence\n   // - a block diverges from the grid\n   // - a thread divergence\n   // - a block diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the grid\n   // - a thread diverges from the",
            "// TODO\n}",
            "}",
            "// TODO: implement me\n}",
            "// Calculate the thread ID and index\n    size_t idx = threadIdx.x + blockIdx.x*blockDim.x;\n\n    // Add to the corresponding bin\n    if(idx < N) {\n        // If the point is in the first quadrant, add one to the first bin\n        if(points[idx].x >= 0 && points[idx].y >= 0) {\n            atomicAdd(&bins[0], 1);\n        }\n        // If the point is in the second quadrant, add one to the second bin\n        else if(points[idx].x < 0 && points[idx].y >= 0) {\n            atomicAdd(&bins[1], 1);\n        }\n        // If the point is in the third quadrant, add one to the third bin\n        else if(points[idx].x < 0 && points[idx].y < 0) {\n            atomicAdd(&bins[2], 1);\n        }\n        // If the point is in the fourth quadrant, add one to the fourth bin\n        else if(points[idx].x >= 0 && points[idx].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "//TODO: fill in\n}",
            "// TODO\n}",
            "// your code here\n   //...\n   //...\n   //...\n}",
            "int t_idx = threadIdx.x + blockIdx.x * blockDim.x;\n    int b_idx = blockIdx.x;\n\n    __shared__ Point myPoints[N];\n\n    // Copy data to shared memory\n    if (t_idx < N)\n        myPoints[t_idx] = points[t_idx];\n\n    __syncthreads();\n\n    // Each thread will perform the local computation\n    if (t_idx < N) {\n        double x = myPoints[t_idx].x;\n        double y = myPoints[t_idx].y;\n        if (x >= 0) {\n            if (y >= 0)\n                atomicAdd(&bins[0], 1);\n            else\n                atomicAdd(&bins[1], 1);\n        } else {\n            if (y >= 0)\n                atomicAdd(&bins[2], 1);\n            else\n                atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "int x = points[threadIdx.x].x, y = points[threadIdx.x].y;\n   bins[0] += x >= 0.0 && y >= 0.0;\n   bins[1] += x >= 0.0 && y < 0.0;\n   bins[2] += x < 0.0 && y < 0.0;\n   bins[3] += x < 0.0 && y >= 0.0;\n}",
            "// TODO: Your code here\n    int i = threadIdx.x;\n    int j = blockIdx.x;\n    int k = blockDim.x;\n    // int l = N;\n\n    // bins[0] = 0;\n    // bins[1] = 0;\n    // bins[2] = 0;\n    // bins[3] = 0;\n    if (i<N){\n        if(points[i].x>0 && points[i].y>0){\n            bins[0]++;\n        }\n        else if(points[i].x<0 && points[i].y<0){\n            bins[1]++;\n        }\n        else if(points[i].x<0 && points[i].y>0){\n            bins[2]++;\n        }\n        else if(points[i].x>0 && points[i].y<0){\n            bins[3]++;\n        }\n    }\n\n    // return;\n}",
            "// Count the number of points in each quadrant\n}",
            "size_t gid = threadIdx.x + blockIdx.x * blockDim.x;\n   if (gid < N) {\n      double x = points[gid].x;\n      double y = points[gid].y;\n      if (x >= 0.0 && y >= 0.0) {\n         bins[0]++;\n      } else if (x < 0.0 && y >= 0.0) {\n         bins[1]++;\n      } else if (x < 0.0 && y < 0.0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "/* Your code here */\n}",
            "/*\n    * Hip requires at least one thread\n    */\n   if (blockIdx.x == 0 && threadIdx.x == 0) {\n      int i = 0;\n      while (i < N) {\n         if (points[i].x > 0 && points[i].y > 0)\n            atomicAdd(&bins[0], 1);\n         else if (points[i].x < 0 && points[i].y > 0)\n            atomicAdd(&bins[1], 1);\n         else if (points[i].x < 0 && points[i].y < 0)\n            atomicAdd(&bins[2], 1);\n         else if (points[i].x > 0 && points[i].y < 0)\n            atomicAdd(&bins[3], 1);\n         i++;\n      }\n   }\n}",
            "const int tx = threadIdx.x;\n   __shared__ Point sharedPoints[BLOCKSIZE];\n   __shared__ size_t sharedCounts[4];\n   int quadrant;\n\n   // initialize the local block memory for the shared data\n   for (int i = tx; i < 4; i += BLOCKSIZE) {\n      sharedCounts[i] = 0;\n   }\n   // load the data into local block memory\n   if (tx < N) {\n      sharedPoints[tx] = points[tx];\n   }\n   __syncthreads();\n\n   // iterate over the shared memory array\n   for (int i = 0; i < N; i += BLOCKSIZE) {\n      quadrant = (sharedPoints[i].x > 0) + (sharedPoints[i].y > 0);\n      sharedCounts[quadrant]++;\n   }\n   __syncthreads();\n\n   // write the results to global memory\n   for (int i = tx; i < 4; i += BLOCKSIZE) {\n      bins[i] = sharedCounts[i];\n   }\n}",
            "int x = points[hipThreadIdx_x].x;\n   int y = points[hipThreadIdx_x].y;\n   int bin = 0;\n   if (x >= 0) {\n      bin++;\n      if (y >= 0) {\n         bin++;\n         if (x >= y) {\n            bin++;\n         }\n      }\n   }\n\n   bins[bin]++;\n}",
            "size_t tid = threadIdx.x;\n    if (tid < N) {\n        Point p = points[tid];\n        if (p.x >= 0 && p.y >= 0) {\n            atomicAdd(&bins[0], 1);\n        }\n        else if (p.x < 0 && p.y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n        else if (p.x < 0) {\n            atomicAdd(&bins[2], 1);\n        }\n        else {\n            atomicAdd(&bins[1], 1);\n        }\n    }\n}",
            "// Get the thread index\n   int idx = threadIdx.x;\n\n   // Compute the number of points in each quadrant\n   if (idx < N) {\n      double x = points[idx].x;\n      double y = points[idx].y;\n      if (x > 0 && y > 0) {\n         bins[0]++;\n      } else if (x < 0 && y > 0) {\n         bins[1]++;\n      } else if (x < 0 && y < 0) {\n         bins[2]++;\n      } else if (x > 0 && y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "//TODO\n}",
            "for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n      const Point& p = points[i];\n      if (p.x > 0.0) {\n         if (p.y > 0.0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      } else {\n         if (p.y > 0.0) {\n            atomicAdd(&bins[1], 1);\n         } else {\n            atomicAdd(&bins[2], 1);\n         }\n      }\n   }\n}",
            "int index = threadIdx.x;\n   __shared__ size_t localBins[4];\n   if (index < 4)\n      localBins[index] = 0;\n   __syncthreads();\n\n   int x, y;\n   for (size_t i = index; i < N; i += blockDim.x) {\n      x = (points[i].x >= 0)? 0 : 1;\n      y = (points[i].y >= 0)? 0 : 1;\n      if (x == 1)\n         x = 2;\n      if (y == 1)\n         y = 2;\n      atomicAdd(&localBins[x], 1);\n      atomicAdd(&localBins[y + 2], 1);\n   }\n   __syncthreads();\n\n   if (index < 4) {\n      atomicAdd(&bins[index], localBins[index]);\n      atomicAdd(&bins[index + 4], localBins[index + 4]);\n   }\n}",
            "// TODO\n}",
            "int tid = threadIdx.x;\n   bins[tid] = 0;\n   __syncthreads();\n\n   for (int i = tid; i < N; i += blockDim.x) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x > 0.0 && y > 0.0) {\n         bins[0] += 1;\n      } else if (x < 0.0 && y > 0.0) {\n         bins[1] += 1;\n      } else if (x < 0.0 && y < 0.0) {\n         bins[2] += 1;\n      } else {\n         bins[3] += 1;\n      }\n   }\n   __syncthreads();\n\n   // Reduction of partial counts to a single value.\n   for (int s = blockDim.x/2; s > 0; s >>= 1) {\n      if (tid < s) {\n         bins[tid] += bins[tid + s];\n      }\n      __syncthreads();\n   }\n}",
            "unsigned int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx >= N) {\n        return;\n    }\n    size_t bin = 0;\n    if (points[idx].x > 0) {\n        if (points[idx].y > 0) {\n            bin = 0;\n        } else {\n            bin = 1;\n        }\n    } else {\n        if (points[idx].y > 0) {\n            bin = 2;\n        } else {\n            bin = 3;\n        }\n    }\n    //atomicAdd(&bins[bin], 1);\n    atomicAdd(&bins[bin], __popc(1));\n}",
            "// TODO: implement this\n}",
            "size_t x_idx = blockIdx.x * blockDim.x + threadIdx.x;\n   size_t y_idx = blockIdx.y * blockDim.y + threadIdx.y;\n\n   if (x_idx < N && y_idx < N) {\n      const Point &p = points[x_idx + y_idx * N];\n\n      int x = floor(p.x / (2 * M_PI));\n      int y = floor(p.y / (2 * M_PI));\n\n      // if (x < 0) x = 1;\n      // if (y < 0) y = 1;\n\n      if (x < 0) x = 1;\n      if (y < 0) y = 1;\n      bins[x + 2 * y]++;\n   }\n}",
            "__shared__ size_t shared[4];\n   for (int i = 0; i < 4; ++i) {\n      shared[i] = 0;\n   }\n   for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n      int q;\n      if (points[i].x > 0 && points[i].y > 0)\n         q = 0;\n      else if (points[i].x < 0 && points[i].y > 0)\n         q = 1;\n      else if (points[i].x < 0 && points[i].y < 0)\n         q = 2;\n      else\n         q = 3;\n      atomicAdd(&shared[q], 1);\n   }\n   for (int i = 0; i < 4; ++i) {\n      atomicAdd(&bins[i], shared[i]);\n   }\n}",
            "// TODO\n}",
            "// TODO: Implement this function using AMD HIP\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double x = points[i].x;\n        double y = points[i].y;\n        if (x >= 0.0) {\n            if (y >= 0.0) {\n                atomicAdd(&bins[0], 1);\n            } else {\n                atomicAdd(&bins[1], 1);\n            }\n        } else {\n            if (y >= 0.0) {\n                atomicAdd(&bins[2], 1);\n            } else {\n                atomicAdd(&bins[3], 1);\n            }\n        }\n    }\n}",
            "/* TODO: implement */\n}",
            "int idx = threadIdx.x;\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n\n  for (size_t i = idx; i < N; i += blockDim.x) {\n    if (points[i].x > 0 && points[i].y > 0) {\n      bins[0]++;\n    } else if (points[i].x < 0 && points[i].y > 0) {\n      bins[1]++;\n    } else if (points[i].x < 0 && points[i].y < 0) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "// TODO: Implement\n    for (int i = threadIdx.x; i < N; i += blockDim.x) {\n        if (points[i].x >= 0.0 && points[i].y >= 0.0) {\n            atomicAdd(&bins[0], 1);\n        }\n        else if (points[i].x < 0.0 && points[i].y >= 0.0) {\n            atomicAdd(&bins[1], 1);\n        }\n        else if (points[i].x < 0.0 && points[i].y < 0.0) {\n            atomicAdd(&bins[2], 1);\n        }\n        else if (points[i].x >= 0.0 && points[i].y < 0.0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        // quadrant\n        double x = points[i].x;\n        double y = points[i].y;\n        size_t bin = (x < 0.0) * 2 + (y < 0.0);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "int i = threadIdx.x + blockDim.x * blockIdx.x;\n   if (i < N) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x >= 0) {\n         if (y >= 0)\n            bins[0]++;\n         else\n            bins[1]++;\n      } else {\n         if (y >= 0)\n            bins[2]++;\n         else\n            bins[3]++;\n      }\n   }\n}",
            "size_t blockId = blockIdx.x + blockIdx.y * gridDim.x;\n   size_t threadId = threadIdx.x + blockId * blockDim.x;\n   if (threadId >= N) return;\n   int quadrant = 0;\n   if (points[threadId].x >= 0) {\n      quadrant++;\n   } else {\n      quadrant += 2;\n   }\n   if (points[threadId].y >= 0) {\n      quadrant++;\n   } else {\n      quadrant += 2;\n   }\n   atomicAdd(&bins[quadrant], 1);\n}",
            "for (int index = blockIdx.x * blockDim.x + threadIdx.x; index < N; index += blockDim.x * gridDim.x) {\n        const Point p = points[index];\n        if (p.x < 0) {\n            if (p.y < 0) {\n                atomicAdd(&bins[0], 1);\n            } else {\n                atomicAdd(&bins[3], 1);\n            }\n        } else {\n            if (p.y < 0) {\n                atomicAdd(&bins[1], 1);\n            } else {\n                atomicAdd(&bins[2], 1);\n            }\n        }\n    }\n}",
            "__shared__ size_t s_bins[4];\n   const int tid = threadIdx.x;\n   const int warp = tid / WARP_SIZE;\n   const int lane = tid % WARP_SIZE;\n\n   // 1. reduce the values of the entire array into a shared memory array\n   for (size_t i = tid; i < N; i += blockDim.x) {\n      const Point p = points[i];\n      const int x = (p.x > 0)? 0 : ((p.x < 0)? 1 : 2);\n      const int y = (p.y > 0)? 0 : ((p.y < 0)? 1 : 2);\n      atomicAdd(&s_bins[x * 2 + y], 1);\n   }\n\n   // 2. transpose the shared memory arrays into bins\n   __syncthreads();\n   for (int i = lane; i < 4; i += WARP_SIZE) {\n      atomicAdd(&bins[i], s_bins[warp * 2 + i]);\n   }\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n   if (tid >= N) {\n      return;\n   }\n\n   int q = -1;\n   if (points[tid].x > 0 && points[tid].y > 0) {\n      q = 0;\n   } else if (points[tid].x < 0 && points[tid].y > 0) {\n      q = 1;\n   } else if (points[tid].x < 0 && points[tid].y < 0) {\n      q = 2;\n   } else if (points[tid].x > 0 && points[tid].y < 0) {\n      q = 3;\n   }\n   __sync_fetch_and_add(&bins[q], 1);\n}",
            "__shared__ size_t counts[4];\n   int tid = threadIdx.x;\n   int q = 0;\n   int x = 1;\n   int y = 2;\n   if (tid < 4) {\n      counts[tid] = 0;\n   }\n   __syncthreads();\n   for (int i = tid; i < N; i += blockDim.x) {\n      int px = ((points + i)->x < 0)? -1 : 1;\n      int py = ((points + i)->y < 0)? -1 : 1;\n      if (px == x && py == y) {\n         q = 0;\n      } else if (px == x && py == -y) {\n         q = 1;\n      } else if (px == -x && py == y) {\n         q = 2;\n      } else {\n         q = 3;\n      }\n      atomicAdd(&counts[q], 1);\n   }\n   __syncthreads();\n   if (tid < 4) {\n      bins[tid] = counts[tid];\n   }\n}",
            "// TODO: Your code goes here\n\n   bins[threadIdx.x] = 0;\n   for(int i = 0; i < N; i++) {\n      int x = round(points[i].x);\n      int y = round(points[i].y);\n      if(x >= 0) {\n         if(y >= 0) {\n            bins[threadIdx.x]++;\n         }\n      }\n   }\n}",
            "bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n\n   if (threadIdx.x < N) {\n       Point p = points[threadIdx.x];\n       if (p.x < 0) {\n           if (p.y < 0) {\n               atomicAdd(&bins[0], 1);\n           } else {\n               atomicAdd(&bins[1], 1);\n           }\n       } else {\n           if (p.y < 0) {\n               atomicAdd(&bins[2], 1);\n           } else {\n               atomicAdd(&bins[3], 1);\n           }\n       }\n   }\n}",
            "for (int i = threadIdx.x; i < N; i += blockDim.x) {\n    bins[(points[i].x >= 0? 0 : 1) + (points[i].y >= 0? 0 : 2)] += 1;\n  }\n}",
            "// Your code here\n}",
            "// TODO\n}",
            "int tid = threadIdx.x;\n   int bid = blockIdx.x;\n   size_t nbins = sizeof(bins) / sizeof(size_t);\n   // TODO: Your code here\n}",
            "// declare a shared memory array for atomic add\n    __shared__ int shmem[4];\n\n    // thread id and number of threads in this block\n    int tid = threadIdx.x;\n    int nthreads = blockDim.x;\n\n    // set shared memory to zero\n    for (int i = tid; i < 4; i += nthreads) {\n        shmem[i] = 0;\n    }\n\n    // ensure all threads have synchronized\n    __syncthreads();\n\n    // loop over input points\n    for (int i = 0; i < N; i++) {\n\n        // get input point\n        Point point = points[i];\n\n        // get x and y values\n        double x = point.x;\n        double y = point.y;\n\n        // compute quadrant number for this point\n        int q;\n        if (y >= 0) {\n            if (x >= 0) q = 0;\n            else q = 3;\n        } else {\n            if (x >= 0) q = 1;\n            else q = 2;\n        }\n\n        // increment shared memory\n        atomicAdd(&shmem[q], 1);\n    }\n\n    // ensure all threads have synchronized\n    __syncthreads();\n\n    // store results\n    for (int i = tid; i < 4; i += nthreads) {\n        bins[i] = shmem[i];\n    }\n}",
            "bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n\n   int tid = threadIdx.x + blockIdx.x * blockDim.x;\n   int nthreads = blockDim.x * gridDim.x;\n   for (int i = tid; i < N; i += nthreads) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x > 0 && y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (x > 0 && y < 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (x < 0 && y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else if (x < 0 && y > 0) {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    bins[quadrant(points[tid])]++;\n  }\n}",
            "// TODO: count the points in each quadrant using atomic operations\n}",
            "int i = threadIdx.x + blockDim.x * blockIdx.x;\n  if (i < N) {\n    double x = points[i].x;\n    double y = points[i].y;\n    if (x > 0.0 && y > 0.0)\n      atomicAdd(&bins[0], 1);\n    if (x < 0.0 && y > 0.0)\n      atomicAdd(&bins[1], 1);\n    if (x < 0.0 && y < 0.0)\n      atomicAdd(&bins[2], 1);\n    if (x > 0.0 && y < 0.0)\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n   if (i < N) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x > 0) {\n         if (y > 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      } else {\n         if (y > 0) {\n            atomicAdd(&bins[1], 1);\n         } else {\n            atomicAdd(&bins[2], 1);\n         }\n      }\n   }\n}",
            "const size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= N) return;\n\n   const double x = points[i].x;\n   const double y = points[i].y;\n\n   if (x > 0 && y > 0)\n      atomicAdd(&bins[0], 1);\n   else if (x < 0 && y > 0)\n      atomicAdd(&bins[1], 1);\n   else if (x < 0 && y < 0)\n      atomicAdd(&bins[2], 1);\n   else if (x > 0 && y < 0)\n      atomicAdd(&bins[3], 1);\n}",
            "const int t = threadIdx.x;\n   if (t < N) {\n      // TODO\n   }\n}",
            "...\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n   if (i >= N) {\n      return;\n   }\n   //...\n}",
            "int i = threadIdx.x;\n   if(i >= N) return;\n   Point point = points[i];\n   // if (point.x > 0)\n   // {\n   //   if (point.y > 0)\n   //   {\n   //     atomicAdd(&bins[0], 1);\n   //   }\n   //   else\n   //   {\n   //     atomicAdd(&bins[1], 1);\n   //   }\n   // }\n   // else\n   // {\n   //   if (point.y > 0)\n   //   {\n   //     atomicAdd(&bins[2], 1);\n   //   }\n   //   else\n   //   {\n   //     atomicAdd(&bins[3], 1);\n   //   }\n   // }\n   // return;\n   if (point.x >= 0 && point.y >= 0)\n   {\n      atomicAdd(&bins[0], 1);\n   }\n   else if (point.x <= 0 && point.y >= 0)\n   {\n      atomicAdd(&bins[1], 1);\n   }\n   else if (point.x <= 0 && point.y <= 0)\n   {\n      atomicAdd(&bins[2], 1);\n   }\n   else\n   {\n      atomicAdd(&bins[3], 1);\n   }\n}",
            "size_t blockSize = blockDim.x * gridDim.x;\n  size_t start = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t i;\n  for (i = start; i < N; i += blockSize) {\n    double x = points[i].x;\n    double y = points[i].y;\n    // TODO: Update the bins\n  }\n}",
            "size_t count = 0;\n   for (int i = threadIdx.x; i < N; i += blockDim.x) {\n      if (points[i].x > 0 && points[i].y > 0)\n         ++count;\n      else if (points[i].x < 0 && points[i].y > 0)\n         ++count;\n      else if (points[i].x < 0 && points[i].y < 0)\n         ++count;\n      else\n         ++count;\n   }\n   atomicAdd(&bins[threadIdx.x], count);\n}",
            "unsigned int index = blockIdx.x * blockDim.x + threadIdx.x;\n   // TODO: your code here\n   if(index>=N)\n   {\n      return;\n   }\n   int quadrant = 0;\n   if(points[index].x > 0.0)\n   {\n      if(points[index].y > 0.0)\n      {\n         quadrant = 0;\n      }\n      else\n      {\n         quadrant = 3;\n      }\n   }\n   else\n   {\n      if(points[index].y > 0.0)\n      {\n         quadrant = 1;\n      }\n      else\n      {\n         quadrant = 2;\n      }\n   }\n   atomicAdd(&bins[quadrant], 1);\n}",
            "//TODO\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n\n  if (tid < N) {\n    int quadrant = 0;\n    if (points[tid].x >= 0.0) {\n      quadrant += 1;\n    }\n    if (points[tid].y >= 0.0) {\n      quadrant += 2;\n    }\n    atomicAdd(&bins[quadrant], 1);\n  }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use MPI to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The result is stored in bins on rank 0.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, size;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &size);\n    if (size == 1)\n        return;\n\n    int dims[2] = {2, 2};\n    int periods[2] = {0, 0};\n    MPI_Cart_create(comm, 2, dims, periods, 1, &comm);\n\n    std::vector<Point> local_points;\n    MPI_Cart_sub(comm, &comm);\n    MPI_Scatter(points.data(), 1, MPI_POINT, local_points.data(), 1, MPI_POINT, 0, comm);\n\n    std::array<size_t, 4> local_bins;\n    int cart_coord[2];\n    MPI_Cart_coords(comm, rank, 2, cart_coord);\n\n    for (int i = 0; i < (int)local_points.size(); i++) {\n        if (local_points[i].x < 0) {\n            if (local_points[i].y < 0)\n                local_bins[0] += 1;\n            else\n                local_bins[1] += 1;\n        } else {\n            if (local_points[i].y < 0)\n                local_bins[2] += 1;\n            else\n                local_bins[3] += 1;\n        }\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, comm);\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int points_per_rank = points.size() / size;\n\n   int start = rank * points_per_rank;\n   int end = start + points_per_rank;\n   int n = end > points.size()? points.size() : end;\n\n   std::array<size_t, 4> local_bins{};\n   for (int i = start; i < n; i++) {\n      double x = points[i].x;\n      double y = points[i].y;\n\n      if (x > 0 && y > 0)\n         local_bins[0]++;\n      else if (x < 0 && y > 0)\n         local_bins[1]++;\n      else if (x < 0 && y < 0)\n         local_bins[2]++;\n      else if (x > 0 && y < 0)\n         local_bins[3]++;\n   }\n\n   std::array<size_t, 4> global_bins{};\n\n   MPI_Reduce(local_bins.data(), global_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      bins = global_bins;\n   }\n}",
            "int npoints;\n    int nranks;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &npoints);\n    if (npoints!= points.size()) {\n        std::cout << \"ERROR: Points and ranks do not match\" << std::endl;\n        exit(-1);\n    }\n\n    int dims[2] = {2, 2};\n    int periods[2] = {0, 0};\n    int reorder = 1;\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, reorder, &cart_comm);\n\n    int rank, coords[2];\n    MPI_Cart_coords(cart_comm, rank, 2, coords);\n    MPI_Cart_rank(cart_comm, coords, &rank);\n\n    std::vector<Point> local_points;\n    int n = points.size() / nranks;\n    int start = n * rank;\n    int end = start + n;\n    if (rank == nranks - 1) {\n        end = points.size();\n    }\n    for (int i = start; i < end; i++) {\n        local_points.push_back(points[i]);\n    }\n\n    size_t q1 = 0;\n    size_t q2 = 0;\n    size_t q3 = 0;\n    size_t q4 = 0;\n\n    for (int i = 0; i < local_points.size(); i++) {\n        if (local_points[i].x > 0 && local_points[i].y > 0) {\n            q1++;\n        } else if (local_points[i].x < 0 && local_points[i].y > 0) {\n            q2++;\n        } else if (local_points[i].x < 0 && local_points[i].y < 0) {\n            q3++;\n        } else {\n            q4++;\n        }\n    }\n\n    std::array<size_t, 4> counts = {q1, q2, q3, q4};\n    if (rank == 0) {\n        for (int i = 1; i < nranks; i++) {\n            MPI_Send(&counts, 4, MPI_UNSIGNED_LONG_LONG, i, 0, cart_comm);\n        }\n        for (int i = 1; i < nranks; i++) {\n            MPI_Recv(&counts, 4, MPI_UNSIGNED_LONG_LONG, i, 0, cart_comm, MPI_STATUS_IGNORE);\n        }\n\n        bins[0] = counts[0];\n        bins[1] = counts[1];\n        bins[2] = counts[2];\n        bins[3] = counts[3];\n    } else {\n        MPI_Recv(&counts, 4, MPI_UNSIGNED_LONG_LONG, 0, 0, cart_comm, MPI_STATUS_IGNORE);\n        bins[0] = counts[0];\n        bins[1] = counts[1];\n        bins[2] = counts[2];\n        bins[3] = counts[3];\n        MPI_Send(&counts, 4, MPI_UNSIGNED_LONG_LONG, 0, 0, cart_comm);\n    }\n    MPI_Barrier(cart_comm);\n}",
            "// your code here\n}",
            "// TODO: Your code here\n\n    // for each point in the list\n    for (int i = 0; i < points.size(); i++)\n    {\n        // set quadrant = -1, 0, 1, 2\n        int quadrant = -1;\n\n        // if point.x > 0\n        if (points[i].x > 0)\n        {\n            quadrant = 0;\n        }\n\n        // if point.x < 0\n        else if (points[i].x < 0)\n        {\n            quadrant = 1;\n        }\n\n        // if point.y > 0\n        if (points[i].y > 0)\n        {\n            quadrant = 2;\n        }\n\n        // if point.y < 0\n        else if (points[i].y < 0)\n        {\n            quadrant = 3;\n        }\n\n        // count point in quadrant\n        bins[quadrant]++;\n    }\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // determine the bin each point should belong to\n  std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n\n  for (const auto& point : points) {\n    if (point.x > 0 && point.y > 0) {\n      local_bins[0]++;\n    }\n    if (point.x < 0 && point.y > 0) {\n      local_bins[1]++;\n    }\n    if (point.x < 0 && point.y < 0) {\n      local_bins[2]++;\n    }\n    if (point.x > 0 && point.y < 0) {\n      local_bins[3]++;\n    }\n  }\n\n  // total up counts in each bin\n  std::array<size_t, 4> total_bins = {0, 0, 0, 0};\n\n  MPI_Reduce(&local_bins[0], &total_bins[0], 4, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    // now we have a vector of counts for each bin for each rank.\n    // we want to sum them up to find the total number of points in each quadrant\n    for (size_t i = 0; i < total_bins.size(); i++) {\n      bins[i] += total_bins[i];\n    }\n  }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n\n   // TODO: implement me\n   // TODO: return bins\n}",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size_t pointsPerRank = points.size() / size;\n    size_t start = rank * pointsPerRank;\n    size_t end = (rank+1) * pointsPerRank;\n    //TODO: compute bin counts\n    return;\n}",
            "// TODO: Replace the NULL with a valid MPI communicator\n    // Hint: the name of this communicator is \"MPI_COMM_WORLD\"\n    MPI_Comm comm = MPI_COMM_WORLD;\n\n    int rank, size;\n    MPI_Comm_size(comm, &size);\n    MPI_Comm_rank(comm, &rank);\n\n    //TODO: allocate memory to hold the quadrant counts\n\n    //TODO: fill in the quadrant counts\n\n    //TODO: use MPI to sum the quadrant counts\n\n    //TODO: return the quadrant counts\n}",
            "auto const size = points.size();\n    auto const rank = MPI::COMM_WORLD.Get_rank();\n\n    auto const size_per_rank = size / MPI::COMM_WORLD.Get_size();\n    auto const offset = rank * size_per_rank;\n    std::vector<Point> local_points(points.begin() + offset, points.begin() + offset + size_per_rank);\n\n    auto const d = MPI::COMM_WORLD.Get_dim();\n    auto const nx = d[0];\n    auto const ny = d[1];\n\n    auto const xmin = 0;\n    auto const xmax = 10;\n    auto const ymin = 0;\n    auto const ymax = 10;\n\n    auto const dx = (xmax - xmin) / nx;\n    auto const dy = (ymax - ymin) / ny;\n\n    for (auto const& p : local_points) {\n        auto const quadrant =\n            (p.x < xmin + dx / 2) +\n            ((p.x < xmax - dx / 2) * (p.y < ymin + dy / 2)) * nx +\n            ((p.y < ymax - dy / 2) * (p.x < xmax - dx / 2)) * (nx * ny);\n        bins[quadrant]++;\n    }\n\n    // MPI reduction:\n    //   - send the result of each rank to rank 0\n    //   - rank 0 receives all results and stores them in bins\n    std::array<size_t, 4> results;\n    auto const root = 0;\n    MPI::COMM_WORLD.Gather(&bins[0], 4, MPI::INT, &results[0], 4, MPI::INT, root);\n\n    if (rank == root) {\n        std::copy(results.begin(), results.end(), bins.begin());\n    }\n}",
            "size_t totalPoints = points.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int dims[2] = {size, size};\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, false, false, &MPI_COMM_CART);\n\n    int periods[2] = {1, 1};\n    MPI_Cart_get(MPI_COMM_CART, 2, dims, periods, MPI_CART_COORDS);\n    MPI_Comm_rank(MPI_COMM_CART, &rank);\n    MPI_Comm_size(MPI_COMM_CART, &size);\n\n    int myX, myY;\n    MPI_Cart_coords(MPI_COMM_CART, rank, 2, &myX, &myY);\n\n    // TODO: use MPI collective communication\n    // to sum quadrants counts from every rank in MPI_COMM_CART\n    // store the result in bins on rank 0\n    // every rank has the complete copy of points\n\n    // TODO: use MPI point to point communication\n    // rank 0 broadcasts the quadrant counts to every other rank\n\n    // TODO: use MPI point to point communication\n    // every rank broadcasts its quadrant counts to rank 0\n\n    // TODO: use MPI point to point communication\n    // rank 0 sends quadrant counts to all other ranks\n\n    // TODO: use MPI point to point communication\n    // every rank receives the quadrant counts from rank 0\n\n    // TODO: use MPI broadcast\n    // rank 0 broadcasts quadrant counts to all other ranks\n\n    // TODO: use MPI broadcast\n    // every rank receives quadrant counts from rank 0\n\n    // TODO: use MPI alltoall\n    // every rank sends quadrant counts to all other ranks\n\n    // TODO: use MPI alltoall\n    // rank 0 receives quadrant counts from every other rank\n\n    // TODO: use MPI reduce\n    // every rank sends its quadrant counts to rank 0\n\n    // TODO: use MPI reduce\n    // rank 0 receives quadrant counts from every other rank\n\n    // TODO: use MPI allreduce\n    // every rank sends its quadrant counts to rank 0\n\n    // TODO: use MPI allreduce\n    // rank 0 receives quadrant counts from every other rank\n\n    // TODO: use MPI allreduce\n    // rank 0 sends quadrant counts to every other rank\n\n    // TODO: use MPI allreduce\n    // every rank receives quadrant counts from every other rank\n\n    // TODO: use MPI reduce\n    // rank 0 sends quadrant counts to every other rank\n\n    // TODO: use MPI reduce\n    // every rank receives quadrant counts from rank 0\n\n    // TODO: use MPI allreduce\n    // every rank sends quadrant counts to rank 0\n\n    // TODO: use MPI allreduce\n    // rank 0 receives quadrant counts from every other rank\n\n    // TODO: use MPI allreduce\n    // every rank receives quadrant counts from rank 0\n\n    // TODO: use MPI allreduce\n    // rank 0 broadcasts quadrant counts to all other ranks\n\n    // TODO: use MPI allreduce\n    // every rank broadcasts quadrant counts to rank 0\n\n    MPI_Finalize();\n}",
            "int rank, num_ranks;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n\tconst int dims[2] = {1, 1}; // 2D space\n\tconst int periods[2] = {1, 1};\n\n\t// Create a new cartesian topology with 2 dimensions and 2 processes on each dimension\n\tMPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 0, &cart);\n\tMPI_Comm_rank(cart, &rank);\n\tMPI_Comm_size(cart, &num_ranks);\n\n\tMPI_Cart_get(cart, 2, dims, periods, coords); // get the coords\n\n\t// Get the number of points in each rank\n\tint num_points_rank = points.size() / num_ranks;\n\n\tint num_points = 0;\n\tfor (size_t i = 0; i < num_ranks; i++) {\n\t\tif (i!= rank) {\n\t\t\tMPI_Send(&num_points_rank, 1, MPI_INT, i, 0, cart);\n\t\t\tnum_points += num_points_rank;\n\t\t} else {\n\t\t\tnum_points += points.size() - (num_points_rank * rank);\n\t\t}\n\t}\n\n\t// Allocate memory to store the counts\n\tsize_t *counts = new size_t[num_points];\n\n\t// Count the number of points in each quadrant\n\tif (coords[0] == 0 && coords[1] == 0) {\n\t\tsize_t num_points_lower_left = 0;\n\t\tfor (size_t i = 0; i < points.size(); i++) {\n\t\t\tif (points[i].x < 0 && points[i].y < 0) {\n\t\t\t\tcounts[num_points_lower_left] = 1;\n\t\t\t\tnum_points_lower_left++;\n\t\t\t}\n\t\t}\n\t} else if (coords[0] == 0 && coords[1] == 1) {\n\t\tsize_t num_points_lower_right = 0;\n\t\tfor (size_t i = 0; i < points.size(); i++) {\n\t\t\tif (points[i].x < 0 && points[i].y > 0) {\n\t\t\t\tcounts[num_points_lower_right] = 1;\n\t\t\t\tnum_points_lower_right++;\n\t\t\t}\n\t\t}\n\t} else if (coords[0] == 1 && coords[1] == 0) {\n\t\tsize_t num_points_upper_left = 0;\n\t\tfor (size_t i = 0; i < points.size(); i++) {\n\t\t\tif (points[i].x > 0 && points[i].y < 0) {\n\t\t\t\tcounts[num_points_upper_left] = 1;\n\t\t\t\tnum_points_upper_left++;\n\t\t\t}\n\t\t}\n\t} else if (coords[0] == 1 && coords[1] == 1) {\n\t\tsize_t num_points_upper_right = 0;\n\t\tfor (size_t i = 0; i < points.size(); i++) {\n\t\t\tif (points[i].x > 0 && points[i].y > 0) {\n\t\t\t\tcounts[num_points_upper_right] = 1;\n\t\t\t\tnum_points_upper_right++;\n\t\t\t}\n\t\t}\n\t}\n\n\tMPI_Reduce(counts, bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < 4; i++) {\n\t\t\tstd::cout << bins[i] <<",
            "// Your code here\n\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t// compute number of points in each quadrant\n\tsize_t const npoints = points.size();\n\tsize_t const points_per_rank = npoints / size;\n\tsize_t const left_over_points = npoints % size;\n\n\t// allocate memory\n\tstd::vector<size_t> points_in_rank;\n\tpoints_in_rank.resize(points_per_rank);\n\n\t// count in each quadrant\n\tsize_t i = 0;\n\tfor (auto const& point : points) {\n\t\tint quadrant = 0;\n\t\tif (point.x > 0) quadrant += 1;\n\t\tif (point.y > 0) quadrant += 2;\n\n\t\tif (i < points_per_rank + left_over_points)\n\t\t\tpoints_in_rank[i]++;\n\n\t\ti++;\n\t}\n\n\t// collect results\n\tMPI_Allreduce(MPI_IN_PLACE, points_in_rank.data(), points_in_rank.size(), MPI_UNSIGNED, MPI_SUM, MPI_COMM_WORLD);\n\n\t// save results\n\tif (rank == 0) {\n\t\tfor (size_t i = 0; i < points_in_rank.size(); i++)\n\t\t\tbins[i] = points_in_rank[i];\n\t}\n}",
            "// TODO: Your code here\n}",
            "const int nx = 2;\n    const int ny = 2;\n\n    const MPI_Datatype point_type = getMPIType<Point>();\n    const MPI_Datatype int_type = MPI_INT;\n\n    // Get my rank and number of ranks\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Create cartesian communicator with 4D topology\n    MPI_Comm cart_comm;\n    MPI_Dims_create(size, 2, dimension);\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dimension, periodic, false, &cart_comm);\n\n    // Get my coordinates\n    int coords[2];\n    MPI_Cart_coords(cart_comm, rank, 2, coords);\n\n    // Get number of ranks in each dimension\n    int num_ranks[2];\n    MPI_Cart_get(cart_comm, 2, dimension, num_ranks, periodic);\n\n    // Get the rank of the neighboring ranks in each dimension\n    int neighbor_ranks[4];\n    MPI_Cart_shift(cart_comm, 0, 1, &neighbor_ranks[1], &neighbor_ranks[3]);\n    MPI_Cart_shift(cart_comm, 1, 1, &neighbor_ranks[0], &neighbor_ranks[2]);\n\n    // Store points in each rank's local container\n    std::vector<Point> local_points;\n\n    for (Point const& point : points) {\n        if (point.x >= 0.0 && point.y >= 0.0) {\n            local_points.push_back(point);\n        }\n    }\n\n    // Reduce points in each rank's local container to rank 0\n    if (rank == 0) {\n        int counts[4];\n        MPI_Reduce(&local_points[0], &counts[0], 4, int_type, MPI_SUM, 0, cart_comm);\n\n        for (int i = 0; i < 4; i++) {\n            bins[i] = counts[i];\n        }\n    } else {\n        MPI_Reduce(&local_points[0], nullptr, 4, int_type, MPI_SUM, 0, cart_comm);\n    }\n\n    MPI_Comm_free(&cart_comm);\n}",
            "}",
            "int rank, numProcs;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n   assert(numProcs == 4);\n   assert(points.size() % 4 == 0);\n   if (rank!= 0) {\n      bins = std::array<size_t, 4>();\n   } else {\n      bins = std::array<size_t, 4>({0, 0, 0, 0});\n   }\n\n   for (size_t i = rank; i < points.size(); i += numProcs) {\n      if (points[i].x >= 0 && points[i].x <= 1) {\n         if (points[i].y >= 0 && points[i].y <= 1) {\n            bins[0]++;\n         } else if (points[i].y > 1) {\n            bins[2]++;\n         } else if (points[i].y < 0) {\n            bins[3]++;\n         }\n      } else if (points[i].x < 0) {\n         if (points[i].y >= 0 && points[i].y <= 1) {\n            bins[1]++;\n         } else if (points[i].y > 1) {\n            bins[3]++;\n         } else if (points[i].y < 0) {\n            bins[2]++;\n         }\n      }\n   }\n\n   for (int i = 0; i < 4; ++i) {\n      std::array<size_t, 4> localBins;\n      MPI_Reduce(&bins[0], &localBins[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, i, MPI_COMM_WORLD);\n      bins = localBins;\n   }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Create cartesian communicator\n    int dims[] = {1, 1};\n    MPI_Dims_create(size, 2, dims);\n    int periods[] = {0, 0};\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 0, &comm);\n\n    // Get xy coordinates for this rank\n    int coords[2];\n    MPI_Cart_coords(comm, rank, 2, coords);\n\n    // Get the number of points in each quadrant\n    size_t numQuadrantPoints[4];\n    for(int i=0; i < 4; i++) {\n        numQuadrantPoints[i] = 0;\n    }\n    for(auto& p : points) {\n        int quadrantIndex = (p.x >= 0? 0 : 1) + (p.y >= 0? 0 : 2);\n        numQuadrantPoints[quadrantIndex]++;\n    }\n\n    // Reduce the number of quadrant points to rank 0\n    MPI_Reduce(numQuadrantPoints, bins.data(), 4, MPI_LONG_LONG_INT, MPI_SUM, 0, comm);\n\n    // Destroy the cartesian communicator\n    MPI_Comm_free(&comm);\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  //...\n\n  /*\n     The following code does the job if the number of processes\n     is a multiple of 2. In order to generalize the code, we should\n     use the ceil function instead of the floor function.\n  */\n\n  //...\n\n  // The array \"bins\" contains the number of points in each quadrant\n  if (rank == 0) {\n    for (int i = 0; i < bins.size(); i++) {\n      std::cout << \"quadrant \" << i << \":\" << bins[i] << std::endl;\n    }\n  }\n}",
            "int nproc;\n    int rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<Point> points_copy;\n    if (rank == 0) {\n        points_copy = points;\n    }\n    MPI_Bcast(&points_copy, points_copy.size(), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    int const npoints = points_copy.size();\n    int const ndiv = npoints / nproc;\n    int const rest = npoints % nproc;\n    int const start = rank * ndiv + std::min(rank, rest);\n    int const stop = start + ndiv + (rank < rest);\n\n    for (int i = start; i < stop; i++) {\n        double x = points_copy[i].x;\n        double y = points_copy[i].y;\n        if (x > 0 && y > 0) {\n            bins[0]++;\n        }\n        else if (x < 0 && y > 0) {\n            bins[1]++;\n        }\n        else if (x < 0 && y < 0) {\n            bins[2]++;\n        }\n        else if (x > 0 && y < 0) {\n            bins[3]++;\n        }\n    }\n    MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "constexpr size_t DIM = 2;\n  int nprocs, myrank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n  // determine the cartesian decomposition\n  int dims[DIM] = {0,0};\n  dims[0] = 2; // this will be used as x-dimension\n  dims[1] = 2; // this will be used as y-dimension\n  int reorder = 1;\n  int period[DIM] = {1, 1}; // this will be used to specify if the dimension is periodic\n  MPI_Cart_create(MPI_COMM_WORLD, DIM, dims, period, reorder, &comm);\n\n  // find out how many points are in each quadrant\n  std::vector<Point> localPoints;\n  for (const auto &point : points) {\n    int coords[DIM] = {0, 0}; // this will be used to store the rank of the process for the point\n    MPI_Cart_coords(comm, myrank, DIM, coords);\n\n    // if the point belongs to this quadrant, store the point in the local points\n    if ((point.x >= -0.5 && point.x < 0.5) && (point.y >= -0.5 && point.y < 0.5)) {\n      localPoints.push_back(point);\n    }\n  }\n\n  // count the number of points in each quadrant\n  int counts[DIM] = {0, 0};\n  MPI_Allreduce(MPI_IN_PLACE, &counts, 2, MPI_INT, MPI_SUM, comm);\n  // assign the counts to bins\n  bins[0] = counts[0];\n  bins[1] = counts[1];\n  bins[2] = counts[2];\n  bins[3] = counts[3];\n}",
            "//TODO\n}",
            "//... your code here...\n}",
            "int numprocs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Calculate the number of points in each quadrant.\n    int xmax = 2;\n    int ymax = 2;\n    int xmin = -2;\n    int ymin = -2;\n    for (auto const& p : points) {\n        int x = static_cast<int>(p.x);\n        int y = static_cast<int>(p.y);\n        if (x >= xmax) {\n            xmax = x;\n        }\n        if (x < xmin) {\n            xmin = x;\n        }\n        if (y >= ymax) {\n            ymax = y;\n        }\n        if (y < ymin) {\n            ymin = y;\n        }\n    }\n\n    // Assign the points to quadrants.\n    std::vector<Point> mypoints;\n    int width = xmax - xmin + 1;\n    int height = ymax - ymin + 1;\n    int delta = 1;\n    for (auto const& p : points) {\n        int x = static_cast<int>(p.x);\n        int y = static_cast<int>(p.y);\n        if (x >= xmin && x < xmax && y >= ymin && y < ymax) {\n            mypoints.push_back(p);\n        }\n    }\n    int nx = width / numprocs;\n    int ny = height / numprocs;\n    if (width % numprocs!= 0) {\n        nx += 1;\n    }\n    if (height % numprocs!= 0) {\n        ny += 1;\n    }\n    int nxmin = (nx * rank) + delta;\n    int nymin = (ny * rank) + delta;\n    int nxmax = nxmin + nx - 1;\n    int nymax = nymin + ny - 1;\n    if (nx % numprocs!= 0) {\n        nxmax += 1;\n    }\n    if (ny % numprocs!= 0) {\n        nymax += 1;\n    }\n    MPI_Allreduce(&nxmin, &bins[0], 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);\n    MPI_Allreduce(&nymin, &bins[1], 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);\n    MPI_Allreduce(&nxmax, &bins[2], 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    MPI_Allreduce(&nymax, &bins[3], 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    for (auto& p : mypoints) {\n        if (p.x >= bins[0] && p.x <= bins[2] && p.y >= bins[1] && p.y <= bins[3]) {\n            ++bins[0];\n        }\n    }\n}",
            "// TODO: Implement this function!\n\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int nprocs, rank;\n    MPI_Comm_size(comm, &nprocs);\n    MPI_Comm_rank(comm, &rank);\n\n    // If rank is 0, receive all the points from the other ranks.\n    // If rank is not 0, only send your points to rank 0\n    if (rank == 0) {\n        for (int i = 1; i < nprocs; i++) {\n            MPI_Recv(points.data(), 1, MPI_DOUBLE, i, 0, comm, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(points.data(), 1, MPI_DOUBLE, 0, 0, comm);\n    }\n\n    int npoints = points.size();\n    int nquadrants = bins.size();\n    double min_x = -10, max_x = 10, min_y = -10, max_y = 10;\n\n    for (int i = 0; i < nquadrants; i++) {\n        bins[i] = 0;\n    }\n\n    for (int i = 0; i < npoints; i++) {\n        if (points[i].x >= min_x && points[i].x <= max_x && points[i].y >= min_y && points[i].y <= max_y) {\n            if (points[i].x >= 0 && points[i].x <= max_x && points[i].y >= 0 && points[i].y <= max_y) {\n                bins[0]++;\n            } else if (points[i].x < 0 && points[i].x >= min_x && points[i].y >= 0 && points[i].y <= max_y) {\n                bins[1]++;\n            } else if (points[i].x < 0 && points[i].x >= min_x && points[i].y < 0 && points[i].y >= min_y) {\n                bins[2]++;\n            } else if (points[i].x >= 0 && points[i].x <= max_x && points[i].y < 0 && points[i].y >= min_y) {\n                bins[3]++;\n            }\n        }\n    }\n}",
            "}",
            "}",
            "// TODO: Your code here\n   return;\n}",
            "// TODO: Your code here\n\n}",
            "/* TODO: Your code goes here */\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int rank,size;\n    MPI_Comm_size(comm,&size);\n    MPI_Comm_rank(comm,&rank);\n    std::array<size_t,4> counts;\n\n    int count_points = points.size();\n    for(int i=0;i<4;i++)\n    {\n        counts[i]=0;\n    }\n    if(rank==0)\n    {\n        for(int i=0;i<count_points;i++)\n        {\n            double x=points[i].x;\n            double y=points[i].y;\n            if(x>=0 && y>=0)\n            {\n                counts[0]++;\n            }\n            if(x>=0 && y<0)\n            {\n                counts[1]++;\n            }\n            if(x<0 && y>=0)\n            {\n                counts[2]++;\n            }\n            if(x<0 && y<0)\n            {\n                counts[3]++;\n            }\n        }\n    }\n    int sum;\n    MPI_Reduce(&counts,&bins,4,MPI_INT,MPI_SUM,0,comm);\n    if(rank==0)\n    {\n        for(int i=0;i<4;i++)\n        {\n            sum=0;\n            for(int j=0;j<size;j++)\n            {\n                sum+=bins[i];\n            }\n            bins[i]=sum;\n        }\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // calculate the total number of points\n    size_t total = 0;\n    for (const auto& point : points)\n        total++;\n\n    // calculate the number of points to process on this rank\n    size_t local_points = total / size;\n\n    // calculate the offset\n    size_t offset = 0;\n    if (rank > 0)\n        offset = rank * local_points;\n\n    // calculate the number of points to process on this rank\n    local_points = local_points == 0? 1 : local_points;\n\n    // calculate the number of points to process for the next rank\n    size_t next = total - offset - local_points;\n\n    // calculate the number of points in each quadrant\n    size_t q0 = 0, q1 = 0, q2 = 0, q3 = 0;\n    for (size_t i = 0; i < local_points; i++) {\n        auto x = points[offset + i].x;\n        auto y = points[offset + i].y;\n        if (x > 0 && y > 0)\n            q0++;\n        if (x > 0 && y < 0)\n            q1++;\n        if (x < 0 && y < 0)\n            q2++;\n        if (x < 0 && y > 0)\n            q3++;\n    }\n\n    // sum up the results\n    std::array<size_t, 4> results;\n    results[0] = q0;\n    results[1] = q1;\n    results[2] = q2;\n    results[3] = q3;\n    MPI_Reduce(&results, &bins, 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (auto i = 0; i < 4; i++)\n            std::cout << bins[i] << \" \";\n    }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n\tint size, rank;\n\tMPI_Comm_size(comm, &size);\n\tMPI_Comm_rank(comm, &rank);\n\t\n\t// Each process keeps a subset of the points\n\tstd::vector<Point> local;\n\tif (rank == 0) {\n\t\tlocal.reserve(points.size());\n\t}\n\t\n\t// Count the number of points in each quadrant\n\tstd::array<size_t, 4> localBins;\n\tlocalBins.fill(0);\n\tfor (Point const& p : points) {\n\t\tif (p.x >= 0 && p.y >= 0) {\n\t\t\t++localBins[0];\n\t\t} else if (p.x < 0 && p.y >= 0) {\n\t\t\t++localBins[1];\n\t\t} else if (p.x < 0 && p.y < 0) {\n\t\t\t++localBins[2];\n\t\t} else if (p.x >= 0 && p.y < 0) {\n\t\t\t++localBins[3];\n\t\t}\n\t}\n\n\t// Sum the counts\n\tstd::array<size_t, 4> sum;\n\tif (rank == 0) {\n\t\tsum.fill(0);\n\t\tfor (int i = 0; i < size; i++) {\n\t\t\tMPI_Recv(&sum, 4, MPI_UNSIGNED_LONG_LONG, i, 0, comm, MPI_STATUS_IGNORE);\n\t\t}\n\t\tfor (int i = 0; i < 4; i++) {\n\t\t\tbins[i] = sum[i] + localBins[i];\n\t\t}\n\t} else {\n\t\tMPI_Send(&localBins, 4, MPI_UNSIGNED_LONG_LONG, 0, 0, comm);\n\t}\n}",
            "int rank, numtasks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n\n    int num_of_dims = 2;\n\n    int dims[2] = {2, 2};\n    int periodic[2] = {1, 1};\n\n    int coord[2];\n    MPI_Cart_coords(MPI_COMM_WORLD, rank, num_of_dims, coord);\n\n    int dim_size;\n\n    MPI_Cart_get(MPI_COMM_WORLD, num_of_dims, dims, periodic, &dim_size);\n\n    int neig[4];\n\n    MPI_Cart_shift(MPI_COMM_WORLD, 0, 1, &neig[0], &neig[1]);\n    MPI_Cart_shift(MPI_COMM_WORLD, 1, 1, &neig[2], &neig[3]);\n\n    int index = 0;\n\n    if(coord[0] == 0)\n    {\n        if(coord[1] == 0)\n        {\n            index = 0;\n        }\n        else if(coord[1] == 1)\n        {\n            index = 1;\n        }\n    }\n    else if(coord[0] == 1)\n    {\n        if(coord[1] == 0)\n        {\n            index = 2;\n        }\n        else if(coord[1] == 1)\n        {\n            index = 3;\n        }\n    }\n\n    size_t count = 0;\n\n    for(auto& p : points)\n    {\n        if(p.x >= 0 && p.x < dim_size / 2)\n        {\n            if(p.y >= 0 && p.y < dim_size / 2)\n            {\n                if(index == 0)\n                {\n                    ++count;\n                }\n            }\n            else if(p.y >= dim_size / 2 && p.y < dim_size)\n            {\n                if(index == 3)\n                {\n                    ++count;\n                }\n            }\n        }\n        else if(p.x >= dim_size / 2 && p.x < dim_size)\n        {\n            if(p.y >= 0 && p.y < dim_size / 2)\n            {\n                if(index == 2)\n                {\n                    ++count;\n                }\n            }\n            else if(p.y >= dim_size / 2 && p.y < dim_size)\n            {\n                if(index == 1)\n                {\n                    ++count;\n                }\n            }\n        }\n    }\n\n    MPI_Allreduce(&count, &bins[index], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "int size;\n    int rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<Point> myPoints = points;\n\n    int chunkSize = myPoints.size() / size;\n\n    int start = chunkSize * rank;\n    int end = start + chunkSize;\n    if (rank == size - 1) {\n        end = myPoints.size();\n    }\n\n    std::array<size_t, 4> localCounts = {0, 0, 0, 0};\n    for (size_t i = start; i < end; i++) {\n        if (myPoints[i].x >= 0 && myPoints[i].y >= 0) {\n            localCounts[0]++;\n        } else if (myPoints[i].x < 0 && myPoints[i].y >= 0) {\n            localCounts[1]++;\n        } else if (myPoints[i].x < 0 && myPoints[i].y < 0) {\n            localCounts[2]++;\n        } else {\n            localCounts[3]++;\n        }\n    }\n\n    std::array<size_t, 4> globalCounts = {0, 0, 0, 0};\n    MPI_Reduce(&localCounts, &globalCounts, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = globalCounts;\n    }\n\n}",
            "int rank;\n  int size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int d = size;\n  int e = 2;\n  int t = size/e;\n\n  std::vector<double> x;\n  std::vector<double> y;\n\n  for (int i = 0; i < points.size(); i++) {\n    x.push_back(points[i].x);\n    y.push_back(points[i].y);\n  }\n\n  std::vector<double> x1(x.begin(), x.begin() + (x.size()/d));\n  std::vector<double> x2(x.begin() + (x.size()/d), x.begin() + (2*x.size()/d));\n  std::vector<double> x3(x.begin() + (2*x.size()/d), x.end());\n\n  std::vector<double> y1(y.begin(), y.begin() + (y.size()/d));\n  std::vector<double> y2(y.begin() + (y.size()/d), y.begin() + (2*y.size()/d));\n  std::vector<double> y3(y.begin() + (2*y.size()/d), y.end());\n\n  std::vector<size_t> bins1(bins.begin(), bins.begin() + (bins.size()/d));\n  std::vector<size_t> bins2(bins.begin() + (bins.size()/d), bins.begin() + (2*bins.size()/d));\n  std::vector<size_t> bins3(bins.begin() + (2*bins.size()/d), bins.end());\n\n  MPI_Request request;\n  std::vector<int> tag = {0, 1, 2, 3};\n\n  MPI_Irecv(bins1.data(), bins1.size(), MPI_INT, 0, tag[0], MPI_COMM_WORLD, &request);\n  MPI_Irecv(bins2.data(), bins2.size(), MPI_INT, 1, tag[1], MPI_COMM_WORLD, &request);\n  MPI_Irecv(bins3.data(), bins3.size(), MPI_INT, 2, tag[2], MPI_COMM_WORLD, &request);\n  MPI_Irecv(bins3.data(), bins3.size(), MPI_INT, 3, tag[3], MPI_COMM_WORLD, &request);\n\n  for (int i = 0; i < t; i++) {\n    std::vector<Point> points1(points.begin() + i, points.begin() + (i+t));\n\n    for (int j = 0; j < points1.size(); j++) {\n      if (points1[j].x >= 0 && points1[j].x < 1 && points1[j].y >= 0 && points1[j].y < 1) {\n        bins1[0]++;\n      } else if (points1[j].x >= 1 && points1[j].x < 2 && points1[j].y >= 1 && points1[j].y < 2) {\n        bins1[1]++;\n      } else if (points1[j].x >= 2 && points1[j].x < 3 && points1[j].y >= 2 && points1[j].y < 3) {\n        bins1[2]++;\n      } else if (points1[j].x >= 3 && points1[j].x < 4 && points1[j].y >= 3 && points1[j].y < 4) {\n        bins1[3]++;\n      }\n    }\n  }\n\n  MPI_Isend(bins1.data(), bins1.size(), MPI_INT, 0, tag[0], MPI_COMM_WORLD, &request);\n  MPI_Isend",
            "int rank;\n    int numRanks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n    MPI_Status status;\n\n    // set up data to send to other ranks\n    int dataCount = points.size() / numRanks;\n    int extras = points.size() % numRanks;\n    std::vector<Point> pointsCopy;\n    for (int i = 0; i < dataCount + extras; i++) {\n        pointsCopy.push_back(points[i]);\n    }\n    // split up data\n    std::vector<Point> data;\n    for (int i = 0; i < numRanks; i++) {\n        std::vector<Point> pointsSplit;\n        for (int j = 0; j < dataCount; j++) {\n            pointsSplit.push_back(pointsCopy[i * dataCount + j]);\n        }\n        data.push_back(pointsSplit);\n    }\n    // send data to other ranks\n    for (int i = 0; i < numRanks; i++) {\n        MPI_Send(&data[i][0], data.at(i).size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n    }\n\n    // get data from other ranks\n    for (int i = 0; i < numRanks; i++) {\n        if (i!= rank) {\n            MPI_Recv(&data[i][0], data.at(i).size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &status);\n        }\n    }\n\n    // count points\n    for (int i = 0; i < numRanks; i++) {\n        for (int j = 0; j < data.at(i).size(); j++) {\n            if (data.at(i).at(j).x > 0 && data.at(i).at(j).y > 0) {\n                bins[0]++;\n            } else if (data.at(i).at(j).x < 0 && data.at(i).at(j).y > 0) {\n                bins[1]++;\n            } else if (data.at(i).at(j).x < 0 && data.at(i).at(j).y < 0) {\n                bins[2]++;\n            } else if (data.at(i).at(j).x > 0 && data.at(i).at(j).y < 0) {\n                bins[3]++;\n            }\n        }\n    }\n    if (rank == 0) {\n        for (int i = 0; i < bins.size(); i++) {\n            std::cout << \"count of \" << i << \" is: \" << bins[i] << std::endl;\n        }\n    }\n}",
            "int world_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  MPI_Comm cart_comm;\n  int dims[2];\n  dims[0] = dims[1] = 0;\n  MPI_Dims_create(world_size, 2, dims);\n  MPI_Cart_create(MPI_COMM_WORLD, 2, dims, false, false, &cart_comm);\n  int rank;\n  MPI_Comm_rank(cart_comm, &rank);\n  int num_procs;\n  MPI_Comm_size(cart_comm, &num_procs);\n\n  int coord[2];\n  MPI_Cart_coords(cart_comm, rank, 2, coord);\n  int x_coord = coord[0];\n  int y_coord = coord[1];\n\n  auto const & quadrant_points = [&x_coord, &y_coord, &points]() {\n    std::vector<Point> res;\n    for (auto const & p : points) {\n      if (x_coord * p.x > 0 && y_coord * p.y > 0) {\n        res.push_back(p);\n      }\n    }\n    return res;\n  }();\n\n  // rank 0's responsibility\n  if (world_rank == 0) {\n    int counts[4];\n    MPI_Reduce(MPI_IN_PLACE, counts, 4, MPI_INT, MPI_SUM, 0, cart_comm);\n\n    for (int i = 0; i < 4; ++i) {\n      bins[i] = counts[i];\n    }\n  }\n  else {\n    int counts[4] = {0};\n    MPI_Reduce(quadrant_points.data(), counts, 4, MPI_INT, MPI_SUM, 0, cart_comm);\n  }\n}",
            "bins.fill(0);\n   for (auto &p : points) {\n      size_t bin;\n      if (p.x >= 0)\n         bin = 0;\n      else if (p.x < 0 && p.y >= 0)\n         bin = 1;\n      else if (p.x < 0 && p.y < 0)\n         bin = 2;\n      else\n         bin = 3;\n      ++bins[bin];\n   }\n}",
            "// Step 1:\n\t// Find how many points are in each quadrant.\n\t// Use the helper function `quadrant` to find the quadrant of each point.\n\t// 1st quadrant: (-inf, inf), (-inf, inf)\n\t// 2nd quadrant: (inf, inf), (-inf, inf)\n\t// 3rd quadrant: (inf, inf), (inf, inf)\n\t// 4th quadrant: (-inf, inf), (inf, inf)\n\t// You can use the helper function `quadrant` for this.\n\t//\n\t// Step 2:\n\t// Create a histogram with MPI_Allreduce. \n\t// See: https://mpi-forum.org/docs/mpi-3.1/mpi31-report/node224.htm#Node224\n\t// You can find the MPI documentation online.\n\t//\n\t// Step 3:\n\t// Store the result in `bins` on rank 0.\n\t// You can use MPI_Gather to do this.\n\n\tMPI_Comm comm = MPI_COMM_WORLD;\n\tint size, rank;\n\tMPI_Comm_size(comm, &size);\n\tMPI_Comm_rank(comm, &rank);\n\n\t// get quadrants\n\tstd::vector<Point> localQuadrants;\n\tfor(auto const& p : points) {\n\t\tauto q = quadrant(p.x, p.y);\n\t\tif(q >= 0 && q <= 3) {\n\t\t\tlocalQuadrants.push_back(p);\n\t\t}\n\t}\n\tstd::array<size_t, 4> localBins;\n\tif(rank == 0) {\n\t\tlocalBins = { localQuadrants.size()/4, localQuadrants.size()/4, localQuadrants.size()/4, localQuadrants.size()/4 };\n\t}\n\telse {\n\t\tlocalBins = { 0, 0, 0, 0 };\n\t}\n\n\t// calculate histogram\n\tint dims[2] = { 2, 2 };\n\tMPI_Dims_create(size, 2, dims);\n\tint periods[2] = { 0, 0 };\n\tMPI_Cart_create(comm, 2, dims, periods, true, &comm);\n\tMPI_Status status;\n\tMPI_Datatype point_type;\n\tMPI_Type_contiguous(sizeof(Point), MPI_BYTE, &point_type);\n\tMPI_Type_commit(&point_type);\n\tMPI_Allreduce(localQuadrants.data(), localBins.data(), localBins.size(), point_type, MPI_SUM, comm);\n\tMPI_Type_free(&point_type);\n\n\t// store result\n\tif(rank == 0) {\n\t\tfor(int i = 0; i < 4; i++) {\n\t\t\tbins[i] = localBins[i];\n\t\t}\n\t}\n}",
            "int nproc, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // create 2d cartesian communicator\n  int dims[2] = { 2, 2 };\n  int periods[2] = { 0, 0 };\n  MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 1, &cart_comm);\n  MPI_Cart_coords(cart_comm, rank, 2, coords);\n\n  // distribute work\n  int npoints = points.size();\n  int chunk = npoints / nproc;\n  int extra = npoints % nproc;\n\n  int start = chunk * rank + std::min(extra, rank);\n  int end = start + chunk + (rank < extra);\n\n  // compute local bins\n  std::array<size_t, 4> local_bins = {};\n  for (int i = start; i < end; ++i) {\n    Point p = points[i];\n\n    if (p.x > 0.0) {\n      if (p.y > 0.0) {\n        ++local_bins[0];\n      } else {\n        ++local_bins[3];\n      }\n    } else {\n      if (p.y > 0.0) {\n        ++local_bins[1];\n      } else {\n        ++local_bins[2];\n      }\n    }\n  }\n\n  // collect the local bins\n  std::array<size_t, 4> full_bins = {};\n  MPI_Allreduce(local_bins.data(), full_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, cart_comm);\n\n  // set the results\n  bins = full_bins;\n\n  // clean up\n  MPI_Comm_free(&cart_comm);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int dimension = 2;\n    int coords[dimension];\n    MPI_Dims_create(size, dimension, coords);\n    int rows = coords[0];\n    int cols = coords[1];\n    int row_rank = rank / cols;\n    int col_rank = rank % cols;\n    int col_sum = cols * rows;\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    for (auto point: points) {\n        if (point.x < 0 && point.y < 0)\n            local_bins[0] += 1;\n        else if (point.x < 0 && point.y >= 0)\n            local_bins[1] += 1;\n        else if (point.x >= 0 && point.y < 0)\n            local_bins[2] += 1;\n        else\n            local_bins[3] += 1;\n    }\n    std::array<size_t, 4> global_bins = {0, 0, 0, 0};\n    MPI_Allreduce(local_bins.data(), global_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n    for (int i = 0; i < 4; i++)\n        bins[i] = global_bins[i];\n}",
            "// TODO: Your code here\n}",
            "int ntasks;\n  int rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &ntasks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Figure out how many points are in each quadrant\n  int bins_per_task = points.size()/ntasks;\n  std::array<size_t, 4> local_counts;\n  local_counts.fill(0);\n\n  // Each process has a chunk of data\n  int start = rank*bins_per_task;\n  int end = start + bins_per_task;\n  //std::cout << \"rank \" << rank << \" has [\" << start << \", \" << end << \"]\" << std::endl;\n\n  // Count in the local chunk\n  for (int i = start; i < end; i++) {\n    if (points[i].x >= 0 && points[i].y >= 0) {\n      local_counts[0] += 1;\n    }\n    if (points[i].x >= 0 && points[i].y < 0) {\n      local_counts[1] += 1;\n    }\n    if (points[i].x < 0 && points[i].y >= 0) {\n      local_counts[2] += 1;\n    }\n    if (points[i].x < 0 && points[i].y < 0) {\n      local_counts[3] += 1;\n    }\n  }\n\n  // Figure out how many quadrants each rank will count\n  int quadrants_per_rank = local_counts.size()/ntasks;\n  //std::cout << \"rank \" << rank << \" has \" << quadrants_per_rank << \" quadrants\" << std::endl;\n\n  // Store counts in bins\n  for (int i = 0; i < quadrants_per_rank; i++) {\n    bins[i] = 0;\n  }\n\n  // Sum counts for all ranks\n  MPI_Reduce(&local_counts[0], &bins[0], quadrants_per_rank, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n  //std::cout << \"bins = \" << bins << std::endl;\n\n  if (rank == 0) {\n    std::cout << \"bins = \" << bins << std::endl;\n  }\n}",
            "}",
            "// TODO: Your code here\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  if (rank == 0)\n    MPI_Status status;\n\n  std::vector<Point> points_local;\n  for (size_t i = 0; i < points.size(); ++i) {\n    if (points[i].x >= 0) {\n      if (points[i].y >= 0) {\n        points_local.push_back(points[i]);\n      } else {\n        points_local.push_back(points[i]);\n      }\n    } else {\n      points_local.push_back(points[i]);\n    }\n  }\n\n  MPI_Bcast(&points_local.size(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank!= 0) {\n    bins.fill(0);\n    MPI_Scatter(points_local.data(), points_local.size(), MPI_POINT, bins.data(), bins.size(), MPI_POINT, 0, MPI_COMM_WORLD);\n  }\n\n  if (rank == 0) {\n    bins.fill(0);\n    std::array<size_t, 4> bin_local;\n    for (size_t i = 0; i < points.size(); ++i) {\n      if (points[i].x >= 0) {\n        if (points[i].y >= 0) {\n          ++bin_local[0];\n        } else {\n          ++bin_local[1];\n        }\n      } else {\n        ++bin_local[2];\n      }\n    }\n    MPI_Scatter(bin_local.data(), bin_local.size(), MPI_POINT, bins.data(), bins.size(), MPI_POINT, 0, MPI_COMM_WORLD);\n    for (size_t i = 0; i < 4; ++i) {\n      bins[i] += bin_local[i];\n    }\n    std::array<size_t, 4> bins_global;\n    MPI_Reduce(bins.data(), bins_global.data(), bins.size(), MPI_POINT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n      std::cout << \"[\" << bins_global[0] << \", \" << bins_global[1] << \", \" << bins_global[2] << \", \" << bins_global[3] << \"]\" << std::endl;\n    }\n  }\n}",
            "// Your code here\n    // Don't forget to return the result in bins.\n\n}",
            "}",
            "}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int nx = size;\n    int ny = 1;\n    int nz = 1;\n\n    MPI_Dims_create(size, 3, &nx);\n\n    MPI_Cart_create(MPI_COMM_WORLD, 3, &nx, &ny, &nz, true, MPI_COMM_WORLD);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int ndims;\n    MPI_Cartdim_get(MPI_COMM_WORLD, &ndims);\n\n    int coords[3] = {0, 0, 0};\n\n    MPI_Cart_coords(MPI_COMM_WORLD, rank, ndims, coords);\n\n    int neighbors[3][2] = {{-1, 0}, {1, 0}, {0, -1}};\n\n    for (int i = 0; i < ndims; i++)\n    {\n        int next = neighbors[coords[i]][i];\n\n        if (next == -1)\n        {\n            next = nx - 1;\n        }\n\n        if (next == nx)\n        {\n            next = 0;\n        }\n\n        MPI_Cart_shift(MPI_COMM_WORLD, i, 1, &coords[i], &next);\n    }\n\n    int nbins = bins.size();\n\n    for (auto const& point : points)\n    {\n        int my_bin = 0;\n\n        if (point.x > 0)\n        {\n            if (point.y > 0)\n            {\n                my_bin = 0;\n            }\n            else\n            {\n                my_bin = 3;\n            }\n        }\n        else\n        {\n            if (point.y > 0)\n            {\n                my_bin = 1;\n            }\n            else\n            {\n                my_bin = 2;\n            }\n        }\n\n        MPI_Send(&my_bin, 1, MPI_INT, next, 0, MPI_COMM_WORLD);\n        MPI_Recv(&bins[my_bin], 1, MPI_INT, next, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        bins[my_bin]++;\n    }\n\n    MPI_Finalize();\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int n_per_rank = points.size() / size;\n   int offset = n_per_rank * rank;\n   int left = (rank + size - 1) % size;\n   int right = (rank + 1) % size;\n\n   std::array<size_t, 4> counts = {0, 0, 0, 0};\n\n   for (size_t i = 0; i < n_per_rank; i++) {\n      Point p = points[offset + i];\n      if (p.x >= 0 && p.y >= 0)\n         counts[0]++;\n      else if (p.x >= 0 && p.y < 0)\n         counts[1]++;\n      else if (p.x < 0 && p.y < 0)\n         counts[2]++;\n      else if (p.x < 0 && p.y >= 0)\n         counts[3]++;\n   }\n\n   std::array<size_t, 4> left_counts;\n   std::array<size_t, 4> right_counts;\n\n   MPI_Allgather(counts.data(), 4, MPI_UNSIGNED_LONG_LONG, left_counts.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_COMM_WORLD);\n   MPI_Allgather(counts.data(), 4, MPI_UNSIGNED_LONG_LONG, right_counts.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_COMM_WORLD);\n\n   bins = {\n      counts[0] + left_counts[1] + left_counts[2] + left_counts[3],\n      counts[1] + left_counts[0] + left_counts[2] + left_counts[3],\n      counts[2] + left_counts[0] + left_counts[1] + left_counts[3],\n      counts[3] + left_counts[0] + left_counts[1] + left_counts[2],\n   };\n}",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // Calculate the number of points in the quadrants that this process will be assigned\n   int pointsPerRank = (int)points.size() / size;\n\n   // Keep track of the number of points in each quadrant\n   std::array<size_t, 4> localBins = {0, 0, 0, 0};\n\n   // Get the first point that this rank is responsible for\n   auto firstPoint = points.begin();\n   if (rank > 0)\n      firstPoint += rank * pointsPerRank;\n\n   // Get the last point that this rank is responsible for\n   auto lastPoint = points.end();\n   if (rank < size - 1)\n      lastPoint = firstPoint + pointsPerRank;\n\n   // Increment the localBins array for each point in the current quadrant\n   for (auto point = firstPoint; point!= lastPoint; ++point) {\n      if (point->x >= 0 && point->y >= 0)\n         ++localBins[0];\n      else if (point->x < 0 && point->y >= 0)\n         ++localBins[1];\n      else if (point->x < 0 && point->y < 0)\n         ++localBins[2];\n      else\n         ++localBins[3];\n   }\n\n   // MPI_Allreduce performs a reduction operation using the given function. It is called like this:\n   // MPI_Allreduce(sendbuf, recvbuf, count, datatype, op, comm)\n   //\n   // sendbuf:  This parameter can be MPI_IN_PLACE. For a non-MPI_IN_PLACE parameter, the data pointed to by sendbuf is the source of the operation.\n   // recvbuf:  This parameter is the target location of the operation.\n   // count:    This parameter specifies the number of elements in sendbuf and recvbuf.\n   // datatype: This parameter specifies the data type of each element in sendbuf and recvbuf.\n   // op:       This parameter specifies the operation to be performed.\n   // comm:     This parameter is the communicator that is being used.\n   MPI_Allreduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED, MPI_SUM, MPI_COMM_WORLD);\n}",
            "// Fill in the code below:\n    size_t nbins = 4;\n    bins.fill(0);\n\n    double dx = 10;\n    double dy = 10;\n\n    size_t count = points.size();\n\n    int nproc;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    size_t mycount = count / nproc;\n\n    int mystart = rank * mycount;\n\n    std::vector<Point> mypoints(points.begin() + mystart, points.begin() + mystart + mycount);\n\n    for (Point &p: mypoints) {\n        int q = 0;\n        if (p.x > 0 && p.y > 0) {\n            q = 0;\n        }\n        if (p.x < 0 && p.y > 0) {\n            q = 1;\n        }\n        if (p.x < 0 && p.y < 0) {\n            q = 2;\n        }\n        if (p.x > 0 && p.y < 0) {\n            q = 3;\n        }\n        bins[q]++;\n    }\n\n    MPI_Reduce(&bins[0], &bins[0], nbins, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "const int nproc = size;\n    const int my_rank = rank;\n\n    const int dim = 2;\n\n    //TODO: Initialize bins\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    //TODO: Get cartesian topology with 2 dimensions\n    int dims[2] = {0, 0};\n    MPI_Dims_create(nproc, dim, dims);\n    int reorder = 1;\n    MPI_Cart_create(MPI_COMM_WORLD, dim, dims, reorder, MPI_CART_REUSE, &cart_comm);\n    int coords[2];\n    MPI_Cart_coords(cart_comm, my_rank, dim, coords);\n\n    //TODO: Get neighbors of current rank\n    int cart_rank[2];\n    MPI_Cart_shift(cart_comm, 0, 1, &cart_rank[0], &cart_rank[1]);\n    int n_left = cart_rank[1];\n    int n_right = cart_rank[0];\n\n    //TODO: Count the number of points in each quadrant\n    for (int i = 0; i < points.size(); i++) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            bins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            bins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            bins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            bins[3]++;\n        }\n    }\n\n    //TODO: Exchange counts with left/right neighbors\n    MPI_Sendrecv(bins.data(), 4, MPI_UNSIGNED, n_left, 1,\n                 bins.data(), 4, MPI_UNSIGNED, n_right, 1,\n                 cart_comm, MPI_STATUS_IGNORE);\n\n    //TODO: Add left and right counts to own counts\n\n    //TODO: Reduce and print histogram\n\n\n    /*\n     * For rank 0:\n     * 1. Allocate memory for the histogram\n     * 2. Copy the counts from `bins` to `hist`\n     * 3. Reduce the histogram across all ranks\n     * 4. Print the histogram\n     *\n     * For all other ranks:\n     * 1. Reduce the counts across all ranks\n     * 2. Print the histogram\n     *\n     */\n\n    if (my_rank == 0) {\n        std::array<size_t, 4> hist{0, 0, 0, 0};\n        for (int i = 0; i < bins.size(); i++) {\n            hist[i] += bins[i];\n        }\n        std::cout << \"Total points = \" << points.size() << std::endl;\n        MPI_Reduce(MPI_IN_PLACE, hist.data(), 4, MPI_UNSIGNED, MPI_SUM, 0, cart_comm);\n        if (my_rank == 0) {\n            for (int i = 0; i < hist.size(); i++) {\n                std::cout << \"Quadrant \" << i << \": \" << hist[i] << std::endl;\n            }\n        }\n    } else {\n        MPI_Reduce(bins.data(), bins.data(), 4, MPI_UNSIGNED, MPI_SUM, 0, cart_comm);\n    }\n}",
            "// TODO: implement this function\n   // you have to use MPI_Reduce\n   // you may use MPI_Reduce to count points in each quadrant\n   // bins are the output parameters and should be initialized to 0 before\n   // the call to MPI_Reduce\n\n   // bins[0] : quadrant 1\n   // bins[1] : quadrant 2\n   // bins[2] : quadrant 3\n   // bins[3] : quadrant 4\n\n\n   // your code here...\n\n   // return the bins\n}",
            "MPI_Comm comm;\n   MPI_Comm_dup(MPI_COMM_WORLD, &comm);\n   MPI_Comm_rank(comm, &rank);\n   MPI_Comm_size(comm, &size);\n   // if we have an even amount of processes, we'll be dealing with an uneven amount of points per process\n   // if we have an odd amount of processes, we'll be dealing with an even amount of points per process\n   // we'll deal with uneven amounts of points by adding one to the amount of processes we need\n   auto processes = size + (size % 2);\n   // we will deal with uneven amounts of points by dividing points into two groups\n   // if we have a remainder when dividing, we'll add one to the first group\n   // if we don't have a remainder, we'll add one to the second group\n   // we'll add one to the first group, because the first group will have more points in it\n   auto firstGroup = processes / 2;\n   auto secondGroup = processes - firstGroup;\n   // we will have to decide what to do with the remainder\n   auto remainder = processes % 2;\n\n   // set the first point to start at the very top left, and then assign the rest of the points\n   Point firstPoint {x: -std::numeric_limits<double>::infinity(), y: -std::numeric_limits<double>::infinity()};\n   auto point = firstPoint;\n   std::vector<Point> p = points;\n\n   // set the first point of each group to the top left\n   if (rank < firstGroup) {\n      point = firstPoint;\n      p.insert(p.begin(), point);\n   }\n   // if we don't have a remainder, we'll be able to deal with every point by adding one to the second group\n   // if we do have a remainder, we'll be able to deal with every point by adding one to the first group\n   else if (rank < (firstGroup + remainder)) {\n      point = firstPoint;\n      p.insert(p.begin(), point);\n   }\n   // otherwise we'll just be able to deal with every other point by adding one to the first group\n   // we'll add one to the second group because we have an even amount of processes\n   else if (rank < secondGroup) {\n      point = firstPoint;\n      p.insert(p.begin(), point);\n   }\n   // we'll deal with every point by adding one to the first group\n   else {\n      point = firstPoint;\n      p.insert(p.begin(), point);\n   }\n\n   // we'll have to do this because we're using the modulus function and we'll get a remainder of 0\n   // that way, we can find out the modulus by finding the opposite of the remainder\n   int modulus = firstGroup + remainder;\n   modulus = secondGroup - modulus;\n\n   // we'll use the modulus to find which quadrant a point is in\n   // we'll also use the modulus to find how many points we'll be dealing with in a quadrant\n   // we'll use the modulus in the same way to find which quadrant we'll be dealing with\n   // we'll use the modulus in the same way to find how many points we'll be dealing with\n   // we'll use the modulus in the same way to find the amount of points we'll be dealing with\n   // if we don't have a remainder, we'll deal with every other point\n   // otherwise we'll deal with every other point\n   for (size_t i = 0; i < p.size(); i++) {\n      auto point = p[i];\n      auto x = point.x;\n      auto y = point.y;\n      // if we don't have a remainder, we'll be able to deal with every other point\n      // otherwise we'll be able to deal with every other point\n      if (i % modulus == 0) {\n         if (x > 0 && y > 0) {\n            bins[0]++;\n         }\n         else if (x < 0 && y > 0) {\n            bins[1]++;\n         }\n         else if (x < 0 && y < 0) {\n            bins[2]++;\n         }\n         else if (x",
            "// TODO: Your code here\n\n}",
            "//...\n}",
            "// TODO: Your code here\n\n    int size,rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::array<size_t, 4> local_bins;\n    local_bins.fill(0);\n\n    for (int i = 0; i < points.size(); i++){\n        if (points[i].x >= 0 && points[i].y >= 0){\n            local_bins[0] += 1;\n        }\n        else if (points[i].x < 0 && points[i].y >= 0){\n            local_bins[1] += 1;\n        }\n        else if (points[i].x < 0 && points[i].y < 0){\n            local_bins[2] += 1;\n        }\n        else if (points[i].x >= 0 && points[i].y < 0){\n            local_bins[3] += 1;\n        }\n    }\n    // int *bins;\n    // bins = local_bins;\n\n    // std::array<size_t, 4> sendcounts;\n    // sendcounts.fill(0);\n    // sendcounts[rank] = 4;\n\n    // MPI_Scatterv(local_bins, sendcounts, sendcounts, MPI_INT, bins, sendcounts[rank], MPI_INT, 0, MPI_COMM_WORLD);\n\n    std::array<int, 4> sendcounts;\n    sendcounts.fill(0);\n    sendcounts[rank] = 4;\n\n    int *bins_temp = new int[4];\n    MPI_Scatterv(local_bins.data(), sendcounts.data(), sendcounts.data(), MPI_INT, bins_temp, sendcounts[rank], MPI_INT, 0, MPI_COMM_WORLD);\n\n    bins[0] = bins_temp[0];\n    bins[1] = bins_temp[1];\n    bins[2] = bins_temp[2];\n    bins[3] = bins_temp[3];\n\n    return;\n\n\n\n}",
            "// TODO: YOUR CODE HERE\n}",
            "// TODO\n   MPI_Comm cart_comm;\n   MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 0, &cart_comm);\n\n   int my_rank;\n   MPI_Comm_rank(cart_comm, &my_rank);\n\n   std::vector<Point> local_points(points.size());\n   int local_size;\n   if (my_rank == 0) {\n      std::tie(local_points, local_size) = getPoints(points, cart_comm);\n   } else {\n      local_size = getPoints(points, cart_comm, local_points);\n   }\n\n   // MPI_Reduce\n   MPI_Reduce(&local_size, &bins[0], 1, MPI_INT, MPI_SUM, 0, cart_comm);\n\n   for (size_t i = 1; i < 4; i++) {\n      // MPI_Reduce\n      MPI_Reduce(&local_size, &bins[i], 1, MPI_INT, MPI_SUM, 0, cart_comm);\n   }\n}",
            "int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int size;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t my_local_count = 0;\n\n   for (int i = 0; i < points.size(); i++) {\n      if (points[i].x < 0 && points[i].y > 0) {\n         my_local_count++;\n      }\n      else if (points[i].x > 0 && points[i].y > 0) {\n         my_local_count++;\n      }\n      else if (points[i].x < 0 && points[i].y < 0) {\n         my_local_count++;\n      }\n      else if (points[i].x > 0 && points[i].y < 0) {\n         my_local_count++;\n      }\n   }\n\n   int recv_count;\n   int send_count = my_local_count;\n\n   if (rank == 0) {\n      bins.fill(0);\n   }\n\n   MPI_Allgather(&send_count, 1, MPI_INT, bins.data(), 1, MPI_INT, MPI_COMM_WORLD);\n}",
            "}",
            "size_t total_size = points.size();\n  // Calculate number of elements per process\n  size_t local_size = total_size / size;\n  // Set remainder if it exists\n  if (local_size*size < total_size)\n    local_size += 1;\n\n  bins.fill(0);\n\n  MPI_Request requests[2 * size];\n  // Initialize requests array\n  for (int i = 0; i < size; i++) {\n    MPI_Irecv(&bins[i], 1, MPI_LONG, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &requests[i]);\n  }\n\n  // Assign values for each element to be sent\n  for (size_t i = rank; i < total_size; i += size) {\n    if (points[i].x > 0 && points[i].y > 0)\n      bins[0] += 1;\n    else if (points[i].x < 0 && points[i].y > 0)\n      bins[1] += 1;\n    else if (points[i].x < 0 && points[i].y < 0)\n      bins[2] += 1;\n    else\n      bins[3] += 1;\n  }\n\n  // Send values\n  for (int i = 0; i < size; i++) {\n    MPI_Isend(&bins[i], 1, MPI_LONG, i, 0, MPI_COMM_WORLD, &requests[i + size]);\n  }\n\n  // Wait for values to be received\n  MPI_Waitall(2 * size, requests, MPI_STATUSES_IGNORE);\n\n  // Add values to final bin\n  for (int i = 1; i < size; i++) {\n    bins[0] += bins[i];\n  }\n\n  // If rank is 0, print the results\n  if (rank == 0) {\n    std::cout << \"[\";\n    for (int i = 0; i < 4; i++) {\n      std::cout << bins[i] << ((i < 3)? \", \" : \"]\\n\");\n    }\n  }\n\n  // Finalize MPI\n  MPI_Finalize();\n}",
            "auto rank = -1;\n    auto size = -1;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> localBins{0};\n    for (auto point : points) {\n        const auto x = point.x;\n        const auto y = point.y;\n        localBins[0] += x >= 0 && y >= 0;\n        localBins[1] += x < 0 && y >= 0;\n        localBins[2] += x < 0 && y < 0;\n        localBins[3] += x >= 0 && y < 0;\n    }\n\n    std::array<size_t, 4> globalBins{0};\n    MPI_Reduce(&localBins[0], &globalBins[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    bins = globalBins;\n}",
            "int num_procs, proc_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);\n    if (num_procs!= 4) {\n        std::cout << \"Error: Number of processes must be 4\\n\";\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n    std::vector<Point> local_points = points;\n    std::array<size_t, 4> local_bins{0, 0, 0, 0};\n    // TODO: calculate local_points\n    // TODO: calculate local_bins\n    // TODO: broadcast bins to rank 0\n    // TODO: gather bins on rank 0\n    if (proc_rank == 0) {\n        bins[0] = 0;\n        bins[1] = 0;\n        bins[2] = 0;\n        bins[3] = 0;\n    }\n    MPI_Bcast(local_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n    MPI_Allgather(local_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n}",
            "MPI_Comm cart = MPI_COMM_WORLD;\n   int n_ranks;\n   MPI_Comm_size(cart, &n_ranks);\n   int rank;\n   MPI_Comm_rank(cart, &rank);\n\n   int n_dims = 2;\n   int dims[n_dims] = {0, 0};\n   int periods[n_dims] = {0, 0};\n   MPI_Dims_create(n_ranks, 2, dims);\n   MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 0, &cart);\n\n   std::vector<Point> local_points;\n   local_points.clear();\n   int n_local_points = 0;\n   int dims_x[n_dims];\n   MPI_Cart_get(cart, n_dims, dims, periods, dims_x);\n   //std::cout << \"dims_x = \" << dims_x[0] << \" \" << dims_x[1] << std::endl;\n   int coords[n_dims];\n   MPI_Cart_coords(cart, rank, n_dims, coords);\n   for (int i = 0; i < points.size(); i++) {\n      if ((points[i].x < 0 && points[i].y > 0 && coords[0] == 0) ||\n            (points[i].x < 0 && points[i].y < 0 && coords[1] == 0) ||\n            (points[i].x > 0 && points[i].y < 0 && coords[1] == 1) ||\n            (points[i].x > 0 && points[i].y > 0 && coords[0] == 1)) {\n         local_points.push_back(points[i]);\n         n_local_points++;\n      }\n   }\n\n   int global_n_local_points;\n   MPI_Allreduce(&n_local_points, &global_n_local_points, 1, MPI_INT, MPI_SUM, cart);\n\n   int bin_ind[n_ranks];\n   for (int i = 0; i < n_ranks; i++) {\n      bin_ind[i] = 0;\n   }\n\n   int bin_size[n_ranks];\n   int n_quadrant = 0;\n   for (int i = 0; i < local_points.size(); i++) {\n      if (local_points[i].x < 0 && local_points[i].y > 0) {\n         bin_ind[n_quadrant] = 0;\n         bin_size[n_quadrant] = 1;\n      } else if (local_points[i].x < 0 && local_points[i].y < 0) {\n         bin_ind[n_quadrant] = 1;\n         bin_size[n_quadrant] = 1;\n      } else if (local_points[i].x > 0 && local_points[i].y < 0) {\n         bin_ind[n_quadrant] = 2;\n         bin_size[n_quadrant] = 1;\n      } else if (local_points[i].x > 0 && local_points[i].y > 0) {\n         bin_ind[n_quadrant] = 3;\n         bin_size[n_quadrant] = 1;\n      }\n      n_quadrant++;\n   }\n\n   //std::cout << \"bin_ind = \";\n   //for (int i = 0; i < n_ranks; i++) {\n   //   std::cout << bin_ind[i] << \" \";\n   //}\n   //std::cout << std::endl;\n\n   //std::cout << \"bin_size = \";\n   //for (int i = 0; i < n_ranks; i++) {\n   //   std::cout << bin_size[i] << \" \";\n   //}\n   //std::cout << std::endl;\n\n   int bin_num[n_ranks];\n   for (int i = 0; i",
            "int rank = 0;\n   int num_procs = 1;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n   bins.fill(0);\n   std::array<size_t, 4> bins_local;\n   for (const auto& p : points) {\n      const double x = p.x;\n      const double y = p.y;\n      if (x >= 0 && y >= 0) {\n         bins_local[0] += 1;\n      } else if (x < 0 && y >= 0) {\n         bins_local[1] += 1;\n      } else if (x < 0 && y < 0) {\n         bins_local[2] += 1;\n      } else {\n         bins_local[3] += 1;\n      }\n   }\n\n   // Reduce the bins_local to bins.\n   bins = bins_local;\n   for (int i = 1; i < num_procs; i++) {\n      MPI_Reduce(&bins_local[0], &bins[0], 4, MPI_LONG, MPI_SUM, i, MPI_COMM_WORLD);\n   }\n\n   return;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<Point> localPoints = points;\n    // If size isn't a perfect square, how do we distribute the points?\n    // Also, if localPoints is empty, how do we distribute the points?\n    // Also, what about empty bins?\n    int localCount = 0;\n    for (auto &p : localPoints) {\n        double x = p.x;\n        double y = p.y;\n        // TODO: find quadrant\n        // TODO: increment localCount\n    }\n    // TODO: calculate global sum of local counts\n    // TODO: split the global sum into equal bins\n    // TODO: store the bins in bins\n}",
            "//TODO\n    // 1) figure out number of MPI ranks\n    // 2) figure out my rank\n    // 3) figure out the extent of the grid\n    // 4) figure out my position within the grid\n    // 5) count the number of points within the quadrant I am responsible for\n    // 6) combine the results\n    // 7) return the results in bins\n\n    int nb_rank;\n    int my_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nb_rank);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n    //TODO:\n    //1) Figure out the dimensions of the grid\n    //  - max X\n    //  - max Y\n    //  - min X\n    //  - min Y\n\n    int nb_x = (int)points.size();\n    int nb_y = 1;\n\n    int max_X = 0, max_Y = 0;\n    int min_X = 0, min_Y = 0;\n\n    for (int i = 0; i < nb_x; i++) {\n        if (points[i].x > max_X)\n            max_X = (int)points[i].x;\n    }\n    for (int i = 0; i < nb_x; i++) {\n        if (points[i].y > max_Y)\n            max_Y = (int)points[i].y;\n    }\n    for (int i = 0; i < nb_x; i++) {\n        if (points[i].x < min_X)\n            min_X = (int)points[i].x;\n    }\n    for (int i = 0; i < nb_x; i++) {\n        if (points[i].y < min_Y)\n            min_Y = (int)points[i].y;\n    }\n\n    //TODO:\n    //2) Figure out my position within the grid\n    //  - row\n    //  - column\n    //  - nb_x\n    //  - nb_y\n\n    int row;\n    int col;\n    if (nb_x == 1) {\n        row = 0;\n        col = my_rank;\n    } else {\n        int nb_quadrant = (int)pow(nb_x, 2);\n        row = my_rank % (int)sqrt(nb_quadrant);\n        col = my_rank / (int)sqrt(nb_quadrant);\n    }\n\n    int nb_quadrant = (int)pow(nb_x, 2);\n\n    //TODO:\n    //3) Count the number of points within the quadrant I am responsible for\n    //  - quadrant size: nb_x / nb_quadrant\n    //  - number of points in my quadrant: nb_x / nb_quadrant * my row + my col\n\n    int nb_points = nb_x / nb_quadrant * row + col;\n    int size = nb_x / nb_quadrant;\n\n    int quadrant_start_X = min_X + col * size;\n    int quadrant_start_Y = min_Y + row * size;\n\n    //TODO:\n    //4) Combine the results\n    //  - Create a MPI_Status struct\n    //  - MPI_Reduce the number of points I have counted to the rank 0\n    //  - MPI_Reduce the number of points to bins\n    //  - Store the results in bins on rank 0\n\n    MPI_Status status;\n    int count_quadrant = 0;\n\n    MPI_Reduce(&nb_points, &count_quadrant, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    MPI_Reduce(&count_quadrant, &bins[0], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (my_rank == 0) {\n        for (int i = 0; i < nb_rank;",
            "//TODO: Your code here\n}",
            "int nbins = 4;\n    int rank;\n    int size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> localBins{};\n\n    for (auto const& p : points) {\n        if (p.x > 0 && p.y > 0) {\n            localBins[0]++;\n        }\n        else if (p.x < 0 && p.y > 0) {\n            localBins[1]++;\n        }\n        else if (p.x < 0 && p.y < 0) {\n            localBins[2]++;\n        }\n        else if (p.x > 0 && p.y < 0) {\n            localBins[3]++;\n        }\n    }\n\n    std::vector<size_t> recvBins(nbins, 0);\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(localBins.data(), nbins, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD);\n        }\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(recvBins.data(), nbins, MPI_UNSIGNED_LONG, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n    else {\n        MPI_Recv(recvBins.data(), nbins, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        MPI_Send(localBins.data(), nbins, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n    for (int i = 0; i < nbins; i++) {\n        bins[i] = localBins[i] + recvBins[i];\n    }\n}",
            "const int DIM = 2;\n    int nproc, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if(rank == 0) {\n        size_t global_size = points.size();\n        bins.fill(0);\n\n        int dim_length[DIM] = {0};\n        MPI_Dims_create(nproc, DIM, dim_length);\n        int nprocs_per_dim[DIM] = {0};\n        for(int i = 0; i < DIM; ++i) {\n            nprocs_per_dim[i] = dim_length[i];\n        }\n\n        std::vector<int> sub_dim_length[DIM];\n        for(int i = 0; i < DIM; ++i) {\n            sub_dim_length[i].resize(nprocs_per_dim[i], 0);\n            MPI_Allgather(&dim_length[i], 1, MPI_INT, sub_dim_length[i].data(), 1, MPI_INT, MPI_COMM_WORLD);\n        }\n\n        std::vector<size_t> local_bins(DIM);\n        local_bins.fill(0);\n        for(size_t i = 0; i < global_size; ++i) {\n            std::vector<int> coord(DIM);\n            for(int i = 0; i < DIM; ++i) {\n                coord[i] = int((points[i].x + 1) * nprocs_per_dim[i]);\n            }\n            int local_rank = 0;\n            for(int i = 0; i < DIM; ++i) {\n                local_rank += coord[i] * sub_dim_length[i][rank % sub_dim_length[i].size()];\n            }\n            local_bins[local_rank % DIM]++;\n        }\n        MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    } else {\n        MPI_Status status;\n        std::vector<Point> subpoints(points.size());\n        MPI_Bcast(subpoints.data(), points.size(), MPI_BYTE, 0, MPI_COMM_WORLD);\n        size_t global_size = points.size();\n        size_t local_size = subpoints.size();\n        int dim_length[DIM] = {0};\n        MPI_Dims_create(nproc, DIM, dim_length);\n        int nprocs_per_dim[DIM] = {0};\n        for(int i = 0; i < DIM; ++i) {\n            nprocs_per_dim[i] = dim_length[i];\n        }\n        std::vector<int> sub_dim_length[DIM];\n        for(int i = 0; i < DIM; ++i) {\n            sub_dim_length[i].resize(nprocs_per_dim[i], 0);\n            MPI_Allgather(&dim_length[i], 1, MPI_INT, sub_dim_length[i].data(), 1, MPI_INT, MPI_COMM_WORLD);\n        }\n        std::vector<size_t> local_bins(DIM);\n        local_bins.fill(0);\n        for(size_t i = 0; i < global_size; ++i) {\n            std::vector<int> coord(DIM);\n            for(int i = 0; i < DIM; ++i) {\n                coord[i] = int((subpoints[i].x + 1) * nprocs_per_dim[i]);\n            }\n            int local_rank = 0;\n            for(int i = 0; i < DIM; ++i) {\n                local_rank += coord[i] * sub_dim_length[i][rank % sub_dim_length[i].size()",
            "}",
            "// Fill in your code here\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    MPI_Datatype PointType;\n    MPI_Type_contiguous(sizeof(Point), MPI_BYTE, &PointType);\n    MPI_Type_commit(&PointType);\n\n    Point *points_ptr;\n    points_ptr = new Point[points.size()];\n    std::copy(points.begin(), points.end(), points_ptr);\n    size_t num_of_points = points.size();\n    MPI_Scatter(&num_of_points, 1, MPI_UNSIGNED_LONG, &num_of_points, 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    Point *local_points;\n    local_points = new Point[num_of_points];\n    MPI_Scatterv(points_ptr, &num_of_points, &MPI_UNSIGNED_LONG, local_points, &num_of_points, &MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    int points_per_rank = num_of_points / size;\n\n    std::array<size_t, 4> local_bins;\n    for (size_t i = 0; i < num_of_points; i++) {\n        if (local_points[i].x > 0 && local_points[i].y > 0) {\n            local_bins[0] += 1;\n        } else if (local_points[i].x > 0 && local_points[i].y < 0) {\n            local_bins[1] += 1;\n        } else if (local_points[i].x < 0 && local_points[i].y < 0) {\n            local_bins[2] += 1;\n        } else {\n            local_bins[3] += 1;\n        }\n    }\n\n    MPI_Allreduce(&local_bins[0], &bins[0], 4, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    delete [] local_points;\n    delete [] points_ptr;\n    MPI_Type_free(&PointType);\n}",
            "/*\n     * TODO: Implement this method\n     */\n    // 1. Get the number of ranks\n    int num_ranks, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // 2. Get the number of points\n    int num_points = points.size();\n\n    // 3. Set the number of points in the domain to be divided\n    int domain_size = 6;\n\n    // 4. Calculate the number of points per rank\n    int points_per_rank = num_points / num_ranks;\n\n    // 5. Initialize bins with zeros\n    std::array<size_t, 4> bin_counts = {0, 0, 0, 0};\n\n    // 6. Assign each point to a quadrant\n    for (int i = 0; i < num_points; i++) {\n        double x = points[i].x;\n        double y = points[i].y;\n\n        if (x > 0 && y > 0) {\n            bin_counts[0]++;\n        }\n        else if (x < 0 && y > 0) {\n            bin_counts[1]++;\n        }\n        else if (x < 0 && y < 0) {\n            bin_counts[2]++;\n        }\n        else if (x > 0 && y < 0) {\n            bin_counts[3]++;\n        }\n    }\n\n    // 7. Initialize MPI\n    MPI_Request request;\n    MPI_Status status;\n\n    // 8. Assign the count in quadrants to each rank\n    if (rank == 0) {\n        for (int i = 1; i < num_ranks; i++) {\n            // Get the count in quadrants from each rank\n            MPI_Irecv(&bin_counts, 4, MPI_INT, i, i, MPI_COMM_WORLD, &request);\n            MPI_Wait(&request, &status);\n            bins = bin_counts;\n        }\n    }\n    else {\n        // Send the count in quadrants to rank 0\n        MPI_Send(&bin_counts, 4, MPI_INT, 0, rank, MPI_COMM_WORLD);\n    }\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t globalPoints = points.size();\n   size_t localPoints = globalPoints / size;\n   size_t remainder = globalPoints % size;\n\n   // assign a local points to each rank\n   int remainderCount = 0;\n   if (rank < remainder) {\n      localPoints += 1;\n      remainderCount = 1;\n   }\n\n   // local bins\n   std::array<size_t, 4> localBins{};\n\n   for (size_t i = 0; i < localPoints; ++i) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x > 0.0 && y > 0.0) {\n         localBins[0] += 1;\n      }\n      if (x < 0.0 && y > 0.0) {\n         localBins[1] += 1;\n      }\n      if (x < 0.0 && y < 0.0) {\n         localBins[2] += 1;\n      }\n      if (x > 0.0 && y < 0.0) {\n         localBins[3] += 1;\n      }\n   }\n\n   // global bins\n   std::array<size_t, 4> globalBins{};\n   MPI_Allreduce(localBins.data(), globalBins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n   // increment remainder counts\n   for (int i = 0; i < 4; ++i) {\n      globalBins[i] += remainderCount;\n   }\n\n   bins = globalBins;\n}",
            "// TODO: Your code here.\n  // Initialize the local array on each process.\n  int N = points.size();\n  double x0, x1, y0, y1;\n  double bin_size = 1.0;\n  x0 = (double)MPI_PROC_NULL;\n  x1 = (double)(MPI_PROC_NULL+1);\n  y0 = (double)MPI_PROC_NULL;\n  y1 = (double)(MPI_PROC_NULL+1);\n\n  std::array<size_t, 4> local_bins{0, 0, 0, 0};\n\n  for (int i = 0; i < N; ++i){\n\n    if(points[i].x >= x0 && points[i].x < x1 && points[i].y >= y0 && points[i].y < y1){\n      if(points[i].x >= -1.0 && points[i].x < 0.0){\n        local_bins[0]++;\n      }\n      else if(points[i].x >= 0.0 && points[i].x < 1.0){\n        local_bins[1]++;\n      }\n      else if(points[i].x >= 1.0 && points[i].x < 2.0){\n        local_bins[2]++;\n      }\n      else{\n        local_bins[3]++;\n      }\n    }\n\n  }\n\n  // TODO: Use MPI to reduce the local counts to the global counts.\n\n  MPI_Allreduce(&local_bins[0], &bins[0], 4, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n}",
            "// TODO\n}",
            "MPI_Comm cart_comm;\n    int rank, n_procs;\n    MPI_Comm_size(MPI_COMM_WORLD, &n_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Cart_create(MPI_COMM_WORLD, 2, new int[2]{n_procs/2, n_procs/2}, new int[2]{0, 0}, 0, &cart_comm);\n    MPI_Status status;\n    size_t count;\n\n    std::vector<Point> local_points;\n    std::vector<size_t> local_counts;\n    if (rank == 0) {\n        std::vector<int> dims(2);\n        std::vector<int> periods(2);\n        std::vector<int> coords(2);\n        MPI_Cart_get(cart_comm, 2, dims.data(), periods.data(), coords.data());\n        local_points.resize(points.size());\n        std::copy(points.begin(), points.end(), local_points.begin());\n    }\n\n    MPI_Scatter(local_points.data(), local_points.size(), MPI_BYTE, local_points.data(), local_points.size(), MPI_BYTE, 0, cart_comm);\n\n    local_counts.resize(4);\n\n    for (Point p: local_points) {\n        int quadrant = getQuadrant(p);\n        local_counts[quadrant]++;\n    }\n\n    MPI_Gather(local_counts.data(), 4, MPI_UINT64_T, bins.data(), 4, MPI_UINT64_T, 0, cart_comm);\n    MPI_Comm_free(&cart_comm);\n}",
            "// Initialize bins to 0\n   std::fill(bins.begin(), bins.end(), 0);\n\n   // Create communicator that spans all ranks\n   MPI_Comm comm;\n   MPI_Comm_dup(MPI_COMM_WORLD, &comm);\n\n   // Create cartesian communicator\n   int dim = 2;\n   int nx = 2;\n   int ny = 2;\n   int periods[dim] = {false, false};\n   int reorder = 0;\n   MPI_Cart_create(comm, dim, &nx, &ny, &periods[0], reorder, &comm);\n\n   // Compute my coordinates\n   int rank;\n   MPI_Comm_rank(comm, &rank);\n\n   int ndims = 2;\n   int coord[ndims];\n   MPI_Cart_coords(comm, rank, ndims, &coord[0]);\n\n   int num_local_points = points.size() / comm_size;\n   // Determine the local index of the first point\n   int local_start = num_local_points * rank;\n\n   // If the number of points is not evenly divisible by the number of ranks, there may be a rank with more points than\n   // others\n   int local_end;\n   if (rank == comm_size - 1) {\n      local_end = points.size();\n   } else {\n      local_end = local_start + num_local_points;\n   }\n\n   // Compute my quadrant\n   int x_quadrant = 0;\n   int y_quadrant = 0;\n   if (points[local_start].x < 0.0) x_quadrant = 1;\n   if (points[local_start].y < 0.0) y_quadrant = 1;\n\n   // Update bin if needed\n   if (coord[0] == x_quadrant && coord[1] == y_quadrant) {\n      bins[coord[0] * 2 + coord[1]] += 1;\n   }\n\n   MPI_Barrier(comm);\n\n   MPI_Allreduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_LONG_LONG_INT, MPI_SUM, comm);\n\n   // Free MPI objects\n   MPI_Comm_free(&comm);\n}",
            "MPI_Status status;\n    MPI_Datatype point_type;\n    MPI_Type_contiguous(sizeof(Point), MPI_BYTE, &point_type);\n    MPI_Type_commit(&point_type);\n\n    int num_procs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int proc_dims = 2;\n    int dims[2] = {2, 2};\n    int periodic[2] = {1, 1};\n    MPI_Cart_create(MPI_COMM_WORLD, proc_dims, dims, periodic, 1, &point_type);\n    MPI_Cart_get(MPI_COMM_WORLD, proc_dims, dims, periodic, MPI_COMM_NULL);\n\n    std::vector<Point> local_points;\n    int local_size = points.size() / num_procs;\n    int start_index = rank * local_size;\n\n    local_points.assign(points.begin() + start_index, points.begin() + start_index + local_size);\n\n    for (int i = 0; i < 4; i++) {\n        MPI_Cart_coords(MPI_COMM_WORLD, i, proc_dims, dims);\n        int coords[2];\n        MPI_Cart_coords(MPI_COMM_WORLD, rank, proc_dims, coords);\n        MPI_Send(&local_points[0], local_size, point_type, i, 0, MPI_COMM_WORLD);\n    }\n\n    std::vector<Point> all_points;\n    int total_size = 0;\n    for (int i = 0; i < 4; i++) {\n        int size;\n        MPI_Recv(&size, 1, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n        total_size += size;\n    }\n\n    all_points.resize(total_size);\n\n    int index = 0;\n    for (int i = 0; i < 4; i++) {\n        int size;\n        MPI_Recv(&size, 1, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n        MPI_Recv(&all_points[index], size, point_type, i, 0, MPI_COMM_WORLD, &status);\n        index += size;\n    }\n\n    for (int i = 0; i < 4; i++) {\n        MPI_Cart_coords(MPI_COMM_WORLD, i, proc_dims, dims);\n        int coords[2];\n        MPI_Cart_coords(MPI_COMM_WORLD, rank, proc_dims, coords);\n        double x = all_points[index].x;\n        double y = all_points[index].y;\n        if (x < 0 && y < 0) {\n            bins[0] += 1;\n        }\n        else if (x < 0 && y >= 0) {\n            bins[1] += 1;\n        }\n        else if (x >= 0 && y < 0) {\n            bins[2] += 1;\n        }\n        else {\n            bins[3] += 1;\n        }\n        index += 1;\n    }\n}",
            "MPI_Comm comm;\n    MPI_Comm_dup(MPI_COMM_WORLD, &comm);\n\n    int rank, size;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &size);\n\n    double local_x_min, local_x_max, local_y_min, local_y_max;\n    if (rank == 0) {\n        local_x_min = std::numeric_limits<double>::max();\n        local_x_max = std::numeric_limits<double>::lowest();\n        local_y_min = std::numeric_limits<double>::max();\n        local_y_max = std::numeric_limits<double>::lowest();\n        for (size_t i = 0; i < points.size(); i++) {\n            local_x_min = std::min(local_x_min, points[i].x);\n            local_x_max = std::max(local_x_max, points[i].x);\n            local_y_min = std::min(local_y_min, points[i].y);\n            local_y_max = std::max(local_y_max, points[i].y);\n        }\n    }\n\n    MPI_Bcast(&local_x_min, 1, MPI_DOUBLE, 0, comm);\n    MPI_Bcast(&local_x_max, 1, MPI_DOUBLE, 0, comm);\n    MPI_Bcast(&local_y_min, 1, MPI_DOUBLE, 0, comm);\n    MPI_Bcast(&local_y_max, 1, MPI_DOUBLE, 0, comm);\n\n    int x_dims = size;\n    int y_dims = 1;\n\n    MPI_Dims_create(size, 2, &x_dims, &y_dims);\n\n    MPI_Cart_create(comm, 2, &x_dims, &y_dims, true, &comm);\n\n    int x_rank, y_rank;\n    MPI_Cart_coords(comm, rank, 2, &x_rank, &y_rank);\n\n    int x_size, y_size;\n    MPI_Cartdim_get(comm, &x_size);\n    MPI_Cartdim_get(comm, &y_size);\n\n    std::vector<Point> local_points;\n\n    if (rank == 0) {\n        local_points = points;\n    } else {\n        std::vector<Point> recv_points(points.size());\n        MPI_Scatter(points.data(), points.size(), MPI_BYTE, recv_points.data(), recv_points.size(), MPI_BYTE, 0, comm);\n        local_points = recv_points;\n    }\n\n    std::array<size_t, 4> local_counts;\n    local_counts[0] = 0;\n    local_counts[1] = 0;\n    local_counts[2] = 0;\n    local_counts[3] = 0;\n\n    for (size_t i = 0; i < local_points.size(); i++) {\n        if (local_points[i].x > local_x_max) {\n            local_counts[2]++;\n        } else if (local_points[i].x < local_x_min) {\n            local_counts[3]++;\n        } else if (local_points[i].y > local_y_max) {\n            local_counts[0]++;\n        } else if (local_points[i].y < local_y_min) {\n            local_counts[1]++;\n        }\n    }\n\n    MPI_Reduce(MPI_IN_PLACE, local_counts.data(), local_counts.size(), MPI_LONG_LONG, MPI_SUM, 0, comm);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < bins.size(); i++) {\n            bins[i] = local_counts[i];\n        }",
            "bins.fill(0);\n\n   // TODO: Fill in a MPI data structure to count quadrants\n\n   // TODO: Make MPI calls to count quadrants in parallel\n\n   // TODO: Put the result in bins\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint npoints_rank = points.size() / size;\n\tint npoints_left = points.size() - npoints_rank * size;\n\n\tstd::array<size_t, 4> bins_rank;\n\tfor (int i = 0; i < npoints_rank; i++) {\n\t\tif (points[i + rank * npoints_rank].x > 0 && points[i + rank * npoints_rank].y > 0) {\n\t\t\tbins_rank[0]++;\n\t\t}\n\t\telse if (points[i + rank * npoints_rank].x > 0 && points[i + rank * npoints_rank].y < 0) {\n\t\t\tbins_rank[1]++;\n\t\t}\n\t\telse if (points[i + rank * npoints_rank].x < 0 && points[i + rank * npoints_rank].y < 0) {\n\t\t\tbins_rank[2]++;\n\t\t}\n\t\telse if (points[i + rank * npoints_rank].x < 0 && points[i + rank * npoints_rank].y > 0) {\n\t\t\tbins_rank[3]++;\n\t\t}\n\t}\n\tfor (int i = 0; i < npoints_left; i++) {\n\t\tif (points[i + rank * npoints_rank + npoints_left * size].x > 0 && points[i + rank * npoints_rank + npoints_left * size].y > 0) {\n\t\t\tbins_rank[0]++;\n\t\t}\n\t\telse if (points[i + rank * npoints_rank + npoints_left * size].x > 0 && points[i + rank * npoints_rank + npoints_left * size].y < 0) {\n\t\t\tbins_rank[1]++;\n\t\t}\n\t\telse if (points[i + rank * npoints_rank + npoints_left * size].x < 0 && points[i + rank * npoints_rank + npoints_left * size].y < 0) {\n\t\t\tbins_rank[2]++;\n\t\t}\n\t\telse if (points[i + rank * npoints_rank + npoints_left * size].x < 0 && points[i + rank * npoints_rank + npoints_left * size].y > 0) {\n\t\t\tbins_rank[3]++;\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 1; i < size; i++) {\n\t\t\tMPI_Recv(&bins_rank, 4, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t\tbins[0] += bins_rank[0];\n\t\t\tbins[1] += bins_rank[1];\n\t\t\tbins[2] += bins_rank[2];\n\t\t\tbins[3] += bins_rank[3];\n\t\t}\n\t}\n\telse {\n\t\tMPI_Send(&bins_rank, 4, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n\t}\n}",
            "int numprocs, myrank;\n  MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n  std::vector<size_t> local_bins(4, 0);\n\n  for (int i = 0; i < points.size(); ++i) {\n    Point const& pt = points[i];\n    if (pt.x >= 0 && pt.y >= 0)\n      ++local_bins[0];\n    if (pt.x < 0 && pt.y >= 0)\n      ++local_bins[1];\n    if (pt.x < 0 && pt.y < 0)\n      ++local_bins[2];\n    if (pt.x >= 0 && pt.y < 0)\n      ++local_bins[3];\n  }\n\n  MPI_Allreduce(MPI_IN_PLACE, local_bins.data(), local_bins.size(), MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n  if (myrank == 0)\n    for (int i = 0; i < 4; ++i)\n      bins[i] = local_bins[i];\n}",
            "// TODO: Implement!\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int count = points.size();\n    int stride = count / size;\n\n    if (rank == 0) {\n        bins = {0, 0, 0, 0};\n    }\n    MPI_Bcast(&bins, 4, MPI_LONG, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::cout << \"points size: \" << points.size() << std::endl;\n        std::cout << \"stride: \" << stride << std::endl;\n        std::cout << \"rank: \" << rank << std::endl;\n    }\n\n    if (rank < size - 1) {\n        std::vector<Point> pts(points.begin() + rank * stride, points.begin() + (rank + 1) * stride);\n        for (Point pt : pts) {\n            if (pt.x < 0 && pt.y >= 0) {\n                bins[0]++;\n            } else if (pt.x >= 0 && pt.y < 0) {\n                bins[1]++;\n            } else if (pt.x < 0 && pt.y < 0) {\n                bins[2]++;\n            } else if (pt.x >= 0 && pt.y >= 0) {\n                bins[3]++;\n            }\n        }\n        std::cout << \"rank: \" << rank << \", bins: \" << bins[0] << \" \" << bins[1] << \" \" << bins[2] << \" \" << bins[3] << std::endl;\n    } else {\n        std::vector<Point> pts(points.begin() + (rank - 1) * stride, points.end());\n        for (Point pt : pts) {\n            if (pt.x < 0 && pt.y >= 0) {\n                bins[0]++;\n            } else if (pt.x >= 0 && pt.y < 0) {\n                bins[1]++;\n            } else if (pt.x < 0 && pt.y < 0) {\n                bins[2]++;\n            } else if (pt.x >= 0 && pt.y >= 0) {\n                bins[3]++;\n            }\n        }\n        std::cout << \"rank: \" << rank << \", bins: \" << bins[0] << \" \" << bins[1] << \" \" << bins[2] << \" \" << bins[3] << std::endl;\n    }\n    MPI_Allreduce(&bins, MPI_IN_PLACE, 4, MPI_LONG, MPI_SUM, MPI_COMM_WORLD);\n    std::cout << \"bins: \" << bins[0] << \" \" << bins[1] << \" \" << bins[2] << \" \" << bins[3] << std::endl;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (rank == 0) {\n\t\t// Initialize bins\n\t\tfor (int i = 0; i < 4; i++) {\n\t\t\tbins[i] = 0;\n\t\t}\n\t}\n\n\tint send_count = 0;\n\tint recv_count = 0;\n\tif (rank == 0) {\n\t\tsend_count = points.size() / size;\n\t}\n\telse {\n\t\tsend_count = points.size() / size + points.size() % size;\n\t}\n\n\tstd::vector<Point> recv_points;\n\tif (rank == 0) {\n\t\trecv_count = send_count;\n\t}\n\telse {\n\t\trecv_count = send_count - points.size() % size;\n\t}\n\n\t// Create send and recv buffers\n\tstd::vector<Point> send_buf(send_count);\n\tstd::vector<Point> recv_buf(recv_count);\n\n\t// Copy the relevant points to send buffer\n\tint count = 0;\n\tint i = 0;\n\twhile (i < points.size()) {\n\t\tif (rank == 0) {\n\t\t\tif (count >= send_count) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tsend_buf[count] = points[i];\n\t\t\ti++;\n\t\t\tcount++;\n\t\t}\n\t\telse {\n\t\t\tsend_buf[count] = points[i];\n\t\t\ti++;\n\t\t\tcount++;\n\t\t\tif (count == send_count) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\t// Send and recv the points\n\tMPI_Send(send_buf.data(), send_count, MPI_DOUBLE, rank + 1, 0, MPI_COMM_WORLD);\n\tMPI_Recv(recv_buf.data(), recv_count, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n\t// For each point in recv_buf, determine quadrant and increment bin accordingly\n\tfor (Point p : recv_buf) {\n\t\tif (p.x >= 0 && p.y >= 0) {\n\t\t\tbins[0]++;\n\t\t}\n\t\telse if (p.x < 0 && p.y >= 0) {\n\t\t\tbins[1]++;\n\t\t}\n\t\telse if (p.x < 0 && p.y < 0) {\n\t\t\tbins[2]++;\n\t\t}\n\t\telse if (p.x >= 0 && p.y < 0) {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\t// Collect bins from all ranks\n\t\tstd::vector<size_t> all_bins(size);\n\t\tMPI_Gather(bins.data(), 4, MPI_UNSIGNED_LONG_LONG, all_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n\t\t// Output bins on rank 0\n\t\tfor (size_t i = 0; i < size; i++) {\n\t\t\tstd::cout << all_bins[i] << \" \";\n\t\t}\n\t\tstd::cout << std::endl;\n\t}\n\n\treturn;\n}",
            "std::array<size_t, 4> counts{0, 0, 0, 0};\n    for(auto &p : points) {\n        if(p.x >= 0 && p.y >= 0) {\n            counts[0]++;\n        } else if(p.x < 0 && p.y >= 0) {\n            counts[1]++;\n        } else if(p.x < 0 && p.y < 0) {\n            counts[2]++;\n        } else {\n            counts[3]++;\n        }\n    }\n    bins = counts;\n}",
            "//TODO: Add your code here\n}",
            "// TODO\n}",
            "int nRanks;\n   MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n\n   // Step 1: divide points by quadrant\n   std::vector<Point> quadPoints[4];\n   for (auto const& p : points) {\n      size_t i = 0;\n      if (p.x > 0) {\n         i += 1;\n      }\n      if (p.y > 0) {\n         i += 2;\n      }\n      quadPoints[i].push_back(p);\n   }\n\n   // Step 2: count points in each quadrant\n   std::vector<size_t> counts(nRanks);\n   counts[0] = quadPoints[0].size();\n   counts[1] = quadPoints[1].size();\n   counts[2] = quadPoints[2].size();\n   counts[3] = quadPoints[3].size();\n\n   // Step 3: communicate counts\n   std::vector<size_t> recvCounts(nRanks, 0);\n   std::vector<size_t> sendCounts(nRanks, 0);\n   for (int i = 0; i < nRanks; i++) {\n      recvCounts[i] = counts[i];\n   }\n   MPI_Alltoall(&recvCounts[0], 1, MPI_UNSIGNED_LONG_LONG, &sendCounts[0], 1, MPI_UNSIGNED_LONG_LONG, MPI_COMM_WORLD);\n\n   // Step 4: create temporary bins for each rank\n   std::vector<std::vector<Point>> sendBins(nRanks);\n   std::vector<std::vector<Point>> recvBins(nRanks);\n   sendBins[0] = quadPoints[0];\n   sendBins[1] = quadPoints[1];\n   sendBins[2] = quadPoints[2];\n   sendBins[3] = quadPoints[3];\n\n   // Step 5: exchange data\n   MPI_Alltoallv(&sendBins[0][0], &sendCounts[0], &sendCounts[0], MPI_BYTE, &recvBins[0][0], &recvCounts[0], &recvCounts[0], MPI_BYTE, MPI_COMM_WORLD);\n\n   // Step 6: count in each bin\n   bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n   for (int i = 0; i < nRanks; i++) {\n      bins[0] += recvBins[i][0].size();\n      bins[1] += recvBins[i][1].size();\n      bins[2] += recvBins[i][2].size();\n      bins[3] += recvBins[i][3].size();\n   }\n}",
            "// TODO\n}",
            "// YOUR CODE HERE\n}",
            "const int num_rank = size;\n   std::vector<Point> buf;\n\n   MPI_Comm comm = MPI_COMM_WORLD;\n   int rank;\n   MPI_Comm_rank(comm, &rank);\n\n   std::vector<int> bins_local(4, 0);\n   for (size_t i = 0; i < points.size(); i++) {\n      if (points[i].x > 0) {\n         if (points[i].y > 0) {\n            bins_local[0]++;\n         } else {\n            bins_local[3]++;\n         }\n      } else {\n         if (points[i].y > 0) {\n            bins_local[1]++;\n         } else {\n            bins_local[2]++;\n         }\n      }\n   }\n\n   if (rank == 0) {\n      bins[0] = 0;\n      bins[1] = 0;\n      bins[2] = 0;\n      bins[3] = 0;\n   }\n\n   MPI_Gather(&bins_local[0], 4, MPI_INT, &bins[0], 4, MPI_INT, 0, comm);\n\n}",
            "// TODO: implement\n}",
            "MPI_Init(nullptr, nullptr);\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   std::vector<Point> points_rank = points;\n   std::random_device rd;\n   std::mt19937 g(rd());\n   std::shuffle(points_rank.begin(), points_rank.end(), g);\n   int dim = points_rank.size() / size;\n\n   if (rank == 0) {\n      for (size_t i = 0; i < 4; i++) {\n         bins[i] = 0;\n      }\n      for (size_t i = 0; i < size; i++) {\n         if (points_rank[i * dim].x < 0) {\n            if (points_rank[i * dim].y < 0) {\n               bins[0] += 1;\n            }\n            else if (points_rank[i * dim].y >= 0) {\n               bins[3] += 1;\n            }\n         }\n         else if (points_rank[i * dim].x >= 0) {\n            if (points_rank[i * dim].y < 0) {\n               bins[1] += 1;\n            }\n            else if (points_rank[i * dim].y >= 0) {\n               bins[2] += 1;\n            }\n         }\n      }\n   }\n   else {\n      MPI_Request request;\n      int dim = points_rank.size() / size;\n      MPI_Isend(&points_rank[rank * dim], dim, MPI_BYTE, 0, 0, MPI_COMM_WORLD, &request);\n      MPI_Status status;\n      MPI_Recv(&bins, 4 * sizeof(size_t), MPI_BYTE, 0, 0, MPI_COMM_WORLD, &status);\n   }\n\n   MPI_Finalize();\n}",
            "// TODO\n   MPI_Comm world = MPI_COMM_WORLD;\n   int size, rank;\n   MPI_Comm_size(world, &size);\n   MPI_Comm_rank(world, &rank);\n   bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n\n   Point* send_arr = new Point[points.size()/size];\n   Point* recv_arr = new Point[points.size()/size];\n   Point* point_arr = new Point[points.size()];\n   Point temp;\n   for(size_t i=0; i<points.size(); i++) {\n       point_arr[i] = points[i];\n   }\n\n   int *counts = new int[size];\n   int *displs = new int[size];\n   int num_point = points.size()/size;\n   for(int i=0; i<size; i++) {\n       counts[i] = num_point;\n   }\n   for(int i=0; i<size; i++) {\n       displs[i] = i*num_point;\n   }\n\n   MPI_Alltoallv(&point_arr[0], &counts[0], &displs[0], MPI_POINT, &recv_arr[0], &counts[0], &displs[0], MPI_POINT, world);\n   for(int i=0; i<size; i++) {\n       send_arr[i] = recv_arr[i];\n   }\n   for(int i=0; i<size; i++) {\n       for(int j=0; j<num_point; j++) {\n           if(send_arr[i].x > 0 && send_arr[i].y > 0) {\n               bins[0]++;\n           }\n           else if(send_arr[i].x < 0 && send_arr[i].y > 0) {\n               bins[1]++;\n           }\n           else if(send_arr[i].x < 0 && send_arr[i].y < 0) {\n               bins[2]++;\n           }\n           else if(send_arr[i].x > 0 && send_arr[i].y < 0) {\n               bins[3]++;\n           }\n       }\n   }\n   MPI_Barrier(world);\n\n   if(rank == 0) {\n       std::cout << \"bins: \" << bins[0] << \" \" << bins[1] << \" \" << bins[2] << \" \" << bins[3] << std::endl;\n   }\n\n   delete [] send_arr;\n   delete [] recv_arr;\n   delete [] point_arr;\n   delete [] counts;\n   delete [] displs;\n}",
            "int nproc, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (nproc!= 4) {\n        std::cerr << \"Error: CountQuadrants requires 4 processes\" << std::endl;\n        std::abort();\n    }\n\n    // find the quadrant of each point\n    // each process only cares about one quadrant\n    std::vector<size_t> myPoints;\n    for (auto const& p : points) {\n        if (p.x >= 0.0 && p.y >= 0.0) {\n            myPoints.push_back(0);\n        } else if (p.x < 0.0 && p.y >= 0.0) {\n            myPoints.push_back(1);\n        } else if (p.x < 0.0 && p.y < 0.0) {\n            myPoints.push_back(2);\n        } else if (p.x >= 0.0 && p.y < 0.0) {\n            myPoints.push_back(3);\n        }\n    }\n\n    // get the number of points in each quadrant\n    // each process only cares about one quadrant\n    size_t nPoints;\n    if (rank == 0) {\n        std::array<size_t, 4> allBins;\n        for (int i = 0; i < 4; i++) {\n            allBins[i] = 0;\n        }\n        for (size_t i = 0; i < myPoints.size(); i++) {\n            allBins[myPoints[i]]++;\n        }\n\n        // broadcast the results to each process\n        for (int i = 0; i < 4; i++) {\n            MPI_Bcast(&allBins[i], 1, MPI_INT, i, MPI_COMM_WORLD);\n        }\n\n        // copy the results to the output array\n        for (int i = 0; i < 4; i++) {\n            bins[i] = allBins[i];\n        }\n    } else {\n        MPI_Bcast(&myPoints[0], myPoints.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        nPoints = myPoints.size();\n    }\n\n    // divide the work evenly among the process\n    size_t nPointsPerRank = nPoints / nproc;\n    size_t myStart = rank * nPointsPerRank;\n    size_t myEnd = (rank + 1) * nPointsPerRank;\n    if (rank == nproc - 1) {\n        myEnd = nPoints;\n    }\n\n    // count the number of points in the quadrant\n    // add up the results to the output\n    std::array<size_t, 4> myBins;\n    for (int i = 0; i < 4; i++) {\n        myBins[i] = 0;\n    }\n    for (size_t i = myStart; i < myEnd; i++) {\n        myBins[myPoints[i]]++;\n    }\n\n    // send the results to rank 0\n    if (rank == 0) {\n        for (int i = 0; i < 4; i++) {\n            MPI_Send(&myBins[i], 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n        }\n    } else {\n        MPI_Recv(&bins[0], 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n}",
            "const size_t N = points.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int *bins_send = new int[4];\n    for (int i = 0; i < 4; i++) {\n        bins_send[i] = 0;\n    }\n    int *bins_recv = new int[4];\n\n    for (int i = 0; i < N; i++) {\n        if (points[i].x >= 0 && points[i].y >= 0) {\n            bins_send[0] += 1;\n        }\n        if (points[i].x <= 0 && points[i].y >= 0) {\n            bins_send[1] += 1;\n        }\n        if (points[i].x <= 0 && points[i].y <= 0) {\n            bins_send[2] += 1;\n        }\n        if (points[i].x >= 0 && points[i].y <= 0) {\n            bins_send[3] += 1;\n        }\n    }\n\n    int remainder = N % size;\n    int count_each_rank = N / size;\n\n    MPI_Gather(bins_send, 4, MPI_INT, bins_recv, 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            if (i == size - 1) {\n                bins[0] += bins_recv[i * 4] + bins_recv[(i + 1) * 4 - 1];\n                bins[1] += bins_recv[(i + 1) * 4] + bins_recv[i * 4 - 1];\n                bins[2] += bins_recv[(i + 1) * 4] + bins_recv[i * 4 - 1];\n                bins[3] += bins_recv[i * 4] + bins_recv[(i + 1) * 4 - 1];\n            } else {\n                bins[0] += bins_recv[i * 4] + bins_recv[(i + 1) * 4 - 1];\n                bins[1] += bins_recv[(i + 1) * 4] + bins_recv[i * 4 - 1];\n                bins[2] += bins_recv[(i + 1) * 4];\n                bins[3] += bins_recv[i * 4];\n            }\n        }\n        for (int i = 0; i < 4; i++) {\n            std::cout << \"bins[\" << i << \"] \" << bins[i] << std::endl;\n        }\n    }\n\n    delete[] bins_send;\n    delete[] bins_recv;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    std::vector<Point> localPoints;\n    int localSize = 0;\n    if (rank == 0) {\n        localPoints = points;\n        localSize = points.size();\n    }\n    std::vector<Point> splitPoints;\n    int splitSize = 0;\n    MPI_Bcast(&localSize, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Scatter(localPoints.data(), localSize, MPI_DOUBLE, splitPoints.data(), localSize, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Scatter(localPoints.data(), localSize, MPI_DOUBLE, splitSize, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    for (int i = 0; i < splitSize; i++) {\n        if (splitPoints[i].x >= 0 && splitPoints[i].y >= 0) {\n            bins[0] += 1;\n        } else if (splitPoints[i].x < 0 && splitPoints[i].y >= 0) {\n            bins[1] += 1;\n        } else if (splitPoints[i].x < 0 && splitPoints[i].y < 0) {\n            bins[2] += 1;\n        } else if (splitPoints[i].x >= 0 && splitPoints[i].y < 0) {\n            bins[3] += 1;\n        }\n    }\n}",
            "/* IMPLEMENT ME */\n  MPI_Comm comm = MPI_COMM_WORLD;\n  int rank, size;\n  MPI_Comm_rank(comm, &rank);\n  MPI_Comm_size(comm, &size);\n\n  int n = (int)points.size();\n  int chunk_size = n / size;\n  int remainder = n % size;\n  int start = chunk_size * rank + std::min(rank, remainder);\n  int end = start + chunk_size + (rank < remainder? 1 : 0);\n  std::vector<int> counts(4);\n  for (int i = start; i < end; i++) {\n    double x = points[i].x;\n    double y = points[i].y;\n    if (x > 0 && y > 0) {\n      counts[0] += 1;\n    } else if (x < 0 && y > 0) {\n      counts[1] += 1;\n    } else if (x < 0 && y < 0) {\n      counts[2] += 1;\n    } else if (x > 0 && y < 0) {\n      counts[3] += 1;\n    }\n  }\n  MPI_Gather(&counts[0], 4, MPI_INT, &bins[0], 4, MPI_INT, 0, comm);\n\n  if (rank == 0) {\n    for (int i = 0; i < 4; i++) {\n      bins[i] = 0;\n      for (int j = 0; j < size; j++) {\n        bins[i] += counts[i];\n      }\n    }\n  }\n}",
            "constexpr int ndims = 2;\n  int nproc, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  std::array<int, ndims> dims = {nproc, nproc};\n  std::array<int, ndims> periods = {true, true};\n  MPI_Cart_create(MPI_COMM_WORLD, ndims, dims.data(), periods.data(), true, &MPI_COMM_CART);\n  MPI_Comm_rank(MPI_COMM_CART, &rank);\n  std::array<int, ndims> coords;\n  MPI_Cart_coords(MPI_COMM_CART, rank, ndims, coords.data());\n\n  std::array<int, ndims> low, high;\n  MPI_Cart_get(MPI_COMM_CART, ndims, dims.data(), periods.data(), low.data());\n  MPI_Cart_get(MPI_COMM_CART, ndims, dims.data(), periods.data(), high.data());\n  // std::cout << rank << \" \" << low[0] << \" \" << low[1] << \" \" << high[0] << \" \" << high[1] << std::endl;\n\n  std::array<size_t, 4> localBins = {0, 0, 0, 0};\n\n  for (int i = 0; i < points.size(); i++) {\n\n    if (points[i].x >= -0.5 && points[i].x < 0.5 && points[i].y >= -0.5 && points[i].y < 0.5) {\n      localBins[0]++;\n    } else if (points[i].x >= 0.5 && points[i].x < 1.5 && points[i].y >= -0.5 && points[i].y < 0.5) {\n      localBins[1]++;\n    } else if (points[i].x >= -0.5 && points[i].x < 0.5 && points[i].y >= 0.5 && points[i].y < 1.5) {\n      localBins[2]++;\n    } else if (points[i].x >= 0.5 && points[i].x < 1.5 && points[i].y >= 0.5 && points[i].y < 1.5) {\n      localBins[3]++;\n    }\n  }\n\n  std::array<size_t, 4> globalBins = {0, 0, 0, 0};\n  MPI_Reduce(&localBins, &globalBins, 4, MPI_LONG, MPI_SUM, 0, MPI_COMM_CART);\n  if (rank == 0) {\n    for (int i = 0; i < 4; i++) {\n      bins[i] = globalBins[i];\n    }\n  }\n\n}",
            "MPI_Datatype point_t;\n    MPI_Type_contiguous(2, MPI_DOUBLE, &point_t);\n    MPI_Type_commit(&point_t);\n\n    auto p = points.data();\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    auto count = [&p](size_t count, Point point) {\n        if (point.x > 0 && point.y > 0) {\n            count++;\n        } else if (point.x < 0 && point.y > 0) {\n            count++;\n        } else if (point.x < 0 && point.y < 0) {\n            count++;\n        } else if (point.x > 0 && point.y < 0) {\n            count++;\n        }\n        return count;\n    };\n\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    size_t part_size = points.size() / size;\n    std::vector<Point> points_rank(part_size);\n    MPI_Scatterv((void *)points.data(), (int *)&part_size, (int *)&point_t, (void *)points_rank.data(),\n                 part_size, point_t, 0, MPI_COMM_WORLD);\n    for (auto i = 0; i < points_rank.size(); i++) {\n        bins[0] = count(bins[0], points_rank[i]);\n    }\n\n    MPI_Reduce(bins.data(), bins.data(), 4, MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n   int n_procs, my_rank, cart_dims[2];\n   MPI_Comm_size(MPI_COMM_WORLD, &n_procs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n   MPI_Dims_create(n_procs, 2, cart_dims);\n   MPI_Comm cart_comm;\n   MPI_Cart_create(MPI_COMM_WORLD, 2, cart_dims, false, false, &cart_comm);\n\n   int my_rank_row, my_rank_col;\n   MPI_Cart_coords(cart_comm, my_rank, 2, &my_rank_row, &my_rank_col);\n\n   int n_rows = cart_dims[0];\n   int n_cols = cart_dims[1];\n\n   // Each rank has a local copy of points\n   std::vector<Point> local_points;\n   size_t points_per_rank = points.size() / n_procs;\n   size_t points_begin = points_per_rank * my_rank;\n   size_t points_end = points_begin + points_per_rank;\n\n   for (size_t i = points_begin; i < points_end; ++i) {\n      local_points.push_back(points[i]);\n   }\n\n   if (my_rank == 0) {\n      for (int i = 1; i < n_procs; ++i) {\n         int recv_rank = i;\n         MPI_Send(&local_points[0], local_points.size(), MPI_INT, recv_rank, 0, cart_comm);\n      }\n   } else {\n      int send_rank = 0;\n      MPI_Status status;\n      MPI_Recv(&local_points[0], local_points.size(), MPI_INT, send_rank, 0, cart_comm, &status);\n   }\n\n   // Get the quadrants for my_rank\n   std::vector<double> local_x(local_points.size());\n   std::vector<double> local_y(local_points.size());\n\n   for (size_t i = 0; i < local_points.size(); ++i) {\n      local_x[i] = local_points[i].x;\n      local_y[i] = local_points[i].y;\n   }\n\n   // Store quadrants on each rank\n   std::vector<double> local_x_quadrant(local_x.size());\n   std::vector<double> local_y_quadrant(local_y.size());\n\n   for (size_t i = 0; i < local_x.size(); ++i) {\n      if (local_x[i] > 0 && local_y[i] > 0) {\n         local_x_quadrant[i] = 0;\n         local_y_quadrant[i] = 0;\n      } else if (local_x[i] > 0 && local_y[i] < 0) {\n         local_x_quadrant[i] = 1;\n         local_y_quadrant[i] = 1;\n      } else if (local_x[i] < 0 && local_y[i] < 0) {\n         local_x_quadrant[i] = 1;\n         local_y_quadrant[i] = 0;\n      } else if (local_x[i] < 0 && local_y[i] > 0) {\n         local_x_quadrant[i] = 0;\n         local_y_quadrant[i] = 1;\n      }\n   }\n\n   // Count the number of points in each quadrant\n   std::vector<int> local_bins(4);\n   for (size_t i = 0; i < local_x_quadrant.size(); ++i) {\n      ++local_bins[local_x_quadrant[i] + local_y_quadrant[i] * 2];\n   }\n\n   // Aggregate the number of points for each quadrant\n   std::vector<",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<Point> local_points;\n\n    if (rank == 0) {\n        std::copy(points.begin(), points.end(), std::back_inserter(local_points));\n    }\n\n    MPI_Bcast(local_points.data(), points.size() * sizeof(Point), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    auto quadrant_count = [](double x, double y) {\n        if (x > 0 && y > 0) {\n            return 0;\n        } else if (x < 0 && y > 0) {\n            return 1;\n        } else if (x < 0 && y < 0) {\n            return 2;\n        } else {\n            return 3;\n        }\n    };\n\n    auto partition = [&](Point const &point, size_t &index) {\n        int quadrant = quadrant_count(point.x, point.y);\n        index = quadrant;\n    };\n\n    auto count = [&](size_t index, size_t &count) {\n        count = 0;\n        for (auto const &point : local_points) {\n            if (partition(point, index) == index) {\n                count++;\n            }\n        }\n    };\n\n    std::array<size_t, 4> local_counts;\n    local_counts.fill(0);\n    std::for_each(local_points.begin(), local_points.end(), [&](Point const &point) {\n        int quadrant = quadrant_count(point.x, point.y);\n        local_counts[quadrant]++;\n    });\n\n    std::array<size_t, 4> global_counts;\n    global_counts.fill(0);\n    MPI_Allreduce(local_counts.data(), global_counts.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    bins = global_counts;\n}",
            "size_t const num_points = points.size();\n    if (num_points == 0) {\n        bins.fill(0);\n        return;\n    }\n\n    // Get the local number of points per rank\n    int npoints_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &npoints_rank);\n\n    // Get the local index of this rank\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Get the number of processes\n    int nprocesses;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocesses);\n\n    // Determine the binning scheme\n    size_t xmax = 10000;\n    size_t ymax = 10000;\n    size_t bin_size_x = xmax / nprocesses;\n    size_t bin_size_y = ymax / nprocesses;\n\n    // Create a 2D array of bin counts\n    std::vector<std::vector<size_t>> counts;\n    for (int i = 0; i < nprocesses; i++) {\n        counts.emplace_back(std::vector<size_t>(2));\n    }\n\n    // Count the number of points in each bin\n    for (size_t i = 0; i < num_points; i++) {\n        auto point = points[i];\n        size_t bin_x = point.x / bin_size_x;\n        size_t bin_y = point.y / bin_size_y;\n        auto& bin = counts[bin_x * nprocesses + bin_y];\n        bin[0]++;\n        bin[1]++;\n    }\n\n    // Combine the bin counts\n    if (rank == 0) {\n        bins.fill(0);\n    }\n    for (int i = 0; i < nprocesses; i++) {\n        auto& bin = counts[rank * nprocesses + i];\n        MPI_Reduce(&bin[0], &bins[0], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        MPI_Reduce(&bin[1], &bins[1], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        MPI_Reduce(&bin[2], &bins[2], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        MPI_Reduce(&bin[3], &bins[3], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO\n   //...\n\n   MPI_Barrier(MPI_COMM_WORLD);\n\n   // TODO\n   //...\n\n}",
            "// TODO: Your code here\n   return;\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n   MPI_Comm_rank(comm, &rank);\n   MPI_Comm_size(comm, &size);\n\n   if (rank == 0) {\n      std::cout << \"countQuadrants\" << std::endl;\n   }\n\n   //...\n\n   // if (rank == 0) {\n   //    std::cout << \"countQuadrants DONE\" << std::endl;\n   // }\n}",
            "// TODO: Replace these two lines with your implementation\n   bins = {0, 0, 0, 0};\n   return;\n}",
            "// TODO\n    // Create a cartesian topology with 2D cartesian topology.\n    // Each process has a quadrant.\n\n    // Get rank and total number of ranks.\n    int rank;\n    int nproc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    // Get cartesian dimensions from the cartesian topology.\n    int ndims;\n    int dims[2];\n    int periods[2];\n    int coords[2];\n    MPI_Cart_get(MPI_COMM_WORLD, ndims, dims, periods, coords);\n\n    // Find the range for each axis.\n    // Get the coordinate and size for each axis.\n    int axis_1 = coords[0];\n    int axis_2 = coords[1];\n    int size_1 = dims[0];\n    int size_2 = dims[1];\n    int lower_x = axis_1 * size_1;\n    int upper_x = lower_x + size_1 - 1;\n    int lower_y = axis_2 * size_2;\n    int upper_y = lower_y + size_2 - 1;\n\n    // Count the number of points in each quadrant.\n    int count_1 = 0;\n    int count_2 = 0;\n    int count_3 = 0;\n    int count_4 = 0;\n    for (auto& point : points) {\n        if (point.x >= lower_x && point.x <= upper_x) {\n            if (point.y >= lower_y && point.y <= upper_y) {\n                count_1++;\n            } else if (point.y < lower_y) {\n                count_3++;\n            } else {\n                count_4++;\n            }\n        } else if (point.x < lower_x) {\n            if (point.y >= lower_y && point.y <= upper_y) {\n                count_2++;\n            } else if (point.y < lower_y) {\n                count_3++;\n            } else {\n                count_4++;\n            }\n        }\n    }\n    bins[0] = count_1;\n    bins[1] = count_2;\n    bins[2] = count_3;\n    bins[3] = count_4;\n\n    // Collect counts from other ranks.\n    if (rank == 0) {\n        int recv[4];\n        MPI_Allgather(bins.data(), 4, MPI_INT, recv, 4, MPI_INT, MPI_COMM_WORLD);\n\n        // Reduce the counts.\n        bins[0] = recv[0] + recv[1] + recv[2] + recv[3];\n        bins[1] = recv[0] + recv[1] + recv[2] + recv[3];\n        bins[2] = recv[0] + recv[1] + recv[2] + recv[3];\n        bins[3] = recv[0] + recv[1] + recv[2] + recv[3];\n    } else {\n        MPI_Reduce(bins.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "MPI_Comm cart_comm =...;\n   // TODO\n}",
            "int nRanks;\n    MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n\n    std::array<size_t, 4> localBins{0, 0, 0, 0};\n    for (auto const& p : points) {\n        if (p.x >= 0 && p.y >= 0) localBins[0]++;\n        else if (p.x >= 0 && p.y < 0) localBins[1]++;\n        else if (p.x < 0 && p.y < 0) localBins[2]++;\n        else localBins[3]++;\n    }\n\n    std::vector<size_t> allBins(4);\n    MPI_Gather(&localBins[0], 4, MPI_UNSIGNED_LONG_LONG, &allBins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n    if (nRanks > 1) {\n        std::array<size_t, 4> tmpBins{0, 0, 0, 0};\n        if (bins.size()!= 4) {\n            bins = tmpBins;\n        }\n\n        if (nRanks == 2) {\n            bins[0] = allBins[0] + allBins[2];\n            bins[1] = allBins[1];\n            bins[2] = allBins[3];\n            bins[3] = allBins[2];\n        } else {\n            int nRanksPerDim = std::sqrt(nRanks);\n            if (nRanksPerDim * nRanksPerDim!= nRanks)\n                throw std::invalid_argument(\"countQuadrants: nRanks must be a perfect square\");\n\n            size_t nbins = allBins[0] + allBins[2] + allBins[1] + allBins[3];\n            bins.resize(nbins);\n\n            std::array<int, 2> dims{0, 0};\n            int nbinsInDim[2];\n            MPI_Dims_create(nRanks, 2, nbinsInDim);\n            dims[0] = nbinsInDim[0];\n            dims[1] = nbinsInDim[1];\n            std::array<int, 2> periods{false, false};\n            int rank;\n\n            MPI_Cart_create(MPI_COMM_WORLD, 2, &dims[0], &periods[0], false, &rank);\n\n            if (rank == 0) {\n                int nRanksInDim = 0;\n                MPI_Cart_rank(MPI_COMM_WORLD, &rank);\n                MPI_Cart_coords(MPI_COMM_WORLD, rank, 2, &dims[0]);\n                MPI_Cart_shift(MPI_COMM_WORLD, 0, 1, &nRanksInDim, &nRanksInDim);\n                std::array<int, 2> nRanksToReceive = {dims[0] - 1, nRanksInDim - 1};\n                MPI_Cart_shift(MPI_COMM_WORLD, 0, 1, &nRanksToReceive[0], &nRanksToReceive[1]);\n\n                // MPI_Recv(MPI_ANY_TAG, 0, MPI_UNSIGNED_LONG_LONG, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &status);\n\n                for (int i = 0; i < 2; i++) {\n                    MPI_Recv(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, nRanksToReceive[i], 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n                }\n                for (int i = 0; i < 4; i++) {\n                    bins[nbins - i - 1] = allBins[i];\n                }\n            } else",
            "int nproc, myrank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n    auto quadrant = [](Point p){\n        return (p.x >= 0 && p.y >= 0)? 0\n             : (p.x <= 0 && p.y >= 0)? 1\n             : (p.x <= 0 && p.y <= 0)? 2\n             : 3;\n    };\n\n    std::vector<int> quadrants;\n    for(auto const& p : points) quadrants.push_back(quadrant(p));\n\n    // count the number of elements in each quadrant\n    int counts[4] = {0};\n    for(auto q : quadrants) {\n        ++counts[q];\n    }\n    std::cout << myrank << \" counts[0] = \" << counts[0] << std::endl;\n    std::cout << myrank << \" counts[1] = \" << counts[1] << std::endl;\n    std::cout << myrank << \" counts[2] = \" << counts[2] << std::endl;\n    std::cout << myrank << \" counts[3] = \" << counts[3] << std::endl;\n\n    // collect the counts\n    MPI_Allreduce(counts, bins.data(), 4, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    // print the output\n    for(int i = 0; i < nproc; ++i) {\n        std::cout << i << \" bins[0] = \" << bins[0] << std::endl;\n        std::cout << i << \" bins[1] = \" << bins[1] << std::endl;\n        std::cout << i << \" bins[2] = \" << bins[2] << std::endl;\n        std::cout << i << \" bins[3] = \" << bins[3] << std::endl;\n    }\n}",
            "int nproc, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    // TODO: Count quadrants\n    //...\n    //...\n    //...\n\n}",
            "// TODO\n}",
            "// FIXME\n    return;\n}",
            "// TODO\n   return;\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    std::vector<Point> local_points;\n\n    if (rank == 0) {\n        size_t idx = 0;\n        for (auto const& point : points) {\n            if (idx % size == rank) {\n                local_points.push_back(point);\n            }\n            idx++;\n        }\n        std::array<size_t, 4> all_bins = bins;\n        for (auto const& point : local_points) {\n            if (point.x >= 0 && point.y >= 0) {\n                all_bins[0]++;\n            }\n            if (point.x < 0 && point.y >= 0) {\n                all_bins[1]++;\n            }\n            if (point.x < 0 && point.y < 0) {\n                all_bins[2]++;\n            }\n            if (point.x >= 0 && point.y < 0) {\n                all_bins[3]++;\n            }\n        }\n        MPI_Allreduce(MPI_IN_PLACE, all_bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n        bins = all_bins;\n    }\n    else {\n        MPI_Bcast(points.data(), points.size(), MPI_BYTE, 0, MPI_COMM_WORLD);\n        std::array<size_t, 4> local_bins;\n        for (auto const& point : points) {\n            if (point.x >= 0 && point.y >= 0) {\n                local_bins[0]++;\n            }\n            if (point.x < 0 && point.y >= 0) {\n                local_bins[1]++;\n            }\n            if (point.x < 0 && point.y < 0) {\n                local_bins[2]++;\n            }\n            if (point.x >= 0 && point.y < 0) {\n                local_bins[3]++;\n            }\n        }\n        MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n\n}",
            "// TODO\n   // compute the number of points in each quadrant\n   // use the MPI_Allreduce to aggregate the counts\n   // write the results in bins\n   int rank, nproc, xdim, ydim, nquadrants;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n   // set up the grid of quadrants\n   // check that the number of ranks matches the number of quadrants\n   // TODO\n\n   // TODO\n   // determine the number of points in each quadrant\n   // TODO\n   // aggregate the counts with MPI_Allreduce\n   // TODO\n   // write the counts in bins\n   // TODO\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Split the ranks into the four quadrants\n    int const quadrant = rank % 4;\n\n    // Determine the index to start the output vector\n    size_t const start = rank / 4;\n\n    // Make a list of the points in the quadrant\n    std::vector<Point> quadrant_points;\n    for (const auto& p : points) {\n        if (quadrant == 0 && p.x >= 0 && p.y >= 0) {\n            quadrant_points.push_back(p);\n        } else if (quadrant == 1 && p.x < 0 && p.y >= 0) {\n            quadrant_points.push_back(p);\n        } else if (quadrant == 2 && p.x < 0 && p.y < 0) {\n            quadrant_points.push_back(p);\n        } else if (quadrant == 3 && p.x >= 0 && p.y < 0) {\n            quadrant_points.push_back(p);\n        }\n    }\n\n    // Figure out the number of points in each quadrant\n    // Each rank contributes points to the quadrant\n    size_t count = 0;\n    for (const auto& p : quadrant_points) {\n        if (p.x >= 0) {\n            ++count;\n        }\n    }\n    size_t n_quadrant_points = count;\n\n    // Gather the counts from all ranks\n    std::vector<size_t> counts(size);\n    MPI_Allgather(&n_quadrant_points, 1, MPI_UNSIGNED_LONG_LONG, counts.data(), 1, MPI_UNSIGNED_LONG_LONG, MPI_COMM_WORLD);\n\n    // Distribute the counts to the proper index in the output vector\n    // Rank 0 has the results of the computation\n    // Rank 0 is responsible for outputting the results\n    if (rank == 0) {\n        for (size_t i = 0; i < size; ++i) {\n            bins[i] = counts[i];\n        }\n    }\n}",
            "// Your code here\n\n    // \u83b7\u53d6\u8fdb\u7a0b\u6570\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // \u8fdb\u7a0b\u6570\u4e3a1\uff0c\u5219\u76f4\u63a5\u8ba1\u7b97\u7ed3\u679c\n    if(size == 1) {\n        for(auto point: points) {\n            int i = 0;\n            if(point.x >= 0 && point.y >= 0) {\n                i = 0;\n            }\n            else if(point.x < 0 && point.y >= 0) {\n                i = 1;\n            }\n            else if(point.x < 0 && point.y < 0) {\n                i = 2;\n            }\n            else if(point.x >= 0 && point.y < 0) {\n                i = 3;\n            }\n\n            ++bins[i];\n        }\n    }\n    // \u8fdb\u7a0b\u6570\u5927\u4e8e1\uff0c\u5219\u6839\u636e\u89c4\u5219\u5206\u914d\u8ba1\u7b97\u4efb\u52a1\n    else {\n        std::vector<Point> send_points(points);\n\n        int delta = points.size() / size;\n\n        // \u6bcf\u4e2a\u8fdb\u7a0b\u5747\u5300\u5730\u5206\u914d\u6570\u636e\n        int start = delta * rank;\n        int end = delta * (rank + 1);\n        if(rank == size - 1) {\n            end = points.size();\n        }\n\n        std::vector<Point> recv_points;\n        std::vector<size_t> recv_count(4);\n        for(auto i = start; i < end; ++i) {\n            int ix = 0;\n            if(points[i].x >= 0 && points[i].y >= 0) {\n                ix = 0;\n            }\n            else if(points[i].x < 0 && points[i].y >= 0) {\n                ix = 1;\n            }\n            else if(points[i].x < 0 && points[i].y < 0) {\n                ix = 2;\n            }\n            else if(points[i].x >= 0 && points[i].y < 0) {\n                ix = 3;\n            }\n\n            ++recv_count[ix];\n        }\n\n        MPI_Gather(recv_count.data(), 4, MPI_INT, bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n        if(rank == 0) {\n            for(int i = 1; i < size; ++i) {\n                for(int j = 0; j < 4; ++j) {\n                    bins[j] += recv_points[j];\n                }\n            }\n        }\n        else {\n            MPI_Gather(send_points.data(), delta, MPI_DOUBLE, nullptr, 0, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        }\n    }\n}",
            "//TODO\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    const double xmin = -2;\n    const double xmax = 4;\n    const double ymin = -2;\n    const double ymax = 4;\n\n    int nx = 2;\n    int ny = 2;\n    int dims[2] = {nx, ny};\n\n    int periods[2] = {false, false};\n\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, true, &comm_quadrant);\n\n    int rank_quadrant;\n    int coords[2];\n\n    MPI_Comm_rank(comm_quadrant, &rank_quadrant);\n    MPI_Cart_coords(comm_quadrant, rank_quadrant, 2, coords);\n\n    int nq = size;\n    int x0, x1;\n    int y0, y1;\n\n    if (coords[0] == 0) {\n        x0 = xmin;\n        x1 = (xmin + xmax) / 2.0;\n    }\n    else {\n        x0 = (xmin + xmax) / 2.0;\n        x1 = xmax;\n    }\n    if (coords[1] == 0) {\n        y0 = ymin;\n        y1 = (ymin + ymax) / 2.0;\n    }\n    else {\n        y0 = (ymin + ymax) / 2.0;\n        y1 = ymax;\n    }\n\n    std::vector<Point> points_local;\n    if (rank == 0) {\n        for (int i = 0; i < points.size(); ++i) {\n            if (points[i].x >= x0 && points[i].x < x1 && points[i].y >= y0 && points[i].y < y1) {\n                points_local.push_back(points[i]);\n            }\n        }\n    }\n    else {\n        points_local = points;\n    }\n\n    int counter_q0 = 0;\n    int counter_q1 = 0;\n    int counter_q2 = 0;\n    int counter_q3 = 0;\n    for (int i = 0; i < points_local.size(); ++i) {\n        if (points_local[i].x >= x0 && points_local[i].x < x1 && points_local[i].y >= y0 && points_local[i].y < y1) {\n            counter_q0++;\n        }\n        if (points_local[i].x >= x1 && points_local[i].x < xmax && points_local[i].y >= y0 && points_local[i].y < y1) {\n            counter_q1++;\n        }\n        if (points_local[i].x >= x0 && points_local[i].x < x1 && points_local[i].y >= y1 && points_local[i].y < ymax) {\n            counter_q2++;\n        }\n        if (points_local[i].x >= x1 && points_local[i].x < xmax && points_local[i].y >= y1 && points_local[i].y < ymax) {\n            counter_q3++;\n        }\n    }\n\n    std::array<size_t, 4> bins_local;\n    bins_local[0] = counter_q0;\n    bins_local[1] = counter_q1;\n    bins_local[2] = counter_q2;\n    bins_local[3] = counter_q3;\n\n    // MPI_Bcast (&bins_local, 4, MPI_INT, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        MPI_Bcast(&bins_local, 4, MPI_LONG_LONG, 0, comm_quadrant);\n        bins = bins_local;\n    }\n    else {\n        MPI_B",
            "// TODO: Your code here\n    return;\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int numProcs, rank;\n    MPI_Comm_size(comm, &numProcs);\n    MPI_Comm_rank(comm, &rank);\n    int numPoints = points.size();\n    int chunkSize = numPoints / numProcs;\n    int start = chunkSize * rank;\n    int end = chunkSize * (rank + 1);\n\n    if (rank == 0) {\n        bins[0] = bins[1] = bins[2] = bins[3] = 0;\n    }\n\n    for (int i = start; i < end; i++) {\n        if (points[i].x >= 0 && points[i].y >= 0) {\n            bins[0]++;\n        } else if (points[i].x < 0 && points[i].y >= 0) {\n            bins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n\n    if (rank == 0) {\n        std::array<size_t, 4> tempBins;\n        MPI_Reduce(MPI_IN_PLACE, &bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Reduce(&bins, &tempBins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n\n    // MPI_Reduce(bins.data(), tempBins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    // std::swap(bins, tempBins);\n    // MPI_Bcast(bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// 2D cartesian grid of quadrants.\n    int ndims = 2;\n    int dims[ndims] = {2, 2};\n    int periods[ndims] = {1, 1};\n    int reorder = 1;\n    MPI_Cart_create(MPI_COMM_WORLD, ndims, dims, periods, reorder, &MPI_CART_COMM);\n    int nprocs;\n    MPI_Comm_size(MPI_CART_COMM, &nprocs);\n    int rank;\n    MPI_Comm_rank(MPI_CART_COMM, &rank);\n    int coords[ndims];\n    MPI_Cart_coords(MPI_CART_COMM, rank, ndims, coords);\n    auto q = std::make_tuple(coords[0], coords[1]);\n\n    // Loop through each quadrant.\n    for (int i = 0; i < 4; i++) {\n        // Find the quadrant of each point.\n        std::array<double, 2> low, high;\n        low[0] = (i == 0 || i == 1)? -1 : 0;\n        low[1] = (i == 0 || i == 2)? -1 : 0;\n        high[0] = (i == 0 || i == 1)? 0 : 1;\n        high[1] = (i == 0 || i == 2)? 0 : 1;\n        std::array<double, 2> pt;\n        bins[i] = 0;\n        for (auto const& p : points) {\n            pt[0] = p.x;\n            pt[1] = p.y;\n            if (pt[0] >= low[0] && pt[0] < high[0] && pt[1] >= low[1] && pt[1] < high[1]) {\n                bins[i]++;\n            }\n        }\n        // Increment the bin counts for the neighboring quadrants.\n        for (int j = 0; j < ndims; j++) {\n            auto new_q = q;\n            if (coords[j] == 0 && low[j] < 0) {\n                new_q = std::make_tuple(0, 0);\n            } else if (coords[j] == dims[j] - 1 && high[j] > 0) {\n                new_q = std::make_tuple(dims[j] - 1, dims[j] - 1);\n            } else if (coords[j] == 0 && low[j] == 0 && high[j] == 0) {\n                new_q = std::make_tuple(1, 1);\n            } else if (coords[j] == dims[j] - 1 && high[j] == 0 && low[j] == 0) {\n                new_q = std::make_tuple(dims[j] - 2, dims[j] - 2);\n            }\n            if (q!= new_q) {\n                auto sendbuf = bins[i];\n                auto recvbuf = 0;\n                MPI_Cart_shift(MPI_CART_COMM, j, 1, &recvbuf, &sendbuf);\n                bins[i] += recvbuf;\n            }\n        }\n    }\n}",
            "size_t size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (rank == 0) {\n        bins.fill(0);\n    }\n    double minX = 0, minY = 0, maxX = 10, maxY = 10;\n\n    for (const auto &point: points) {\n        if (point.x < minX) {\n            minX = point.x;\n        }\n        if (point.x > maxX) {\n            maxX = point.x;\n        }\n        if (point.y < minY) {\n            minY = point.y;\n        }\n        if (point.y > maxY) {\n            maxY = point.y;\n        }\n    }\n\n    int dimX = 2, dimY = 2;\n    int periodX = 1, periodY = 1;\n    MPI_Dims_create(size, 2, &dimX);\n    MPI_Cart_create(MPI_COMM_WORLD, 2, &dimX, &periodX, &periodY, MPI_COMM_CART);\n\n    int x, y;\n    MPI_Cart_coords(MPI_COMM_WORLD, rank, 2, &x, &y);\n\n    double widthX = (maxX - minX) / dimX;\n    double widthY = (maxY - minY) / dimY;\n\n    double bX = minX + x * widthX, bY = minY + y * widthY;\n    double eX = minX + (x + 1) * widthX, eY = minY + (y + 1) * widthY;\n    double sizeX = (eX - bX) / 2, sizeY = (eY - bY) / 2;\n\n    for (const auto &point: points) {\n        if (point.x >= bX && point.x < bX + sizeX && point.y >= bY && point.y < bY + sizeY) {\n            if (point.x < bX + sizeX / 2) {\n                if (point.y < bY + sizeY / 2) {\n                    MPI_Send(&bins[0], 1, MPI_UNSIGNED, rank + dimY, 1, MPI_COMM_WORLD);\n                    bins[0] += 1;\n                } else {\n                    MPI_Send(&bins[2], 1, MPI_UNSIGNED, rank + dimX, 2, MPI_COMM_WORLD);\n                    bins[2] += 1;\n                }\n            } else {\n                if (point.y < bY + sizeY / 2) {\n                    MPI_Send(&bins[1], 1, MPI_UNSIGNED, rank + 1, 3, MPI_COMM_WORLD);\n                    bins[1] += 1;\n                } else {\n                    MPI_Send(&bins[3], 1, MPI_UNSIGNED, rank + dimX, 4, MPI_COMM_WORLD);\n                    bins[3] += 1;\n                }\n            }\n        }\n    }\n\n    if (rank == 0) {\n        MPI_Request requests[4];\n        MPI_Status statuses[4];\n        MPI_Irecv(&bins[0], 1, MPI_UNSIGNED, rank + dimY, 1, MPI_COMM_WORLD, requests + 0);\n        MPI_Irecv(&bins[1], 1, MPI_UNSIGNED, rank + 1, 3, MPI_COMM_WORLD, requests + 1);\n        MPI_Irecv(&bins[2], 1, MPI_UNSIGNED, rank + dimX, 2, MPI_COMM_WORLD, requests + 2);\n        MPI_Irecv(&bins[3], 1, MPI_UNSIGNED, rank + dimX, 4",
            "int my_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    size_t num_points = points.size();\n\n    size_t chunk_size = num_points / 4;\n    size_t left_over = num_points % 4;\n\n    // Determine starting and ending index for this rank\n    size_t start_idx, end_idx;\n    if (my_rank == 0) {\n        start_idx = 0;\n        end_idx = start_idx + chunk_size + left_over;\n    } else {\n        int offset = my_rank - 1;\n        start_idx = chunk_size * offset + chunk_size;\n        end_idx = start_idx + chunk_size + left_over;\n    }\n\n    // Create a count vector\n    size_t count[4] = {0, 0, 0, 0};\n\n    // Count the points in this rank's range\n    for (size_t i = start_idx; i < end_idx; i++) {\n        if (points[i].x >= 0 && points[i].y >= 0) {\n            count[0]++;\n        } else if (points[i].x < 0 && points[i].y >= 0) {\n            count[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            count[2]++;\n        } else {\n            count[3]++;\n        }\n    }\n\n    MPI_Reduce(count, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (my_rank == 0) {\n        std::cout << \"Counts: \" << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << std::endl;\n    }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // create cartesian topology\n    int dims[2] = {size, size};\n    int periods[2] = {false, false};\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, true, &MPI_COMM_QUADRANTS);\n\n    std::vector<Point> myPoints;\n    // split points among ranks\n    int pointsPerRank = points.size() / size;\n    int remaining = points.size() % size;\n    int myStart = pointsPerRank * rank;\n    int mySize = pointsPerRank;\n    if (rank < remaining) {\n        myStart += rank;\n        mySize++;\n    }\n    for (int i = myStart; i < myStart + mySize; i++) {\n        myPoints.push_back(points.at(i));\n    }\n\n    int localBins[4] = {0, 0, 0, 0};\n    // count points in quadrants\n    for (auto& point : myPoints) {\n        int quadrant;\n        if (point.x > 0 && point.y > 0) quadrant = 0;\n        if (point.x > 0 && point.y < 0) quadrant = 1;\n        if (point.x < 0 && point.y < 0) quadrant = 2;\n        if (point.x < 0 && point.y > 0) quadrant = 3;\n        localBins[quadrant]++;\n    }\n\n    // gather counts\n    MPI_Allgather(localBins, 4, MPI_INT, bins.data(), 4, MPI_INT, MPI_COMM_QUADRANTS);\n}",
            "// TODO:\n}",
            "auto size = static_cast<int>(points.size());\n    int rank, num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    std::array<int, 2> local_size;\n    local_size[0] = size / num_ranks;\n    local_size[1] = size % num_ranks;\n\n    int left_border = rank * local_size[0] + local_size[0] * rank;\n    int right_border = left_border + local_size[0] + local_size[1];\n    int num_local_points = local_size[0] + local_size[1];\n\n    std::vector<Point> local_points;\n    for (int i = left_border; i < right_border; ++i) {\n        local_points.push_back(points[i]);\n    }\n    // std::cout << \"points on rank \" << rank << \":\" << std::endl;\n    // for (auto& p : local_points) {\n    //     std::cout << p.x << \", \" << p.y << std::endl;\n    // }\n\n    std::array<size_t, 4> local_bins{0, 0, 0, 0};\n    for (auto& p : local_points) {\n        if (p.x >= 0 && p.y >= 0) {\n            ++local_bins[0];\n        } else if (p.x < 0 && p.y >= 0) {\n            ++local_bins[1];\n        } else if (p.x < 0 && p.y < 0) {\n            ++local_bins[2];\n        } else {\n            ++local_bins[3];\n        }\n    }\n    // std::cout << \"bins on rank \" << rank << \":\" << std::endl;\n    // for (auto b : local_bins) {\n    //     std::cout << b << std::endl;\n    // }\n\n    std::array<size_t, 4> global_bins{0, 0, 0, 0};\n    MPI_Allreduce(local_bins.data(), global_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < num_local_points; ++i) {\n            bins[i] = global_bins[i];\n        }\n    }\n}",
            "int rank = 0, numRanks = 1;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n   // Use a vector of shared_ptrs to store a copy of the data on each process.\n   std::vector<std::shared_ptr<std::vector<Point>>> localData(numRanks);\n\n   for(int i=0; i<numRanks; i++){\n      if(i == rank){\n         localData[rank] = std::make_shared<std::vector<Point>>(points);\n      }\n      else{\n         localData[i] = std::make_shared<std::vector<Point>>();\n      }\n   }\n\n   MPI_Barrier(MPI_COMM_WORLD);\n\n   // Each process uses its local data to count the number of points in its quadrant.\n   // Store the count in the bin corresponding to its quadrant.\n   // For example, if a process is in the first quadrant, and the point {x=1.5, y=0.1} is contained in the point, the bin[0] should be increased by 1.\n   for(size_t i=0; i<points.size(); i++){\n      int q = 0;\n      if(points[i].x >= 0){\n         if(points[i].y >= 0){\n            q = 0;\n         }\n         else{\n            q = 3;\n         }\n      }\n      else{\n         if(points[i].y >= 0){\n            q = 1;\n         }\n         else{\n            q = 2;\n         }\n      }\n      bins[q]++;\n   }\n\n   // MPI_Reduce(sendbuf, recvbuf, count, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n   // A reduction operation is an operation that reduces each element in a message from multiple processes into a single value.\n   // It sums all the elements in an array and stores the result in the element of an array.\n   MPI_Reduce(MPI_IN_PLACE, &bins, 4, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "// TODO: replace this with a proper implementation\n    std::array<double, 4> x_min{0,0,0,0};\n    std::array<double, 4> x_max{1,1,1,1};\n    std::array<double, 4> y_min{0,0,0,0};\n    std::array<double, 4> y_max{1,1,1,1};\n\n    // TODO: initialize x_min, x_max, y_min, y_max\n\n    int nproc, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    std::vector<Point> subpoints;\n    if (rank == 0) {\n        subpoints = points;\n        for (int i = 0; i < nproc; i++) {\n            double x_min_i = x_min[i];\n            double x_max_i = x_max[i];\n            double y_min_i = y_min[i];\n            double y_max_i = y_max[i];\n\n            int count = 0;\n            for (int j = 0; j < points.size(); j++) {\n                if ((points[j].x >= x_min_i) && (points[j].x <= x_max_i) && (points[j].y >= y_min_i) && (points[j].y <= y_max_i)) {\n                    count++;\n                }\n            }\n            bins[i] = count;\n        }\n    } else {\n        MPI_Status status;\n        int count = 0;\n        for (int j = 0; j < points.size(); j++) {\n            if ((points[j].x >= x_min[rank]) && (points[j].x <= x_max[rank]) && (points[j].y >= y_min[rank]) && (points[j].y <= y_max[rank])) {\n                count++;\n            }\n        }\n        MPI_Send(&count, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        std::vector<int> recv_counts(nproc);\n        for (int i = 0; i < nproc; i++) {\n            MPI_Status status;\n            int count;\n            MPI_Recv(&count, 1, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n            recv_counts[i] = count;\n        }\n        int total = 0;\n        for (int i = 0; i < nproc; i++) {\n            total += recv_counts[i];\n        }\n        int s = total / nproc;\n        for (int i = 0; i < nproc; i++) {\n            MPI_Status status;\n            int count = s;\n            if (i < total % nproc) {\n                count++;\n            }\n            MPI_Send(&count, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n        }\n        for (int i = 0; i < nproc; i++) {\n            MPI_Status status;\n            int count;\n            MPI_Recv(&count, 1, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n            bins[i] += count;\n        }\n    } else {\n        MPI_Status status;\n        int count;\n        MPI_Recv(&count, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n        MPI_Send(&count, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "/* Implement this function */\n}",
            "int num_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        std::cout << \"Number of ranks: \" << num_ranks << std::endl;\n    }\n\n    int dims = 2;\n    int periods[dims] = {0, 0};\n\n    MPI_Dims_create(num_ranks, dims, periods);\n\n    MPI_Cart_create(MPI_COMM_WORLD, dims, periods, 0, 0, &MPI_COMM_CART);\n\n    // Create a 2D Cartesian topology\n    int dims_out[2];\n    MPI_Cartdim_get(MPI_COMM_CART, &dims_out[0]);\n    MPI_Cart_get(MPI_COMM_CART, 2, dims_out, periods, &rank);\n\n    if (rank == 0) {\n        std::cout << \"My rank is \" << rank << \" with cartesian coordinates (\" << dims_out[0] << \", \" << dims_out[1] << \").\" << std::endl;\n    }\n\n    // This will be used to receive data\n    std::array<size_t, 4> bins_out;\n\n    // Send data to rank 0\n    if (rank == 0) {\n        for (int i = 1; i < num_ranks; i++) {\n            MPI_Send(&points[0], points.size(), MPI_INT, i, 0, MPI_COMM_CART);\n        }\n    }\n\n    // Receive data from other ranks\n    if (rank!= 0) {\n        MPI_Recv(&bins_out, sizeof(bins_out), MPI_INT, 0, 0, MPI_COMM_CART, MPI_STATUS_IGNORE);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::cout << \"Quadrant counts: \" << bins_out[0] << \", \" << bins_out[1] << \", \" << bins_out[2] << \", \" << bins_out[3] << std::endl;\n    }\n}",
            "MPI_Comm cartcomm = MPI_COMM_NULL;\n   MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &cartcomm);\n   int rank, nproc;\n   MPI_Comm_rank(cartcomm, &rank);\n   MPI_Comm_size(cartcomm, &nproc);\n\n   MPI_Group group_world, group_cart;\n   MPI_Comm_group(MPI_COMM_WORLD, &group_world);\n   MPI_Comm_group(cartcomm, &group_cart);\n\n   // rank 0 gets the count\n   std::array<size_t, 4> local_count = {0,0,0,0};\n   for (auto& p : points) {\n      int quadrant;\n      if (p.x > 0) {\n         if (p.y > 0) quadrant = 0;\n         else quadrant = 1;\n      }\n      else {\n         if (p.y > 0) quadrant = 2;\n         else quadrant = 3;\n      }\n      local_count[quadrant] += 1;\n   }\n\n   size_t global_count[4] = {0,0,0,0};\n   MPI_Allreduce(&local_count, global_count, 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, cartcomm);\n\n   if (rank == 0) {\n      for (int i = 0; i < 4; i++) {\n         bins[i] = global_count[i];\n      }\n   }\n   MPI_Group_free(&group_world);\n   MPI_Group_free(&group_cart);\n   MPI_Comm_free(&cartcomm);\n}",
            "}",
            "// TODO: Your code here\n  // You may need to initialize bins to 0\n  bins = {0, 0, 0, 0};\n  // Loop over all points and increment bins[0] if x >= 0 and y >= 0, bins[1] if x >= 0 and y < 0,\n  // bins[2] if x < 0 and y >= 0, and bins[3] if x < 0 and y < 0\n  for (auto i = 0; i < points.size(); ++i) {\n    if (points[i].x >= 0 && points[i].y >= 0) bins[0]++;\n    else if (points[i].x >= 0 && points[i].y < 0) bins[1]++;\n    else if (points[i].x < 0 && points[i].y >= 0) bins[2]++;\n    else bins[3]++;\n  }\n}",
            "int size = points.size();\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int numprocs;\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\n    int dims[2];\n    dims[0] = 2;\n    dims[1] = 2;\n\n    int periods[2];\n    periods[0] = 0;\n    periods[1] = 0;\n\n    int reorder = 1;\n\n    int coord[2];\n    MPI_Dims_create(numprocs, 2, dims);\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, reorder, &comm);\n    MPI_Cart_coords(comm, rank, 2, coord);\n\n    MPI_Datatype pointType;\n    MPI_Type_contiguous(2, MPI_DOUBLE, &pointType);\n    MPI_Type_commit(&pointType);\n\n    int num = points.size() / 4;\n    int xpos, ypos;\n    MPI_Cart_coords(comm, 0, 2, xpos);\n    MPI_Cart_coords(comm, 0, 2, ypos);\n    std::vector<Point> buffer;\n    if (rank == 0)\n    {\n        for (int i = 0; i < num; i++)\n        {\n            buffer.push_back(points.at(i));\n        }\n    }\n    MPI_Scatterv(&buffer[0], &num, MPI_DOUBLE, &buffer[0], &num, MPI_DOUBLE, 0, comm);\n\n    Point p;\n    int count = 0;\n    for (int i = 0; i < num; i++)\n    {\n        p = buffer.at(i);\n        if (p.x >= 0 && p.y >= 0)\n        {\n            count++;\n        }\n        else if (p.x >= 0 && p.y < 0)\n        {\n            count++;\n        }\n        else if (p.x < 0 && p.y >= 0)\n        {\n            count++;\n        }\n        else if (p.x < 0 && p.y < 0)\n        {\n            count++;\n        }\n    }\n    bins[0] = count;\n\n    int disp[2];\n    MPI_Cart_shift(comm, 1, 1, &disp[1], &disp[0]);\n\n    int recv_count;\n    if (rank == 0)\n    {\n        recv_count = num;\n    }\n    if (rank == disp[1])\n    {\n        MPI_Recv(&buffer[0], &recv_count, pointType, disp[1], 0, comm, MPI_STATUS_IGNORE);\n        count = 0;\n        for (int i = 0; i < num; i++)\n        {\n            p = buffer.at(i);\n            if (p.x >= 0 && p.y >= 0)\n            {\n                count++;\n            }\n            else if (p.x >= 0 && p.y < 0)\n            {\n                count++;\n            }\n            else if (p.x < 0 && p.y >= 0)\n            {\n                count++;\n            }\n            else if (p.x < 0 && p.y < 0)\n            {\n                count++;\n            }\n        }\n        bins[1] = count;\n    }\n\n    MPI_Cart_shift(comm, 0, 1, &disp[1], &disp[0]);\n\n    if (rank == 0)\n    {\n        recv_count = num;\n    }\n    if (rank == disp[0])\n    {\n        MPI_Recv(&buffer[0], &recv_count, pointType, disp[0], 0, comm, MPI_STATUS_IGNORE);\n        count = 0;\n        for (int i = 0; i < num; i++)\n        {",
            "auto points_size = points.size();\n\t//std::cout << \"Size of vector: \" << points_size << std::endl;\n\t\n\tMPI_Status status;\n\tint n_points, n_points_rank;\n\t//n_points = points_size;\n\tn_points = 0;\n\tMPI_Reduce(&points_size, &n_points, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\tn_points_rank = n_points / size;\n\t//std::cout << \"Points per rank: \" << n_points_rank << std::endl;\n\n\t//int n_points_rank = points_size / size;\n\t//int n_points_rank = points_size / size;\n\t//std::cout << \"Points per rank: \" << n_points_rank << std::endl;\n\n\tstd::vector<Point> points_per_rank;\n\tpoints_per_rank.reserve(n_points_rank);\n\n\tint start_index = n_points_rank * rank;\n\tfor (int i = start_index; i < start_index + n_points_rank; ++i) {\n\t\tpoints_per_rank.push_back(points[i]);\n\t}\n\t\n\t//std::vector<Point> points_per_rank = std::vector<Point>(points.begin() + start_index, points.begin() + start_index + n_points_rank);\n\n\tint x_quadrant = 0, y_quadrant = 0;\n\n\tfor (auto point : points_per_rank) {\n\t\tif (point.x < 0) {\n\t\t\tx_quadrant = 1;\n\t\t}\n\t\telse {\n\t\t\tx_quadrant = 2;\n\t\t}\n\n\t\tif (point.y < 0) {\n\t\t\ty_quadrant = 1;\n\t\t}\n\t\telse {\n\t\t\ty_quadrant = 2;\n\t\t}\n\t}\n\n\t//std::cout << \"Quadrant: \" << x_quadrant << y_quadrant << std::endl;\n\n\tMPI_Reduce(&x_quadrant, &bins[0], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\tMPI_Reduce(&y_quadrant, &bins[1], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\tMPI_Reduce(&x_quadrant, &bins[2], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\tMPI_Reduce(&y_quadrant, &bins[3], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\t//std::cout << \"Rank \" << rank << \": \" << x_quadrant << y_quadrant << std::endl;\n\n\t//if (rank == 0) {\n\t//\tstd::cout << \"Rank \" << rank << \": \" << x_quadrant << y_quadrant << std::endl;\n\t//}\n\n\t//std::cout << \"Bins: \" << bins[0] << \" \" << bins[1] << \" \" << bins[2] << \" \" << bins[3] << std::endl;\n\n}",
            "bins.fill(0);\n\n   // TODO: your code here\n\n}",
            "auto n = points.size();\n    std::array<size_t, 4> local_counts;\n    local_counts.fill(0);\n\n    // TODO\n    // MPI_Reduce(local_counts.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    for (size_t i = 0; i < n; ++i) {\n        Point p = points[i];\n        if (p.x >= 0 && p.y >= 0) local_counts[0] += 1;\n        else if (p.x >= 0 && p.y < 0) local_counts[1] += 1;\n        else if (p.x < 0 && p.y >= 0) local_counts[2] += 1;\n        else local_counts[3] += 1;\n    }\n\n    if (bins.data() == nullptr) {\n        bins = local_counts;\n    } else {\n        MPI_Reduce(local_counts.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  //TODO: count quadrants and store in bins\n}",
            "// TODO: add your code here\n   int mpi_rank, mpi_size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\n   int bin = 0;\n   for(size_t i = 0; i < points.size(); i++) {\n      if(points[i].x > 0 && points[i].y > 0) {\n         bin = 0;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         bin = 1;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         bin = 2;\n      } else if (points[i].x > 0 && points[i].y < 0) {\n         bin = 3;\n      }\n      bins[bin]++;\n   }\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        auto const num_processes = points.size();\n        auto const quadrants = num_processes / 4;\n        size_t const points_per_process = num_processes / quadrants;\n        auto const remainder = num_processes % quadrants;\n\n        std::array<size_t, 4> counts;\n        size_t const bins_per_process = 4 / quadrants;\n        std::array<size_t, 4> bins_counts;\n\n        std::vector<std::pair<Point, size_t>> points_and_ranks;\n        points_and_ranks.reserve(points_per_process);\n\n        for (size_t process_index = 0; process_index < quadrants; process_index++) {\n            auto const process_offset = process_index * points_per_process;\n            auto const process_points = points.begin() + process_offset;\n\n            for (size_t i = 0; i < points_per_process; i++) {\n                auto const x = process_points[i].x;\n                auto const y = process_points[i].y;\n\n                if (x < 0 && y < 0) {\n                    points_and_ranks.emplace_back(std::make_pair(process_points[i], 1));\n                    counts[0]++;\n                } else if (x > 0 && y > 0) {\n                    points_and_ranks.emplace_back(std::make_pair(process_points[i], 2));\n                    counts[1]++;\n                } else if (x < 0 && y > 0) {\n                    points_and_ranks.emplace_back(std::make_pair(process_points[i], 3));\n                    counts[2]++;\n                } else {\n                    points_and_ranks.emplace_back(std::make_pair(process_points[i], 4));\n                    counts[3]++;\n                }\n            }\n        }\n\n        if (remainder > 0) {\n            auto const process_offset = quadrants * points_per_process;\n            auto const process_points = points.begin() + process_offset;\n\n            for (size_t i = 0; i < remainder; i++) {\n                auto const x = process_points[i].x;\n                auto const y = process_points[i].y;\n\n                if (x < 0 && y < 0) {\n                    points_and_ranks.emplace_back(std::make_pair(process_points[i], 1));\n                    counts[0]++;\n                } else if (x > 0 && y > 0) {\n                    points_and_ranks.emplace_back(std::make_pair(process_points[i], 2));\n                    counts[1]++;\n                } else if (x < 0 && y > 0) {\n                    points_and_ranks.emplace_back(std::make_pair(process_points[i], 3));\n                    counts[2]++;\n                } else {\n                    points_and_ranks.emplace_back(std::make_pair(process_points[i], 4));\n                    counts[3]++;\n                }\n            }\n        }\n\n        std::vector<std::pair<size_t, size_t>> counts_and_ranks;\n        counts_and_ranks.reserve(counts.size());\n\n        for (size_t process_index = 0; process_index < quadrants; process_index++) {\n            auto const process_offset = process_index * points_per_process;\n            auto const process_counts = counts.begin() + process_offset;\n\n            for (size_t i = 0; i < bins_per_process; i++) {\n                counts_and_ranks.emplace_back(std::make_pair(process_counts[i], process_index + 1));\n            }\n        }\n\n        if (remainder > 0) {\n            auto const",
            "}",
            "int nproc;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    size_t npoints = points.size();\n    size_t my_points = npoints / nproc;\n\n    std::vector<size_t> my_counts(4);\n    for (auto &p : points) {\n        if (p.x > 0 && p.y > 0) {\n            my_counts[0] += 1;\n        } else if (p.x < 0 && p.y > 0) {\n            my_counts[1] += 1;\n        } else if (p.x < 0 && p.y < 0) {\n            my_counts[2] += 1;\n        } else {\n            my_counts[3] += 1;\n        }\n    }\n\n    // Send and receive messages to/from all other ranks.\n    // TODO: add code\n\n    // Compute the sum of all the counts and store it in bins\n    // TODO: add code\n\n    // Reduce the counts to a single value on rank 0\n    // TODO: add code\n\n    // Check that the counts are the same on all ranks\n    // TODO: add code\n}",
            "// FIXME\n  int nproc, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int dims[2] = {nproc, nproc};\n  int periods[2] = {0, 0};\n\n  MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 1, &my_cart);\n\n  std::vector<Point> points_rank;\n  for (int i = 0; i < points.size(); i++)\n    points_rank.push_back(points[i]);\n\n  int quadrant[2];\n  MPI_Cart_coords(my_cart, rank, 2, quadrant);\n\n  MPI_Datatype point_type;\n  MPI_Type_contiguous(2, MPI_DOUBLE, &point_type);\n  MPI_Type_commit(&point_type);\n\n  std::array<Point, 4> quadrant_points;\n\n  MPI_Cart_shift(my_cart, 0, 1, &quadrant_points[0].x, &quadrant_points[2].x);\n  MPI_Cart_shift(my_cart, 1, 1, &quadrant_points[1].y, &quadrant_points[3].y);\n\n  for (int i = 0; i < 4; i++) {\n    int count;\n    MPI_Reduce(&points_rank[0], &count, 1, MPI_INT, MPI_SUM, i, my_cart);\n    bins[i] = count;\n  }\n\n  MPI_Type_free(&point_type);\n\n  MPI_Cart_free(&my_cart);\n}",
            "// TODO\n}",
            "}",
            "}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, size;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &size);\n\n    bins = std::array<size_t, 4> {0};\n\n    double low = 0, high = 0;\n    if (rank == 0) {\n        low = -10;\n        high = 10;\n    }\n    int part_size = size;\n    if (rank == size - 1) part_size = size - rank;\n    size_t n = points.size();\n    size_t part_n = n / part_size;\n\n    std::vector<Point> local_points(points.begin() + rank * part_n,\n                                    points.begin() + (rank + 1) * part_n);\n    for (int i = 0; i < local_points.size(); i++) {\n        Point p = local_points[i];\n        if (p.x > high)\n            bins[3]++;\n        else if (p.x > 0)\n            bins[2]++;\n        else if (p.x < low)\n            bins[1]++;\n        else\n            bins[0]++;\n    }\n\n    MPI_Reduce(MPI_IN_PLACE, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0,\n               MPI_COMM_WORLD);\n}",
            "// Create MPI_COMM_WORLD communicator\n    MPI_Comm comm;\n    MPI_Comm_dup(MPI_COMM_WORLD, &comm);\n\n    // Find the rank and number of processors\n    int nproc, rank;\n    MPI_Comm_size(comm, &nproc);\n    MPI_Comm_rank(comm, &rank);\n\n    // Determine the number of elements to process in the rank\n    const auto chunk_size = std::ceil(points.size() / static_cast<double>(nproc));\n\n    // Calculate the starting and ending points for the rank\n    size_t start = rank * chunk_size;\n    size_t end = start + chunk_size;\n    if(rank == nproc - 1) {\n        end = points.size();\n    }\n\n    // Initialize the array of quadrants\n    bins.fill(0);\n\n    for(size_t i = start; i < end; ++i) {\n        // Get x and y values\n        const auto x = points[i].x;\n        const auto y = points[i].y;\n\n        // Determine the quadrant\n        if (x > 0 && y > 0) {\n            ++bins[0];\n        } else if (x < 0 && y > 0) {\n            ++bins[1];\n        } else if (x < 0 && y < 0) {\n            ++bins[2];\n        } else if (x > 0 && y < 0) {\n            ++bins[3];\n        }\n    }\n\n    // Use MPI to reduce the counts from all ranks\n    MPI_Reduce(MPI_IN_PLACE, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, comm);\n\n    // Deallocate memory\n    MPI_Comm_free(&comm);\n}",
            "MPI_Init(nullptr, nullptr);\n    int size = -1, rank = -1;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    bins.fill(0);\n\n    MPI_Request req[2];\n\n    // Step 1: exchange points with other processes\n    size_t num_points = points.size();\n    size_t num_points_per_proc = num_points / size;\n    size_t num_points_to_send = 0;\n\n    if (rank == 0) {\n        std::vector<Point> points_to_send;\n        for (int i = 1; i < size; i++) {\n            for (int j = 0; j < num_points_per_proc; j++) {\n                points_to_send.push_back(points[i * num_points_per_proc + j]);\n            }\n            num_points_to_send += num_points_per_proc;\n        }\n        MPI_Isend(&points_to_send[0], num_points_to_send, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &req[0]);\n        MPI_Recv(&points[num_points], num_points, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    } else {\n        MPI_Isend(&points[num_points_per_proc * (rank - 1)], num_points_per_proc, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &req[0]);\n        MPI_Recv(&points[num_points_per_proc * rank], num_points_per_proc, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        MPI_Isend(&points[num_points_per_proc * rank], num_points_per_proc, MPI_DOUBLE, (rank + 1) % size, 0, MPI_COMM_WORLD, &req[1]);\n        MPI_Recv(&points[num_points_per_proc * rank + num_points_per_proc], num_points_per_proc, MPI_DOUBLE, (rank - 1) % size, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    // Step 2: perform a local count\n    for (auto i : points) {\n        if (i.x > 0 && i.y > 0) {\n            bins[0]++;\n        } else if (i.x < 0 && i.y > 0) {\n            bins[1]++;\n        } else if (i.x < 0 && i.y < 0) {\n            bins[2]++;\n        } else if (i.x > 0 && i.y < 0) {\n            bins[3]++;\n        }\n    }\n\n    // Step 3: reduce the bins to a single value on rank 0.\n    if (rank == 0) {\n        for (auto i = 1; i < size; i++) {\n            MPI_Status status;\n            MPI_Recv(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, &status);\n        }\n    } else {\n        MPI_Status status;\n        MPI_Send(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    MPI_Finalize();\n}",
            "int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   bins.fill(0);\n\n   // determine the number of ranks in the Y direction\n   int dims[2];\n   MPI_Dims_create(MPI_COMM_WORLD, 2, dims);\n   int ny = dims[1];\n\n   // determine the number of ranks in the X direction\n   int dims_x[2];\n   MPI_Dims_create(MPI_COMM_WORLD, 1, dims_x);\n   int nx = dims_x[1];\n\n   // determine the rank's coordinates in the 2D grid\n   int coords[2];\n   MPI_Cart_coords(MPI_COMM_WORLD, rank, 2, coords);\n\n   // determine the X range of this rank\n   int low_x = coords[1] * nx;\n   int high_x = low_x + nx;\n\n   // determine the Y range of this rank\n   int low_y = coords[0] * ny;\n   int high_y = low_y + ny;\n\n   // loop over all points\n   for (auto const& p : points) {\n      // determine the quadrant\n      int qx = (p.x >= 0.0)? 1 : 2;\n      int qy = (p.y >= 0.0)? 1 : 2;\n\n      // determine the rank that contains this quadrant\n      int i = 1 + qx + 2 * qy - 3;\n\n      // determine the local coordinates in the 2D quadrant\n      double x = p.x - low_x;\n      double y = p.y - low_y;\n\n      // determine the local index of this point in the quadrant\n      int ix = int(x / nx);\n      int iy = int(y / ny);\n\n      // increment the counter\n      bins[i] += 1;\n   }\n\n   // sum the counts\n   if (rank == 0) {\n      MPI_Reduce(MPI_IN_PLACE, bins.data(), 4, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n   } else {\n      MPI_Reduce(bins.data(), bins.data(), 4, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n}",
            "int npoints = points.size();\n   int rank, num_ranks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n   // compute number of points per rank\n   int count = npoints / num_ranks;\n   int extra = npoints % num_ranks;\n\n   // compute the local point range\n   int start = rank * count;\n   if (rank < extra)\n      start += rank;\n   else\n      start += extra;\n   int end = start + count;\n   if (rank < extra)\n      end += 1;\n\n   // compute quadrant\n   for (int i = start; i < end; ++i) {\n      auto& point = points[i];\n      if (point.x >= 0 && point.y >= 0) {\n         bins[0]++;\n      } else if (point.x >= 0 && point.y < 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y >= 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n\n   // MPI_Gather() is used here to accumulate quadrants across MPI ranks.\n   // For gathering, the buffers of senders and receivers must be the same size.\n   MPI_Gather(&bins[0], 4, MPI_UNSIGNED, &bins[0], 4, MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n   if (rank == 0) {\n      for (int i = 0; i < num_ranks; ++i) {\n         std::cout << \"Rank \" << i << \": \" << bins[i] << std::endl;\n      }\n   }\n}",
            "// Your code here.\n\n}",
            "int numProcs, myRank;\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n\n    if (numProcs < 2) {\n        bins.fill(points.size());\n        return;\n    }\n\n    int dims[] = {2, 2};\n    MPI_Dims_create(numProcs, 2, dims);\n\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, true, true, &MPI_COMM_WORLD);\n\n    auto const xBin = [](Point const& p) {\n        return p.x >= 0.0? 0 : 1;\n    };\n\n    auto const yBin = [](Point const& p) {\n        return p.y >= 0.0? 0 : 1;\n    };\n\n    int myCoords[2];\n    MPI_Cart_coords(MPI_COMM_WORLD, myRank, 2, myCoords);\n\n    int xBinIndex = xBin(points[0]);\n    int yBinIndex = yBin(points[0]);\n\n    std::array<size_t, 4> binCounts{};\n\n    for (auto & p : points) {\n        int x = xBin(p);\n        int y = yBin(p);\n\n        binCounts[x * 2 + y]++;\n\n        xBinIndex = std::min(xBinIndex, x);\n        yBinIndex = std::min(yBinIndex, y);\n    }\n\n    int coords[2];\n    MPI_Cart_coords(MPI_COMM_WORLD, 0, 2, coords);\n\n    MPI_Allreduce(MPI_IN_PLACE, binCounts.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    MPI_Allreduce(MPI_IN_PLACE, &xBinIndex, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    MPI_Allreduce(MPI_IN_PLACE, &yBinIndex, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n\n    int const xPeriods = coords[0] * 2 + 1;\n    int const yPeriods = coords[1] * 2 + 1;\n\n    for (int i = 0; i < xPeriods; i++) {\n        for (int j = 0; j < yPeriods; j++) {\n            int bin = i * 2 + j;\n            if (binCounts[bin] == 0) continue;\n\n            int xRank, yRank;\n            MPI_Cart_rank(MPI_COMM_WORLD, coords, &xRank);\n            MPI_Cart_rank(MPI_COMM_WORLD, coords, &yRank);\n\n            MPI_Send(binCounts.data(), 4, MPI_UNSIGNED_LONG_LONG, yRank, 1, MPI_COMM_WORLD);\n            MPI_Send(binCounts.data(), 4, MPI_UNSIGNED_LONG_LONG, xRank, 1, MPI_COMM_WORLD);\n        }\n    }\n\n    if (myRank == 0) {\n        bins[xBinIndex * 2 + yBinIndex] = std::accumulate(binCounts.begin(), binCounts.end(), 0);\n    }\n\n    MPI_Finalize();\n}",
            "// TODO: Your code here\n}",
            "int rank;\n  int size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  if (rank == 0) {\n    //TODO:\n    //  - create a 2D cartesian topology with 2 rows and 2 columns\n    //  - count the points in the quadrants, which are in the corners of the matrix\n    //  - store the count for each quadrant in the bins\n    //  - use MPI_Scatterv to distribute the counts to all processes\n    //    - sendcounts: the number of points in each quadrant, for example {3, 1, 0, 2}\n    //    - senddispls: the index of the first point in each quadrant, for example {0, 3, 3, 6}\n    //    - sendtype: MPI_LONG_LONG, the type of each count, for example MPI_LONG_LONG\n    //    - recvcount: the total number of counts, for example 4\n    //    - recvtype: MPI_LONG_LONG, the type of the counts, for example MPI_LONG_LONG\n    //    - recvbuf: the array to receive the counts, for example &bins[0]\n\n    //create cartesian topology\n    int dims[2] = {2,2};\n    int periods[2] = {false,false};\n    MPI_Cart_create(MPI_COMM_WORLD,2,dims,periods,false,&bins);\n\n    //count points in quadrants\n    for (int i=0; i<points.size(); i++){\n      if (points[i].x >= 0 && points[i].y >= 0)\n        bins[0] += 1;\n      else if (points[i].x < 0 && points[i].y >= 0)\n        bins[1] += 1;\n      else if (points[i].x < 0 && points[i].y < 0)\n        bins[2] += 1;\n      else if (points[i].x >= 0 && points[i].y < 0)\n        bins[3] += 1;\n    }\n\n    //store counts in bins\n    int counts[4] = {bins[0], bins[1], bins[2], bins[3]};\n    int displs[4] = {0, bins[0], bins[0]+bins[1], bins[0]+bins[1]+bins[2]};\n\n    //use MPI_Scatterv to distribute counts to all ranks\n    MPI_Scatterv(counts,displs,2,MPI_LONG_LONG,&bins[0],4,MPI_LONG_LONG,0,MPI_COMM_WORLD);\n  }\n  else{\n    //TODO:\n    //  - count the points in the quadrants, which are in the corners of the matrix\n    //  - store the count for each quadrant in the bins\n    //  - use MPI_Scatterv to distribute the counts to all processes\n    //    - sendcounts: the number of points in each quadrant, for example {3, 1, 0, 2}\n    //    - senddispls: the index of the first point in each quadrant, for example {0, 3, 3, 6}\n    //    - sendtype: MPI_LONG_LONG, the type of each count, for example MPI_LONG_LONG\n    //    - recvcount: the total number of counts, for example 4\n    //    - recvtype: MPI_LONG_LONG, the type of the counts, for example MPI_LONG_LONG\n    //    - recvbuf: the array to receive the counts, for example &bins[0]\n  }\n\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n   int rank, size;\n   MPI_Comm_rank(comm, &rank);\n   MPI_Comm_size(comm, &size);\n\n   const size_t n = points.size();\n\n   // 2D distribution\n   int dims[2] = {size, 1};\n   int periods[2] = {0, 0};\n   int coords[2];\n\n   // MPI_Cart_create(comm, 2, dims, periods, false, comm);\n   MPI_Dims_create(size, 2, dims);\n   MPI_Cart_create(comm, 2, dims, periods, false, comm);\n   MPI_Cart_coords(comm, rank, 2, coords);\n\n   size_t myBins[4] = {0};\n   size_t totalPoints = 0;\n\n   for (size_t i = 0; i < n; i++) {\n      Point p = points[i];\n      if (p.x >= 0 && p.y >= 0) {\n         myBins[0]++;\n      }\n      if (p.x < 0 && p.y < 0) {\n         myBins[1]++;\n      }\n      if (p.x < 0 && p.y >= 0) {\n         myBins[2]++;\n      }\n      if (p.x >= 0 && p.y < 0) {\n         myBins[3]++;\n      }\n      totalPoints++;\n   }\n\n   MPI_Allreduce(myBins, bins.data(), 4, MPI_LONG_LONG_INT, MPI_SUM, comm);\n\n   MPI_Comm_free(&comm);\n}",
            "size_t n = points.size();\n   std::array<size_t, 4> bins_local = {0};\n\n   // TODO: parallelize using MPI\n   for (auto &point : points) {\n      double x = point.x;\n      double y = point.y;\n      if (x > 0.0 && y > 0.0) bins_local[0]++;\n      else if (x < 0.0 && y > 0.0) bins_local[1]++;\n      else if (x < 0.0 && y < 0.0) bins_local[2]++;\n      else bins_local[3]++;\n   }\n   // TODO: compute bins[0] = sum_i bins_local[i]\n   // TODO: compute bins[1] =...\n   // TODO: compute bins[2] =...\n   // TODO: compute bins[3] =...\n\n   MPI_Reduce(&bins_local, &bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int world_size = 0;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   int world_rank = 0;\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n   if (world_rank == 0) {\n      int dims[1] = {2};\n      int periods[1] = {0};\n      MPI_Cart_create(MPI_COMM_WORLD, 1, dims, periods, 0, &MPI_COMM_QUADRANTS);\n      std::array<int, 2> coord = {0, 0};\n      MPI_Cart_coords(MPI_COMM_QUADRANTS, world_rank, 2, coord.data());\n\n      MPI_Group group_world, group_quadrants;\n      MPI_Comm_group(MPI_COMM_WORLD, &group_world);\n      MPI_Comm_group(MPI_COMM_QUADRANTS, &group_quadrants);\n      MPI_Group_incl(group_world, points.size(), world_rank, &group_quadrants);\n      MPI_Comm_create(MPI_COMM_WORLD, group_quadrants, &MPI_COMM_QUADRANTS_LOCAL);\n   }\n\n   int dims[1] = {2};\n   int periods[1] = {0};\n   MPI_Cart_sub(MPI_COMM_QUADRANTS, coord.data(), &MPI_COMM_QUADRANTS_LOCAL);\n\n   std::vector<Point> points_local;\n   std::array<size_t, 4> bins_local;\n   for (int i = 0; i < points.size(); i++) {\n      Point p = points[i];\n      if (p.x >= 0) {\n         if (p.y >= 0) {\n            bins_local[0]++;\n         } else {\n            bins_local[3]++;\n         }\n      } else {\n         if (p.y >= 0) {\n            bins_local[1]++;\n         } else {\n            bins_local[2]++;\n         }\n      }\n   }\n\n   MPI_Reduce(bins_local.data(), bins.data(), 4, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_QUADRANTS);\n}",
            "int num_ranks, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // if not root rank, return\n    if (rank!= 0)\n        return;\n\n    int bin = 0;\n    int quadrant = 0;\n\n    for (Point& p : points) {\n        if (p.x >= 0)\n            if (p.y >= 0)\n                quadrant = 0;\n            else\n                quadrant = 3;\n        else\n            if (p.y >= 0)\n                quadrant = 1;\n            else\n                quadrant = 2;\n        bin = quadrant;\n        bins[bin] += 1;\n    }\n\n    return;\n}",
            "const int MPI_COMM_WORLD_size = 4;\n\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // bins[0] counts points with y > 0 and x > 0\n    // bins[1] counts points with y > 0 and x = 0\n    // bins[2] counts points with y < 0 and x < 0\n    // bins[3] counts points with y < 0 and x = 0\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; i++) {\n            bins[i] = 0;\n        }\n        // Count the number of points in each quadrant.\n        for (auto& p : points) {\n            if (p.x > 0 && p.y > 0) {\n                bins[0]++;\n            } else if (p.x < 0 && p.y > 0) {\n                bins[1]++;\n            } else if (p.x < 0 && p.y < 0) {\n                bins[2]++;\n            } else if (p.x > 0 && p.y < 0) {\n                bins[3]++;\n            }\n        }\n    }\n\n    // Broadcast the bins to other ranks.\n    MPI_Bcast(bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int n = 4;\n    int rank;\n    int size;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_bins[4];\n\n    for (size_t i = 0; i < 4; i++)\n        local_bins[i] = 0;\n\n    size_t bins_per_rank = points.size() / size;\n    for (int i = 0; i < size; i++) {\n        for (size_t j = 0; j < bins_per_rank; j++) {\n            int x = points[i * bins_per_rank + j].x;\n            int y = points[i * bins_per_rank + j].y;\n            if (x >= 0 && y >= 0)\n                local_bins[0]++;\n            else if (x < 0 && y >= 0)\n                local_bins[1]++;\n            else if (x < 0 && y < 0)\n                local_bins[2]++;\n            else if (x >= 0 && y < 0)\n                local_bins[3]++;\n        }\n    }\n    MPI_Allreduce(MPI_IN_PLACE, local_bins, n, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n    bins = local_bins;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tint dims[2] = {4, 2};\n\tint periods[2] = {false, true};\n\tMPI_Dims_create(size, 2, dims);\n\tint coords[2];\n\tMPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, true, MPI_INFO_NULL, &cart_comm);\n\tMPI_Cart_coords(cart_comm, rank, 2, coords);\n\tint dim_size = dims[0] * dims[1];\n\tstd::vector<Point> local_points(points.begin() + coords[0] * dim_size, points.begin() + coords[0] * dim_size + dim_size);\n\tfor (int i = 0; i < 2; ++i) {\n\t\tfor (int j = 0; j < 2; ++j) {\n\t\t\tif (i == 0 && j == 0) continue;\n\t\t\tMPI_Cart_shift(cart_comm, i, j, &rank, &rank);\n\t\t\tstd::vector<Point> recv_points;\n\t\t\tMPI_Recv(&recv_points[0], 0, MPI_POINT, rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t\tlocal_points.insert(local_points.end(), recv_points.begin(), recv_points.end());\n\t\t}\n\t}\n\tstd::vector<Point> sorted_points;\n\tint quadrants[4] = {0, 0, 0, 0};\n\tfor (auto& p : local_points) {\n\t\tif (p.x < 0) {\n\t\t\tif (p.y < 0) {\n\t\t\t\tquadrants[0]++;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tquadrants[1]++;\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\tif (p.y < 0) {\n\t\t\t\tquadrants[2]++;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tquadrants[3]++;\n\t\t\t}\n\t\t}\n\t}\n\tbins[0] = quadrants[0];\n\tbins[1] = quadrants[1];\n\tbins[2] = quadrants[2];\n\tbins[3] = quadrants[3];\n\tMPI_Reduce(&quadrants[0], &bins[0], 4, MPI_INT, MPI_SUM, 0, cart_comm);\n}",
            "// TODO\n}",
            "MPI_Comm cart_comm;\n   MPI_Cart_create(MPI_COMM_WORLD, 2, dimension, period, false, &cart_comm);\n\n   int rank, num_proc;\n   MPI_Comm_rank(cart_comm, &rank);\n   MPI_Comm_size(cart_comm, &num_proc);\n   int n = points.size();\n\n   if (rank == 0) {\n      bins.fill(0);\n   }\n\n   // Broadcast the size of the points vector to all ranks.\n   MPI_Bcast(&n, 1, MPI_INT, 0, cart_comm);\n\n   std::vector<Point> local_points;\n   int x, y;\n\n   // Split the points vector into local_points.\n   MPI_Cart_coords(cart_comm, rank, 2, &x, &y);\n   for (int i = 0; i < n; i++) {\n      Point p = points[i];\n      if (x == p.x > 0) {\n         local_points.push_back(p);\n      }\n   }\n\n   int size = local_points.size();\n   MPI_Allreduce(MPI_IN_PLACE, &size, 1, MPI_INT, MPI_SUM, cart_comm);\n\n   int bin = 0;\n\n   // Count the number of points in each quadrant.\n   for (int i = 0; i < size; i++) {\n      Point p = local_points[i];\n      if (p.x >= 0) {\n         if (p.y >= 0) {\n            bin = 0;\n         }\n         else {\n            bin = 1;\n         }\n      }\n      else {\n         if (p.y >= 0) {\n            bin = 2;\n         }\n         else {\n            bin = 3;\n         }\n      }\n\n      MPI_Reduce(&bin, &bins[0], 1, MPI_INT, MPI_SUM, 0, cart_comm);\n   }\n\n   MPI_Barrier(cart_comm);\n   if (rank == 0) {\n      MPI_Finalize();\n   }\n\n   //bins = {3, 1, 0, 2};\n\n   MPI_Barrier(cart_comm);\n   MPI_Finalize();\n}",
            "// TODO\n}",
            "// Write your code here\n  // The implementation should be in the `points_on_quadrants.cpp` file.\n  // The main function should be in the `points_on_quadrants.hpp` file.\n  // You should not modify `points_on_quadrants.cpp` or `points_on_quadrants.hpp` files.\n\n\n\n  return;\n}",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int* bin_count = new int[4]();\n    int n = points.size();\n    if(n == 0) {\n        bins = std::array<size_t, 4>();\n        return;\n    }\n    int num_per_rank = n/size;\n    int leftovers = n%size;\n    int start = rank*num_per_rank + std::min(rank, leftovers);\n    int end = start + num_per_rank;\n    if(rank == size - 1) {\n        end += leftovers;\n    }\n\n    for (int i = start; i < end; i++) {\n        Point p = points[i];\n        int x = (p.x < 0)? 0 : (p.x > 0)? 2 : 1;\n        int y = (p.y < 0)? 0 : (p.y > 0)? 2 : 1;\n        bin_count[x + y * 2]++;\n    }\n\n    if(rank == 0) {\n        for(int i = 0; i < 4; i++) {\n            MPI_Reduce(MPI_IN_PLACE, &bin_count[i], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n        }\n        for(int i = 0; i < 4; i++) {\n            bins[i] = bin_count[i];\n        }\n    }\n    else {\n        MPI_Reduce(bin_count, bin_count, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   // TODO: your code here\n   MPI_Barrier(MPI_COMM_WORLD);\n   std::cout << rank << \" before: \" << std::endl;\n   for (int i = 0; i < points.size(); i++)\n   {\n      if (points[i].x > 0.0 && points[i].y > 0.0)\n      {\n         bins[0]++;\n      }\n      else if (points[i].x < 0.0 && points[i].y > 0.0)\n      {\n         bins[1]++;\n      }\n      else if (points[i].x < 0.0 && points[i].y < 0.0)\n      {\n         bins[2]++;\n      }\n      else\n      {\n         bins[3]++;\n      }\n   }\n   MPI_Barrier(MPI_COMM_WORLD);\n   if (rank == 0)\n   {\n      for (int i = 0; i < 4; i++)\n      {\n         std::cout << \"bins[\" << i << \"]=\" << bins[i] << std::endl;\n      }\n   }\n}",
            "// Initialize MPI\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    // Get cartesian topology of size rows x cols.\n    // Use MPI_Dims_create to compute this.\n    int dims[2], periods[2];\n    MPI_Dims_create(size, 2, dims);\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 0, &MPI_COMM_CART);\n    // Get cartesian coordinates of rank.\n    int coords[2];\n    MPI_Cart_coords(MPI_COMM_CART, rank, 2, coords);\n    // Get ranks of neighbours.\n    int neighbour[4];\n    MPI_Cart_shift(MPI_COMM_CART, 0, 1, &neighbour[0], &neighbour[2]);\n    MPI_Cart_shift(MPI_COMM_CART, 1, 1, &neighbour[1], &neighbour[3]);\n    // Get rank of neighbour if in top row.\n    MPI_Cart_shift(MPI_COMM_CART, 0, 1, &neighbour[4], &neighbour[5]);\n    // Get rank of neighbour if in left column.\n    MPI_Cart_shift(MPI_COMM_CART, 1, 1, &neighbour[6], &neighbour[7]);\n    int x, y;\n    x = coords[0];\n    y = coords[1];\n    // Create local array to store local counts.\n    int counts[4];\n    counts[0] = 0;\n    counts[1] = 0;\n    counts[2] = 0;\n    counts[3] = 0;\n    for (int i = 0; i < points.size(); i++) {\n        if (points[i].x > 0) {\n            if (points[i].y > 0) {\n                counts[0]++;\n            }\n            else if (points[i].y < 0) {\n                counts[3]++;\n            }\n        }\n        else if (points[i].x < 0) {\n            if (points[i].y > 0) {\n                counts[1]++;\n            }\n            else if (points[i].y < 0) {\n                counts[2]++;\n            }\n        }\n    }\n    // Send counts to all neighbours.\n    MPI_Send(counts, 4, MPI_INT, neighbour[0], 0, MPI_COMM_WORLD);\n    MPI_Send(counts, 4, MPI_INT, neighbour[1], 0, MPI_COMM_WORLD);\n    MPI_Send(counts, 4, MPI_INT, neighbour[2], 0, MPI_COMM_WORLD);\n    MPI_Send(counts, 4, MPI_INT, neighbour[3], 0, MPI_COMM_WORLD);\n    MPI_Send(counts, 4, MPI_INT, neighbour[4], 0, MPI_COMM_WORLD);\n    MPI_Send(counts, 4, MPI_INT, neighbour[5], 0, MPI_COMM_WORLD);\n    MPI_Send(counts, 4, MPI_INT, neighbour[6], 0, MPI_COMM_WORLD);\n    MPI_Send(counts, 4, MPI_INT, neighbour[7], 0, MPI_COMM_WORLD);\n    // Get counts from all neighbours.\n    int count_data[4];\n    MPI_Recv(count_data, 4, MPI_INT, neighbour[0], 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    MPI_Recv(count_data, 4, MPI_INT, neighbour[1], 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    MPI_",
            "int n;\n    int dims[2];\n    int coords[2];\n    int size;\n    int rank;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    MPI_Dims_create(size, 2, dims);\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, NULL, true, &n);\n\n    MPI_Cart_coords(n, rank, 2, coords);\n\n    std::vector<Point> subPoints;\n    std::vector<double> subX;\n    std::vector<double> subY;\n\n    for (int i = 0; i < points.size(); i++) {\n        if (coords[0] == (points[i].x > 0? 0 : 1) &&\n            coords[1] == (points[i].y > 0? 0 : 1)) {\n            subPoints.push_back(points[i]);\n        }\n    }\n\n    int subSize;\n    int subRank;\n\n    MPI_Comm_size(n, &subSize);\n    MPI_Comm_rank(n, &subRank);\n\n    int *bin = new int[4];\n    int x = subRank % dims[0];\n    int y = subRank / dims[0];\n\n    for (int i = 0; i < subPoints.size(); i++) {\n        if (subPoints[i].x > 0 && subPoints[i].y > 0)\n            bin[0]++;\n        else if (subPoints[i].x > 0 && subPoints[i].y < 0)\n            bin[1]++;\n        else if (subPoints[i].x < 0 && subPoints[i].y < 0)\n            bin[2]++;\n        else if (subPoints[i].x < 0 && subPoints[i].y > 0)\n            bin[3]++;\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < size; i++) {\n            MPI_Recv(bins.data(), 4, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(bin, 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < size; i++) {\n            MPI_Send(&bins[0], 4, MPI_INT, i, 1, MPI_COMM_WORLD);\n        }\n    } else {\n        MPI_Recv(&bins[0], 4, MPI_INT, 0, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    MPI_Finalize();\n    return;\n}",
            "constexpr int DIM = 2;\n    int rank, nranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n\n    MPI_Datatype pointType;\n    MPI_Type_contiguous(sizeof(Point), MPI_CHAR, &pointType);\n    MPI_Type_commit(&pointType);\n\n    int npoints = points.size();\n    int chunk = npoints / nranks;\n\n    int start = rank * chunk;\n    int end = start + chunk;\n\n    if (rank == nranks - 1) end = npoints;\n\n    // Initialize the count for this rank\n    for (int i = 0; i < 4; ++i) {\n        bins[i] = 0;\n    }\n\n    // First pass:\n    // Count the number of points in each quadrant\n    for (int i = start; i < end; ++i) {\n        double x = points[i].x;\n        double y = points[i].y;\n\n        int quadrant = 0;\n        if (x >= 0) {\n            if (y >= 0) {\n                quadrant = 0;\n            } else {\n                quadrant = 3;\n            }\n        } else {\n            if (y >= 0) {\n                quadrant = 1;\n            } else {\n                quadrant = 2;\n            }\n        }\n\n        bins[quadrant]++;\n    }\n\n    // Barrier: wait for all ranks to finish first pass\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    // Second pass:\n    // Gather counts for all quadrants.\n    // Note: use MPI_ALLGATHER\n    // https://mpi-forum.org/docs/mpi-3.0/mpi30-report/node120.htm\n    MPI_Allgather(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, &bins[0], 4 * sizeof(size_t), MPI_CHAR, MPI_COMM_WORLD);\n\n    MPI_Type_free(&pointType);\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // find the maximum x value\n   double maxx = 0.0;\n   for (auto &p : points) {\n      if (p.x > maxx)\n         maxx = p.x;\n   }\n\n   // get the number of points in each quadrant\n   // TODO: use MPI_Allreduce() to do this\n\n   // create a histogram based on quadrants\n   // TODO: use MPI_Allgather() to do this\n\n   // only rank 0 will get the results\n   if (rank == 0) {\n      for (int i = 0; i < size; ++i) {\n         if (i == 0) {\n            bins[i] = (hist[0] + hist[1]) / 2.0;\n         } else if (i == 1) {\n            bins[i] = (hist[1] + hist[2]) / 2.0;\n         } else if (i == 2) {\n            bins[i] = (hist[2] + hist[3]) / 2.0;\n         } else {\n            bins[i] = (hist[3] + hist[0]) / 2.0;\n         }\n      }\n   }\n}",
            "size_t nproc;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    int dims[2] = {nproc, nproc};\n\n    MPI_Dims_create(nproc, 2, dims);\n\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, false, false, &comm);\n\n    int coords[2];\n    MPI_Cart_coords(comm, rank, 2, coords);\n\n    std::array<size_t, 4> local_bins{0};\n\n    for (auto &p : points) {\n        int quadrant;\n        if (p.x < 0 && p.y > 0)\n            quadrant = 0;\n        else if (p.x > 0 && p.y > 0)\n            quadrant = 1;\n        else if (p.x < 0 && p.y < 0)\n            quadrant = 2;\n        else\n            quadrant = 3;\n\n        local_bins[quadrant]++;\n    }\n\n    MPI_Gather(&local_bins[0], 4, MPI_LONG_LONG_INT, &bins[0], 4, MPI_LONG_LONG_INT, 0, comm);\n}",
            "}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n  int rank, size;\n  MPI_Comm_rank(comm, &rank);\n  MPI_Comm_size(comm, &size);\n\n  // calculate the quadrant for each point and count them\n  size_t local_bins[4] = {0, 0, 0, 0};\n  for(auto const& p : points) {\n    int x_quadrant, y_quadrant;\n    if(p.x > 0.0) {\n      x_quadrant = 0;\n    } else {\n      x_quadrant = 1;\n    }\n    if(p.y > 0.0) {\n      y_quadrant = 0;\n    } else {\n      y_quadrant = 1;\n    }\n    local_bins[x_quadrant + 2*y_quadrant]++;\n  }\n\n  // sum the counts\n  size_t counts[4] = {0, 0, 0, 0};\n  for(int i = 0; i < 4; ++i) {\n    MPI_Reduce(&local_bins[i], &counts[i], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, i, comm);\n  }\n  if(rank == 0) {\n    bins[0] = counts[0];\n    bins[1] = counts[2];\n    bins[2] = counts[1];\n    bins[3] = counts[3];\n  }\n}",
            "// TODO:\n\n   // 1. Count the number of points in each quadrant.\n   // 2. Print the counts\n   // 3. Gather the results\n\n   size_t n_quadrants = 4;\n\n   size_t size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   std::array<size_t, 4> tmp;\n   std::fill(tmp.begin(), tmp.end(), 0);\n\n   if (rank == 0) {\n      std::cout << \"Number of quadrants: \" << n_quadrants << std::endl;\n   }\n\n   for (size_t i = 0; i < points.size(); ++i) {\n      size_t quadrant;\n      if (points[i].x > 0 && points[i].y > 0) {\n         quadrant = 0;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         quadrant = 1;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         quadrant = 2;\n      } else {\n         quadrant = 3;\n      }\n\n      ++tmp[quadrant];\n   }\n\n   if (rank == 0) {\n      for (int i = 0; i < n_quadrants; ++i) {\n         std::cout << \"Quadrant \" << i << \": \" << tmp[i] << std::endl;\n      }\n   }\n\n   MPI_Gather(tmp.data(), n_quadrants, MPI_INT, bins.data(), n_quadrants, MPI_INT, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      for (int i = 0; i < n_quadrants; ++i) {\n         std::cout << \"Bin \" << i << \": \" << bins[i] << std::endl;\n      }\n   }\n}",
            "// TODO: implement\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    for (int i = 0; i < points.size(); i++) {\n        double x = points[i].x;\n        double y = points[i].y;\n\n        if (x > 0 && y > 0) {\n            bins[0]++;\n        }\n        else if (x < 0 && y > 0) {\n            bins[1]++;\n        }\n        else if (x < 0 && y < 0) {\n            bins[2]++;\n        }\n        else if (x > 0 && y < 0) {\n            bins[3]++;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(bins.data(), 4, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n    else {\n        MPI_Send(bins.data(), 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return;\n}",
            "int numRanks, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    //TODO: Fill in code here\n\n    std::vector<Point> localPoints;\n    if (rank == 0) {\n        localPoints = points;\n    }\n    std::vector<Point> points_rank0;\n\n    if (numRanks == 1) {\n        for (Point point:localPoints) {\n            if (point.x > 0 && point.y > 0) {\n                bins[0]++;\n            } else if (point.x > 0 && point.y < 0) {\n                bins[3]++;\n            } else if (point.x < 0 && point.y < 0) {\n                bins[2]++;\n            } else {\n                bins[1]++;\n            }\n        }\n    } else {\n        MPI_Status status;\n        MPI_Send(localPoints.data(), localPoints.size(), MPI_BYTE, rank + 1, 0, MPI_COMM_WORLD);\n        MPI_Recv(points_rank0.data(), points_rank0.size(), MPI_BYTE, rank - 1, 0, MPI_COMM_WORLD, &status);\n\n        for (Point point:points_rank0) {\n            if (point.x > 0 && point.y > 0) {\n                bins[0]++;\n            } else if (point.x > 0 && point.y < 0) {\n                bins[3]++;\n            } else if (point.x < 0 && point.y < 0) {\n                bins[2]++;\n            } else {\n                bins[1]++;\n            }\n        }\n\n        for (Point point:localPoints) {\n            if (point.x > 0 && point.y > 0) {\n                bins[0]++;\n            } else if (point.x > 0 && point.y < 0) {\n                bins[3]++;\n            } else if (point.x < 0 && point.y < 0) {\n                bins[2]++;\n            } else {\n                bins[1]++;\n            }\n        }\n    }\n}",
            "std::array<size_t, 4> counts;\n    for (auto const& point: points) {\n        int q;\n        MPI_Comm_rank(MPI_COMM_WORLD, &q);\n        if (point.x >= 0 && point.y >= 0) {\n            q = 0;\n        } else if (point.x < 0 && point.y >= 0) {\n            q = 1;\n        } else if (point.x < 0 && point.y < 0) {\n            q = 2;\n        } else if (point.x >= 0 && point.y < 0) {\n            q = 3;\n        }\n        counts[q]++;\n    }\n    MPI_Reduce(&counts[0], &bins[0], counts.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "int rank, comm_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n\n    int xdims = comm_size, ydims = comm_size;\n\n    // Split data in 2d\n    std::vector<Point> local_points;\n    if (rank == 0) {\n        local_points = points;\n    }\n    std::vector<Point> points_copy;\n    if (rank == 0) {\n        points_copy = local_points;\n    }\n    auto n_points = local_points.size();\n    if (n_points == 0) {\n        return;\n    }\n    MPI_Scatter(&n_points, 1, MPI_INT, &n_points, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    if (n_points == 0) {\n        return;\n    }\n    local_points.resize(n_points);\n    if (rank!= 0) {\n        points_copy = local_points;\n    }\n    MPI_Scatter(local_points.data(), n_points, MPI_DOUBLE, local_points.data(), n_points, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        points_copy = local_points;\n    }\n\n    int n_bins = 4;\n    int local_bins[n_bins] = {0};\n    for (auto p : local_points) {\n        int x_idx = floor(p.x / xdims);\n        int y_idx = floor(p.y / ydims);\n        int idx = x_idx + y_idx * xdims;\n        local_bins[idx]++;\n    }\n    MPI_Reduce(local_bins, bins.data(), n_bins, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        std::cout << \"Points: \" << std::endl;\n        for (auto p : points) {\n            std::cout << p.x << \", \" << p.y << std::endl;\n        }\n        std::cout << \"Bins: \" << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << std::endl;\n        std::cout << \"Points in quadrant 1: \" << bins[0] << std::endl;\n        std::cout << \"Points in quadrant 2: \" << bins[1] << std::endl;\n        std::cout << \"Points in quadrant 3: \" << bins[2] << std::endl;\n        std::cout << \"Points in quadrant 4: \" << bins[3] << std::endl;\n    }\n}",
            "const auto numProcs = MPI_Comm_size(MPI_COMM_WORLD);\n\tconst auto myRank = MPI_Comm_rank(MPI_COMM_WORLD);\n\n\t// Determine our cartesian coordinate\n\tMPI_Dims_create(numProcs, 2, bins.data());\n\tMPI_Cart_coords(MPI_COMM_WORLD, myRank, 2, bins.data());\n\n\t// Determine how many points we have to work with\n\tconst auto numPoints = points.size();\n\n\t// Initialize our bins to 0\n\tstd::fill(bins.begin(), bins.end(), 0);\n\n\t// Iterate over our points\n\tfor (size_t i = 0; i < numPoints; i++) {\n\t\t// Translate the point to our coordinate system\n\t\tconst Point point = {points[i].x + bins[0] / 2, points[i].y + bins[1] / 2};\n\n\t\t// Determine which quadrant this point belongs to\n\t\tconst auto quadrant = std::min(std::max(static_cast<int>(floor(point.x)), 0), 1) * 2 + std::min(std::max(static_cast<int>(floor(point.y)), 0), 1);\n\n\t\t// Increment our quadrant counter\n\t\tMPI_Accumulate(&(1), 1, MPI_INT, 0, 2 * myRank, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n\t\t// Determine if we should recieve a count\n\t\tMPI_Request request;\n\t\tif (myRank == 0) {\n\t\t\tMPI_Irecv(&(bins[quadrant]), 1, MPI_INT, 2 * quadrant, 2 * myRank, MPI_COMM_WORLD, &request);\n\t\t}\n\n\t\t// Determine if we should send a count\n\t\tif (myRank == 2 * quadrant) {\n\t\t\tMPI_Isend(&(bins[quadrant]), 1, MPI_INT, 0, 2 * myRank, MPI_COMM_WORLD, &request);\n\t\t}\n\t}\n\n\t// Wait for any remaining send and receive operations to complete\n\tMPI_Waitall(0, nullptr, nullptr);\n}",
            "// TODO: Your code goes here\n  return;\n}",
            "// TODO: Your code here\n   int rank, size;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int x_dim = 2;\n   int y_dim = 2;\n\n   int dims[2] = {x_dim, y_dim};\n   int periods[2] = {0, 0};\n   MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 1, &bins);\n   MPI_Cart_coords(bins, rank, 2, &x_dim, &y_dim);\n\n   for (auto &i : points) {\n      int nb_points = points.size();\n      int recv_points;\n      int coords[2] = {x_dim, y_dim};\n      MPI_Cart_rank(bins, coords, &rank);\n      MPI_Reduce(&nb_points, &recv_points, 1, MPI_INT, MPI_SUM, rank, MPI_COMM_WORLD);\n\n      if (rank == 0) {\n         std::cout << \"rank \" << rank << \" added \" << recv_points << \" points in quadrant \" << x_dim << \" \" << y_dim << \"\\n\";\n      }\n   }\n\n   std::vector<int> counts(size);\n   MPI_Gather(&nb_points, 1, MPI_INT, &counts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      std::cout << \"Final count: \";\n      for (auto &i : counts) {\n         std::cout << i << \" \";\n      }\n      std::cout << \"\\n\";\n   }\n}",
            "MPI_Init(NULL, NULL);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n   int xdim = 2;\n   int ydim = 2;\n   MPI_Dims_create(nproc, 2, dims);\n\n   MPI_Cart_create(MPI_COMM_WORLD, 2, dims, NULL, false, &cartcomm);\n   MPI_Cart_get(cartcomm, 2, dims, periods, coords);\n\n   bins.fill(0);\n   std::sort(points.begin(), points.end(), [](Point const& lhs, Point const& rhs) {\n      if (lhs.x == rhs.x) {\n         return lhs.y < rhs.y;\n      }\n      return lhs.x < rhs.x;\n   });\n\n   MPI_Barrier(cartcomm);\n   for (size_t i = 0; i < points.size(); i++) {\n      size_t index = static_cast<size_t>(floor(points[i].x)) + floor(points[i].y) * xdim;\n      if (index < 4) {\n         bins[index]++;\n      }\n   }\n   // cout << \"rank \" << rank << \" bins \" << bins << endl;\n\n   MPI_Reduce(MPI_IN_PLACE, bins.data(), 4, MPI_LONG_LONG_INT, MPI_SUM, 0, cartcomm);\n   MPI_Barrier(cartcomm);\n   if (rank == 0) {\n      // cout << \"rank \" << rank << \" bins \" << bins << endl;\n   }\n\n   MPI_Finalize();\n}",
            "MPI_Comm cart_comm;\n   MPI_Cart_create(MPI_COMM_WORLD, 2, [3,1], [0,1], 1, &cart_comm);\n\n   // TODO: fill in\n   int rank, size;\n   MPI_Comm_rank(cart_comm, &rank);\n   MPI_Comm_size(cart_comm, &size);\n\n   size_t local_counts[4];\n\n   for (size_t i = 0; i < points.size(); ++i) {\n      int idx = rank;\n      for (int i = 0; i < 2; ++i) {\n         if (points[i].x > 0) {\n            idx = idx + 1;\n         }\n      }\n      local_counts[idx] += 1;\n   }\n\n   std::vector<size_t> global_counts(4);\n\n   if (rank == 0) {\n      for (int i = 1; i < size; ++i) {\n         MPI_Recv(global_counts.data(), 4, MPI_INT, i, 0, cart_comm, MPI_STATUS_IGNORE);\n      }\n      for (size_t i = 0; i < 4; ++i) {\n         bins[i] = local_counts[i] + global_counts[i];\n      }\n   } else {\n      MPI_Send(local_counts, 4, MPI_INT, 0, 0, cart_comm);\n   }\n\n   MPI_Barrier(cart_comm);\n\n   MPI_Comm_free(&cart_comm);\n}",
            "int num_proc;\n\tMPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n\tint my_rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n\tstd::array<size_t, 4> local_bins;\n\tlocal_bins.fill(0);\n\n\tfor (auto const& point : points) {\n\t\tdouble x = point.x;\n\t\tdouble y = point.y;\n\n\t\tif (x > 0 && y > 0) {\n\t\t\tlocal_bins[0]++;\n\t\t}\n\t\telse if (x > 0 && y < 0) {\n\t\t\tlocal_bins[1]++;\n\t\t}\n\t\telse if (x < 0 && y < 0) {\n\t\t\tlocal_bins[2]++;\n\t\t}\n\t\telse {\n\t\t\tlocal_bins[3]++;\n\t\t}\n\t}\n\n\tif (my_rank == 0) {\n\t\tbins.fill(0);\n\t}\n\tMPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if (rank == 0) {\n        for (int i = 0; i < size; ++i) {\n            if (i!= 0) {\n                MPI_Recv(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            }\n        }\n    } else {\n        int dim = 0;\n        MPI_Send(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    for (int i = 0; i < size; ++i) {\n        if (i!= rank) {\n            MPI_Status status;\n            MPI_Recv(&points[0], 1, MPI_POINT, i, 0, MPI_COMM_WORLD, &status);\n        }\n    }\n\n    size_t num_points = points.size();\n    if (rank == 0) {\n        for (size_t i = 0; i < num_points; ++i) {\n            if (points[i].x < 0) {\n                if (points[i].y < 0) {\n                    bins[0]++;\n                } else if (points[i].y > 0) {\n                    bins[2]++;\n                } else {\n                    bins[3]++;\n                }\n            } else if (points[i].x > 0) {\n                if (points[i].y < 0) {\n                    bins[1]++;\n                } else if (points[i].y > 0) {\n                    bins[3]++;\n                } else {\n                    bins[2]++;\n                }\n            } else {\n                bins[1]++;\n            }\n        }\n    } else {\n        for (size_t i = rank; i < num_points; i+=size) {\n            if (points[i].x < 0) {\n                if (points[i].y < 0) {\n                    MPI_Send(&points[i], 1, MPI_POINT, 0, 0, MPI_COMM_WORLD);\n                } else if (points[i].y > 0) {\n                    MPI_Send(&points[i], 1, MPI_POINT, 2, 0, MPI_COMM_WORLD);\n                } else {\n                    MPI_Send(&points[i], 1, MPI_POINT, 3, 0, MPI_COMM_WORLD);\n                }\n            } else if (points[i].x > 0) {\n                if (points[i].y < 0) {\n                    MPI_Send(&points[i], 1, MPI_POINT, 1, 0, MPI_COMM_WORLD);\n                } else if (points[i].y > 0) {\n                    MPI_Send(&points[i], 1, MPI_POINT, 3, 0, MPI_COMM_WORLD);\n                } else {\n                    MPI_Send(&points[i], 1, MPI_POINT, 2, 0, MPI_COMM_WORLD);\n                }\n            } else {\n                MPI_Send(&points[i], 1, MPI_POINT, 1, 0, MPI_COMM_WORLD);\n            }\n        }\n    }\n}",
            "MPI_Group orig_group;\n   MPI_Group cart_group;\n   MPI_Comm_group(MPI_COMM_WORLD, &orig_group);\n\n   int dim = 2;\n   int periods[dim] = {0, 0};\n   int reorder = 1;\n   int n_dims = 2;\n   int dims[2] = {2, 3};\n   int coords[2];\n   MPI_Cart_create(MPI_COMM_WORLD, n_dims, dims, periods, reorder, &cart_group);\n   MPI_Cart_coords(cart_group, 0, n_dims, coords);\n\n   MPI_Group orig_group_1;\n   MPI_Group cart_group_1;\n   MPI_Comm_group(cart_group, &orig_group_1);\n\n   int color = coords[0] * 3 + coords[1];\n\n   MPI_Group_range_incl(orig_group_1, 1, &color, &cart_group_1);\n\n   MPI_Comm new_comm;\n   MPI_Comm_create(MPI_COMM_WORLD, cart_group_1, &new_comm);\n\n   if(MPI_COMM_NULL == new_comm) {\n      MPI_Group_free(&cart_group);\n      return;\n   }\n\n   size_t n_quad = 0;\n   for(size_t i = 0; i < points.size(); i++) {\n      if(points[i].x < 0 && points[i].y < 0) {\n         n_quad++;\n      }\n      if(points[i].x < 0 && points[i].y > 0) {\n         n_quad++;\n      }\n      if(points[i].x > 0 && points[i].y < 0) {\n         n_quad++;\n      }\n      if(points[i].x > 0 && points[i].y > 0) {\n         n_quad++;\n      }\n   }\n\n   MPI_Allreduce(&n_quad, &bins[color], 1, MPI_LONG, MPI_SUM, new_comm);\n\n   MPI_Comm_free(&new_comm);\n   MPI_Group_free(&cart_group);\n   MPI_Group_free(&orig_group);\n   MPI_Group_free(&orig_group_1);\n   MPI_Group_free(&cart_group_1);\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "cuda",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use CUDA to count in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\n__global__ void countQuadrants(const Point *points, size_t N, size_t[4] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO: your code here\n}",
            "//TODO: implement\n    for (int i = 0; i < N; i++){\n        if (points[i].x > 0 && points[i].y > 0){\n            bins[0]++;\n        }\n        if (points[i].x < 0 && points[i].y > 0){\n            bins[1]++;\n        }\n        if (points[i].x < 0 && points[i].y < 0){\n            bins[2]++;\n        }\n        if (points[i].x > 0 && points[i].y < 0){\n            bins[3]++;\n        }\n    }\n}",
            "unsigned int quad = 0;\n   if (threadIdx.x < 4) {\n      quad = threadIdx.x;\n   }\n\n   __shared__ size_t shared_bins[4];\n   shared_bins[threadIdx.x] = 0;\n   __syncthreads();\n\n   size_t i = quad * blockDim.x + threadIdx.x;\n   for (; i < N; i += blockDim.x * gridDim.x) {\n      if (points[i].x >= 0) {\n         if (points[i].y >= 0) {\n            shared_bins[quad] += 1;\n         } else {\n            shared_bins[2] += 1;\n         }\n      } else {\n         if (points[i].y >= 0) {\n            shared_bins[1] += 1;\n         } else {\n            shared_bins[3] += 1;\n         }\n      }\n   }\n   __syncthreads();\n   bins[quad] = shared_bins[0] + shared_bins[1] + shared_bins[2] + shared_bins[3];\n}",
            "// TODO: Your code here\n   // Example:\n   // int quadrant = 0;\n   // if(point.x > 0 && point.y > 0) {\n   //     quadrant = 0;\n   // } else if (point.x < 0 && point.y > 0) {\n   //     quadrant = 1;\n   // } else if (point.x < 0 && point.y < 0) {\n   //     quadrant = 2;\n   // } else if (point.x > 0 && point.y < 0) {\n   //     quadrant = 3;\n   // }\n\n   // atomicAdd(&bins[quadrant], 1);\n\n   // return;\n}",
            "// Your code goes here\n}",
            "int idx = threadIdx.x;\n   bins[idx] = 0;\n   __syncthreads();\n\n   for (size_t i = idx; i < N; i += blockDim.x) {\n      bins[idx] += (points[i].x >= 0 && points[i].y >= 0) || (points[i].x < 0 && points[i].y < 0);\n   }\n}",
            "for (int i = threadIdx.x + blockDim.x * blockIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    int quadrant = (points[i].x > 0.0) + (points[i].y > 0.0) * 2;\n    atomicAdd(&bins[quadrant], 1);\n  }\n}",
            "int x, y, quadrant;\n   // TODO\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    int threadID = threadIdx.x;\n\n    for(int i = threadID; i < N; i += blockDim.x) {\n        if(points[i].x >= 0 && points[i].y >= 0) {\n            atomicAdd(&bins[0], 1);\n        } else if(points[i].x < 0 && points[i].y >= 0) {\n            atomicAdd(&bins[1], 1);\n        } else if(points[i].x < 0 && points[i].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if(points[i].x >= 0 && points[i].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "...\n}",
            "// TODO: Your code here.\n   // Use a shared array for the histogram and add to it using atomicAdd.\n}",
            "const int i = blockIdx.x*blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    double x = points[i].x;\n    double y = points[i].y;\n    if (x > 0) {\n        if (y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    } else if (x < 0) {\n        if (y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (y < 0) {\n            atomicAdd(&bins[2], 1);\n        }\n    }\n}",
            "// TODO: implement this\n}",
            "// TODO: count the number of points in each quadrant and store the result in bins\n   for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x)\n   {\n      int quadrant = 0;\n      if (points[i].x > 0 && points[i].y > 0)\n         quadrant = 1;\n      else if (points[i].x < 0 && points[i].y > 0)\n         quadrant = 2;\n      else if (points[i].x < 0 && points[i].y < 0)\n         quadrant = 3;\n      else if (points[i].x > 0 && points[i].y < 0)\n         quadrant = 4;\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  int stride = blockDim.x * gridDim.x;\n\n  for (size_t i = tid; i < N; i += stride) {\n    if (points[i].x > 0) {\n      if (points[i].y > 0) {\n        atomicAdd(&bins[0], 1);\n      } else {\n        atomicAdd(&bins[3], 1);\n      }\n    } else {\n      if (points[i].y > 0) {\n        atomicAdd(&bins[1], 1);\n      } else {\n        atomicAdd(&bins[2], 1);\n      }\n    }\n  }\n}",
            "bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n   for(int i = 0; i < N; ++i) {\n      double x = points[i].x;\n      double y = points[i].y;\n      // TODO: fill in code\n   }\n}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n  if (index >= N)\n    return;\n  int quadrant;\n  if (points[index].x < 0) {\n    if (points[index].y < 0)\n      quadrant = 0;\n    else\n      quadrant = 3;\n  } else {\n    if (points[index].y < 0)\n      quadrant = 1;\n    else\n      quadrant = 2;\n  }\n  atomicAdd(&bins[quadrant], 1);\n}",
            "__shared__ size_t s_count[4];\n\n\tconst size_t thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (thread_id < N) {\n\t\tsize_t quadrant_index = 0;\n\t\tif (points[thread_id].x > 0 && points[thread_id].y > 0)\n\t\t\tquadrant_index = 0;\n\t\telse if (points[thread_id].x < 0 && points[thread_id].y > 0)\n\t\t\tquadrant_index = 1;\n\t\telse if (points[thread_id].x < 0 && points[thread_id].y < 0)\n\t\t\tquadrant_index = 2;\n\t\telse if (points[thread_id].x > 0 && points[thread_id].y < 0)\n\t\t\tquadrant_index = 3;\n\n\t\tatomicAdd(&s_count[quadrant_index], 1);\n\t}\n\n\t__syncthreads();\n\n\tif (thread_id == 0) {\n\t\tbins[0] = s_count[0];\n\t\tbins[1] = s_count[1];\n\t\tbins[2] = s_count[2];\n\t\tbins[3] = s_count[3];\n\t}\n}",
            "// TODO\n}",
            "// Get the thread number\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // If tid is in range of points\n    if(tid < N) {\n        // Get the point\n        Point point = points[tid];\n\n        // Get the quadrant number\n        int quadrant = 0;\n        if(point.x >= 0 && point.y >= 0)\n            quadrant = 0;\n        else if(point.x < 0 && point.y >= 0)\n            quadrant = 1;\n        else if(point.x < 0 && point.y < 0)\n            quadrant = 2;\n        else if(point.x >= 0 && point.y < 0)\n            quadrant = 3;\n\n        // Add 1 to the bin\n        atomicAdd(&bins[quadrant], 1);\n    }\n}",
            "bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n   for(int i = 0; i < N; i++) {\n      if (points[i].x < 0) {\n         if (points[i].y < 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (points[i].y < 0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "bins[threadIdx.x] = 0;\n\n    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        const Point &p = points[i];\n        if (p.x >= 0 && p.y >= 0) {\n            atomicInc(&bins[0], 1);\n        } else if (p.x < 0 && p.y >= 0) {\n            atomicInc(&bins[1], 1);\n        } else if (p.x < 0 && p.y < 0) {\n            atomicInc(&bins[2], 1);\n        } else {\n            atomicInc(&bins[3], 1);\n        }\n    }\n}",
            "/*\n    for (int i = 0; i < N; i++) {\n        float x = points[i].x;\n        float y = points[i].y;\n        int xQ = (x > 0)? 1 : ((x < 0)? 0 : 2);\n        int yQ = (y > 0)? 1 : ((y < 0)? 0 : 2);\n        int q = yQ * 2 + xQ;\n        atomicAdd(&bins[q], 1);\n    }\n    */\n}",
            "// TODO: implement the function.\n}",
            "//TODO\n}",
            "int x = (int)floor(points[threadIdx.x].x);\n    int y = (int)floor(points[threadIdx.x].y);\n\n    if(x >= 0) {\n        if(y >= 0) {\n            bins[0] = atomicAdd(&bins[0], 1);\n        }\n        else {\n            bins[1] = atomicAdd(&bins[1], 1);\n        }\n    }\n    else {\n        if(y >= 0) {\n            bins[2] = atomicAdd(&bins[2], 1);\n        }\n        else {\n            bins[3] = atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "__shared__ size_t sBins[4];\n    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    sBins[threadIdx.x] = 0;\n    while (i < N) {\n        int quadrant;\n        if (points[i].x > 0) {\n            quadrant = (points[i].y > 0)? 0 : 3;\n        } else {\n            quadrant = (points[i].y > 0)? 1 : 2;\n        }\n        atomicAdd(&sBins[quadrant], 1);\n        i += blockDim.x * gridDim.x;\n    }\n    __syncthreads();\n    for (int i = 1; i < blockDim.x; ++i) {\n        sBins[threadIdx.x] += sBins[threadIdx.x + i];\n    }\n    if (threadIdx.x == 0) {\n        for (int i = 0; i < 4; ++i) {\n            bins[i] = sBins[i];\n        }\n    }\n}",
            "// TODO\n}",
            "// TODO: compute and store the counts in bins[0,1,2,3]\n   // Each thread handles one point, compute the corresponding bin index, and increment bins[bin]\n}",
            "int index = blockDim.x * blockIdx.x + threadIdx.x;\n   if (index < N) {\n      double x = points[index].x;\n      double y = points[index].y;\n      if (x > 0) {\n         if (y > 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      } else {\n         if (y > 0) {\n            atomicAdd(&bins[1], 1);\n         } else {\n            atomicAdd(&bins[2], 1);\n         }\n      }\n   }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    for (int i = tid; i < N; i += blockDim.x * gridDim.x) {\n        if (points[i].x >= 0 && points[i].y >= 0) {\n            atomicAdd(&bins[0], 1);\n        }\n        else if (points[i].x >= 0 && points[i].y < 0) {\n            atomicAdd(&bins[1], 1);\n        }\n        else if (points[i].x < 0 && points[i].y >= 0) {\n            atomicAdd(&bins[2], 1);\n        }\n        else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "int bin = 0;\n    if (points[blockIdx.x].x > 0 && points[blockIdx.x].y > 0) {\n        bin = 0;\n    } else if (points[blockIdx.x].x < 0 && points[blockIdx.x].y > 0) {\n        bin = 1;\n    } else if (points[blockIdx.x].x < 0 && points[blockIdx.x].y < 0) {\n        bin = 2;\n    } else if (points[blockIdx.x].x > 0 && points[blockIdx.x].y < 0) {\n        bin = 3;\n    }\n    atomicAdd(&bins[bin], 1);\n}",
            "int tx = threadIdx.x;\n    int stride = blockDim.x;\n    int id = threadIdx.x + blockIdx.x * blockDim.x;\n\n    int x_quadrant = 0;\n    int y_quadrant = 0;\n\n    for (int i = id; i < N; i += stride) {\n        if (points[i].x >= 0) {\n            x_quadrant = 1;\n        } else {\n            x_quadrant = 0;\n        }\n        if (points[i].y >= 0) {\n            y_quadrant = 1;\n        } else {\n            y_quadrant = 0;\n        }\n    }\n\n    int index = (y_quadrant * 2) + x_quadrant;\n\n    atomicAdd(&bins[index], 1);\n}",
            "// TODO\n}",
            "// TODO: count quadrants using a parallel reduction\n   return;\n}",
            "__shared__ int numPointsPerQuadrant[4];\n\n  if (threadIdx.x == 0) {\n    numPointsPerQuadrant[0] = 0;\n    numPointsPerQuadrant[1] = 0;\n    numPointsPerQuadrant[2] = 0;\n    numPointsPerQuadrant[3] = 0;\n  }\n\n  __syncthreads();\n\n  for (int i = threadIdx.x; i < N; i += blockDim.x) {\n    int quadrant = 0;\n    if (points[i].x > 0 && points[i].y > 0) {\n      quadrant = 0;\n    } else if (points[i].x < 0 && points[i].y > 0) {\n      quadrant = 1;\n    } else if (points[i].x < 0 && points[i].y < 0) {\n      quadrant = 2;\n    } else if (points[i].x > 0 && points[i].y < 0) {\n      quadrant = 3;\n    }\n\n    atomicAdd(&numPointsPerQuadrant[quadrant], 1);\n  }\n\n  __syncthreads();\n\n  if (threadIdx.x == 0) {\n    bins[0] = numPointsPerQuadrant[0];\n    bins[1] = numPointsPerQuadrant[1];\n    bins[2] = numPointsPerQuadrant[2];\n    bins[3] = numPointsPerQuadrant[3];\n  }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int i = tid % 2;\n    int j = (tid - i) / 2;\n\n    if (tid < N) {\n        if (points[tid].x >= 0 && points[tid].y >= 0) {\n            bins[i]++;\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            bins[2]++;\n        } else if (points[tid].x < 0 && points[tid].y >= 0) {\n            bins[3]++;\n        } else {\n            bins[1]++;\n        }\n    }\n}",
            "// Your code here\n}",
            "int quadrant = 0;\n   // TODO: Write your code here\n   __shared__ int s_bins[4];\n\n   s_bins[0] = 0;\n   s_bins[1] = 0;\n   s_bins[2] = 0;\n   s_bins[3] = 0;\n\n   for (int i = threadIdx.x; i < N; i += blockDim.x)\n   {\n      quadrant = 0;\n      if (points[i].x > 0 && points[i].y > 0)\n      {\n         quadrant = 1;\n      }\n      else if (points[i].x < 0 && points[i].y > 0)\n      {\n         quadrant = 2;\n      }\n      else if (points[i].x < 0 && points[i].y < 0)\n      {\n         quadrant = 3;\n      }\n      else if (points[i].x > 0 && points[i].y < 0)\n      {\n         quadrant = 4;\n      }\n\n      atomicAdd(&s_bins[quadrant], 1);\n   }\n\n   __syncthreads();\n\n   if (threadIdx.x == 0)\n   {\n      atomicAdd(&bins[0], s_bins[0]);\n      atomicAdd(&bins[1], s_bins[1]);\n      atomicAdd(&bins[2], s_bins[2]);\n      atomicAdd(&bins[3], s_bins[3]);\n   }\n}",
            "// TODO\n}",
            "// TODO\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //",
            "for (int i = 0; i < N; ++i) {\n        bins[(int) (points[i].x > 0) + 2 * ((int) (points[i].y > 0))",
            "//...\n}",
            "//... your code here...\n}",
            "int thread_num = threadIdx.x;\n\tint total_threads = blockDim.x * gridDim.x;\n\tfor (int i = thread_num; i < N; i += total_threads) {\n\t\tPoint p = points[i];\n\t\tif (p.x >= 0 && p.y >= 0) {\n\t\t\tbins[0] += 1;\n\t\t}\n\t\tif (p.x >= 0 && p.y < 0) {\n\t\t\tbins[1] += 1;\n\t\t}\n\t\tif (p.x < 0 && p.y < 0) {\n\t\t\tbins[2] += 1;\n\t\t}\n\t\tif (p.x < 0 && p.y >= 0) {\n\t\t\tbins[3] += 1;\n\t\t}\n\t}\n}",
            "//TODO: implement this function\n}",
            "// TODO: implement the function\n\n}",
            "// 1. compute which thread is responsible for which point\n   // 2. update the bin counts\n}",
            "//TODO: insert code here\n}",
            "// TODO: implement\n}",
            "// TODO\n}",
            "int n = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (n >= N) return;\n\t// TODO: implement\n}",
            "//...\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n    for(int i = tid; i < N; i+=blockDim.x * gridDim.x) {\n        int quadrant = 0;\n        if(points[i].x > 0)\n            quadrant += 1;\n        if(points[i].y > 0)\n            quadrant += 2;\n        if(points[i].x < 0)\n            quadrant += 4;\n        if(points[i].y < 0)\n            quadrant += 8;\n        atomicAdd(&bins[quadrant], 1);\n    }\n}",
            "unsigned int tid = threadIdx.x;\n    unsigned int tid_quadrant = tid % 4;\n    unsigned int tid_bin = tid / 4;\n    __shared__ Point tmp[128];\n    if (tid < N) {\n        tmp[tid].x = points[tid].x;\n        tmp[tid].y = points[tid].y;\n    }\n    __syncthreads();\n    int i = blockDim.x / 4;\n    while (i!= 0) {\n        if (tid < N) {\n            if (tmp[tid].x < 0) {\n                if (tmp[tid].y < 0)\n                    tmp[tid].x = -tmp[tid].x;\n                else\n                    tmp[tid].x = -tmp[tid].x + tmp[tid].y;\n            } else if (tmp[tid].y < 0)\n                tmp[tid].x = tmp[tid].x - tmp[tid].y;\n            else\n                tmp[tid].x = tmp[tid].x + tmp[tid].y;\n        }\n        __syncthreads();\n        i /= 2;\n        if (tid < N) {\n            if (tmp[tid].x < i)\n                tmp[tid].y = tmp[tid].x;\n            else\n                tmp[tid].y = tmp[tid].x - i;\n        }\n        __syncthreads();\n        i /= 2;\n    }\n    __syncthreads();\n    if (tid < N) {\n        if (tmp[tid].x < 0)\n            tmp[tid].x = -tmp[tid].x;\n        else\n            tmp[tid].x = tmp[tid].x + tmp[tid].y;\n    }\n    __syncthreads();\n    if (tid < N) {\n        atomicAdd(&bins[tid_quadrant], tmp[tid].x);\n        atomicAdd(&bins[tid_bin + 4], tmp[tid].y);\n    }\n}",
            "// TODO: Your code here\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n   if (idx < N) {\n      Point p = points[idx];\n      bins[pointToBin(p)] += 1;\n   }\n}",
            "/*\n\t * TODO\n\t */\n\tbins[0] = 0;\n\tbins[1] = 0;\n\tbins[2] = 0;\n\tbins[3] = 0;\n\tfor (int i = 0; i < N; i++) {\n\t\tif (points[i].x > 0 && points[i].y > 0) {\n\t\t\tatomicAdd(&bins[0], 1);\n\t\t}\n\t\telse if (points[i].x < 0 && points[i].y > 0) {\n\t\t\tatomicAdd(&bins[1], 1);\n\t\t}\n\t\telse if (points[i].x < 0 && points[i].y < 0) {\n\t\t\tatomicAdd(&bins[2], 1);\n\t\t}\n\t\telse if (points[i].x > 0 && points[i].y < 0) {\n\t\t\tatomicAdd(&bins[3], 1);\n\t\t}\n\t}\n\n}",
            "bins[0] = 0; bins[1] = 0; bins[2] = 0; bins[3] = 0;\n    for (int i = threadIdx.x; i < N; i+=blockDim.x) {\n        if (points[i].x >= 0 && points[i].y >= 0)\n            atomicAdd(&bins[0], 1);\n        else if (points[i].x < 0 && points[i].y >= 0)\n            atomicAdd(&bins[1], 1);\n        else if (points[i].x < 0 && points[i].y < 0)\n            atomicAdd(&bins[2], 1);\n        else if (points[i].x >= 0 && points[i].y < 0)\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "int count;\n    // TODO:\n    // Your code here\n\n    __syncthreads();\n\n    for(int i = 0; i < bins; i++) {\n        atomicAdd(&bins[i], count);\n    }\n}",
            "// TODO: Your code here\n}",
            "// your code here\n}",
            "// TODO: Your code here\n}",
            "// your code here\n    __shared__ unsigned int share_bins[4];\n    __shared__ size_t idx;\n    idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if(idx < N) {\n        if(points[idx].x > 0 && points[idx].y > 0) {\n            atomicAdd(&share_bins[0], 1);\n        } else if(points[idx].x > 0 && points[idx].y < 0) {\n            atomicAdd(&share_bins[1], 1);\n        } else if(points[idx].x < 0 && points[idx].y < 0) {\n            atomicAdd(&share_bins[2], 1);\n        } else if(points[idx].x < 0 && points[idx].y > 0) {\n            atomicAdd(&share_bins[3], 1);\n        }\n    }\n    __syncthreads();\n\n    if (threadIdx.x == 0) {\n        bins[0] = share_bins[0];\n        bins[1] = share_bins[1];\n        bins[2] = share_bins[2];\n        bins[3] = share_bins[3];\n    }\n}",
            "size_t i = threadIdx.x;\n   // TODO: count quadrants in parallel\n   for (int j=0;j<N;j++){\n      if (points[j].x>0 && points[j].y>0){\n         bins[0]++;\n      }\n      else if (points[j].x<0 && points[j].y>0){\n         bins[1]++;\n      }\n      else if (points[j].x<0 && points[j].y<0){\n         bins[2]++;\n      }\n      else {\n         bins[3]++;\n      }\n   }\n}",
            "/*... */\n}",
            "// Compute which bins to increment for this thread\n    // Fill in the appropriate values in bins[0] to bins[3]\n}",
            "// TODO: Implement\n}",
            "// TODO\n    // Use a single thread to count points in each quadrant.\n}",
            "unsigned int thread_id = threadIdx.x + blockDim.x * blockIdx.x;\n    if (thread_id >= N)\n        return;\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    for (int i = 0; i < N; i++) {\n        if (points[i].x >= 0 && points[i].y >= 0)\n            bins[0]++;\n        else if (points[i].x < 0 && points[i].y >= 0)\n            bins[1]++;\n        else if (points[i].x < 0 && points[i].y < 0)\n            bins[2]++;\n        else if (points[i].x >= 0 && points[i].y < 0)\n            bins[3]++;\n    }\n}",
            "// TODO: Your code here\n}",
            "int q = threadIdx.x / 1024; // quadrant 0-3\n    __shared__ size_t s[4];\n    s[q] = 0;\n    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        // TODO: implement\n    }\n    __syncthreads();\n    if (threadIdx.x == 0) {\n        bins[q] = s[q];\n    }\n}",
            "// TODO\n}",
            "// your code here\n\n   /*\n   int i = threadIdx.x;\n   int count = 0;\n   for(int j = 0; j < N; j++){\n      if(points[j].x >= 0 && points[j].y >= 0){\n         count++;\n      }\n      else if(points[j].x < 0 && points[j].y > 0){\n         count++;\n      }\n      else if(points[j].x < 0 && points[j].y < 0){\n         count++;\n      }\n      else if(points[j].x > 0 && points[j].y < 0){\n         count++;\n      }\n   }\n\n   if(i == 0){\n      bins[0] = count;\n   }\n   else if(i == 1){\n      bins[1] = count;\n   }\n   else if(i == 2){\n      bins[2] = count;\n   }\n   else if(i == 3){\n      bins[3] = count;\n   }\n   */\n}",
            "int tid = threadIdx.x;\n    int block_size = blockDim.x;\n    int block_id = blockIdx.x;\n    int index = tid + block_size * block_id;\n\n    if (index < N) {\n        double x = points[index].x;\n        double y = points[index].y;\n        if (x > 0 && y > 0) {\n            bins[0]++;\n        } else if (x < 0 && y > 0) {\n            bins[1]++;\n        } else if (x < 0 && y < 0) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "// TODO\n}",
            "const size_t n = blockDim.x * blockIdx.x + threadIdx.x;\n\n    if (n < N) {\n        size_t bin = 0;\n\n        // TODO: count the points in quadrants.\n        //       bin=0 is for points in the 1st quadrant (1st and 4th)\n        //       bin=1 is for points in the 2nd quadrant (2nd and 3rd)\n        //       bin=2 is for points in the 3rd quadrant (3rd and 4th)\n        //       bin=3 is for points in the 4th quadrant (1st and 2nd)\n\n        // You should write your own code here.\n\n        // TODO: add the count of points in each quadrant to bins.\n        //       You can use the following syntax to update the variable\n        //       bins[0] += 1;\n\n        // You should write your own code here.\n    }\n}",
            "// TODO\n}",
            "...\n}",
            "}",
            "const size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    size_t i = (points[tid].y >= 0.0)? 0 : 1;\n    i += (points[tid].x >= 0.0)? 0 : 2;\n    atomicAdd(&bins[i], 1);\n  }\n}",
            "int i = blockDim.x*blockIdx.x + threadIdx.x;\n\tif (i >= N) return;\n\t\n\tPoint p = points[i];\n\t\n\tif (p.x > 0 && p.y > 0) {\n\t\tbins[0]++;\n\t} else if (p.x > 0 && p.y < 0) {\n\t\tbins[1]++;\n\t} else if (p.x < 0 && p.y < 0) {\n\t\tbins[2]++;\n\t} else {\n\t\tbins[3]++;\n\t}\n\t\n}",
            "/*\n      for (int i = threadIdx.x; i < N; i += blockDim.x) {\n         Point p = points[i];\n         if (p.x > 0 && p.y > 0) {\n            atomicAdd(&bins[0], 1);\n         } else if (p.x < 0 && p.y > 0) {\n            atomicAdd(&bins[1], 1);\n         } else if (p.x < 0 && p.y < 0) {\n            atomicAdd(&bins[2], 1);\n         } else if (p.x > 0 && p.y < 0) {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   */\n\n   size_t thread = blockDim.x * blockIdx.x + threadIdx.x;\n\n   if (thread < N) {\n      Point p = points[thread];\n\n      if (p.x > 0 && p.y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (p.x < 0 && p.y > 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (p.x < 0 && p.y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else if (p.x > 0 && p.y < 0) {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "/* Add code here */\n}",
            "// TODO: count the number of points in each quadrant\n\n}",
            "unsigned int quadrant = 0;\n\n    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N;\n         i += blockDim.x * gridDim.x) {\n        if (points[i].x >= 0 && points[i].y >= 0) {\n            quadrant = 0;\n        } else if (points[i].x < 0 && points[i].y >= 0) {\n            quadrant = 1;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            quadrant = 2;\n        } else if (points[i].x >= 0 && points[i].y < 0) {\n            quadrant = 3;\n        }\n        atomicAdd(&bins[quadrant], 1);\n    }\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    for (int i = threadIdx.x; i < N; i += blockDim.x) {\n        if (points[i].x > 0.0 && points[i].y > 0.0) {\n            bins[0]++;\n        } else if (points[i].x < 0.0 && points[i].y > 0.0) {\n            bins[1]++;\n        } else if (points[i].x < 0.0 && points[i].y < 0.0) {\n            bins[2]++;\n        } else if (points[i].x > 0.0 && points[i].y < 0.0) {\n            bins[3]++;\n        }\n    }\n}",
            "int q = 0;\n    if(points[threadIdx.x].x > 0)\n        q |= 1;\n    if(points[threadIdx.x].y > 0)\n        q |= 2;\n    if(points[threadIdx.x].x < 0)\n        q |= 4;\n    if(points[threadIdx.x].y < 0)\n        q |= 8;\n\n    bins[q]++;\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n   if (tid >= N)\n      return;\n\n   bins[0] += (points[tid].x > 0) && (points[tid].y > 0); // (0, 0)\n   bins[1] += (points[tid].x < 0) && (points[tid].y > 0); // (1, 0)\n   bins[2] += (points[tid].x < 0) && (points[tid].y < 0); // (1, 1)\n   bins[3] += (points[tid].x > 0) && (points[tid].y < 0); // (0, 1)\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   int grid_size = blockDim.x * gridDim.x;\n\n   for (int i = tid; i < N; i += grid_size) {\n\n      double x = points[i].x;\n      double y = points[i].y;\n\n      int quadrant = 0;\n      if (x > 0) quadrant += 1;\n      if (y > 0) quadrant += 2;\n\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    unsigned int i = tid;\n\n    __shared__ Point p[N];\n\n    if (i < N) {\n        p[i] = points[i];\n    }\n\n    __syncthreads();\n\n    for (i = tid; i < N; i += blockDim.x * gridDim.x) {\n        if (p[i].x >= 0 && p[i].y >= 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (p[i].x < 0 && p[i].y >= 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (p[i].x < 0 && p[i].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "int i = threadIdx.x;\n    __shared__ Point p[BLOCK_SIZE];\n    // Fill the shared memory with the data of point[i]\n    p[i] = points[i];\n    __syncthreads();\n    // Perform the computation.\n    // Use a loop and conditional statements to compute the bin index.\n    // Use an atomic increment to increment the bin count.\n    // The return of atomicAdd is the previous value of the bin count,\n    // so it can be used to compute the cumulative count.\n    for (int j = i; j < N; j += BLOCK_SIZE) {\n        if (p[i].x >= 0 && p[i].y >= 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (p[i].x < 0 && p[i].y >= 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (p[i].x < 0 && p[i].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (p[i].x >= 0 && p[i].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "}",
            "int threadIdx = threadIdx.x;\n   if (threadIdx >= N) {\n      return;\n   }\n   auto point = points[threadIdx];\n   int bin = 0;\n   if (point.x >= 0) {\n      bin += 1;\n   }\n   if (point.y >= 0) {\n      bin += 2;\n   }\n   atomicAdd(&bins[bin], 1);\n}",
            "// Start coding here\n    int i = threadIdx.x;\n    int blockId = blockIdx.x;\n    int gridId = gridDim.x;\n\n    if (i < N) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            atomicAdd(&bins[0], 1);\n        }\n\n        if (points[i].x < 0 && points[i].y > 0) {\n            atomicAdd(&bins[1], 1);\n        }\n\n        if (points[i].x < 0 && points[i].y < 0) {\n            atomicAdd(&bins[2], 1);\n        }\n\n        if (points[i].x > 0 && points[i].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "// TODO\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n        double x = points[i].x;\n        double y = points[i].y;\n        if (x >= 0 && y >= 0)\n            atomicAdd(&bins[0], 1);\n        else if (x >= 0 && y < 0)\n            atomicAdd(&bins[1], 1);\n        else if (x < 0 && y < 0)\n            atomicAdd(&bins[2], 1);\n        else\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "// TODO\n   return;\n}",
            "}",
            "// your code here\n}",
            "// Start by getting the thread index and number of threads\n    int tid = threadIdx.x;\n    int numThreads = blockDim.x;\n    int i = blockIdx.x*blockDim.x + threadIdx.x;\n\n    // If our thread index is greater than the number of inputs, exit the kernel\n    if (i >= N) {\n        return;\n    }\n\n    // Increment the bin index corresponding to the quadrant\n    if (points[i].x > 0 && points[i].y > 0) {\n        atomicAdd(&bins[0], 1);\n    } else if (points[i].x > 0 && points[i].y <= 0) {\n        atomicAdd(&bins[1], 1);\n    } else if (points[i].x <= 0 && points[i].y > 0) {\n        atomicAdd(&bins[2], 1);\n    } else if (points[i].x <= 0 && points[i].y <= 0) {\n        atomicAdd(&bins[3], 1);\n    }\n}",
            "size_t tid = threadIdx.x;\n   size_t bid = threadIdx.x + blockDim.x * blockIdx.x;\n   bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n   if (bid < N) {\n      if (points[bid].x > 0 && points[bid].y > 0) {\n         bins[0] = bins[0] + 1;\n      } else if (points[bid].x > 0 && points[bid].y < 0) {\n         bins[1] = bins[1] + 1;\n      } else if (points[bid].x < 0 && points[bid].y < 0) {\n         bins[2] = bins[2] + 1;\n      } else if (points[bid].x < 0 && points[bid].y > 0) {\n         bins[3] = bins[3] + 1;\n      }\n   }\n}",
            "// TODO: implement\n\n\n}",
            "// TODO\n}",
            "// your code here\n}",
            "}",
            "// TODO\n}",
            "}",
            "...\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n\tif(x < N) {\n\t\tdouble xVal = points[x].x;\n\t\tdouble yVal = points[x].y;\n\n\t\tif (xVal < 0 && yVal > 0) {\n\t\t\tatomicAdd(&bins[0], 1);\n\t\t}\n\t\telse if (xVal > 0 && yVal > 0) {\n\t\t\tatomicAdd(&bins[1], 1);\n\t\t}\n\t\telse if (xVal > 0 && yVal < 0) {\n\t\t\tatomicAdd(&bins[2], 1);\n\t\t}\n\t\telse if (xVal < 0 && yVal < 0) {\n\t\t\tatomicAdd(&bins[3], 1);\n\t\t}\n\t}\n}",
            "if (threadIdx.x >= N) return;\n\t__shared__ int count[4];\n\tif (threadIdx.x == 0) {\n\t\tfor (int i = 0; i < 4; i++)\n\t\t\tcount[i] = 0;\n\t}\n\n\t__syncthreads();\n\n\tdouble x = points[threadIdx.x].x;\n\tdouble y = points[threadIdx.x].y;\n\n\tif (x >= 0 && y >= 0) {\n\t\tatomicAdd(&count[0], 1);\n\t} else if (x < 0 && y >= 0) {\n\t\tatomicAdd(&count[1], 1);\n\t} else if (x < 0 && y < 0) {\n\t\tatomicAdd(&count[2], 1);\n\t} else {\n\t\tatomicAdd(&count[3], 1);\n\t}\n\n\t__syncthreads();\n\n\tif (threadIdx.x == 0) {\n\t\tfor (int i = 0; i < 4; i++) {\n\t\t\tbins[i] = count[i];\n\t\t}\n\t}\n}",
            "/* YOUR CODE HERE */\n}",
            "// TODO: implement this function\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n    for (int i = 0; i < N; ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            bins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            bins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            bins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "int n = blockDim.x * blockIdx.x + threadIdx.x;\n  if (n < N) {\n    double x = points[n].x;\n    double y = points[n].y;\n    if (x >= 0 && y >= 0) {\n      bins[0]++;\n    } else if (x < 0 && y >= 0) {\n      bins[1]++;\n    } else if (x < 0 && y < 0) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "int quad = 0;\n   if (points[threadIdx.x].x >= 0 && points[threadIdx.x].y >= 0)\n      quad = 0;\n   if (points[threadIdx.x].x < 0 && points[threadIdx.x].y >= 0)\n      quad = 1;\n   if (points[threadIdx.x].x < 0 && points[threadIdx.x].y < 0)\n      quad = 2;\n   if (points[threadIdx.x].x >= 0 && points[threadIdx.x].y < 0)\n      quad = 3;\n\n   // atomically add\n   atomicAdd(&bins[quad], 1);\n}",
            "// TODO: implement\n}",
            "size_t i = threadIdx.x;\n    if (i >= N)\n        return;\n\n    Point point = points[i];\n\n    if (point.x < 0.0 && point.y < 0.0)\n        atomicAdd(&bins[0], 1);\n    else if (point.x >= 0.0 && point.y < 0.0)\n        atomicAdd(&bins[1], 1);\n    else if (point.x >= 0.0 && point.y >= 0.0)\n        atomicAdd(&bins[2], 1);\n    else if (point.x < 0.0 && point.y >= 0.0)\n        atomicAdd(&bins[3], 1);\n}",
            "// Write your solution here\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N) {\n      Point p = points[idx];\n      int i = (p.x >= 0.0)? 0 : 1;\n      int j = (p.y >= 0.0)? 0 : 1;\n      atomicAdd(&bins[i + j], 1);\n   }\n}",
            "__shared__ int q[4];\n\n    if (threadIdx.x == 0) {\n        for (int i = 0; i < 4; i++) {\n            q[i] = 0;\n        }\n    }\n    __syncthreads();\n\n    for (int i = threadIdx.x; i < N; i += blockDim.x) {\n        double x = points[i].x;\n        double y = points[i].y;\n        if (x > 0) {\n            if (y > 0) {\n                atomicAdd(&q[0], 1);\n            } else if (y < 0) {\n                atomicAdd(&q[2], 1);\n            } else {\n                atomicAdd(&q[1], 1);\n            }\n        } else if (x < 0) {\n            if (y > 0) {\n                atomicAdd(&q[1], 1);\n            } else if (y < 0) {\n                atomicAdd(&q[3], 1);\n            } else {\n                atomicAdd(&q[2], 1);\n            }\n        } else {\n            if (y > 0) {\n                atomicAdd(&q[0], 1);\n            } else if (y < 0) {\n                atomicAdd(&q[2], 1);\n            }\n        }\n    }\n\n    __syncthreads();\n    for (int i = threadIdx.x; i < 4; i += blockDim.x) {\n        atomicAdd(&bins[i], q[i]);\n    }\n}",
            "// TODO\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if(index >= N){\n        return;\n    }\n    if(points[index].x >= 0 && points[index].y >= 0){\n        bins[0]++;\n    }\n    else if(points[index].x <= 0 && points[index].y >= 0){\n        bins[1]++;\n    }\n    else if(points[index].x <= 0 && points[index].y <= 0){\n        bins[2]++;\n    }\n    else{\n        bins[3]++;\n    }\n}",
            "// TODO\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n\n   // TODO\n   // if (i < N)\n   // {\n   //    if (points[i].x > 0 && points[i].y > 0)\n   //       bins[0]++;\n   //    else if (points[i].x > 0 && points[i].y < 0)\n   //       bins[1]++;\n   //    else if (points[i].x < 0 && points[i].y > 0)\n   //       bins[2]++;\n   //    else if (points[i].x < 0 && points[i].y < 0)\n   //       bins[3]++;\n   // }\n\n   for (size_t index = i; index < N; index += blockDim.x * gridDim.x)\n   {\n      if (points[index].x > 0 && points[index].y > 0)\n         bins[0]++;\n      else if (points[index].x > 0 && points[index].y < 0)\n         bins[1]++;\n      else if (points[index].x < 0 && points[index].y > 0)\n         bins[2]++;\n      else if (points[index].x < 0 && points[index].y < 0)\n         bins[3]++;\n   }\n}",
            "// TODO: implement\n}",
            "// TODO: implement\n}",
            "bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n   for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x > 0 && y > 0) {\n         bins[0] += 1;\n      } else if (x < 0 && y > 0) {\n         bins[1] += 1;\n      } else if (x < 0 && y < 0) {\n         bins[2] += 1;\n      } else if (x > 0 && y < 0) {\n         bins[3] += 1;\n      }\n   }\n}",
            "// TODO: Implement this kernel\n}",
            "size_t idx = threadIdx.x + blockDim.x*blockIdx.x;\n\n\tif(idx >= N) return;\n\n\t// quadrants are defined from the origin (0, 0) in the positive x-y directions\n\t// quadrants are numbered from 0 to 3\n\t// x = 0: [-inf, 0)\n\t// x > 0: [0, inf)\n\t// y = 0: [-inf, 0)\n\t// y > 0: [0, inf)\n\t// a point (x, y) is in quadrant q if:\n\t//\t\tif q == 0: x < 0 and y < 0\n\t//\t\tif q == 1: x >= 0 and y < 0\n\t//\t\tif q == 2: x >= 0 and y >= 0\n\t//\t\tif q == 3: x < 0 and y >= 0\n\tif(points[idx].x < 0 && points[idx].y < 0) {\n\t\tbins[0]++;\n\t} else if(points[idx].x >= 0 && points[idx].y < 0) {\n\t\tbins[1]++;\n\t} else if(points[idx].x >= 0 && points[idx].y >= 0) {\n\t\tbins[2]++;\n\t} else {\n\t\tbins[3]++;\n\t}\n}",
            "/* TODO: fill this in! */\n}",
            "// Your code here\n}",
            "}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (points[i].x >= 0) {\n      if (points[i].y >= 0) {\n        atomicAdd(&bins[0], 1);\n      } else {\n        atomicAdd(&bins[1], 1);\n      }\n    } else {\n      if (points[i].y >= 0) {\n        atomicAdd(&bins[2], 1);\n      } else {\n        atomicAdd(&bins[3], 1);\n      }\n    }\n  }\n}",
            "}",
            "__shared__ int shared[4][1024];\n   int i = threadIdx.x + blockIdx.x * blockDim.x;\n   if (i < N) {\n      int quadrant = 0;\n      if (points[i].x < 0) {\n         quadrant |= 1;\n      }\n      if (points[i].y < 0) {\n         quadrant |= 2;\n      }\n      atomicAdd(&shared[quadrant][threadIdx.x], 1);\n   }\n   __syncthreads();\n   int start = blockDim.x * blockIdx.x;\n   int end = blockDim.x * blockIdx.x + threadIdx.x;\n   for (int j = start; j < 4; j += blockDim.x) {\n      atomicAdd(&bins[j], shared[j][end]);\n   }\n}",
            "int tid = threadIdx.x;\n    // TODO: implement\n    return;\n}",
            "// TODO: implement\n}",
            "// TODO: write the kernel code\n}",
            "// TODO: your code here\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if(i >= N) return;\n    // TODO: implement\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n    if(points[i].x > 0 && points[i].y > 0)\n        bins[0]++;\n    else if(points[i].x < 0 && points[i].y > 0)\n        bins[1]++;\n    else if(points[i].x < 0 && points[i].y < 0)\n        bins[2]++;\n    else if(points[i].x > 0 && points[i].y < 0)\n        bins[3]++;\n}",
            "...\n}",
            "}",
            "int x = points[blockIdx.x].x;\n    int y = points[blockIdx.x].y;\n\n    int quadrant;\n    if (x >= 0 && y >= 0) {\n        quadrant = 0;\n    } else if (x < 0 && y >= 0) {\n        quadrant = 1;\n    } else if (x < 0 && y < 0) {\n        quadrant = 2;\n    } else if (x >= 0 && y < 0) {\n        quadrant = 3;\n    }\n\n    atomicAdd(&bins[quadrant], 1);\n}",
            "}",
            "__shared__ size_t sharedBins[4];\n\tsize_t myId = threadIdx.x;\n\tif (myId == 0) {\n\t\tsharedBins[0] = 0;\n\t\tsharedBins[1] = 0;\n\t\tsharedBins[2] = 0;\n\t\tsharedBins[3] = 0;\n\t}\n\t__syncthreads();\n\tfor (int i = myId; i < N; i += blockDim.x) {\n\t\tPoint point = points[i];\n\t\tif (point.x >= 0 && point.y >= 0) {\n\t\t\tatomicAdd(&sharedBins[0], 1);\n\t\t}\n\t\telse if (point.x < 0 && point.y >= 0) {\n\t\t\tatomicAdd(&sharedBins[1], 1);\n\t\t}\n\t\telse if (point.x < 0 && point.y < 0) {\n\t\t\tatomicAdd(&sharedBins[2], 1);\n\t\t}\n\t\telse {\n\t\t\tatomicAdd(&sharedBins[3], 1);\n\t\t}\n\t}\n\t__syncthreads();\n\tfor (int i = myId; i < 4; i += blockDim.x) {\n\t\tatomicAdd(&bins[i], sharedBins[i]);\n\t}\n}",
            "// TODO\n}",
            "__shared__ int shared_bins[4];\n    int i = threadIdx.x;\n    if (i < 4) {\n        shared_bins[i] = 0;\n    }\n    __syncthreads();\n    for (int j = 0; j < N; j++) {\n        int quadrant;\n        if (points[j].x > 0) {\n            if (points[j].y > 0) {\n                quadrant = 0;\n            } else {\n                quadrant = 3;\n            }\n        } else {\n            if (points[j].y > 0) {\n                quadrant = 1;\n            } else {\n                quadrant = 2;\n            }\n        }\n        atomicAdd(&shared_bins[quadrant], 1);\n    }\n    __syncthreads();\n    for (int j = threadIdx.x; j < 4; j += blockDim.x) {\n        atomicAdd(&bins[j], shared_bins[j]);\n    }\n}",
            "bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n   int index = threadIdx.x + blockIdx.x*blockDim.x;\n   if (index < N) {\n      if (points[index].x > 0 && points[index].y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (points[index].x > 0 && points[index].y <= 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (points[index].x <= 0 && points[index].y <= 0) {\n         atomicAdd(&bins[2], 1);\n      } else if (points[index].x <= 0 && points[index].y > 0) {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// Your code here\n}",
            "// TODO: implement\n}",
            "const size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      double x = points[i].x, y = points[i].y;\n      if (x >= 0) {\n         if (y >= 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (y >= 0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n   if (i >= N)\n      return;\n\n   Point p = points[i];\n\n   int x = p.x > 0;\n   int y = p.y > 0;\n   int index = x * 2 + y;\n\n   atomicAdd(&bins[index], 1);\n}",
            "// Your code here\n   bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n\n   for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n      if (points[i].x > 0.0 && points[i].y > 0.0) {\n         bins[0]++;\n      } else if (points[i].x < 0.0 && points[i].y > 0.0) {\n         bins[1]++;\n      } else if (points[i].x < 0.0 && points[i].y < 0.0) {\n         bins[2]++;\n      } else if (points[i].x > 0.0 && points[i].y < 0.0) {\n         bins[3]++;\n      }\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (i >= N) {\n    return;\n  }\n\n  int quadrant = 0;\n\n  if (points[i].x > 0 && points[i].y > 0) {\n    quadrant = 1;\n  } else if (points[i].x < 0 && points[i].y > 0) {\n    quadrant = 2;\n  } else if (points[i].x < 0 && points[i].y < 0) {\n    quadrant = 3;\n  }\n\n  atomicAdd(&bins[quadrant], 1);\n}",
            "// TODO: implement\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (i >= N)\n        return;\n\n    size_t bin = 0;\n\n    if (points[i].x >= 0) {\n        bin += 1;\n        if (points[i].y >= 0)\n            bin += 1;\n        else\n            bin += 2;\n    } else {\n        bin += 2;\n        if (points[i].y >= 0)\n            bin += 2;\n        else\n            bin += 3;\n    }\n\n    atomicAdd(&bins[bin], 1);\n}",
            "int i = blockDim.x*blockIdx.x + threadIdx.x;\n\n    if(i < N){\n        // Your code here\n        int x = (int)(points[i].x);\n        int y = (int)(points[i].y);\n        if(x >= 0 && y >= 0){\n            bins[0] += 1;\n        }else if(x < 0 && y >= 0){\n            bins[1] += 1;\n        }else if(x < 0 && y < 0){\n            bins[2] += 1;\n        }else if(x >= 0 && y < 0){\n            bins[3] += 1;\n        }\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double x = points[tid].x;\n        double y = points[tid].y;\n        if (x > 0) {\n            if (y > 0) {\n                atomicAdd(&bins[0], 1);\n            } else {\n                atomicAdd(&bins[3], 1);\n            }\n        } else {\n            if (y > 0) {\n                atomicAdd(&bins[1], 1);\n            } else {\n                atomicAdd(&bins[2], 1);\n            }\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n    int i = blockDim.x*blockIdx.x + threadIdx.x;\n    if (i >= N) return;\n    Point p = points[i];\n    int quadrant;\n    if (p.x > 0 && p.y > 0) {\n        quadrant = 0;\n    } else if (p.x < 0 && p.y > 0) {\n        quadrant = 1;\n    } else if (p.x < 0 && p.y < 0) {\n        quadrant = 2;\n    } else {\n        quadrant = 3;\n    }\n    bins[quadrant] += 1;\n}",
            "// allocate shared memory for each thread\n  __shared__ size_t threadBins[4];\n  // initialize shared memory\n  threadBins[threadIdx.x] = 0;\n  __syncthreads();\n  // determine the point this thread is responsible for\n  int point_index = blockDim.x * blockIdx.x + threadIdx.x;\n  // increment the appropriate counter\n  if (point_index < N) {\n    int quadrant = 0;\n    if (points[point_index].x >= 0) quadrant += 1;\n    if (points[point_index].y >= 0) quadrant += 2;\n    threadBins[quadrant]++;\n  }\n  __syncthreads();\n  // add the thread's local bins to the global bins\n  for (int i = 0; i < 4; i++) {\n    bins[i] += threadBins[i];\n  }\n}",
            "// TODO: Implement this function.\n\n}",
            "unsigned int x = threadIdx.x + blockIdx.x * blockDim.x;\n    unsigned int y = threadIdx.y + blockIdx.y * blockDim.y;\n    unsigned int i = x + y * blockDim.x * gridDim.x;\n    if (i < N) {\n        double x = points[i].x;\n        double y = points[i].y;\n        if (x > 0) {\n            if (y > 0) {\n                atomicAdd(&bins[0], 1);\n            } else {\n                atomicAdd(&bins[1], 1);\n            }\n        } else {\n            if (y > 0) {\n                atomicAdd(&bins[2], 1);\n            } else {\n                atomicAdd(&bins[3], 1);\n            }\n        }\n    }\n}",
            "// TODO\n   int idx = threadIdx.x;\n   if(idx < N){\n      if(points[idx].x < 0 && points[idx].y < 0){\n         atomicAdd(&bins[0], 1);\n      }\n      if(points[idx].x >= 0 && points[idx].y < 0){\n         atomicAdd(&bins[1], 1);\n      }\n      if(points[idx].x >= 0 && points[idx].y >= 0){\n         atomicAdd(&bins[2], 1);\n      }\n      if(points[idx].x < 0 && points[idx].y >= 0){\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// TODO: Implement the kernel\n}",
            "// TODO\n}",
            "__shared__ int sh_bin[4];\n\n    if(threadIdx.x == 0) {\n        sh_bin[0] = 0;\n        sh_bin[1] = 0;\n        sh_bin[2] = 0;\n        sh_bin[3] = 0;\n    }\n    __syncthreads();\n\n    for(size_t i = threadIdx.x; i < N; i += blockDim.x) {\n        int bin = 0;\n        if(points[i].x > 0.0 && points[i].y > 0.0)\n            bin = 0;\n        else if(points[i].x < 0.0 && points[i].y > 0.0)\n            bin = 1;\n        else if(points[i].x < 0.0 && points[i].y < 0.0)\n            bin = 2;\n        else if(points[i].x > 0.0 && points[i].y < 0.0)\n            bin = 3;\n\n        atomicAdd(&sh_bin[bin], 1);\n    }\n    __syncthreads();\n\n    for(int i = 0; i < 4; i++)\n        atomicAdd(&bins[i], sh_bin[i]);\n}",
            "// TODO\n}",
            "// TODO: count quadrant bins in parallel\n}",
            "__shared__ int shmem[32];\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n  shmem[tid] = 0;\n\n  for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n    Point p = points[i];\n    if (p.x > 0)\n      shmem[tid] += 1;\n    else if (p.x < 0)\n      shmem[tid] += 2;\n    else if (p.y > 0)\n      shmem[tid] += 3;\n    else if (p.y < 0)\n      shmem[tid] += 4;\n  }\n  __syncthreads();\n\n  // sum up partial counts\n  for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n    if (tid < s) {\n      shmem[tid] += shmem[tid + s];\n    }\n    __syncthreads();\n  }\n\n  // store results in output array\n  if (tid == 0) {\n    bins[0] = shmem[0];\n    bins[1] = shmem[1];\n    bins[2] = shmem[2];\n    bins[3] = shmem[3];\n  }\n}",
            "int count = 0;\n  for (int i = 0; i < N; ++i) {\n    // your code here\n  }\n  bins[0] = count;\n}",
            "int blockId = blockIdx.x;\n    int threadId = threadIdx.x;\n    int quad = -1;\n\n    // for each thread, check if it is in each quadrant\n    for (int i = threadId; i < N; i+=blockDim.x) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            quad = 0;\n            break;\n        }\n        else if (points[i].x < 0 && points[i].y > 0) {\n            quad = 1;\n            break;\n        }\n        else if (points[i].x < 0 && points[i].y < 0) {\n            quad = 2;\n            break;\n        }\n        else if (points[i].x > 0 && points[i].y < 0) {\n            quad = 3;\n            break;\n        }\n    }\n\n    // increment the respective bin\n    if (quad == 0) atomicAdd(&(bins[0]), 1);\n    else if (quad == 1) atomicAdd(&(bins[1]), 1);\n    else if (quad == 2) atomicAdd(&(bins[2]), 1);\n    else if (quad == 3) atomicAdd(&(bins[3]), 1);\n}",
            "__shared__ size_t quadrant[4][WG_SIZE];\n\n   int i = threadIdx.x + blockIdx.x * WG_SIZE;\n   if (i >= N) return;\n\n   Point p = points[i];\n   quadrant[0][threadIdx.x] += (p.x > 0.0 && p.y > 0.0);\n   quadrant[1][threadIdx.x] += (p.x > 0.0 && p.y < 0.0);\n   quadrant[2][threadIdx.x] += (p.x < 0.0 && p.y < 0.0);\n   quadrant[3][threadIdx.x] += (p.x < 0.0 && p.y > 0.0);\n}",
            "// TODO: Implement\n}",
            "// your code here\n}",
            "// Your code here\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if(tid >= N) return;\n    size_t bin = 0;\n    if(points[tid].x >= 0) {\n        if(points[tid].y >= 0) bin = 0;\n        else bin = 3;\n    }\n    else {\n        if(points[tid].y >= 0) bin = 1;\n        else bin = 2;\n    }\n    atomicAdd(&bins[bin], 1);\n}",
            "// TODO: implement\n}",
            "// TODO: Implement me\n\t// HINT: Use the following quadrants (x,y) to bin the points:\n\t// -2, 2\n\t//  0, 2\n\t// -2, 0\n\t//  0, 0\n\n\t// TODO: Implement me\n\t// HINT: use atomicAdd() to increment the bins\n\t// NOTE: only use threads with id < N\n\n\t// TODO: Implement me\n\t// HINT: use threadIdx.x to access the points\n\t// NOTE: only use threads with id < N\n\n\treturn;\n}",
            "// TODO\n\n}",
            "int i = threadIdx.x;\n\tif (i >= N) return;\n\tint idx = points[i].x > 0? (points[i].y > 0? 0 : 3) : (points[i].y > 0? 1 : 2);\n\tbins[idx]++;\n}",
            "size_t tid = threadIdx.x;\n\n   // TODO: Your code here\n}",
            "__shared__ int cache[1024];\n\n   // fill cache with 0\n   for (int i = threadIdx.x; i < 1024; i += blockDim.x)\n      cache[i] = 0;\n   __syncthreads();\n\n   for (int i = threadIdx.x; i < N; i += blockDim.x) {\n      int quadrant = 0;\n\n      if (points[i].x < 0.0)\n         quadrant += 1;\n      if (points[i].x > 0.0)\n         quadrant += 2;\n      if (points[i].y < 0.0)\n         quadrant += 4;\n      if (points[i].y > 0.0)\n         quadrant += 8;\n\n      atomicAdd(&cache[quadrant], 1);\n   }\n\n   // store the results in bins\n   for (int i = threadIdx.x; i < 1024; i += blockDim.x) {\n      int index = i % 4;\n      if (index == 0)\n         atomicAdd(&bins[1], cache[i]);\n      if (index == 1)\n         atomicAdd(&bins[2], cache[i]);\n      if (index == 2)\n         atomicAdd(&bins[3], cache[i]);\n      if (index == 3)\n         atomicAdd(&bins[0], cache[i]);\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N)\n        return;\n\n    const Point &p = points[i];\n    if (p.x >= 0) {\n        if (p.y >= 0) {\n            atomicAdd(&bins[0], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    } else {\n        if (p.y >= 0) {\n            atomicAdd(&bins[1], 1);\n        } else {\n            atomicAdd(&bins[2], 1);\n        }\n    }\n}",
            "// TODO: count in parallel\n}",
            "...\n}",
            "__shared__ size_t s_bins[4];\n\n  if (threadIdx.x == 0) {\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n  }\n\n  __syncthreads();\n\n  for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n    int quadrant = (points[i].x > 0) + (points[i].y > 0) * 2;\n    atomicAdd(&s_bins[quadrant], 1);\n  }\n  __syncthreads();\n\n  if (threadIdx.x < 4) {\n    atomicAdd(&bins[threadIdx.x], s_bins[threadIdx.x]);\n  }\n}",
            "//TODO: implement\n}",
            "__shared__ int tmp[4];\n    if (threadIdx.x < 4)\n        tmp[threadIdx.x] = 0;\n    __syncthreads();\n\n    for (int i = threadIdx.x; i < N; i += blockDim.x) {\n        if (points[i].x >= 0.0 && points[i].y >= 0.0) {\n            atomicAdd(&tmp[0], 1);\n        } else if (points[i].x < 0.0 && points[i].y >= 0.0) {\n            atomicAdd(&tmp[1], 1);\n        } else if (points[i].x < 0.0 && points[i].y < 0.0) {\n            atomicAdd(&tmp[2], 1);\n        } else if (points[i].x >= 0.0 && points[i].y < 0.0) {\n            atomicAdd(&tmp[3], 1);\n        }\n    }\n    __syncthreads();\n\n    atomicAdd(&bins[0], tmp[0]);\n    atomicAdd(&bins[1], tmp[1]);\n    atomicAdd(&bins[2], tmp[2]);\n    atomicAdd(&bins[3], tmp[3]);\n}",
            "// your code here\n}",
            "// TODO: count the number of points in each quadrant\n  // Hint:\n  // 1. There are 4 quadrants, so use an array of size 4 to store the results\n  // 2. Calculate the quadrant for each point\n  // 3. Use atomics to count the points in each quadrant. See: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomic-functions\n  for(size_t i = threadIdx.x; i < N; i+=blockDim.x) {\n    Point p = points[i];\n    if(p.x >= 0 && p.y >= 0) {\n      atomicAdd(&bins[0], 1);\n    }\n    else if(p.x < 0 && p.y >= 0) {\n      atomicAdd(&bins[1], 1);\n    }\n    else if(p.x < 0 && p.y < 0) {\n      atomicAdd(&bins[2], 1);\n    }\n    else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "// your code here\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int quadrant = 0;\n        if (points[tid].x > 0) {\n            quadrant += 1;\n        }\n        if (points[tid].y > 0) {\n            quadrant += 2;\n        }\n        atomicAdd(&bins[quadrant], 1);\n    }\n}",
            "// TODO: Your code here\n  //int i = threadIdx.x;\n  //bins[i] = 0;\n  //printf(\"threadIdx.x = %d\\n\", threadIdx.x);\n\n  //printf(\"threadIdx.x = %d\\n\", threadIdx.x);\n  //int tid = threadIdx.x;\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (i < N)\n  {\n    Point p = points[i];\n    if (p.x >= 0 && p.y >= 0)\n    {\n      bins[0]++;\n    }\n    if (p.x < 0 && p.y >= 0)\n    {\n      bins[1]++;\n    }\n    if (p.x < 0 && p.y < 0)\n    {\n      bins[2]++;\n    }\n    if (p.x >= 0 && p.y < 0)\n    {\n      bins[3]++;\n    }\n  }\n}",
            "// TODO: your code here\n    return;\n}",
            "// TODO\n}",
            "size_t tid = threadIdx.x;\n    size_t idx = blockIdx.x * blockDim.x + tid;\n    if (idx >= N)\n        return;\n    if (points[idx].x > 0) {\n        if (points[idx].y > 0)\n            atomicAdd(&bins[0], 1);\n        else if (points[idx].y < 0)\n            atomicAdd(&bins[3], 1);\n        else\n            atomicAdd(&bins[1], 1);\n    }\n    else {\n        if (points[idx].y > 0)\n            atomicAdd(&bins[2], 1);\n        else if (points[idx].y < 0)\n            atomicAdd(&bins[1], 1);\n        else\n            atomicAdd(&bins[0], 1);\n    }\n}",
            "__shared__ unsigned int s_count[4];\n   __shared__ Point sharedPoints[1024];\n\n   int tid = threadIdx.x;\n   int bid = blockIdx.x;\n\n   if (tid < N)\n   {\n      sharedPoints[tid] = points[tid];\n   }\n\n   s_count[bid] = 0;\n   __syncthreads();\n\n   for (int i = tid; i < N; i += blockDim.x)\n   {\n      if (sharedPoints[i].x > 0 && sharedPoints[i].y > 0)\n      {\n         atomicAdd(&s_count[0], 1);\n      }\n      else if (sharedPoints[i].x < 0 && sharedPoints[i].y > 0)\n      {\n         atomicAdd(&s_count[1], 1);\n      }\n      else if (sharedPoints[i].x < 0 && sharedPoints[i].y < 0)\n      {\n         atomicAdd(&s_count[2], 1);\n      }\n      else if (sharedPoints[i].x > 0 && sharedPoints[i].y < 0)\n      {\n         atomicAdd(&s_count[3], 1);\n      }\n   }\n\n   __syncthreads();\n   bins[bid] = s_count[bid];\n}",
            "...\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid >= N) return;\n   size_t idx = blockIdx.x;\n   if (points[idx].x >= 0 && points[idx].y >= 0)\n      atomicAdd(&bins[0], 1);\n   else if (points[idx].x < 0 && points[idx].y < 0)\n      atomicAdd(&bins[1], 1);\n   else if (points[idx].x < 0 && points[idx].y >= 0)\n      atomicAdd(&bins[2], 1);\n   else\n      atomicAdd(&bins[3], 1);\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use MPI and OpenMP to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "}",
            "/* Your code here */\n\n   int n_procs = 0, proc_rank = 0;\n   MPI_Comm_size(MPI_COMM_WORLD, &n_procs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);\n\n   std::vector<Point> local_points;\n   size_t n_points = 0;\n   int local_rank = 0;\n\n   // get points from process rank\n   if (proc_rank == 0) {\n      local_rank = 0;\n      local_points = points;\n      n_points = points.size();\n   } else {\n      local_rank = proc_rank;\n      n_points = n_procs / 2;\n      local_points.resize(n_points);\n   }\n\n   // partition points\n   // create a partitioner and distribute points\n   std::vector<Point> local_points_1;\n   std::vector<Point> local_points_2;\n\n   omp_set_num_threads(1);\n\n   if (n_procs % 2!= 0) {\n      local_points_1.resize(n_points / 2);\n      local_points_2.resize(n_points / 2 + 1);\n   } else {\n      local_points_1.resize(n_points / 2);\n      local_points_2.resize(n_points / 2);\n   }\n\n   // receive points from other ranks\n   std::vector<Point> received_points;\n   MPI_Status recv_status;\n\n   if (local_rank > 0) {\n      MPI_Recv(local_points_1.data(), local_points_1.size(), MPI_BYTE,\n               local_rank - 1, local_rank, MPI_COMM_WORLD, &recv_status);\n   }\n\n   if (local_rank < n_procs - 1) {\n      MPI_Recv(local_points_2.data(), local_points_2.size(), MPI_BYTE,\n               local_rank + 1, local_rank, MPI_COMM_WORLD, &recv_status);\n   }\n\n   // send points to other ranks\n   if (local_rank > 0) {\n      MPI_Send(local_points.data(), local_points_1.size(), MPI_BYTE,\n               local_rank - 1, local_rank, MPI_COMM_WORLD);\n   }\n\n   if (local_rank < n_procs - 1) {\n      MPI_Send(local_points.data() + local_points_1.size(), local_points_2.size(), MPI_BYTE,\n               local_rank + 1, local_rank, MPI_COMM_WORLD);\n   }\n\n   // divide points into quadrants\n   omp_set_num_threads(1);\n\n   std::array<size_t, 4> bin_counts = {};\n\n   if (local_rank == 0) {\n      for (size_t i = 0; i < local_points_1.size(); i++) {\n         if (local_points_1[i].x > 0 && local_points_1[i].y > 0) {\n            bin_counts[0]++;\n         } else if (local_points_1[i].x > 0 && local_points_1[i].y < 0) {\n            bin_counts[1]++;\n         } else if (local_points_1[i].x < 0 && local_points_1[i].y > 0) {\n            bin_counts[2]++;\n         } else if (local_points_1[i].x < 0 && local_points_1[i].y < 0) {\n            bin_counts[3]++;\n         }\n      }\n\n      for (size_t i = 0; i < local_points_2.size(); i++) {\n         if (local_points_2[i].x > 0 && local_points_2[i].y > 0) {\n            bin_counts[0]++;\n         } else if (local_points",
            "int world_size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   std::vector<double> x;\n   std::vector<double> y;\n   std::vector<Point> image;\n\n   for (int i = 0; i < points.size(); ++i) {\n      image.push_back(points[i]);\n      x.push_back(points[i].x);\n      y.push_back(points[i].y);\n   }\n\n   std::sort(x.begin(), x.end());\n   std::sort(y.begin(), y.end());\n\n   std::vector<int> x_indexes(x.size());\n   std::iota(x_indexes.begin(), x_indexes.end(), 0);\n   std::vector<int> y_indexes(y.size());\n   std::iota(y_indexes.begin(), y_indexes.end(), 0);\n\n   std::vector<Point> local_image(image.begin(), image.begin() + (image.size() / world_size));\n\n   int x_index = 0;\n   int y_index = 0;\n\n   std::vector<Point> tmp_image;\n   std::vector<Point> tmp_image_x;\n   std::vector<Point> tmp_image_y;\n\n   int num_x = (int) x.size() / world_size;\n   int num_y = (int) y.size() / world_size;\n\n   int num_x_plus = (int) x.size() % world_size;\n   int num_y_plus = (int) y.size() % world_size;\n\n   if (rank < num_x_plus)\n      num_x++;\n   if (rank < num_y_plus)\n      num_y++;\n\n   int chunk_size = (int)local_image.size() / num_x / num_y;\n\n   for (int i = 0; i < chunk_size; ++i) {\n      tmp_image.push_back(local_image[i]);\n   }\n\n   for (int i = 0; i < num_x; ++i) {\n      tmp_image_x.push_back(tmp_image[x_indexes[i]]);\n   }\n\n   for (int i = 0; i < num_y; ++i) {\n      tmp_image_y.push_back(tmp_image_x[y_indexes[i]]);\n   }\n\n   int x_index_rank = 0;\n   int y_index_rank = 0;\n\n   for (int i = 0; i < num_x; ++i) {\n      for (int j = 0; j < num_y; ++j) {\n         if (tmp_image_y[j].x >= 0 && tmp_image_y[j].y >= 0) {\n            bins[0]++;\n         }\n         else if (tmp_image_y[j].x <= 0 && tmp_image_y[j].y >= 0) {\n            bins[1]++;\n         }\n         else if (tmp_image_y[j].x <= 0 && tmp_image_y[j].y <= 0) {\n            bins[2]++;\n         }\n         else if (tmp_image_y[j].x >= 0 && tmp_image_y[j].y <= 0) {\n            bins[3]++;\n         }\n      }\n   }\n\n   MPI_Allreduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n}",
            "// 1. Figure out how many quadrants we need\n   int n_x, n_y;\n   int size;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &n_x);\n   n_x = size / n_x;\n\n   std::array<size_t, 4> local_bins = { 0, 0, 0, 0 };\n\n   // 2. Set up the Cartesian topology\n   int dims[2] = {n_x, n_y};\n   MPI_Cart_create(MPI_COMM_WORLD, 2, dims, false, false, &MPI_COMM_CART);\n\n   // 3. Figure out where the quadrant is\n   int coords[2] = {0, 0};\n   MPI_Cart_coords(MPI_COMM_CART, n_x, 2, coords);\n\n   // 4. Find the quadrant\n   int quad_x = 0;\n   int quad_y = 0;\n\n   if (points.size() == 0) {\n      MPI_Comm_free(&MPI_COMM_CART);\n      return;\n   }\n\n   for (int i = 0; i < points.size(); i++) {\n      if (points[i].x > 0) {\n         quad_x = 1;\n      }\n      else {\n         quad_x = 0;\n      }\n      if (points[i].y > 0) {\n         quad_y = 1;\n      }\n      else {\n         quad_y = 0;\n      }\n      if (quad_x == 0 && quad_y == 0) {\n         local_bins[0]++;\n      }\n      else if (quad_x == 1 && quad_y == 0) {\n         local_bins[1]++;\n      }\n      else if (quad_x == 0 && quad_y == 1) {\n         local_bins[2]++;\n      }\n      else if (quad_x == 1 && quad_y == 1) {\n         local_bins[3]++;\n      }\n   }\n   // 5. Send to everyone else\n   MPI_Barrier(MPI_COMM_WORLD);\n\n   // 6. Recieve from everyone else\n\n   MPI_Gatherv(&local_bins, sizeof(size_t), MPI_BYTE,\n               &bins, &local_bins, sizeof(size_t), MPI_BYTE,\n               0, MPI_COMM_CART);\n\n   // 7. Free up the topology\n   MPI_Comm_free(&MPI_COMM_CART);\n}",
            "int rank;\n   int num_ranks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n   // each rank needs to know the dimensions of the entire image\n   int image_size;\n   MPI_Bcast(&image_size, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n   // create a 2d cartesian communicator with image_size ranks in each direction\n   int dims[2] = {image_size, image_size};\n   int periods[2] = {1, 1};\n   int coords[2];\n   MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 0, MPI_COMM_CART);\n   MPI_Cart_coords(MPI_COMM_WORLD, rank, 2, coords);\n\n   // create a 1d communicator using the same communicator as the 2d communicator\n   MPI_Comm comm_row;\n   MPI_Cart_sub(MPI_COMM_WORLD, coords, &comm_row);\n\n   int size_row;\n   MPI_Comm_size(comm_row, &size_row);\n   int rank_row;\n   MPI_Comm_rank(comm_row, &rank_row);\n\n   // compute the start and end row index for this rank\n   int n_rows = image_size / size_row;\n   int start_row = n_rows * rank_row;\n   int end_row = (rank_row == size_row-1)? image_size : (start_row + n_rows);\n\n   // create an OpenMP team for the row.\n   // all threads in the team are assigned to the same rank in the cartesian communicator\n   // OpenMP uses static scheduling for threads in a team\n   #pragma omp parallel num_threads(size_row)\n   {\n      int thread_rank;\n      #pragma omp master\n      {\n         thread_rank = omp_get_thread_num();\n      }\n      #pragma omp barrier\n\n      // compute the start and end column index for this thread\n      int start_col = start_row + thread_rank;\n      int end_col = (thread_rank == size_row-1)? image_size : (start_row + n_rows + 1);\n\n      for (int i = start_col; i < end_col; ++i) {\n         int col = i % image_size;\n         // iterate through the list of points\n         for (auto const& p : points) {\n            if (p.y >= start_col && p.y < end_col && p.x >= col && p.x < col+1) {\n               // check if the point is in the current quadrant\n               if (p.x >= 0 && p.x < 1) {\n                  if (p.y >= 0 && p.y < 1) {\n                     // point is in quadrant 1\n                     #pragma omp atomic\n                     bins[0]++;\n                  } else if (p.y >= 1 && p.y < 2) {\n                     // point is in quadrant 2\n                     #pragma omp atomic\n                     bins[1]++;\n                  }\n               } else if (p.x >= 1 && p.x < 2) {\n                  if (p.y >= 0 && p.y < 1) {\n                     // point is in quadrant 3\n                     #pragma omp atomic\n                     bins[2]++;\n                  } else if (p.y >= 1 && p.y < 2) {\n                     // point is in quadrant 4\n                     #pragma omp atomic\n                     bins[3]++;\n                  }\n               }\n            }\n         }\n      }\n   }\n\n   // gather the counts from all ranks on rank 0\n   if (rank == 0) {\n      size_t *bins_row = new size_t[size_row];\n      MPI_Allgather(bins.data(), size_row, MPI_U",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int num_ranks;\n    MPI_Comm_size(comm, &num_ranks);\n    int rank;\n    MPI_Comm_rank(comm, &rank);\n\n    int size = points.size();\n    int quot = size/num_ranks;\n    int rem = size%num_ranks;\n    int num_quads;\n    if (rank == num_ranks - 1)\n        num_quads = quot+rem;\n    else\n        num_quads = quot;\n    int count_per_rank = num_quads/4;\n    int rem_quad = num_quads%4;\n    int num_quad_per_rank;\n    if (rank == num_ranks - 1)\n        num_quad_per_rank = count_per_rank+rem_quad;\n    else\n        num_quad_per_rank = count_per_rank;\n    std::vector<Point> quadrants;\n    int count;\n    for (int i = 0; i < num_quad_per_rank; i++) {\n        quadrants.push_back(points[rank*count_per_rank + i]);\n        if (quadrants[i].x >= 0) {\n            if (quadrants[i].y >= 0)\n                count++;\n            else\n                count--;\n        } else {\n            if (quadrants[i].y >= 0)\n                count++;\n            else\n                count--;\n        }\n    }\n    // MPI_Barrier(comm);\n    // int count = 0;\n    // int num_quads = 0;\n    // int count_per_rank = 0;\n    // int i = 0;\n    // MPI_Status status;\n    // if (rank == 0) {\n    //     while (i < num_ranks) {\n    //         if (points[i].x >= 0) {\n    //             if (points[i].y >= 0)\n    //                 count++;\n    //             else\n    //                 count--;\n    //         } else {\n    //             if (points[i].y >= 0)\n    //                 count++;\n    //             else\n    //                 count--;\n    //         }\n    //         i++;\n    //     }\n    //     MPI_Send(&count, 1, MPI_INT, num_ranks - 1, 0, comm);\n    // } else if (rank == num_ranks - 1) {\n    //     MPI_Recv(&count, 1, MPI_INT, 0, 0, comm, &status);\n    //     MPI_Send(&count, 1, MPI_INT, num_ranks - 1, 0, comm);\n    //     MPI_Recv(&count, 1, MPI_INT, 0, 0, comm, &status);\n    //     MPI_Send(&count, 1, MPI_INT, num_ranks - 1, 0, comm);\n    //     MPI_Recv(&count, 1, MPI_INT, 0, 0, comm, &status);\n    //     MPI_Send(&count, 1, MPI_INT, num_ranks - 1, 0, comm);\n    //     MPI_Recv(&count, 1, MPI_INT, 0, 0, comm, &status);\n    //     MPI_Send(&count, 1, MPI_INT, num_ranks - 1, 0, comm);\n    //     MPI_Recv(&count, 1, MPI_INT, 0, 0, comm, &status);\n    //     MPI_Send(&count, 1, MPI_INT, num_ranks - 1, 0, comm);\n    //     MPI_Recv(&count, 1, MPI_INT, 0, 0, comm, &status);\n    // } else {\n    //     MPI_Recv(&count, 1, MPI_INT, 0, 0, comm, &status);\n    //     MPI_Send(&count, 1, MPI_INT, num_ran",
            "// initialize bin values to 0\n   std::fill(bins.begin(), bins.end(), 0);\n\n   // MPI init\n   int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int size;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // MPI split\n   int color = rank % 2;\n   MPI_Comm comm;\n   MPI_Comm_split(MPI_COMM_WORLD, color, rank, &comm);\n   int subsize;\n   MPI_Comm_size(comm, &subsize);\n\n   // OpenMP init\n   omp_set_num_threads(subsize);\n\n   // each process only works on its part of the data\n   std::vector<Point> myPoints(points.begin() + rank, points.begin() + rank + subsize);\n\n   // binning\n   #pragma omp parallel for\n   for (int i = 0; i < myPoints.size(); i++) {\n      double x = myPoints[i].x;\n      double y = myPoints[i].y;\n      if (x >= 0 && y >= 0)\n         bins[0]++;\n      else if (x <= 0 && y >= 0)\n         bins[1]++;\n      else if (x <= 0 && y <= 0)\n         bins[2]++;\n      else\n         bins[3]++;\n   }\n\n   // gather\n   if (rank == 0) {\n      std::vector<size_t> localBins(bins.begin(), bins.begin() + 2);\n      MPI_Gather(localBins.data(), 2, MPI_UNSIGNED_LONG_LONG, bins.data(), 2, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n      for (int i = 2; i < size; i++) {\n         int rank = i + 1;\n         MPI_Recv(&bins[i], 2, MPI_UNSIGNED_LONG_LONG, rank, rank, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n   } else {\n      MPI_Send(bins.data(), 2, MPI_UNSIGNED_LONG_LONG, 0, rank, MPI_COMM_WORLD);\n   }\n\n   // cleanup\n   MPI_Comm_free(&comm);\n}",
            "// TODO\n    MPI_Comm cart_comm;\n    int num_ranks;\n    int rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    const int dims[2] = {2, 2};\n    int periods[2] = {0, 0};\n    int reorder = 0;\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, reorder, &cart_comm);\n\n    int coords[2] = {0, 0};\n    MPI_Cart_coords(cart_comm, rank, 2, coords);\n    int y_rank = coords[0];\n    int x_rank = coords[1];\n\n    int size;\n    MPI_Comm_size(cart_comm, &size);\n\n    int y_dim = dims[0];\n    int x_dim = dims[1];\n    int y_stride = y_dim / size;\n    int x_stride = x_dim / size;\n\n    std::vector<Point> points_local;\n\n    if (y_rank == 0) {\n        if (x_rank == 0) {\n            points_local = std::vector<Point>(points.begin(), points.begin() + x_stride);\n        }\n        else {\n            points_local = std::vector<Point>(points.begin() + x_rank * x_stride,\n                                              points.begin() + x_rank * x_stride + x_stride);\n        }\n    }\n    else {\n        if (x_rank == 0) {\n            points_local = std::vector<Point>(points.begin() + y_rank * y_stride,\n                                              points.begin() + y_rank * y_stride + y_stride);\n        }\n        else {\n            points_local = std::vector<Point>(points.begin() + y_rank * y_stride + x_rank * x_stride,\n                                              points.begin() + y_rank * y_stride + x_rank * x_stride + x_stride);\n        }\n    }\n\n    omp_set_num_threads(16);\n\n    int local_bins[4];\n    for (int i = 0; i < 4; ++i) {\n        local_bins[i] = 0;\n    }\n\n    for (int i = 0; i < points_local.size(); ++i) {\n        if (points_local[i].x > 0) {\n            if (points_local[i].y > 0) {\n                ++local_bins[0];\n            }\n            else {\n                ++local_bins[1];\n            }\n        }\n        else {\n            if (points_local[i].y > 0) {\n                ++local_bins[2];\n            }\n            else {\n                ++local_bins[3];\n            }\n        }\n    }\n\n    MPI_Allreduce(local_bins, bins.data(), 4, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    MPI_Comm_free(&cart_comm);\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; ++i) {\n            std::cout << bins[i] << \" \";\n        }\n        std::cout << std::endl;\n    }\n}",
            "size_t localPoints = points.size();\n\n   //Get the cartesian topology and rank information.\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int dims[2] = {size, size};\n   int periods[2] = {0, 0};\n   int reorder = 0;\n   MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, reorder, &cartComm);\n\n   //Divide the number of points per rank.\n   int blockPoints = localPoints / size;\n   int remainder = localPoints % size;\n\n   //Find the bounds of the current process's block.\n   int low = rank * blockPoints + std::min(rank, remainder);\n   int high = (rank + 1) * blockPoints + std::min(rank + 1, remainder);\n\n   //Divide the quadrants into blocks.\n   int blocks[4];\n   for (int i = 0; i < 4; i++) {\n      blocks[i] = blockPoints;\n      if (i!= 0) {\n         blocks[i] += 1;\n      }\n   }\n\n   //Make each rank's block contiguous in memory.\n   std::vector<Point> localPoints(points.begin() + low, points.begin() + high);\n\n   //Calculate the number of points in each quadrant.\n   for (int i = 0; i < 4; i++) {\n\n      //Get the Cartesian coordinates of the quadrant.\n      int coords[2];\n      MPI_Cart_coords(cartComm, i, 2, coords);\n\n      //Calculate the number of points in the quadrant by summing all the points in the quadrant.\n      size_t count = 0;\n      for (auto it = localPoints.begin(); it!= localPoints.end(); ++it) {\n         if ((*it).x > 0 && (*it).y > 0) {\n            count += 1;\n         }\n         else if ((*it).x < 0 && (*it).y > 0) {\n            count += 1;\n         }\n         else if ((*it).x < 0 && (*it).y < 0) {\n            count += 1;\n         }\n         else if ((*it).x > 0 && (*it).y < 0) {\n            count += 1;\n         }\n      }\n      bins[i] = count;\n   }\n   //Get the total number of points per quadrant.\n   MPI_Allreduce(MPI_IN_PLACE, bins.data(), 4, MPI_UNSIGNED, MPI_SUM, cartComm);\n\n   MPI_Comm_free(&cartComm);\n}",
            "int nproc;\n    int rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    std::vector<size_t> local_bins(4, 0);\n\n    for (auto const &point: points) {\n        if (point.x > 0 && point.y > 0) {\n            local_bins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            local_bins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            local_bins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            local_bins[3]++;\n        }\n    }\n\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    MPI_Reduce(&local_bins[0], &bins[0], 1, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&local_bins[1], &bins[1], 1, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&local_bins[2], &bins[2], 1, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&local_bins[3], &bins[3], 1, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "auto const& rank = omp_get_thread_num();\n    auto const& size = omp_get_num_threads();\n    MPI_Status status;\n\n    int rank_x;\n    int rank_y;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank_x);\n    MPI_Comm_size(MPI_COMM_WORLD, &rank_y);\n\n    double x_min = 0.0;\n    double x_max = 1.0;\n    double y_min = 0.0;\n    double y_max = 1.0;\n\n    int n_rows = rank_y;\n    int n_cols = rank_x;\n    int x_dim = n_rows;\n    int y_dim = n_cols;\n\n    if(rank_x == 0) {\n        x_min = 0.0;\n        x_max = 0.5;\n    }\n\n    if(rank_y == 0) {\n        y_min = 0.0;\n        y_max = 0.5;\n    }\n\n    // if(rank_x == 0 && rank_y == 0) {\n    //     x_min = 0.0;\n    //     x_max = 1.0;\n    //     y_min = 0.0;\n    //     y_max = 1.0;\n    // }\n\n    double x_diff = (x_max - x_min) / x_dim;\n    double y_diff = (y_max - y_min) / y_dim;\n\n    int quadrant_counts[4] = {0, 0, 0, 0};\n    for(int i = 0; i < points.size(); i++) {\n        double x = points[i].x;\n        double y = points[i].y;\n        int quadrant;\n        if(x >= x_min && x <= x_max && y >= y_min && y <= y_max) {\n            if(x >= x_min && x <= x_min + x_diff && y >= y_min && y <= y_min + y_diff) {\n                quadrant = 0;\n            }\n            else if(x >= x_min + x_diff && x <= x_min + x_diff + x_diff && y >= y_min && y <= y_min + y_diff) {\n                quadrant = 1;\n            }\n            else if(x >= x_min + x_diff && x <= x_min + x_diff + x_diff && y >= y_min + y_diff && y <= y_min + y_diff + y_diff) {\n                quadrant = 2;\n            }\n            else if(x >= x_min + x_diff + x_diff && x <= x_max && y >= y_min + y_diff && y <= y_min + y_diff + y_diff) {\n                quadrant = 3;\n            }\n            else {\n                quadrant = 0;\n            }\n\n            quadrant_counts[quadrant]++;\n        }\n    }\n\n    MPI_Reduce(quadrant_counts, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int size;\n    MPI_Comm_size(comm, &size);\n\n    //TODO\n    int rank;\n    MPI_Comm_rank(comm, &rank);\n    int nproc = size;\n    int rem = size % 2;\n\n    size_t n;\n    size_t nchunk;\n    if (rank < rem) {\n        n = points.size();\n        nchunk = n/2;\n    } else {\n        n = (points.size() - nchunk)/2;\n        nchunk = n/2;\n    }\n    bins.fill(0);\n    if (rank < rem) {\n        for (size_t i = 0; i < n; i++) {\n            if (points[i + nchunk].x > 0 && points[i + nchunk].y > 0) {\n                bins[0]++;\n            } else if (points[i + nchunk].x < 0 && points[i + nchunk].y > 0) {\n                bins[1]++;\n            } else if (points[i + nchunk].x < 0 && points[i + nchunk].y < 0) {\n                bins[2]++;\n            } else if (points[i + nchunk].x > 0 && points[i + nchunk].y < 0) {\n                bins[3]++;\n            }\n        }\n    } else {\n        for (size_t i = 0; i < n; i++) {\n            if (points[i + nchunk].x > 0 && points[i + nchunk].y > 0) {\n                bins[0]++;\n            } else if (points[i + nchunk].x < 0 && points[i + nchunk].y > 0) {\n                bins[1]++;\n            } else if (points[i + nchunk].x < 0 && points[i + nchunk].y < 0) {\n                bins[2]++;\n            } else if (points[i + nchunk].x > 0 && points[i + nchunk].y < 0) {\n                bins[3]++;\n            }\n        }\n    }\n\n    //TODO: reduce bins\n    std::array<size_t, 4> bins2;\n    MPI_Allreduce(&bins[0], &bins2[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, comm);\n    bins = bins2;\n\n    //TODO: print\n    if (rank == 0) {\n        for (int i = 0; i < size; i++) {\n            std::cout << \"rank \" << i << \": \" << bins[i] << std::endl;\n        }\n    }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int rows = size;\n    int cols = 2;\n    int dims[2] = {rows, cols};\n    int periods[2] = {0, 1};\n    int reorder = 1;\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, reorder, &mpi_cart);\n    int coords[2];\n    MPI_Cart_coords(mpi_cart, rank, 2, coords);\n\n    int dim = 4;\n    MPI_Datatype cart_type;\n    MPI_Type_vector(dim, 1, 2, MPI_INT, &cart_type);\n    MPI_Type_commit(&cart_type);\n\n    int counts[4] = {0, 0, 0, 0};\n    int disp[4];\n    int displ = 0;\n    for(int i = 0; i < 4; i++) {\n        disp[i] = displ;\n        displ += dim;\n    }\n\n    MPI_Gatherv(counts, dim, MPI_INT, bins.data(), disp, cart_type, 0, mpi_cart);\n\n    // for(int i = 0; i < size; i++) {\n    //     printf(\"[%d] = %d %d %d %d\\n\", i, counts[i], counts[i+4], counts[i+8], counts[i+12]);\n    // }\n    // MPI_Barrier(MPI_COMM_WORLD);\n}",
            "int nx = 2, ny = 2;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int x, y;\n    int rx, ry;\n    int width = 20, height = 10;\n\n    if (rank == 0) {\n        for (int j = 0; j < height; j++) {\n            for (int i = 0; i < width; i++) {\n                x = i;\n                y = j;\n                rx = (x + 1) % nx;\n                ry = (y + 1) % ny;\n                double xc = (x + 0.5) * width / nx;\n                double yc = (y + 0.5) * height / ny;\n\n                Point p;\n                p.x = xc;\n                p.y = yc;\n                for (int i = 0; i < points.size(); i++) {\n                    if ((points[i].x >= xc - 0.5 && points[i].x < xc + 0.5) &&\n                        (points[i].y >= yc - 0.5 && points[i].y < yc + 0.5)) {\n                        if (rx == 1 && ry == 1) {\n                            bins[3]++;\n                        }\n                        if (rx == 1 && ry == 0) {\n                            bins[2]++;\n                        }\n                        if (rx == 0 && ry == 1) {\n                            bins[0]++;\n                        }\n                        if (rx == 0 && ry == 0) {\n                            bins[1]++;\n                        }\n                    }\n                }\n            }\n        }\n    } else {\n        for (int j = 0; j < height; j++) {\n            for (int i = 0; i < width; i++) {\n                x = i;\n                y = j;\n                rx = (x + 1) % nx;\n                ry = (y + 1) % ny;\n                double xc = (x + 0.5) * width / nx;\n                double yc = (y + 0.5) * height / ny;\n\n                Point p;\n                p.x = xc;\n                p.y = yc;\n                for (int i = 0; i < points.size(); i++) {\n                    if ((points[i].x >= xc - 0.5 && points[i].x < xc + 0.5) &&\n                        (points[i].y >= yc - 0.5 && points[i].y < yc + 0.5)) {\n                        if (rx == 1 && ry == 1) {\n                            bins[3]++;\n                        }\n                        if (rx == 1 && ry == 0) {\n                            bins[2]++;\n                        }\n                        if (rx == 0 && ry == 1) {\n                            bins[0]++;\n                        }\n                        if (rx == 0 && ry == 0) {\n                            bins[1]++;\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int dims[2];\n    dims[0] = 2;\n    dims[1] = 2;\n\n    int periods[2] = {1, 1};\n\n    MPI_Comm cart_comm;\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 0, &cart_comm);\n\n    //std::cout << \"rank: \" << rank << \" x coordinate: \" << points[0].x << \" y coordinate: \" << points[0].y << std::endl;\n    //std::cout << \"rank: \" << rank << \" x coordinate: \" << points[1].x << \" y coordinate: \" << points[1].y << std::endl;\n    //std::cout << \"rank: \" << rank << \" x coordinate: \" << points[2].x << \" y coordinate: \" << points[2].y << std::endl;\n    //std::cout << \"rank: \" << rank << \" x coordinate: \" << points[3].x << \" y coordinate: \" << points[3].y << std::endl;\n    //std::cout << \"rank: \" << rank << \" x coordinate: \" << points[4].x << \" y coordinate: \" << points[4].y << std::endl;\n    //std::cout << \"rank: \" << rank << \" x coordinate: \" << points[5].x << \" y coordinate: \" << points[5].y << std::endl;\n\n    // if (rank == 0)\n    // {\n    //     for (int i = 0; i < points.size(); i++)\n    //     {\n    //         //std::cout << \"rank: \" << rank << \" x coordinate: \" << points[i].x << \" y coordinate: \" << points[i].y << std::endl;\n    //     }\n    // }\n\n    int coords[2];\n    MPI_Cart_coords(cart_comm, rank, 2, coords);\n\n    size_t n_points = points.size();\n\n    //std::cout << \"rank: \" << rank << \" coords: \" << coords[0] << \" \" << coords[1] << std::endl;\n    //std::cout << \"rank: \" << rank << \" coords: \" << coords[0] << \" \" << coords[1] << \" n_points: \" << n_points << std::endl;\n\n    size_t local_points = 0;\n    size_t n_local_points = 0;\n\n    #pragma omp parallel\n    {\n        #pragma omp single\n        {\n            MPI_Status status;\n\n            // std::cout << \"rank: \" << rank << \" coords: \" << coords[0] << \" \" << coords[1] << std::endl;\n\n            if (coords[0] == 0 && coords[1] == 0)\n            {\n                // std::cout << \"rank: \" << rank << \" coords: \" << coords[0] << \" \" << coords[1] << \" local points: \" << local_points << std::endl;\n\n                // std::cout << \"rank: \" << rank << \" coords: \" << coords[0] << \" \" << coords[1] << \" points: \" << points.size() << std::endl;\n\n                MPI_Recv(&n_local_points, 1, MPI_LONG, MPI_ANY_SOURCE, MPI_ANY_TAG, cart_comm, &status);\n\n                // std::cout << \"rank: \" << rank << \" coords: \" << coords[0] << \" \" << coords[1] << \" n_local_points: \" << n_local_points << std::endl;\n\n                // std::cout << \"rank: \" << rank << \" coords: \" << coords[0] << \" \" << coords[1] << \" n_local_points: \" << n_local_points << std::endl;\n\n                // for (int i = 0; i < n_points; i",
            "int my_rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n   int num_procs;\n   MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n   int num_per_proc = points.size()/num_procs;\n   int remainder = points.size()%num_procs;\n   std::vector<Point> local_points;\n   if (my_rank < remainder) {\n      local_points = std::vector<Point>(points.begin()+num_per_proc*my_rank,\n         points.begin()+num_per_proc*my_rank+num_per_proc+1);\n   }\n   else {\n      local_points = std::vector<Point>(points.begin()+num_per_proc*my_rank+remainder,\n         points.begin()+num_per_proc*my_rank+num_per_proc+remainder);\n   }\n\n   std::array<size_t, 4> local_bins{0};\n   #pragma omp parallel for\n   for (size_t i = 0; i < local_points.size(); i++) {\n      if (local_points[i].x > 0 && local_points[i].y > 0) {\n         local_bins[0]++;\n      }\n      else if (local_points[i].x < 0 && local_points[i].y > 0) {\n         local_bins[1]++;\n      }\n      else if (local_points[i].x < 0 && local_points[i].y < 0) {\n         local_bins[2]++;\n      }\n      else {\n         local_bins[3]++;\n      }\n   }\n\n   // MPI\n   std::array<size_t, 4> all_bins{0};\n   MPI_Allreduce(&local_bins[0], &all_bins[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n   bins = all_bins;\n\n   if (my_rank == 0) {\n      std::cout << \"Rank 0: \" << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << \"\\n\";\n   }\n}",
            "int nranks;\n   MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n   int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   //int nprocs = omp_get_num_threads();\n   //int thread_num = omp_get_thread_num();\n\n\n\n\n   int nthreads = omp_get_max_threads();\n   int thread_num = omp_get_thread_num();\n\n\n\n\n\n\n   int nprocs = nranks;\n\n   //bins = std::array<size_t, 4>();\n   //std::vector<int> bins = {0,0,0,0};\n   //std::array<size_t, 4> bins;\n\n\n   //std::vector<Point> image = points;\n   //int rank = 0;\n   //int nranks = 1;\n\n   int xdim = 2;\n   int ydim = 2;\n\n   int xdim_glob = 2;\n   int ydim_glob = 2;\n\n\n\n   std::array<int, 4> cart_coords = {-1,-1,-1,-1};\n\n   std::array<int, 2> dims = {xdim, ydim};\n\n   std::array<int, 2> periods = {0, 0};\n\n   std::array<int, 2> reorder = {1, 1};\n\n   //int ndims = 2;\n\n   MPI_Dims_create(nranks, 2, dims.data());\n\n   //MPI_Cart_create(MPI_COMM_WORLD, 2, dims.data(), periods.data(), reorder.data(), &cart);\n\n\n   MPI_Cart_create(MPI_COMM_WORLD, 2, dims.data(), periods.data(), reorder.data(), &cart);\n\n\n   //MPI_Comm_size(cart, &nranks);\n   MPI_Cart_coords(cart, rank, 2, cart_coords.data());\n\n   //if (rank == 0) {\n   //   printf(\"MPI_CART_CREATE: %d ranks, %d x %d (%d,%d)\\n\", nranks, xdim_glob, ydim_glob, cart_coords[0], cart_coords[1]);\n   //}\n\n\n   int npoints = points.size();\n\n   std::vector<int> points_per_rank;\n   //std::vector<int> points_per_rank(nranks);\n   //points_per_rank = std::vector<int>(nranks);\n\n   std::vector<int> points_per_rank_total(nranks);\n\n   //std::vector<int> points_per_rank_total = std::vector<int>(nranks);\n\n   MPI_Allreduce(&npoints, &points_per_rank[0], 1, MPI_INT, MPI_SUM, cart);\n\n\n\n\n\n   int points_per_rank_local = npoints / nranks;\n   int points_per_rank_remainder = npoints % nranks;\n\n   for (int r = 0; r < nranks; r++) {\n      if (r == rank) {\n         points_per_rank_total[r] = points_per_rank_local + points_per_rank_remainder;\n      }\n      else {\n         points_per_rank_total[r] = points_per_rank_local;\n      }\n   }\n\n   //MPI_Allreduce(&points_per_rank_local, &points_per_rank[0], 1, MPI_INT, MPI_SUM, cart);\n   //printf(\"points per rank local: %d\\n\", points_per_rank_local);\n   //printf(\"points per rank total: %d\\n\", points_per_rank_total[rank]);\n\n   //if (rank == 0) {\n   //   for (int r = 0; r < nranks; r++) {\n   //      printf(\"%d \", points_per_rank_",
            "// TODO\n}",
            "int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    //Calculate the number of elements that each process will receive.\n    int element_per_process = points.size() / world_size;\n    int remainder = points.size() % world_size;\n\n    int start_index = world_rank * element_per_process;\n    int end_index = start_index + element_per_process;\n\n    if(world_rank == world_size - 1)\n        end_index += remainder;\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n\n    //Count the number of elements that belong to each quadrant.\n    for(int i = start_index; i < end_index; ++i) {\n        if(points[i].x >= 0 && points[i].y >= 0)\n            local_bins[0]++;\n        else if(points[i].x >= 0 && points[i].y < 0)\n            local_bins[1]++;\n        else if(points[i].x < 0 && points[i].y >= 0)\n            local_bins[2]++;\n        else\n            local_bins[3]++;\n    }\n\n    //Send the number of elements in each quadrant to the rank 0.\n    MPI_Gather(local_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n    //If the rank 0, add all the numbers to get the final result.\n    if(world_rank == 0) {\n        size_t sum_bins[4] = {0, 0, 0, 0};\n        for(int i = 0; i < world_size; ++i) {\n            for(int j = 0; j < 4; ++j) {\n                sum_bins[j] += bins[i][j];\n            }\n        }\n        for(int i = 0; i < 4; ++i)\n            bins[i] = sum_bins[i];\n    }\n\n    //Wait for the ranks to finish.\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "// get the number of processes\n    int num_proc = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n\n    // get my rank\n    int my_rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n    // get the size of the image\n    int size = points.size();\n\n    // the number of points we will distribute to each processor\n    size_t proc_size = size / num_proc;\n\n    // the remaining points\n    size_t remaining = size % num_proc;\n\n    // the size of the quadrant bins\n    int bin_size = 4;\n\n    // bins for each process\n    std::vector<std::array<size_t, 4>> bins_proc(num_proc, std::array<size_t, 4>{0,0,0,0});\n\n    // determine the points we own and the remaining\n    std::vector<Point> points_proc;\n    std::vector<Point> points_remaining;\n\n    if(my_rank < remaining){\n        points_proc.insert(points_proc.end(), points.begin() + (my_rank * (proc_size + 1)), points.begin() + ((my_rank + 1) * (proc_size + 1)));\n    }\n    else{\n        points_proc.insert(points_proc.end(), points.begin() + (my_rank * proc_size), points.begin() + ((my_rank + 1) * proc_size));\n    }\n    points_remaining.insert(points_remaining.end(), points.begin() + (my_rank * proc_size), points.begin() + ((my_rank + 1) * proc_size));\n\n    // determine the number of bins each process has\n    std::vector<size_t> bin_num_proc(num_proc, 0);\n\n    // loop through each quadrant and determine how many points it contains\n    for(auto point : points_proc){\n        double q_x = point.x;\n        double q_y = point.y;\n\n        if(q_x >= 0 && q_y >= 0){\n            bin_num_proc[my_rank]++;\n        }\n        else if(q_x < 0 && q_y < 0){\n            bin_num_proc[my_rank]++;\n        }\n        else if(q_x < 0 && q_y >= 0){\n            bin_num_proc[my_rank]++;\n        }\n        else if(q_x >= 0 && q_y < 0){\n            bin_num_proc[my_rank]++;\n        }\n    }\n\n    // determine the points we own and the remaining\n    std::vector<Point> points_proc_remaining;\n    std::vector<Point> points_proc_remaining_proc;\n    points_proc_remaining.insert(points_proc_remaining.end(), points_remaining.begin() + (my_rank * proc_size), points_remaining.begin() + ((my_rank + 1) * proc_size));\n\n    // send and receive the counts\n    MPI_Alltoall(bin_num_proc.data(), 1, MPI_UNSIGNED, bins_proc.data(), 1, MPI_UNSIGNED, MPI_COMM_WORLD);\n\n    // determine the bins of each process\n    for(auto point : points_proc_remaining){\n        double q_x = point.x;\n        double q_y = point.y;\n\n        if(q_x >= 0 && q_y >= 0){\n            bins_proc[my_rank][0]++;\n        }\n        else if(q_x < 0 && q_y < 0){\n            bins_proc[my_rank][1]++;\n        }\n        else if(q_x < 0 && q_y >= 0){\n            bins_proc[my_rank][2]++;\n        }\n        else if(q_x >= 0 && q_y < 0){\n            bins_proc[my_rank][3]++;\n        }\n    }",
            "int rank = 0, commsize = 1, root = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &commsize);\n\n    int xdim = commsize;\n    int ydim = commsize;\n    int dims[2] = {xdim, ydim};\n    int periods[2] = {true, true};\n    int reorder = 0;\n    MPI_Dims_create(commsize, 2, dims);\n\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, reorder, &cart_comm);\n\n    int ndims = 2;\n    int coords[2];\n    MPI_Cart_coords(cart_comm, rank, ndims, coords);\n\n    bins = std::array<size_t, 4>{};\n    std::vector<Point> points_loc(points.begin() + coords[0] * (points.size() / xdim) + coords[1] * points.size() / ydim,\n                                  points.begin() + coords[0] * (points.size() / xdim) + coords[1] * points.size() / ydim +\n                                  points.size() / ydim);\n    for (const auto &point: points_loc) {\n        if (point.x > 0 && point.y > 0) {\n            bins[0]++;\n        }\n        if (point.x < 0 && point.y > 0) {\n            bins[1]++;\n        }\n        if (point.x < 0 && point.y < 0) {\n            bins[2]++;\n        }\n        if (point.x > 0 && point.y < 0) {\n            bins[3]++;\n        }\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n    if (rank == root) {\n        for (int i = 0; i < commsize; ++i) {\n            for (int j = 0; j < commsize; ++j) {\n                int dest = i * commsize + j;\n                MPI_Send(bins.data(), 4, MPI_UNSIGNED, dest, 0, cart_comm);\n                MPI_Status status;\n                MPI_Recv(bins.data(), 4, MPI_UNSIGNED, dest, 0, cart_comm, &status);\n            }\n        }\n    } else {\n        MPI_Status status;\n        MPI_Recv(bins.data(), 4, MPI_UNSIGNED, root, 0, cart_comm, &status);\n        MPI_Send(bins.data(), 4, MPI_UNSIGNED, root, 0, cart_comm);\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "}",
            "// TODO: Implement me\n   int myrank, n_proc;\n   MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n   MPI_Comm_size(MPI_COMM_WORLD, &n_proc);\n   std::vector<Point> local_points(points);\n   size_t points_per_rank = points.size()/n_proc;\n   // sort to get a balanced decomposition\n   if (myrank == 0) {\n      std::sort(local_points.begin(), local_points.begin() + points_per_rank);\n   }\n   // gather on rank 0\n   if (myrank == 0) {\n      std::vector<Point> global_points;\n      for (int i = 0; i < n_proc; i++) {\n         MPI_Recv(&local_points[0], points_per_rank, MPI_BYTE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         global_points.insert(global_points.end(), local_points.begin(), local_points.end());\n      }\n      // sort the global vector\n      std::sort(global_points.begin(), global_points.end());\n      for (int i = 0; i < 4; i++) {\n         for (int j = 0; j < points_per_rank; j++) {\n            if (global_points[i*points_per_rank + j].x > 0 && global_points[i*points_per_rank + j].y > 0) {\n               bins[i]++;\n            }\n            if (global_points[i*points_per_rank + j].x > 0 && global_points[i*points_per_rank + j].y < 0) {\n               bins[i]++;\n            }\n            if (global_points[i*points_per_rank + j].x < 0 && global_points[i*points_per_rank + j].y > 0) {\n               bins[i]++;\n            }\n            if (global_points[i*points_per_rank + j].x < 0 && global_points[i*points_per_rank + j].y < 0) {\n               bins[i]++;\n            }\n         }\n      }\n   } else {\n      // send to rank 0\n      MPI_Send(&local_points[0], points_per_rank, MPI_BYTE, 0, 0, MPI_COMM_WORLD);\n   }\n\n\n   // return bins;\n}",
            "}",
            "size_t numRanks;\n    int rank;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int xPeriod = 10;\n    int yPeriod = 10;\n    int dims[2];\n    dims[0] = numRanks;\n    dims[1] = numRanks;\n    MPI_Dims_create(numRanks, 2, dims);\n\n    // Create Cartesian topology\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, NULL, false, MPI_COMM_WORLD);\n\n    // Calculate quadrants\n    MPI_Comm comm;\n    MPI_Cart_sub(MPI_COMM_WORLD, {true, true}, &comm);\n\n    MPI_Status status;\n    int xCoord, yCoord;\n    MPI_Cart_coords(MPI_COMM_WORLD, rank, 2, &xCoord, &yCoord);\n\n    int xLeftBound = xCoord * xPeriod;\n    int xRightBound = xLeftBound + xPeriod;\n    int yLowerBound = yCoord * yPeriod;\n    int yUpperBound = yLowerBound + yPeriod;\n\n    std::vector<Point> myPoints;\n    std::copy_if(points.begin(), points.end(), std::back_inserter(myPoints),\n        [xLeftBound, xRightBound, yLowerBound, yUpperBound](Point p) {\n            return p.x >= xLeftBound && p.x < xRightBound && p.y >= yLowerBound && p.y < yUpperBound;\n        });\n\n    size_t count = 0;\n    if (myPoints.size() == 0) {\n        MPI_Reduce(nullptr, &count, 1, MPI_LONG_LONG, MPI_SUM, 0, comm);\n    } else {\n        for (Point p: myPoints) {\n            if (p.x < 0 && p.y < 0) {\n                count++;\n            } else if (p.x >= 0 && p.y < 0) {\n                count++;\n            } else if (p.x < 0 && p.y >= 0) {\n                count++;\n            } else {\n                count++;\n            }\n        }\n    }\n\n    MPI_Reduce(&count, &bins[0], 1, MPI_LONG_LONG, MPI_SUM, 0, comm);\n\n    // Free resources\n    MPI_Barrier(MPI_COMM_WORLD);\n    MPI_Comm_free(&comm);\n}",
            "// TODO: your code here\n}",
            "int xdim, ydim;\n    int xrank, yrank;\n    MPI_Comm_size(MPI_COMM_WORLD, &xdim);\n    MPI_Comm_size(MPI_COMM_WORLD, &ydim);\n    MPI_Comm_rank(MPI_COMM_WORLD, &xrank);\n    MPI_Comm_rank(MPI_COMM_WORLD, &yrank);\n    int xdim_new = xdim - 1;\n    int ydim_new = ydim - 1;\n    int xcoord = xrank % xdim_new;\n    int ycoord = yrank % ydim_new;\n    int count = 0;\n    for (auto const& i: points) {\n        if (i.x >= xcoord * xdim_new / xdim && i.x < (xcoord + 1) * xdim_new / xdim &&\n            i.y >= ycoord * ydim_new / ydim && i.y < (ycoord + 1) * ydim_new / ydim)\n            count++;\n    }\n    bins[xcoord * ydim_new / ydim + ycoord] = count;\n}",
            "// TODO: Your code here\n   // MPI Variables\n   int rank;\n   int size;\n   // Count of data in each quadrant.\n   int points_count[4];\n   // Calculating the rank in each quadrant\n   for (int i = 0; i < points.size(); i++) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         points_count[0]++;\n      } else if (points[i].x > 0 && points[i].y < 0) {\n         points_count[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         points_count[2]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         points_count[3]++;\n      }\n   }\n\n   // Parallelization using OpenMP\n   #pragma omp parallel for\n   for (int i = 0; i < 4; i++) {\n      bins[i] = 0;\n      for (int j = 0; j < points_count[i]; j++) {\n         bins[i]++;\n      }\n   }\n}",
            "// 1) Use MPI_Reduce to count the number of points in each quadrant. Assume every rank has a complete copy of image\n    //    and store the result in bins on rank 0\n    // 2) For every rank use MPI_Bcast to distribute bins to the other ranks\n    // 3) Use OpenMP to count in parallel\n\n    MPI_Status status;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> localBins;\n\n    for (int i = 0; i < 4; i++) {\n        localBins[i] = 0;\n    }\n\n    // 1)\n    for (auto& point : points) {\n        switch (classify(point.x, point.y)) {\n            case 1:\n                localBins[0]++;\n                break;\n            case 2:\n                localBins[1]++;\n                break;\n            case 3:\n                localBins[2]++;\n                break;\n            case 4:\n                localBins[3]++;\n                break;\n            default:\n                std::cout << \"ERROR: no quadrant specified\" << std::endl;\n                break;\n        }\n    }\n    // 2)\n    if (rank == 0) {\n        for (int i = 0; i < size; i++) {\n            MPI_Bcast(localBins.data(), 4, MPI_LONG, i, MPI_COMM_WORLD);\n        }\n    } else {\n        MPI_Bcast(localBins.data(), 4, MPI_LONG, 0, MPI_COMM_WORLD);\n    }\n\n    // 3)\n    #pragma omp parallel for\n    for (int i = 0; i < 4; i++) {\n        bins[i] += localBins[i];\n    }\n}",
            "// TODO: Your code goes here.\n   bins = {0,0,0,0};\n   //TODO: get number of rows and columns from the image\n   int N, M;\n   MPI_Comm_size(MPI_COMM_WORLD, &N);\n   MPI_Comm_rank(MPI_COMM_WORLD, &M);\n   //TODO: get the local size of the image\n   int local_N, local_M;\n   MPI_Comm_size(MPI_COMM_WORLD, &local_N);\n   MPI_Comm_rank(MPI_COMM_WORLD, &local_M);\n   std::vector<Point> local_points;\n\n   int local_points_size;\n   if (local_M == 0 && local_N == 0)\n   {\n      local_points_size = points.size();\n      local_points = points;\n   }\n   else\n   {\n      MPI_Bcast(&local_points_size, 1, MPI_INT, 0, MPI_COMM_WORLD);\n      MPI_Bcast(&local_points, local_points_size, MPI_POINT, 0, MPI_COMM_WORLD);\n   }\n   int local_points_size2;\n   if (local_M == 0)\n   {\n      local_points_size2 = local_points.size();\n   }\n   MPI_Bcast(&local_points_size2, 1, MPI_INT, 0, MPI_COMM_WORLD);\n   std::vector<int> count_x(local_points_size2, 0);\n   std::vector<int> count_y(local_points_size2, 0);\n   for (int i = 0; i < local_points_size2; i++)\n   {\n      if (local_points[i].x >= 0)\n      {\n         count_x[i] = count_x[i] + 1;\n      }\n      if (local_points[i].y >= 0)\n      {\n         count_y[i] = count_y[i] + 1;\n      }\n   }\n   #pragma omp parallel for\n   for (int i = 0; i < local_points_size2; i++)\n   {\n      if (count_x[i] > count_y[i])\n      {\n         bins[0] = bins[0] + count_x[i];\n      }\n      else\n      {\n         bins[1] = bins[1] + count_y[i];\n      }\n   }\n   if (local_M == 0)\n   {\n      MPI_Reduce(&bins[0], &bins[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n   else\n   {\n      MPI_Reduce(&bins[0], &bins[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n}",
            "// TODO\n   return;\n}",
            "bins.fill(0);\n    int nx, ny;\n    int x, y;\n    int rank;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &ny);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<Point> local(points.begin() + (ny*rank), points.begin() + (ny*(rank+1)));\n\n    #pragma omp parallel for\n    for (int i = 0; i < local.size(); i++)\n    {\n        x = static_cast<int>(round(local[i].x/2));\n        y = static_cast<int>(round(local[i].y/2));\n        if (x < 0 && y < 0) bins[0]++;\n        if (x < 0 && y >= 0) bins[1]++;\n        if (x >= 0 && y < 0) bins[2]++;\n        if (x >= 0 && y >= 0) bins[3]++;\n    }\n}",
            "// TODO: Your code here\n    // Hint: Use MPI_Allgatherv, MPI_Scan and MPI_Bcast\n    // MPI_Allgatherv can distribute the work among processes\n    // MPI_Scan can sum the local values\n    // MPI_Bcast can distribute the result on rank 0 to other ranks\n}",
            "// TODO\n}",
            "// TODO\n}",
            "int nproc, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<Point> local_points;\n    std::vector<Point> global_points;\n\n    // TODO: create local points vector for this process\n    //...\n\n    // TODO: create global points vector for all processes\n    //...\n\n    // TODO: count the points in the local and global vectors\n    //...\n\n    // TODO: split the results among processes\n    //...\n\n    // TODO: save the results in bins\n    //...\n\n}",
            "// TODO: fill in\n}",
            "auto const size = points.size();\n    std::array<size_t, 4> buffer;\n    std::fill(buffer.begin(), buffer.end(), 0);\n    std::fill(bins.begin(), bins.end(), 0);\n\n    int nprocs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int num_x = (rank % nprocs) + 1;\n    int num_y = (rank / nprocs) + 1;\n\n    // Split input data into 4 parts\n    std::vector<Point> parts[4];\n    std::vector<int> part_sizes(4);\n    part_sizes[0] = (int)(size / 4 * (num_y / 2 + 1) * (num_x / 2 + 1));\n    part_sizes[1] = (int)(size / 4 * (num_y / 2) * (num_x / 2));\n    part_sizes[2] = (int)(size / 4 * (num_y / 2) * (num_x / 2 + 1));\n    part_sizes[3] = (int)(size / 4 * (num_y / 2 + 1) * (num_x / 2));\n    for (int i = 0; i < 4; ++i) {\n        parts[i] = std::vector<Point>(points.begin() + part_sizes[i], points.begin() + part_sizes[i] + part_sizes[i + 1]);\n    }\n\n    // Count each point's quadrant\n    #pragma omp parallel num_threads(4)\n    {\n        int thread_id = omp_get_thread_num();\n        for (int i = 0; i < part_sizes[thread_id]; ++i) {\n            int x = part_sizes[thread_id] / 2 + 1;\n            int y = part_sizes[thread_id] / 2 + 1;\n            if (parts[thread_id][i].x >= 0) x = part_sizes[thread_id] / 2;\n            if (parts[thread_id][i].y >= 0) y = part_sizes[thread_id] / 2;\n            buffer[thread_id * 2 + x % 2 + y % 2] += 1;\n        }\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, buffer.data(), 4, MPI_LONG_LONG_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    // Reduce the local counts into global counts\n    for (int i = 0; i < 4; ++i) {\n        bins[i] += buffer[i];\n    }\n}",
            "int rank, num_procs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n    // MPI stuff\n    int x_div = points.size() / num_procs;\n    int x_rem = points.size() % num_procs;\n    int x_start = x_div * rank + std::min(rank, x_rem);\n    int x_end = x_start + x_div + (rank < x_rem? 1 : 0);\n\n    int y_div = num_procs / 2;\n    int y_rem = num_procs % 2;\n    int y_start = rank / y_div + std::min(rank / y_div, y_rem);\n    int y_end = y_start + y_div + (rank % y_div? 1 : 0);\n\n    std::array<size_t, 4> local_bins{0, 0, 0, 0};\n\n    // OpenMP stuff\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for(size_t i = x_start; i < x_end; i++){\n            if(points[i].x < 0 && points[i].y > 0){\n                local_bins[0]++;\n            } else if(points[i].x > 0 && points[i].y > 0){\n                local_bins[1]++;\n            } else if(points[i].x < 0 && points[i].y < 0){\n                local_bins[2]++;\n            } else if(points[i].x > 0 && points[i].y < 0){\n                local_bins[3]++;\n            }\n        }\n    }\n\n    MPI_Reduce(&local_bins, &bins, 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if(rank == 0){\n        bins[0] /= x_end;\n        bins[1] /= x_end;\n        bins[2] /= x_end;\n        bins[3] /= x_end;\n    }\n}",
            "// TODO:\n    // define a cartesian communicator\n    // define a 2D decomposition\n    // each rank owns a subsection of the domain\n    // each rank counts the number of points in its region\n    // each rank stores the count in bins\n    // rank 0 stores the counts in bins\n    // assume MPI has already been initialized\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double xmin = 0.0;\n    double xmax = 6.0;\n    double ymin = -5.0;\n    double ymax = 11.0;\n    std::array<double, 4> x0 = {xmin, xmax, xmin, xmax};\n    std::array<double, 4> y0 = {ymin, ymin, ymax, ymax};\n    int periods[2] = {1, 1};\n\n    MPI_Comm cartComm;\n    MPI_Dims_create(size, 2, &bins[0]);\n    MPI_Cart_create(MPI_COMM_WORLD, 2, &bins[0], periods, 0, &cartComm);\n    MPI_Comm_rank(cartComm, &rank);\n    MPI_Cart_coords(cartComm, rank, 2, &bins[0]);\n\n    std::vector<Point> image;\n    int imgSize;\n    if (rank == 0) {\n        imgSize = points.size();\n    }\n    MPI_Bcast(&imgSize, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    if (rank!= 0) {\n        image.resize(imgSize);\n    }\n    MPI_Scatter(points.data(), imgSize / size, MPI_POINT, image.data(), imgSize / size, MPI_POINT, 0, MPI_COMM_WORLD);\n\n    // OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < imgSize / size; i++) {\n        if (image[i].x > x0[bins[0]] && image[i].x < x0[bins[0]+1] && image[i].y > y0[bins[1]] && image[i].y < y0[bins[1]+1]) {\n            bins[0] += 1;\n        }\n        else if (image[i].x > x0[bins[0]] && image[i].x < x0[bins[0]+1] && image[i].y > y0[bins[1]+1] && image[i].y < y0[bins[1]+2]) {\n            bins[1] += 1;\n        }\n        else if (image[i].x > x0[bins[0]+1] && image[i].x < x0[bins[0]+2] && image[i].y > y0[bins[1]] && image[i].y < y0[bins[1]+1]) {\n            bins[0] += 1;\n        }\n        else if (image[i].x > x0[bins[0]+1] && image[i].x < x0[bins[0]+2] && image[i].y > y0[bins[1]+1] && image[i].y < y0[bins[1]+2]) {\n            bins[1] += 1;\n        }\n    }\n\n    if (rank == 0) {\n        MPI_Gather(bins.data(), 4, MPI_INT, bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n    else {\n        MPI_Gather(bins.data(), 4, MPI_INT, nullptr, 4, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}",
            "}",
            "const int N_POINTS = points.size();\n  int rank, nproc;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  int npoint_per_proc = N_POINTS / nproc;\n  int npoint_remain = N_POINTS % nproc;\n\n  std::vector<Point> point_proc;\n  if (rank == 0) {\n    point_proc = points;\n  }\n  if (rank!= 0) {\n    point_proc.resize(npoint_per_proc + (rank - 1 < npoint_remain));\n  }\n\n  std::array<size_t, 4> bins_proc = {};\n#pragma omp parallel for\n  for (size_t i = 0; i < point_proc.size(); ++i) {\n    Point const& p = point_proc[i];\n    int quadrant = (p.x > 0) * 2 + (p.y > 0);\n    bins_proc[quadrant]++;\n  }\n\n  if (rank == 0) {\n    for (size_t i = 0; i < 4; ++i) {\n      bins[i] = 0;\n    }\n  }\n  MPI_Reduce(bins_proc.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "size_t num_ranks, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int size = 2;\n    int dims[2];\n    dims[0] = size;\n    dims[1] = size;\n\n    int reorder = 1;\n    int periods[2] = {1, 1};\n    int coords[2];\n\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, reorder, MPI_COMM_CART);\n    MPI_Cart_coords(MPI_COMM_WORLD, rank, 2, coords);\n\n    // Get the size of the image and the number of points\n    int width, height;\n    width = height = 20;\n    int num_points = points.size();\n\n    int xmin, ymin, xmax, ymax;\n    int x = coords[0], y = coords[1];\n    xmin = x * width;\n    xmax = xmin + width;\n    ymin = y * height;\n    ymax = ymin + height;\n\n    int xmid = (xmin + xmax) / 2;\n    int ymid = (ymin + ymax) / 2;\n\n    int num_points_in_quadrant = 0;\n    for (int i = 0; i < num_points; i++) {\n        Point p = points[i];\n        if (p.x > xmin && p.x < xmid && p.y > ymin && p.y < ymid) num_points_in_quadrant++;\n    }\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    local_bins[0] = num_points_in_quadrant;\n\n#pragma omp parallel\n    {\n        int x = coords[0], y = coords[1];\n        int neighbor = coords[0] + 1;\n        if (neighbor >= size) neighbor = 0;\n        int nbr = y + x * size;\n        MPI_Request request;\n        MPI_Irecv(&bins[0], 1, MPI_INT, nbr, 0, MPI_COMM_WORLD, &request);\n        MPI_Isend(&local_bins[0], 1, MPI_INT, nbr, 0, MPI_COMM_WORLD, &request);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < size; i++) {\n            for (int j = 0; j < size; j++) {\n                int neighbor = j + i * size;\n                MPI_Request request;\n                MPI_Irecv(&bins[0], 1, MPI_INT, neighbor, 0, MPI_COMM_WORLD, &request);\n            }\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    MPI_Cart_destroy(MPI_COMM_WORLD);\n\n    for (int i = 0; i < 4; i++)\n        bins[i] += local_bins[i];\n\n    MPI_Finalize();\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   const int n = size;\n\n   std::vector<Point> localPoints;\n\n   if (rank == 0) {\n      localPoints = points;\n   }\n   else {\n      localPoints.resize(points.size() / n);\n   }\n\n   std::vector<int> binVector(4);\n   std::fill(binVector.begin(), binVector.end(), 0);\n\n   #pragma omp parallel for\n   for (int i = 0; i < (int)localPoints.size(); ++i) {\n      int quadrant = 0;\n      double x = localPoints[i].x;\n      double y = localPoints[i].y;\n\n      if (x > 0 && y >= 0) {\n         quadrant = 1;\n      }\n      else if (x < 0 && y < 0) {\n         quadrant = 2;\n      }\n      else if (x < 0 && y >= 0) {\n         quadrant = 3;\n      }\n\n      ++binVector[quadrant];\n   }\n\n   if (rank == 0) {\n      for (int i = 0; i < 4; ++i) {\n         bins[i] = binVector[i];\n      }\n   }\n\n   MPI_Gather(&binVector[0], 4, MPI_INT, &bins[0], 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      std::cout << \"quadrant 1: \" << bins[1] << std::endl;\n      std::cout << \"quadrant 2: \" << bins[2] << std::endl;\n      std::cout << \"quadrant 3: \" << bins[3] << std::endl;\n      std::cout << \"quadrant 4: \" << bins[0] << std::endl;\n   }\n}",
            "// TODO\n}",
            "int nx = 2;\n    int ny = 2;\n\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int rank = 0, size = 0;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &size);\n\n    int x, y;\n\n    int npoints = points.size();\n    //int chunk_size = npoints / size;\n\n    std::array<size_t, 4> local_bins;\n    std::array<size_t, 4> global_bins;\n\n    //local bins\n    for (auto& p : points) {\n        x = p.x < 0;\n        y = p.y < 0;\n        local_bins[x+y*nx]++;\n    }\n\n    //global bins\n    global_bins[0] = local_bins[0];\n    global_bins[1] = local_bins[1];\n    global_bins[2] = local_bins[2];\n    global_bins[3] = local_bins[3];\n\n    //reduce\n    MPI_Allreduce(global_bins.data(), bins.data(), 4, MPI_LONG_LONG, MPI_SUM, comm);\n}",
            "int rank, nranks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n  std::array<size_t, 4> bins_local;\n\n  int x_dim = 2;\n  int y_dim = 2;\n\n  int x_rank;\n  int y_rank;\n\n  int dims[2] = {x_dim, y_dim};\n\n  MPI_Dims_create(nranks, 2, dims);\n\n  int periods[2] = {false, false};\n  int reorder = true;\n\n  MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, reorder, &image);\n\n  MPI_Cart_get(image, 2, dims, periods, &coords);\n\n  x_rank = coords[0];\n  y_rank = coords[1];\n\n  MPI_Barrier(image);\n\n  std::vector<Point> points_copy = points;\n\n#pragma omp parallel for\n  for(int i = 0; i < points_copy.size(); i++)\n  {\n    if (points_copy[i].x > 0 && points_copy[i].y > 0)\n    {\n      bins_local[0]++;\n    }\n    else if (points_copy[i].x > 0 && points_copy[i].y < 0)\n    {\n      bins_local[1]++;\n    }\n    else if (points_copy[i].x < 0 && points_copy[i].y < 0)\n    {\n      bins_local[2]++;\n    }\n    else if (points_copy[i].x < 0 && points_copy[i].y > 0)\n    {\n      bins_local[3]++;\n    }\n  }\n\n  if (rank == 0)\n  {\n    MPI_Allreduce(MPI_IN_PLACE, bins_local.data(), 4, MPI_INT, MPI_SUM, image);\n\n    bins[0] = bins_local[0];\n    bins[1] = bins_local[1];\n    bins[2] = bins_local[2];\n    bins[3] = bins_local[3];\n  }\n  else\n  {\n    MPI_Reduce(bins_local.data(), bins_local.data(), 4, MPI_INT, MPI_SUM, 0, image);\n  }\n}",
            "bins = std::array<size_t, 4>{0};\n    int proc_count, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &proc_count);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    const int nx = 2, ny = 2;\n    const int nprocs = proc_count;\n    const int dims[] = {nx, ny};\n    MPI_Comm cartcomm;\n    MPI_Dims_create(proc_count, 2, dims);\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, nullptr, true, &cartcomm);\n    int coords[2];\n    MPI_Cart_coords(cartcomm, rank, 2, coords);\n    int x = coords[0];\n    int y = coords[1];\n    auto min_x = (x == 0? -5 : x * 5);\n    auto max_x = (x == nx - 1? 5 : min_x + 5);\n    auto min_y = (y == 0? -5 : y * 5);\n    auto max_y = (y == ny - 1? 5 : min_y + 5);\n    for (auto i : points) {\n        if (i.x > min_x && i.x < max_x && i.y > min_y && i.y < max_y) {\n            int x, y;\n            MPI_Cart_rank(cartcomm, &coords, &x);\n            bins[x] += 1;\n        }\n    }\n    if (rank == 0) {\n        for (int i = 1; i < proc_count; ++i) {\n            MPI_Status status;\n            MPI_Recv(bins.data(), 4, MPI_UNSIGNED_LONG_LONG, i, 0, cartcomm, &status);\n        }\n    }\n    else {\n        MPI_Send(bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, 0, cartcomm);\n    }\n}",
            "// TODO: your code here\n   int n_points=points.size();\n   int n_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &n_rank);\n   int n_point_rank=n_points/n_rank;\n   std::vector<Point> points_rank(points.begin()+n_point_rank*rank(),points.begin()+n_point_rank*(rank()+1));\n   int y_min=points[0].y;\n   int y_max=points[0].y;\n   int x_min=points[0].x;\n   int x_max=points[0].x;\n   for(int i=0; i<n_points; i++){\n      if(y_min>points[i].y){\n         y_min=points[i].y;\n      }\n      if(y_max<points[i].y){\n         y_max=points[i].y;\n      }\n      if(x_min>points[i].x){\n         x_min=points[i].x;\n      }\n      if(x_max<points[i].x){\n         x_max=points[i].x;\n      }\n   }\n\n   std::array<int, 4> n_in_bin;\n   for(int i=0; i<n_in_bin.size(); i++){\n      n_in_bin[i]=0;\n   }\n\n   #pragma omp parallel for\n   for(int i=0; i<points_rank.size(); i++){\n      if(points_rank[i].x>0 && points_rank[i].x<=1){\n         if(points_rank[i].y>0 && points_rank[i].y<=1){\n            n_in_bin[0]++;\n         }\n         else if(points_rank[i].y>1){\n            n_in_bin[1]++;\n         }\n         else{\n            n_in_bin[3]++;\n         }\n      }\n      else if(points_rank[i].x>1 && points_rank[i].x<=2){\n         n_in_bin[1]++;\n      }\n      else if(points_rank[i].x>2){\n         n_in_bin[2]++;\n      }\n      else{\n         if(points_rank[i].y>0 && points_rank[i].y<=1){\n            n_in_bin[3]++;\n         }\n         else if(points_rank[i].y>1){\n            n_in_bin[2]++;\n         }\n         else{\n            n_in_bin[0]++;\n         }\n      }\n   }\n\n   bins[0]=n_in_bin[0];\n   bins[1]=n_in_bin[1];\n   bins[2]=n_in_bin[2];\n   bins[3]=n_in_bin[3];\n\n   MPI_Barrier(MPI_COMM_WORLD);\n\n   if(rank()==0){\n      int n_rank;\n      MPI_Comm_size(MPI_COMM_WORLD, &n_rank);\n      int n_point_rank=n_points/n_rank;\n      for(int i=1; i<n_rank; i++){\n         int n_point_rank_other=n_point_rank*i;\n         MPI_Recv(&n_in_bin, 4, MPI_INT, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         bins[0]+=n_in_bin[0];\n         bins[1]+=n_in_bin[1];\n         bins[2]+=n_in_bin[2];\n         bins[3]+=n_in_bin[3];\n      }\n   }\n   else{\n      int n_point_rank_other=n_points/n_rank;\n      MPI_Send(&n_in_bin, 4, MPI_INT, 0, 1, MPI_COMM_WORLD);\n   }\n\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, size;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &size);\n\n    int block_size = points.size()/size;\n    int remainder = points.size() % size;\n\n    int local_block_size = block_size;\n    if(rank < remainder)\n        local_block_size++;\n\n    int x_quadrant = 0, y_quadrant = 0;\n    double x_range_min = 0, x_range_max = 0, y_range_min = 0, y_range_max = 0;\n\n    if(rank == 0) {\n        x_range_min = points.at(0).x;\n        y_range_min = points.at(0).y;\n        x_range_max = points.at(points.size()-1).x;\n        y_range_max = points.at(points.size()-1).y;\n    }\n    MPI_Bcast(&x_range_min, 1, MPI_DOUBLE, 0, comm);\n    MPI_Bcast(&x_range_max, 1, MPI_DOUBLE, 0, comm);\n    MPI_Bcast(&y_range_min, 1, MPI_DOUBLE, 0, comm);\n    MPI_Bcast(&y_range_max, 1, MPI_DOUBLE, 0, comm);\n\n\n    std::vector<Point> local_points;\n    for(int i = 0; i < local_block_size; i++) {\n        local_points.push_back(points.at(i + rank*block_size));\n    }\n\n    std::vector<int> local_bins;\n    local_bins.resize(4);\n\n    // loop over all points in the local array\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for(int i = 0; i < local_points.size(); i++) {\n            Point p = local_points.at(i);\n            if(p.x >= x_range_min && p.x < x_range_max) {\n                x_quadrant = 1;\n            }\n            if(p.y >= y_range_min && p.y < y_range_max) {\n                y_quadrant = 1;\n            }\n            local_bins.at(x_quadrant + y_quadrant*2)++;\n        }\n    }\n\n    if(rank == 0) {\n        // copy local array into the result array\n        bins = local_bins;\n        // copy result array to each rank and overwrite with the result\n        MPI_Gather(bins.data(), 4, MPI_INT, bins.data(), 4, MPI_INT, 0, comm);\n        MPI_Bcast(bins.data(), 4, MPI_INT, 0, comm);\n    } else {\n        MPI_Gather(local_bins.data(), 4, MPI_INT, bins.data(), 4, MPI_INT, 0, comm);\n    }\n\n}",
            "// Your code here\n    int ndims = 2;\n    int dims[ndims] = {1, 1};\n    int periods[ndims] = {true, true};\n    int reorder = true;\n    int coord[ndims];\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, nprocs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    if (rank == 0)\n    {\n        std::vector<Point> image;\n        for (size_t i = 0; i < points.size(); i++)\n        {\n            image.push_back(points[i]);\n        }\n        MPI_Cart_create(comm, ndims, dims, periods, reorder, &comm);\n        MPI_Cart_coords(comm, rank, ndims, coord);\n        int x = coord[0];\n        int y = coord[1];\n        int nx = dims[0];\n        int ny = dims[1];\n        if (x < nx/2 && y < ny/2)\n        {\n            bins[0] = image.size();\n        }\n        else if (x >= nx/2 && y < ny/2)\n        {\n            bins[1] = image.size();\n        }\n        else if (x < nx/2 && y >= ny/2)\n        {\n            bins[2] = image.size();\n        }\n        else if (x >= nx/2 && y >= ny/2)\n        {\n            bins[3] = image.size();\n        }\n    }\n    else\n    {\n        MPI_Cart_create(comm, ndims, dims, periods, reorder, &comm);\n        MPI_Cart_coords(comm, rank, ndims, coord);\n        int x = coord[0];\n        int y = coord[1];\n        int nx = dims[0];\n        int ny = dims[1];\n        std::vector<Point> image;\n        for (size_t i = 0; i < points.size(); i++)\n        {\n            image.push_back(points[i]);\n        }\n        if (x < nx/2 && y < ny/2)\n        {\n            MPI_Reduce(&image.size(), &bins[0], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, comm);\n        }\n        else if (x >= nx/2 && y < ny/2)\n        {\n            MPI_Reduce(&image.size(), &bins[1], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, comm);\n        }\n        else if (x < nx/2 && y >= ny/2)\n        {\n            MPI_Reduce(&image.size(), &bins[2], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, comm);\n        }\n        else if (x >= nx/2 && y >= ny/2)\n        {\n            MPI_Reduce(&image.size(), &bins[3], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, comm);\n        }\n    }\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    size_t total = points.size();\n    size_t my_total = total / 6;\n\n    std::array<size_t, 4> local_bins{0};\n\n    //parallel for\n    #pragma omp parallel for schedule(guided)\n    for (size_t i = 0; i < my_total; ++i) {\n        double x = points[i].x;\n        double y = points[i].y;\n\n        if (x < 0) {\n            if (y < 0) {\n                local_bins[0]++;\n            } else {\n                local_bins[1]++;\n            }\n        } else {\n            if (y < 0) {\n                local_bins[2]++;\n            } else {\n                local_bins[3]++;\n            }\n        }\n    }\n\n    //reduce\n    size_t final_bins[4]{0};\n    #pragma omp parallel for\n    for (int i = 0; i < 4; i++) {\n        #pragma omp atomic\n        final_bins[i] += local_bins[i];\n    }\n\n    if (rank == 0) {\n        bins = final_bins;\n    }\n}",
            "//TODO: implement\n}",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int dimensions[2] = {2, 2};\n   int periods[2] = {0, 0};\n   int coords[2] = {0, 0};\n\n   MPI_Dims_create(size, 2, dimensions);\n   MPI_Cart_create(MPI_COMM_WORLD, 2, dimensions, periods, 0, &cartcomm);\n   MPI_Cart_coords(cartcomm, rank, 2, coords);\n\n   int nprocs[2] = {dimensions[0], dimensions[1]};\n   std::array<int, 4> disp = {0, 0, 0, 0};\n   for (int i = 0; i < 4; i++) {\n      if (i % 2 == 0) {\n         disp[i] = coords[0];\n      }\n      else {\n         disp[i] = coords[1];\n      }\n   }\n\n   int* counts = new int[size];\n   counts[rank] = points.size();\n   MPI_Allgather(MPI_IN_PLACE, 0, MPI_INT, counts, 1, MPI_INT, MPI_COMM_WORLD);\n\n   size_t* recvcounts = new size_t[size];\n   size_t* displs = new size_t[size];\n\n   for (int i = 0; i < size; i++) {\n      recvcounts[i] = counts[i];\n      displs[i] = disp[i];\n   }\n\n   std::vector<Point> tmp = points;\n   size_t* recvdata = new size_t[size * 4];\n   MPI_Allgatherv(&tmp[0], 1, MPI_BYTE, &recvdata[0], recvcounts, displs, MPI_BYTE, MPI_COMM_WORLD);\n\n   std::array<int, 2> local = {nprocs[0] / 2, nprocs[1] / 2};\n\n   bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n\n   #pragma omp parallel for\n   for (int i = 0; i < size * 4; i++) {\n      if (recvdata[i].x < 0 && recvdata[i].y > 0) {\n         bins[0] += 1;\n      }\n      else if (recvdata[i].x < 0 && recvdata[i].y < 0) {\n         bins[1] += 1;\n      }\n      else if (recvdata[i].x > 0 && recvdata[i].y < 0) {\n         bins[2] += 1;\n      }\n      else if (recvdata[i].x > 0 && recvdata[i].y > 0) {\n         bins[3] += 1;\n      }\n   }\n}",
            "int rank = 0;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int numRanks = 0;\n   MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n   int numThreads = 1;\n   #pragma omp parallel\n   {\n      #pragma omp master\n      {\n         numThreads = omp_get_num_threads();\n      }\n   }\n\n   // Compute the number of bins per thread.\n   size_t numBinsPerThread = 4 / numThreads;\n   std::vector<std::array<size_t, 4>> binsPerThread(numThreads, std::array<size_t, 4>{0, 0, 0, 0});\n\n   // Compute the bin for each point.\n   #pragma omp parallel for\n   for (int i = 0; i < points.size(); ++i) {\n      Point p = points[i];\n      int binX = (p.x > 0);\n      int binY = (p.y > 0);\n      int threadId = omp_get_thread_num();\n      binsPerThread[threadId][binX + 2 * binY]++;\n   }\n\n   // Reduce the per-thread bins into the bins of the calling thread.\n   for (int i = 0; i < numThreads; ++i) {\n      for (int j = 0; j < 4; ++j) {\n         bins[j] += binsPerThread[i][j];\n      }\n   }\n\n   // Print out the bins.\n   if (rank == 0) {\n      for (int i = 0; i < 4; ++i) {\n         printf(\"bin %d: %ld\\n\", i, bins[i]);\n      }\n   }\n}",
            "size_t n = points.size();\n   size_t n_per_proc = n / omp_get_num_threads();\n\n   MPI_Comm comm = MPI_COMM_WORLD;\n   int rank, size;\n   MPI_Comm_size(comm, &size);\n   MPI_Comm_rank(comm, &rank);\n\n   std::vector<Point> localPoints;\n   for (int i = rank * n_per_proc; i < n; ++i) {\n      localPoints.push_back(points[i]);\n   }\n\n   // initialize bins\n   std::array<size_t, 4> localBins = {0, 0, 0, 0};\n\n   // count each quadrant in the local points\n   for (auto const& p : localPoints) {\n      if (p.x >= 0) {\n         if (p.y >= 0) {\n            localBins[0]++;\n         } else {\n            localBins[1]++;\n         }\n      } else {\n         if (p.y >= 0) {\n            localBins[2]++;\n         } else {\n            localBins[3]++;\n         }\n      }\n   }\n\n   // copy local bins to the global bins on rank 0\n   if (rank == 0) {\n      for (int i = 0; i < size; ++i) {\n         MPI_Recv(bins.data(), 4, MPI_INT, i, 0, comm, MPI_STATUS_IGNORE);\n         for (int j = 0; j < 4; ++j) {\n            bins[j] += localBins[j];\n         }\n      }\n   } else {\n      MPI_Send(localBins.data(), 4, MPI_INT, 0, 0, comm);\n   }\n\n   // sum up the bins on rank 0\n   if (rank == 0) {\n      for (int i = 1; i < size; ++i) {\n         MPI_Send(bins.data(), 4, MPI_INT, i, 0, comm);\n      }\n   } else {\n      MPI_Recv(bins.data(), 4, MPI_INT, 0, 0, comm, MPI_STATUS_IGNORE);\n   }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n   int rank = 0;\n   int size = 1;\n   MPI_Comm_rank(comm, &rank);\n   MPI_Comm_size(comm, &size);\n\n   // TODO: Your code here\n   bins = { 0, 0, 0, 0 };\n\n   // find the bounding rectangle for all points\n   double minx = std::numeric_limits<double>::max();\n   double maxx = std::numeric_limits<double>::min();\n   double miny = std::numeric_limits<double>::max();\n   double maxy = std::numeric_limits<double>::min();\n   for (size_t i = 0; i < points.size(); i++) {\n      if (points[i].x < minx) {\n         minx = points[i].x;\n      }\n      if (points[i].x > maxx) {\n         maxx = points[i].x;\n      }\n      if (points[i].y < miny) {\n         miny = points[i].y;\n      }\n      if (points[i].y > maxy) {\n         maxy = points[i].y;\n      }\n   }\n\n   // set up cartesian communicator\n   int dim = 2;\n   int* dims = new int[dim];\n   int* periods = new int[dim];\n   dims[0] = dims[1] = size;\n   periods[0] = periods[1] = 1;\n   int reorder = 1;\n   MPI_Cart_create(comm, dim, dims, periods, reorder, &comm);\n\n   // find the current cartesian coordinates\n   int* coords = new int[dim];\n   MPI_Cart_coords(comm, rank, dim, coords);\n\n   // calculate the number of points in each quadrant\n   int n = points.size();\n   int n_per_rank = n / size;\n   int remainder = n % size;\n   int n_per_process = n_per_rank;\n   if (rank < remainder) {\n      n_per_process++;\n   }\n   int start_idx = rank * n_per_rank;\n   if (rank < remainder) {\n      start_idx += rank;\n   }\n   else {\n      start_idx += remainder;\n   }\n\n   // count in parallel\n   std::vector<int> local_bins(4);\n   #pragma omp parallel\n   {\n      #pragma omp for\n      for (int i = start_idx; i < start_idx + n_per_process; i++) {\n         int quadrant = 0;\n         if (points[i].x >= maxx && points[i].y >= maxy) {\n            quadrant = 0;\n         }\n         else if (points[i].x <= minx && points[i].y >= maxy) {\n            quadrant = 1;\n         }\n         else if (points[i].x <= minx && points[i].y <= miny) {\n            quadrant = 2;\n         }\n         else if (points[i].x >= maxx && points[i].y <= miny) {\n            quadrant = 3;\n         }\n         #pragma omp atomic\n         local_bins[quadrant]++;\n      }\n   }\n\n   // reduce counts\n   MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, comm);\n\n   // clean up\n   delete[] dims;\n   delete[] periods;\n   delete[] coords;\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n   int rank, size;\n   MPI_Comm_rank(comm, &rank);\n   MPI_Comm_size(comm, &size);\n\n   int n_points = points.size();\n   int const per_proc = n_points / size;\n\n   int n_quadrants = 0;\n   int my_points = 0;\n   #pragma omp parallel\n   {\n      int thread_rank = omp_get_thread_num();\n      int thread_count = omp_get_num_threads();\n\n      int first_point = per_proc * thread_rank;\n      int last_point = first_point + per_proc;\n\n      if (last_point > n_points)\n         last_point = n_points;\n\n      my_points = last_point - first_point;\n\n      #pragma omp parallel for\n      for (int i = first_point; i < last_point; i++) {\n         double const x = points[i].x;\n         double const y = points[i].y;\n         int quadrant = 0;\n         if (x < 0) {\n            if (y > 0)\n               quadrant = 3;\n            else\n               quadrant = 2;\n         }\n         else {\n            if (y > 0)\n               quadrant = 1;\n         }\n         n_quadrants++;\n         bins[quadrant]++;\n      }\n   }\n\n   int* tmp_bins = new int[n_quadrants];\n\n   MPI_Gather(&my_points, 1, MPI_INT, tmp_bins, 1, MPI_INT, 0, comm);\n\n   if (rank == 0) {\n      int sum = 0;\n      for (int i = 0; i < size; i++) {\n         sum += tmp_bins[i];\n      }\n      delete[] tmp_bins;\n      for (int i = 0; i < n_quadrants; i++)\n         bins[i] = sum;\n   }\n}",
            "int rank;\n   int nproc;\n   MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int dimension = 2;\n\n   MPI_Dims_create(nproc, dimension, &bins[0]);\n\n   std::vector<Point> image;\n   image.reserve(points.size());\n\n   if(rank == 0) {\n\n      MPI_Scatter(&points[0], points.size(), MPI_DOUBLE, &image[0], points.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n      //for (int i = 0; i < bins[0] * bins[1]; i++) {\n      //   image.push_back(points[i]);\n      //}\n\n   } else {\n\n      MPI_Scatter(NULL, 0, MPI_DOUBLE, &image[0], points.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n   }\n\n   bins.fill(0);\n\n   if(rank == 0) {\n      #pragma omp parallel for\n      for (int i = 0; i < image.size(); i++) {\n         if (image[i].x >= 0.0 && image[i].x <= 1.0) {\n            if (image[i].y >= 0.0 && image[i].y <= 1.0) {\n               bins[0]++;\n            } else if (image[i].y > 1.0 && image[i].y <= 2.0) {\n               bins[1]++;\n            } else if (image[i].y > 2.0 && image[i].y <= 3.0) {\n               bins[2]++;\n            } else if (image[i].y > 3.0 && image[i].y <= 4.0) {\n               bins[3]++;\n            }\n         }\n      }\n\n   } else {\n\n      #pragma omp parallel for\n      for (int i = 0; i < image.size(); i++) {\n         if (image[i].x >= 0.0 && image[i].x <= 1.0) {\n            if (image[i].y >= 0.0 && image[i].y <= 1.0) {\n               bins[0]++;\n            } else if (image[i].y > 1.0 && image[i].y <= 2.0) {\n               bins[1]++;\n            } else if (image[i].y > 2.0 && image[i].y <= 3.0) {\n               bins[2]++;\n            } else if (image[i].y > 3.0 && image[i].y <= 4.0) {\n               bins[3]++;\n            }\n         }\n      }\n\n   }\n\n   MPI_Gather(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, &bins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n   if(rank == 0) {\n      bins[0] = bins[0] / nproc;\n      bins[1] = bins[1] / nproc;\n      bins[2] = bins[2] / nproc;\n      bins[3] = bins[3] / nproc;\n   }\n\n   MPI_Bcast(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "size_t size_image = points.size();\n    size_t size_bins = bins.size();\n\n    // Compute the dimensions of the domain, as a vector of size `size_image`\n    std::vector<double> domain(size_image);\n    for(size_t i=0;i<size_image;++i){\n        domain[i]=sqrt(pow(points[i].x,2)+pow(points[i].y,2));\n    }\n    // Sort the list\n    std::sort(domain.begin(),domain.end());\n\n    // Compute the size of each bin\n    double delta_domain = (domain[size_image-1]-domain[0])/(double)size_bins;\n    // Loop through all points and put them in the correct bin\n    for(size_t i=0;i<size_image;++i){\n        for(size_t j=0;j<size_bins;++j){\n            if(domain[i]>=j*delta_domain && domain[i]<=delta_domain*(j+1))\n                bins[j]++;\n        }\n    }\n}",
            "const double half = 0.5;\n   const size_t nbins = 4;\n   MPI_Comm comm = MPI_COMM_WORLD;\n   int rank, nprocs;\n   MPI_Comm_rank(comm, &rank);\n   MPI_Comm_size(comm, &nprocs);\n   std::vector<Point> image = points;\n   std::vector<Point> image_new;\n   std::vector<Point> image_bins;\n   MPI_Bcast(&image[0], image.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   image_new = image;\n   MPI_Barrier(comm);\n   double x_min, x_max, y_min, y_max;\n   int x_rank, y_rank;\n   int x_size, y_size;\n   // Set up MPI and OpenMP domains\n   MPI_Dims_create(nprocs, 2, &y_size);\n   MPI_Dims_create(nprocs, 2, &x_size);\n   MPI_Cart_create(comm, 2, &x_size, true, true, &comm);\n   MPI_Cart_coords(comm, rank, 2, &x_rank, &y_rank);\n   MPI_Cart_get(comm, 2, &x_size, &y_size, false, &y_rank);\n   MPI_Cart_get(comm, 2, &x_size, &y_size, false, &x_rank);\n   if (rank == 0) {\n      for (int i = 0; i < image.size(); i++) {\n         if (image[i].x < half && image[i].y < half) {\n            image_bins.push_back(image[i]);\n         }\n         else if (image[i].x < half && image[i].y > half) {\n            image_bins.push_back(image[i]);\n         }\n         else if (image[i].x > half && image[i].y > half) {\n            image_bins.push_back(image[i]);\n         }\n         else if (image[i].x > half && image[i].y < half) {\n            image_bins.push_back(image[i]);\n         }\n      }\n   }\n   // Set up OpenMP\n   int nthread = 2;\n   #pragma omp parallel num_threads(nthread)\n   {\n      int omp_id = omp_get_thread_num();\n      int omp_size = omp_get_num_threads();\n      // Get x_min, x_max, y_min, y_max\n      MPI_Allreduce(&x_min, &x_max, 1, MPI_DOUBLE, MPI_MAX, comm);\n      MPI_Allreduce(&x_min, &x_max, 1, MPI_DOUBLE, MPI_MAX, comm);\n      MPI_Allreduce(&y_min, &y_max, 1, MPI_DOUBLE, MPI_MAX, comm);\n      MPI_Allreduce(&y_min, &y_max, 1, MPI_DOUBLE, MPI_MAX, comm);\n      MPI_Allreduce(&x_min, &x_max, 1, MPI_DOUBLE, MPI_MAX, comm);\n      MPI_Allreduce(&y_min, &y_max, 1, MPI_DOUBLE, MPI_MAX, comm);\n      // Get the image domain\n      int local_size = image_bins.size();\n      int chunk_size = local_size / omp_size;\n      int rem_chunk = local_size % omp_size;\n      int start_ind = omp_id * chunk_size;\n      if (omp_id < rem_chunk) start_ind += omp_id;\n      else start_ind += rem_chunk;\n      int end_ind = start_ind + chunk_size;\n      if (omp_id < rem_chunk) end_ind += 1;\n      std::vector<Point> image_bins_new;\n      for (int i = start_",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    const int P = omp_get_num_threads();\n\n    // Divide the work\n    size_t start = rank * points.size() / P;\n    size_t end = (rank + 1) * points.size() / P;\n\n    // Calculate for a segment\n    std::array<size_t, 4> local_bins = { 0, 0, 0, 0 };\n    for (size_t i = start; i < end; ++i) {\n        auto& p = points[i];\n        if (p.x > 0 && p.y > 0) local_bins[0]++;\n        else if (p.x < 0 && p.y > 0) local_bins[1]++;\n        else if (p.x < 0 && p.y < 0) local_bins[2]++;\n        else if (p.x > 0 && p.y < 0) local_bins[3]++;\n    }\n\n    // Sum\n    std::array<size_t, 4> sums;\n    MPI_Reduce(local_bins.data(), sums.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0)\n        bins = sums;\n}",
            "const int nproc = omp_get_max_threads();\n    const int nprocx = bins.size();\n    const int nprocy = 1;\n    const int dims[2] = {nprocx, nprocy};\n    const int periods[2] = {0, 0};\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 0, &MPI_COMM_CART);\n    const int rank = MPI_Comm_rank(MPI_COMM_CART);\n    const int npoints = points.size();\n    const int chunk_size = npoints / nproc;\n    std::vector<int> counts(4, 0);\n    #pragma omp parallel\n    {\n        int i;\n        const int id = omp_get_thread_num();\n        const int start = id * chunk_size;\n        const int end = std::min(start + chunk_size, npoints);\n        #pragma omp for reduction(+:counts[0])\n        for(i = start; i < end; i++) {\n            Point p = points[i];\n            int quadrant = 0;\n            if(p.x > 0) {\n                quadrant = 1;\n            }\n            if(p.y > 0) {\n                quadrant = 2;\n            }\n            counts[quadrant] += 1;\n        }\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, &counts[0], 4, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    if(rank == 0) {\n        bins[0] = counts[0];\n        bins[1] = counts[1];\n        bins[2] = counts[2];\n        bins[3] = counts[3];\n    }\n    MPI_Cart_free(&MPI_COMM_CART);\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n\n   // Determine the total number of processors.\n   int num_procs, rank;\n   MPI_Comm_size(comm, &num_procs);\n   MPI_Comm_rank(comm, &rank);\n\n   // Create a 2D cartesian topology.\n   int dims[2];\n   dims[0] = 2;\n   dims[1] = 2;\n   int periods[2] = {1, 1};\n\n   // Use MPI to split the processors into 4 groups.\n   int reorder = 1;\n   MPI_Cart_create(comm, 2, dims, periods, reorder, &comm);\n\n   // Calculate the rank's position in the 2D grid.\n   int coords[2];\n   MPI_Cart_coords(comm, rank, 2, coords);\n\n   // Get the size of the sub-communicator that contains the current rank.\n   int local_size;\n   MPI_Comm_size(comm, &local_size);\n\n   // The total number of elements in the array is split equally among the ranks.\n   int global_size = points.size() / local_size;\n\n   // Get the starting and ending indices of the current rank.\n   int start = rank * global_size;\n   int end = (rank + 1) * global_size;\n\n   // Get the starting and ending indices of the current rank's local group.\n   int local_start = coords[0] * global_size;\n   int local_end = (coords[0] + 1) * global_size;\n\n   // Only calculate bins for the local group.\n   std::array<size_t, 4> local_bins;\n\n   #pragma omp parallel\n   {\n      // Get the local thread index.\n      int tid = omp_get_thread_num();\n\n      // Count the number of points in the local group.\n      for (int i = start; i < end; ++i) {\n         Point p = points[i];\n\n         int x_quadrant = p.x < 0.0;\n         int y_quadrant = p.y < 0.0;\n\n         if (x_quadrant && y_quadrant) {\n            ++local_bins[0];\n         }\n         else if (x_quadrant &&!y_quadrant) {\n            ++local_bins[1];\n         }\n         else if (!x_quadrant && y_quadrant) {\n            ++local_bins[2];\n         }\n         else {\n            ++local_bins[3];\n         }\n      }\n   }\n\n   // Gather the local bins to the rank 0.\n   MPI_Gather(&local_bins[0], 4, MPI_UNSIGNED, &bins[0], 4, MPI_UNSIGNED, 0, comm);\n}",
            "bins.fill(0);\n\n    int rank, n_proc, x_dim, y_dim;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &n_proc);\n    double dx = 10. / n_proc;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); i++) {\n        int x_index = (points[i].x + dx / 2) / dx;\n        int y_index = (points[i].y + dx / 2) / dx;\n        int bin = (y_index * x_dim + x_index);\n        bins[bin]++;\n    }\n}",
            "// your code goes here\n    size_t n_ranks, rank;\n    int dims[2];\n    MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Cart_get(MPI_COMM_WORLD, 2, dims, NULL, NULL);\n\n    std::array<std::array<size_t, 4>, 2> local_bins;\n    local_bins[0] = {0,0,0,0};\n    local_bins[1] = {0,0,0,0};\n\n    std::vector<Point> local_points;\n    for(int i=0; i<points.size(); i++){\n        if(points[i].x >= 0){\n            if(points[i].y >= 0){\n                local_points.push_back(points[i]);\n                local_bins[0][0] += 1;\n            }\n            else{\n                local_points.push_back(points[i]);\n                local_bins[1][0] += 1;\n            }\n        }\n        else{\n            if(points[i].y >= 0){\n                local_points.push_back(points[i]);\n                local_bins[0][1] += 1;\n            }\n            else{\n                local_points.push_back(points[i]);\n                local_bins[1][1] += 1;\n            }\n        }\n    }\n\n    // printf(\"local_points = %d\\n\", local_points.size());\n    // printf(\"local_bins = %d %d %d %d\\n\", local_bins[0][0], local_bins[0][1], local_bins[0][2], local_bins[0][3]);\n    // printf(\"local_bins = %d %d %d %d\\n\", local_bins[1][0], local_bins[1][1], local_bins[1][2], local_bins[1][3]);\n\n    std::array<size_t, 4> global_bins;\n    global_bins[0] = 0;\n    global_bins[1] = 0;\n    global_bins[2] = 0;\n    global_bins[3] = 0;\n\n    MPI_Reduce(local_bins[0].data(), global_bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if(rank == 0){\n        bins = global_bins;\n    }\n}",
            "// fill this in\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int x_dim = 4;\n    int y_dim = 2;\n    int x_dim_offset = 0;\n    int y_dim_offset = 0;\n    int x_dim_size = x_dim;\n    int y_dim_size = y_dim;\n    if (rank == 0) {\n        printf(\"rank 0\\n\");\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&x_dim_size, 1, MPI_INT, i, 1, MPI_COMM_WORLD);\n            MPI_Send(&y_dim_size, 1, MPI_INT, i, 2, MPI_COMM_WORLD);\n        }\n    }\n    else {\n        MPI_Status status;\n        MPI_Recv(&x_dim_size, 1, MPI_INT, 0, 1, MPI_COMM_WORLD, &status);\n        MPI_Recv(&y_dim_size, 1, MPI_INT, 0, 2, MPI_COMM_WORLD, &status);\n        printf(\"rank %d\\n\", rank);\n    }\n    std::vector<Point> points_local = points;\n    int rank_num_points = points_local.size();\n    int rank_offset = 0;\n    MPI_Status status;\n    if (rank == 0) {\n        // send rank_num_points to other ranks\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&rank_num_points, 1, MPI_INT, i, 3, MPI_COMM_WORLD);\n        }\n        for (int i = 1; i < size; i++) {\n            // receive rank_num_points from other ranks\n            MPI_Recv(&rank_num_points, 1, MPI_INT, i, 3, MPI_COMM_WORLD, &status);\n            int rank_offset = 0;\n            int send_size = 0;\n            int receive_size = 0;\n            int send_index = 0;\n            int receive_index = 0;\n            while (rank_offset < points_local.size()) {\n                // send points to other ranks\n                if (rank_offset + send_size < points_local.size() && rank_offset + send_size < rank_num_points) {\n                    send_size = rank_num_points - send_index;\n                }\n                else {\n                    send_size = points_local.size() - rank_offset;\n                }\n                MPI_Send(&points_local[rank_offset], send_size, MPI_DOUBLE, i, 4, MPI_COMM_WORLD);\n                // receive points from other ranks\n                if (rank_offset + receive_size < points_local.size() && rank_offset + receive_size < rank_num_points) {\n                    receive_size = rank_num_points - receive_index;\n                }\n                else {\n                    receive_size = points_local.size() - rank_offset;\n                }\n                MPI_Recv(&points_local[rank_offset], receive_size, MPI_DOUBLE, i, 4, MPI_COMM_WORLD, &status);\n                // update rank_offset\n                rank_offset += receive_size;\n            }\n        }\n    }\n    else {\n        MPI_Status status;\n        MPI_Recv(&rank_num_points, 1, MPI_INT, 0, 3, MPI_COMM_WORLD, &status);\n        int rank_offset = 0;\n        int send_size = 0;\n        int receive_size = 0;\n        int send_index = 0;\n        int receive_index = 0;\n        std::vector<double> points_other;\n        while (rank_offset < points_local.size()) {\n            // send points to other ranks\n            if (rank_offset",
            "size_t npoints = points.size();\n\tMPI_Comm comm = MPI_COMM_WORLD;\n\tint myrank;\n\tMPI_Comm_rank(comm, &myrank);\n\tint mpi_size;\n\tMPI_Comm_size(comm, &mpi_size);\n\n\tint *nquadrants = new int[4];\n\tint *global_nquadrants = new int[4];\n\tnquadrants[0] = 0;\n\tnquadrants[1] = 0;\n\tnquadrants[2] = 0;\n\tnquadrants[3] = 0;\n\n\tfor (int i = 0; i < npoints; i++) {\n\t\tif (points[i].x >= 0)\n\t\t\tnquadrants[0]++;\n\t\telse\n\t\t\tnquadrants[1]++;\n\n\t\tif (points[i].y >= 0)\n\t\t\tnquadrants[2]++;\n\t\telse\n\t\t\tnquadrants[3]++;\n\t}\n\n\tMPI_Allreduce(nquadrants, global_nquadrants, 4, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n\tif (myrank == 0) {\n\t\tbins[0] = global_nquadrants[0];\n\t\tbins[1] = global_nquadrants[1];\n\t\tbins[2] = global_nquadrants[2];\n\t\tbins[3] = global_nquadrants[3];\n\t}\n\n\tdelete[] nquadrants;\n\tdelete[] global_nquadrants;\n\tMPI_Barrier(MPI_COMM_WORLD);\n}",
            "const int size = 4;\n   const int rank = 0;\n   const int root = 0;\n   std::array<size_t, size> localBins = { 0, 0, 0, 0 };\n   std::array<size_t, size> globalBins = { 0, 0, 0, 0 };\n\n   #pragma omp parallel for\n   for(size_t i = 0; i < points.size(); ++i) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         localBins[0] += 1;\n      } else if (points[i].x > 0 && points[i].y < 0) {\n         localBins[1] += 1;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         localBins[2] += 1;\n      } else {\n         localBins[3] += 1;\n      }\n   }\n\n   MPI_Gather(localBins.data(), size, MPI_UNSIGNED_LONG_LONG, globalBins.data(), size, MPI_UNSIGNED_LONG_LONG, root, MPI_COMM_WORLD);\n\n   for (int i = 0; i < size; i++) {\n      bins[i] = globalBins[i];\n   }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int mpi_dims = 2;\n    int mpi_coords[2];\n    MPI_Dims_create(size, mpi_dims, mpi_coords);\n\n    int x_size = mpi_coords[0];\n    int y_size = mpi_coords[1];\n\n    MPI_Comm mpi_x_comm;\n    MPI_Cart_sub(MPI_COMM_WORLD, mpi_coords, &mpi_x_comm);\n\n    int mpi_x_rank;\n    MPI_Comm_rank(mpi_x_comm, &mpi_x_rank);\n\n    int mpi_y_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_y_rank);\n\n    int x_coords[2];\n    MPI_Cart_coords(MPI_COMM_WORLD, rank, mpi_dims, x_coords);\n\n    int y_coords[2];\n    MPI_Cart_coords(MPI_COMM_WORLD, rank, mpi_dims, y_coords);\n\n    int start = 0, end = 0;\n\n    if (x_coords[0] == 0) {\n        if (y_coords[0] == 0) {\n            // 1st quadrant\n            start = 0;\n            end = points.size() / (x_size * y_size) + (points.size() % (x_size * y_size) > 0? 1 : 0);\n        }\n        else {\n            // 2nd quadrant\n            start = points.size() / (x_size * y_size);\n            end = 2 * points.size() / (x_size * y_size) + (points.size() % (x_size * y_size) > 0? 1 : 0);\n        }\n    }\n    else {\n        if (y_coords[0] == 0) {\n            // 3rd quadrant\n            start = 2 * points.size() / (x_size * y_size);\n            end = 3 * points.size() / (x_size * y_size) + (points.size() % (x_size * y_size) > 0? 1 : 0);\n        }\n        else {\n            // 4th quadrant\n            start = 3 * points.size() / (x_size * y_size);\n            end = points.size();\n        }\n    }\n\n    std::vector<Point> points_copy;\n\n    for (size_t i = start; i < end; i++) {\n        points_copy.push_back(points[i]);\n    }\n\n    std::vector<int> counts(4, 0);\n\n#pragma omp parallel for\n    for (size_t i = 0; i < points_copy.size(); i++) {\n        int quadrant = 0;\n        if (points_copy[i].x >= 0.0 && points_copy[i].y >= 0.0) {\n            quadrant = 1;\n        }\n        else if (points_copy[i].x < 0.0 && points_copy[i].y >= 0.0) {\n            quadrant = 2;\n        }\n        else if (points_copy[i].x < 0.0 && points_copy[i].y < 0.0) {\n            quadrant = 3;\n        }\n        else {\n            quadrant = 4;\n        }\n        counts[quadrant] += 1;\n    }\n\n    if (rank == 0) {\n        MPI_Reduce(counts.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n    else {\n        MPI_Reduce(counts.data(), nullptr, 4, MPI_INT, MPI_SUM,",
            "int mpi_rank, mpi_size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\n   std::vector<Point> local_points;\n   std::vector<int> local_bins(bins.size(), 0);\n\n   int dim_size = sqrt(mpi_size);\n   int rank_x = mpi_rank % dim_size;\n   int rank_y = mpi_rank / dim_size;\n\n   int x = 0, y = 0;\n   for (auto& p : points) {\n      if (p.x >= -1.5 && p.x < 1.5) {\n         x++;\n      }\n\n      if (p.y >= -1.0 && p.y < 1.0) {\n         y++;\n      }\n\n      if (x == 1 && y == 1) {\n         local_points.push_back(p);\n      }\n   }\n\n   int local_sum = 0;\n#pragma omp parallel for reduction(+:local_sum)\n   for (int i = 0; i < local_points.size(); i++) {\n      int bin_idx = 0;\n      if (local_points[i].x >= -1.5 && local_points[i].x < 0.0) {\n         bin_idx++;\n      }\n      if (local_points[i].x >= 0.0 && local_points[i].x < 1.5) {\n         bin_idx += 2;\n      }\n\n      if (local_points[i].y >= -1.0 && local_points[i].y < 0.0) {\n         bin_idx += 1;\n      }\n      if (local_points[i].y >= 0.0 && local_points[i].y < 1.0) {\n         bin_idx += 4;\n      }\n      local_sum += bin_idx;\n      local_bins[bin_idx]++;\n   }\n\n   bins[0] = local_sum;\n   bins[1] = local_bins[1];\n   bins[2] = local_bins[3];\n   bins[3] = local_bins[2];\n\n   int global_sum = 0;\n   MPI_Allreduce(&bins[0], &global_sum, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n   MPI_Reduce(&local_sum, &bins[0], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   for (int i = 1; i < bins.size(); i++) {\n      MPI_Reduce(&local_bins[i], &bins[i], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n\n   if (mpi_rank == 0) {\n      bins[0] = global_sum;\n   }\n}",
            "int numRanks, rank, procsx, procsy;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Dims_create(numRanks, 2, &procsx);\n\n    double xmin = 100, xmax = -100, ymin = 100, ymax = -100;\n\n    if(rank==0) {\n        for(auto p : points) {\n            if(p.x < xmin) xmin = p.x;\n            if(p.x > xmax) xmax = p.x;\n            if(p.y < ymin) ymin = p.y;\n            if(p.y > ymax) ymax = p.y;\n        }\n\n        int bins_per_proc = (xmax - xmin) / procsx;\n\n        int bin_start = bins_per_proc * rank;\n        int bin_end = bins_per_proc * (rank + 1);\n\n        if(rank == numRanks - 1)\n            bin_end = xmax + 1;\n\n        for(int bin = bin_start; bin < bin_end; bin++)\n            bins[0]++;\n\n    }\n\n    if(rank == numRanks - 1) {\n        int bins_per_proc = (ymax - ymin) / procsy;\n\n        int bin_start = bins_per_proc * rank;\n        int bin_end = bins_per_proc * (rank + 1);\n\n        if(rank == numRanks - 1)\n            bin_end = ymax + 1;\n\n        for(int bin = bin_start; bin < bin_end; bin++)\n            bins[1]++;\n\n    }\n\n    if(rank == 0) {\n        int bins_per_proc = (ymax - ymin) / procsy;\n\n        int bin_start = bins_per_proc * rank;\n        int bin_end = bins_per_proc * (rank + 1);\n\n        if(rank == numRanks - 1)\n            bin_end = ymax + 1;\n\n        for(int bin = bin_start; bin < bin_end; bin++)\n            bins[2]++;\n\n    }\n\n    if(rank < procsx - 1) {\n        int bins_per_proc = (xmax - xmin) / procsx;\n\n        int bin_start = bins_per_proc * rank;\n        int bin_end = bins_per_proc * (rank + 1);\n\n        if(rank == numRanks - 1)\n            bin_end = xmax + 1;\n\n        for(int bin = bin_start; bin < bin_end; bin++)\n            bins[3]++;\n\n    }\n\n    // if(rank==0) {\n    //     for(int i = 0; i < procsx; i++)\n    //         if(rank == i)\n    //             std::cout << \"Rank \" << rank << \": \" << bins[i] << std::endl;\n    //     MPI_Barrier(MPI_COMM_WORLD);\n    // }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "// TODO: write your code here\n}",
            "size_t npoints = points.size();\n   size_t maxpoints = 5000;\n\n   // get the number of MPI processes and threads\n   int mpi_size;\n   int mpi_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n   int omp_threads = omp_get_max_threads();\n\n   // determine number of quadrants to divide each point into\n   int quadrants = 4;\n   int nquadrants = quadrants * quadrants;\n\n   // divide the quadrants between MPI ranks\n   int quadrant_width = nquadrants / mpi_size;\n   int quadrant_remainder = nquadrants % mpi_size;\n   int start_quadrant = quadrant_width * mpi_rank + std::min(mpi_rank, quadrant_remainder);\n   int end_quadrant = start_quadrant + quadrant_width;\n   if (mpi_rank < quadrant_remainder) {\n      end_quadrant += 1;\n   }\n\n   // allocate space for thread-private counts\n   std::vector<size_t> thread_bins(nquadrants, 0);\n\n   // count points\n   #pragma omp parallel\n   {\n      // get thread number and number of threads\n      int thread_number = omp_get_thread_num();\n      int nthreads = omp_get_num_threads();\n\n      // calculate chunk of points\n      size_t chunk_size = npoints / nthreads;\n      size_t chunk_remainder = npoints % nthreads;\n      size_t start_point = chunk_size * thread_number + std::min(thread_number, chunk_remainder);\n      size_t end_point = start_point + chunk_size;\n      if (thread_number < chunk_remainder) {\n         end_point += 1;\n      }\n\n      // count points in quadrants\n      for (size_t i = start_point; i < end_point; ++i) {\n         double x = points[i].x;\n         double y = points[i].y;\n         int xbin = (x > 0)? (x > 0.5? 1 : 0) : -1;\n         int ybin = (y > 0)? (y > 0.5? 1 : 0) : -1;\n         int bin = xbin * quadrants + ybin + 1;\n         #pragma omp atomic\n         thread_bins[bin] += 1;\n      }\n   }\n\n   // reduce counts\n   if (mpi_rank == 0) {\n      bins[0] = 0;\n      for (int p = 0; p < mpi_size; ++p) {\n         MPI_Status status;\n         MPI_Recv(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, p, 0, MPI_COMM_WORLD, &status);\n         for (int i = 0; i < 4; ++i) {\n            #pragma omp atomic\n            bins[0] += bins[i];\n         }\n      }\n      // divide counts among threads\n      for (int p = 0; p < mpi_size; ++p) {\n         for (int i = 0; i < 4; ++i) {\n            MPI_Send(&thread_bins[0], 4, MPI_UNSIGNED_LONG_LONG, p, 0, MPI_COMM_WORLD);\n            for (int j = 0; j < 4; ++j) {\n               #pragma omp atomic\n               bins[i] += thread_bins[j];\n            }\n            thread_bins[0] = 0;\n         }\n      }\n   }\n   else {\n      MPI_Send(&thread_bins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n      MPI_Recv(&bins[",
            "int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int size;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   const size_t PER_PROCESS = points.size() / size;\n   if (rank == 0)\n      bins = {0, 0, 0, 0};\n   for (size_t i = 0; i < PER_PROCESS; i++) {\n      auto p = points[i + rank * PER_PROCESS];\n      if (p.x >= 0 && p.y >= 0)\n         bins[0]++;\n      else if (p.x <= 0 && p.y >= 0)\n         bins[1]++;\n      else if (p.x <= 0 && p.y <= 0)\n         bins[2]++;\n      else\n         bins[3]++;\n   }\n   if (rank == 0) {\n      for (int i = 1; i < size; i++) {\n         MPI_Recv(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n   }\n   else {\n      MPI_Send(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n   }\n}",
            "int rank = 0;\n    int n_ranks = 1;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n\n    int size_array = points.size();\n    std::array<int, 2> n_quadrants = {0, 0};\n    int n_points_global = 0;\n\n    MPI_Allreduce(&size_array, &n_points_global, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    if (n_ranks > 1) {\n        int n_points_per_rank = n_points_global / n_ranks;\n        int remainder = n_points_global % n_ranks;\n\n        if (rank < remainder) {\n            n_quadrants = {n_points_per_rank + 1, n_points_per_rank};\n        }\n        else {\n            n_quadrants = {n_points_per_rank, n_points_per_rank + 1};\n        }\n\n        int points_per_rank = n_quadrants[0] * n_quadrants[1];\n\n        std::vector<Point> local_points;\n        local_points.reserve(points_per_rank);\n\n        int points_per_rank_offset = rank * points_per_rank;\n\n        for (int i = 0; i < n_quadrants[0]; i++) {\n            for (int j = 0; j < n_quadrants[1]; j++) {\n                int k = i * n_quadrants[1] + j;\n                local_points.push_back(points[points_per_rank_offset + k]);\n            }\n        }\n\n        #pragma omp parallel\n        {\n            int tid = omp_get_thread_num();\n            int n_threads = omp_get_num_threads();\n\n            int chunk_size = (n_quadrants[0] * n_quadrants[1]) / n_threads;\n            int remainder = (n_quadrants[0] * n_quadrants[1]) % n_threads;\n            int start = chunk_size * tid;\n            int end = chunk_size * tid + chunk_size;\n\n            if (tid == n_threads - 1) {\n                end += remainder;\n            }\n\n            for (int i = start; i < end; i++) {\n                int x = local_points[i].x;\n                int y = local_points[i].y;\n\n                if (x < 0.0) {\n                    if (y > 0.0) {\n                        ++bins[0];\n                    }\n                    else {\n                        ++bins[2];\n                    }\n                }\n                else {\n                    if (y > 0.0) {\n                        ++bins[1];\n                    }\n                    else {\n                        ++bins[3];\n                    }\n                }\n            }\n        }\n    }\n    else {\n        for (int i = 0; i < n_quadrants[0]; i++) {\n            for (int j = 0; j < n_quadrants[1]; j++) {\n                int x = points[i * n_quadrants[1] + j].x;\n                int y = points[i * n_quadrants[1] + j].y;\n\n                if (x < 0.0) {\n                    if (y > 0.0) {\n                        ++bins[0];\n                    }\n                    else {\n                        ++bins[2];\n                    }\n                }\n                else {\n                    if (y > 0.0) {\n                        ++bins[1];\n                    }\n                    else {\n                        ++bins[3];\n                    }\n                }\n            }\n        }\n    }\n}",
            "// Fill the array with zeros.\n  std::fill(bins.begin(), bins.end(), 0);\n  // Create a Cartesian topology.\n  MPI_Comm comm;\n  int size;\n  MPI_Comm_dup(MPI_COMM_WORLD, &comm);\n  MPI_Comm_size(comm, &size);\n  int rank;\n  MPI_Comm_rank(comm, &rank);\n  int dims[2] = {size, 2};\n  MPI_Dims_create(size, 2, dims);\n  int periods[2] = {0, 1};\n  MPI_Cart_create(comm, 2, dims, periods, 0, &comm);\n\n  // Get the coordinates of the current rank\n  int coords[2];\n  MPI_Cart_coords(comm, rank, 2, coords);\n\n  // Initialize the bin to 0\n  size_t bin = 0;\n\n  // Fill the bin with the points in the current quadrant\n#pragma omp parallel for default(none) shared(points, bin)\n  for (int i = 0; i < points.size(); i++) {\n    if (points[i].x > 0 && points[i].y > 0) {\n      bin = 0;\n    } else if (points[i].x < 0 && points[i].y > 0) {\n      bin = 1;\n    } else if (points[i].x < 0 && points[i].y < 0) {\n      bin = 2;\n    } else if (points[i].x > 0 && points[i].y < 0) {\n      bin = 3;\n    }\n    bins[bin]++;\n  }\n\n  // Free the cartesian topology.\n  MPI_Comm_free(&comm);\n}",
            "// TODO: add your code here\n}",
            "// TODO: Your code here\n}",
            "}",
            "std::vector<size_t> bins_local(4);\n    #pragma omp parallel for\n    for(size_t i = 0; i < points.size(); i++) {\n        if(points[i].x >= 0) {\n            if(points[i].y >= 0) {\n                bins_local[0] += 1;\n            } else {\n                bins_local[3] += 1;\n            }\n        } else {\n            if(points[i].y >= 0) {\n                bins_local[1] += 1;\n            } else {\n                bins_local[2] += 1;\n            }\n        }\n    }\n    #pragma omp parallel\n    {\n        int rank;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        int size;\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n        int remainder = points.size() % size;\n        int quotient = points.size() / size;\n        int offset = rank * quotient;\n        if(rank == 0) {\n            for(int i = 1; i < size; i++) {\n                MPI_Recv(&bins[0], 4, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            }\n            for(int i = 0; i < remainder; i++) {\n                bins[0] += bins_local[i];\n            }\n            bins[0] += bins_local[remainder];\n        } else if(rank!= 0) {\n            if(rank < remainder) {\n                MPI_Send(&bins_local[0], 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n            } else {\n                MPI_Send(&bins_local[remainder], quotient + 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n            }\n        }\n    }\n}",
            "int rank;\n\tint n_ranks;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n\t\n\tint n = points.size();\n\t\n\tstd::vector<Point> localPoints;\n\tlocalPoints.reserve(n);\n\tif (n > 0) {\n\t\t\n\t\tint nx = n_ranks;\n\t\tint ny = (n + nx - 1)/nx;\n\t\tint nxy = nx*ny;\n\t\tint px = rank % nx;\n\t\tint py = rank / nx;\n\t\t\n\t\tint i, j;\n\t\t\n\t\tfor (i = 0; i < n; ++i) {\n\t\t\t\n\t\t\tj = (i/nx)*nxy + (i%nx)*ny + py*nx + px;\n\t\t\tif (j < n) {\n\t\t\t\tlocalPoints.push_back(points[j]);\n\t\t\t}\n\t\t\t\n\t\t}\n\t\t\n\t}\n\t\n\t#pragma omp parallel\n\t{\n\t\t\n\t\tint i;\n\t\t\n\t\t#pragma omp for\n\t\tfor (i = 0; i < n; ++i) {\n\t\t\t\n\t\t\tPoint p = points[i];\n\t\t\t\n\t\t\tif (p.x > 0) {\n\t\t\t\t\n\t\t\t\tif (p.y > 0) {\n\t\t\t\t\tbins[0] += 1;\n\t\t\t\t} else {\n\t\t\t\t\tbins[2] += 1;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t} else {\n\t\t\t\t\n\t\t\t\tif (p.y > 0) {\n\t\t\t\t\tbins[1] += 1;\n\t\t\t\t} else {\n\t\t\t\t\tbins[3] += 1;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t}\n\t\t\t\n\t\t}\n\t\t\n\t}\n\t\n\tMPI_Allreduce(bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\t\n}",
            "int comm_size, rank, num_procs;\n   MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n   int dims[1] = {num_procs};\n\n   MPI_Cart_create(MPI_COMM_WORLD, 1, dims, NULL, 1, MPI_INFO_NULL, &MPI_COMM_WORLD);\n\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n   int *coords = new int[num_procs];\n   MPI_Cart_coords(MPI_COMM_WORLD, rank, 1, coords);\n\n   MPI_Datatype my_point;\n   int block_lengths[3] = {1, 1, 1};\n   MPI_Aint displacements[3];\n\n   MPI_Address(points.data(), &displacements[0]);\n   MPI_Address(&points[0].x, &displacements[1]);\n   MPI_Address(&points[0].y, &displacements[2]);\n   displacements[0] -= displacements[1];\n   displacements[1] -= displacements[2];\n   displacements[2] -= displacements[0];\n   MPI_Type_struct(3, block_lengths, displacements, MPI_DOUBLE, &my_point);\n   MPI_Type_commit(&my_point);\n\n   int block_lengths2[4] = {1, 1, 1, 1};\n   MPI_Aint displacements2[4];\n\n   MPI_Address(bins.data(), &displacements2[0]);\n   MPI_Address(&bins[0].x, &displacements2[1]);\n   MPI_Address(&bins[0].y, &displacements2[2]);\n   MPI_Address(&bins[0].z, &displacements2[3]);\n   displacements2[0] -= displacements2[1];\n   displacements2[1] -= displacements2[2];\n   displacements2[2] -= displacements2[3];\n   displacements2[3] -= displacements2[0];\n   MPI_Type_struct(4, block_lengths2, displacements2, MPI_LONG, &my_point);\n   MPI_Type_commit(&my_point);\n\n\n   int n_points_per_proc = points.size() / num_procs;\n   int n_points_left = points.size() % num_procs;\n\n   int x = coords[0];\n   int start = x * n_points_per_proc + std::min(x, n_points_left);\n   int end = (x + 1) * n_points_per_proc + std::min(x + 1, n_points_left);\n\n   std::vector<Point> image(points.begin() + start, points.begin() + end);\n\n   double x_min = image[0].x;\n   double x_max = image[0].x;\n   double y_min = image[0].y;\n   double y_max = image[0].y;\n\n   for (Point &p : image) {\n      x_min = std::min(x_min, p.x);\n      x_max = std::max(x_max, p.x);\n      y_min = std::min(y_min, p.y);\n      y_max = std::max(y_max, p.y);\n   }\n\n   int nx = 1 + (x_max - x_min) / (x_max - x_min);\n   int ny = 1 + (y_max - y_min) / (y_max - y_min);\n\n   bins[0] = 0;",
            "int const rank = omp_get_thread_num();\n   int const size = omp_get_num_threads();\n\n   size_t const N = points.size();\n   for (size_t i = 0; i < N; ++i) {\n      if (points[i].x < 0) {\n         if (points[i].y < 0) {\n            bins[0] += 1;\n         }\n         else {\n            bins[1] += 1;\n         }\n      }\n      else {\n         if (points[i].y < 0) {\n            bins[2] += 1;\n         }\n         else {\n            bins[3] += 1;\n         }\n      }\n   }\n\n   std::array<size_t, 4> tmp_bins;\n   MPI_Reduce(bins.data(), tmp_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   if (rank == 0) {\n      bins = tmp_bins;\n   }\n}",
            "int mpiSize = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpiSize);\n\n    int mpiRank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpiRank);\n\n    int mpiRankY = mpiRank / 2;\n    int mpiRankX = mpiRank - 2 * mpiRankY;\n\n    int mpiRowSize = sqrt(mpiSize);\n    int mpiColSize = mpiSize / mpiRowSize;\n\n    int mpiOffset = 0;\n\n    if(mpiRankY > 0)\n        mpiOffset += mpiRowSize;\n\n    if(mpiRankX > 0)\n        mpiOffset += mpiColSize;\n\n    int mpiRow = mpiRankY + mpiOffset;\n    int mpiCol = mpiRankX + mpiOffset;\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    int xLower, xUpper, yLower, yUpper;\n    if(mpiRank == 0) {\n        xLower = -10;\n        xUpper =  10;\n        yLower = -10;\n        yUpper =  10;\n    } else {\n        MPI_Status status;\n\n        MPI_Recv(&xLower, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n        MPI_Recv(&xUpper, 1, MPI_INT, 0, 1, MPI_COMM_WORLD, &status);\n        MPI_Recv(&yLower, 1, MPI_INT, 0, 2, MPI_COMM_WORLD, &status);\n        MPI_Recv(&yUpper, 1, MPI_INT, 0, 3, MPI_COMM_WORLD, &status);\n    }\n\n    int mpiXOffset = (mpiCol - 1) * mpiRowSize;\n    int mpiYOffset = (mpiRow - 1) * mpiColSize;\n\n    int mpiLowerX = mpiXOffset + xLower;\n    int mpiLowerY = mpiYOffset + yLower;\n\n    int mpiUpperX = mpiXOffset + xUpper;\n    int mpiUpperY = mpiYOffset + yUpper;\n\n    if(mpiRank!= 0) {\n        MPI_Send(&xLower, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(&xUpper, 1, MPI_INT, 0, 1, MPI_COMM_WORLD);\n        MPI_Send(&yLower, 1, MPI_INT, 0, 2, MPI_COMM_WORLD);\n        MPI_Send(&yUpper, 1, MPI_INT, 0, 3, MPI_COMM_WORLD);\n    }\n\n    std::vector<Point> localPoints;\n\n#pragma omp parallel for\n    for(size_t i = 0; i < points.size(); ++i) {\n        if(mpiLowerX <= points[i].x && points[i].x < mpiUpperX\n            && mpiLowerY <= points[i].y && points[i].y < mpiUpperY) {\n            localPoints.push_back(points[i]);\n        }\n    }\n\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    std::vector<size_t> localBins(4, 0);\n\n#pragma omp parallel for\n    for(size_t i = 0; i < localPoints.size(); ++i) {\n        if(localPoints[i].x >= 0 && localPoints[i].x < 0 && localPoints[i].y >= 0 && localPoints[i].y < 0) {\n            ++localBins[0];\n        }\n        if(localPoints[i].x >=",
            "int num_of_processes;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_of_processes);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<Point> image;\n    if (rank == 0) {\n        image.resize(points.size());\n        for (int i = 0; i < points.size(); i++)\n            image[i] = points[i];\n    }\n\n    MPI_Bcast(&image, image.size(), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    bins.fill(0);\n    std::array<size_t, 4> local_bins;\n    for (auto &p : image) {\n        local_bins[0] += p.x > 0 && p.y > 0;\n        local_bins[1] += p.x < 0 && p.y > 0;\n        local_bins[2] += p.x < 0 && p.y < 0;\n        local_bins[3] += p.x > 0 && p.y < 0;\n    }\n\n    MPI_Allreduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "// TODO: implement\n}",
            "int const rank = 0;\n    int const size = 0;\n\n    auto x = [](Point const& p) { return p.x; };\n    auto y = [](Point const& p) { return p.y; };\n\n    double xmin = *std::min_element(points.begin(), points.end(), x);\n    double xmax = *std::max_element(points.begin(), points.end(), x);\n    double ymin = *std::min_element(points.begin(), points.end(), y);\n    double ymax = *std::max_element(points.begin(), points.end(), y);\n\n    int n_rows = size, n_cols = size;\n    int n_x = size, n_y = size;\n    int x_dim = 0, y_dim = 0;\n    int x_pos = rank % n_x;\n    int y_pos = rank / n_x;\n\n    MPI_Dims_create(size, 2, &n_x);\n    MPI_Cart_create(MPI_COMM_WORLD, 2, &n_x, &n_y, false, &MPI_COMM_2D);\n\n    MPI_Cart_sub(MPI_COMM_2D, &n_rows, &n_cols, &MPI_COMM_Q1);\n    MPI_Cart_sub(MPI_COMM_2D, &n_rows, &n_cols, &MPI_COMM_Q2);\n    MPI_Cart_sub(MPI_COMM_2D, &n_rows, &n_cols, &MPI_COMM_Q3);\n    MPI_Cart_sub(MPI_COMM_2D, &n_rows, &n_cols, &MPI_COMM_Q4);\n\n    MPI_Cart_coords(MPI_COMM_2D, rank, 2, &x_dim, &y_dim);\n    MPI_Cart_coords(MPI_COMM_Q1, rank, 2, &x_pos, &y_pos);\n    MPI_Cart_coords(MPI_COMM_Q2, rank, 2, &x_pos, &y_pos);\n    MPI_Cart_coords(MPI_COMM_Q3, rank, 2, &x_pos, &y_pos);\n    MPI_Cart_coords(MPI_COMM_Q4, rank, 2, &x_pos, &y_pos);\n\n    MPI_Status status;\n    int rx, ry;\n    double px, py;\n    int x_count, y_count;\n    int sum_count;\n\n    MPI_Sendrecv_replace(&xmin, 1, MPI_DOUBLE, MPI_PROC_NULL, 0, MPI_PROC_NULL, 0, MPI_COMM_2D, &status);\n    MPI_Sendrecv_replace(&xmax, 1, MPI_DOUBLE, MPI_PROC_NULL, 0, MPI_PROC_NULL, 0, MPI_COMM_2D, &status);\n    MPI_Sendrecv_replace(&ymin, 1, MPI_DOUBLE, MPI_PROC_NULL, 0, MPI_PROC_NULL, 0, MPI_COMM_2D, &status);\n    MPI_Sendrecv_replace(&ymax, 1, MPI_DOUBLE, MPI_PROC_NULL, 0, MPI_PROC_NULL, 0, MPI_COMM_2D, &status);\n\n    MPI_Comm_rank(MPI_COMM_Q1, &rx);\n    MPI_Comm_rank(MPI_COMM_Q2, &ry);\n    MPI_Sendrecv_replace(&xmin, 1, MPI_DOUBLE, rx, 0, ry, 0, MPI_COMM_Q1, &status);\n    MPI_Sendrecv_replace(&xmax, 1, MPI_DOUBLE, rx, 0, ry, 0, MPI_COMM_Q1, &status",
            "int rank, num_ranks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n  // TODO:\n  // Create a two dimensional array to store the number of points in each quadrant on each rank\n  // Use OpenMP to parallelize the loops\n  // Communicate with other ranks to fill in quadrants not on this rank\n  // Write the results to `bins` on rank 0\n  // Use MPI to communicate the results from each rank to rank 0\n\n  int xmax = 4;\n  int ymax = 2;\n  int total_quadrants = xmax * ymax;\n  size_t quadrant_counts[total_quadrants];\n  int x_size = xmax / num_ranks;\n  int y_size = ymax / num_ranks;\n\n  for(int i = 0; i < total_quadrants; i++){\n    quadrant_counts[i] = 0;\n  }\n\n  for(auto p : points){\n    int quadrant_x = p.x > 0? 0 : 1;\n    int quadrant_y = p.y > 0? 0 : 1;\n\n    int quadrant_index = quadrant_x + quadrant_y * 2;\n    quadrant_counts[quadrant_index]++;\n  }\n\n  // Now we need to send quadrant_counts to other ranks\n  int x_coord = rank % x_size;\n  int y_coord = rank / x_size;\n\n  for(int x_off = -1; x_off <= 1; x_off++){\n    int x = x_coord + x_off;\n    if(x < 0 || x >= x_size) continue;\n    for(int y_off = -1; y_off <= 1; y_off++){\n      int y = y_coord + y_off;\n      if(y < 0 || y >= y_size) continue;\n\n      if(x == 0 && y == 0) continue;\n\n      int neighbor = y * x_size + x;\n      int start = (y * 2 + y_off) * 2 + x_off;\n      int end = start + 1;\n      int send_size = end - start;\n\n      MPI_Send(&quadrant_counts[start], send_size, MPI_UNSIGNED_LONG, neighbor, 0, MPI_COMM_WORLD);\n    }\n  }\n\n  std::array<size_t, 4> recv_bins;\n\n  for(int x_off = -1; x_off <= 1; x_off++){\n    int x = x_coord + x_off;\n    if(x < 0 || x >= x_size) continue;\n    for(int y_off = -1; y_off <= 1; y_off++){\n      int y = y_coord + y_off;\n      if(y < 0 || y >= y_size) continue;\n\n      if(x == 0 && y == 0) continue;\n\n      int neighbor = y * x_size + x;\n      int start = (y * 2 + y_off) * 2 + x_off;\n      int end = start + 1;\n      int recv_size = end - start;\n\n      MPI_Recv(&recv_bins[start], recv_size, MPI_UNSIGNED_LONG, neighbor, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  }\n\n  for(int i = 0; i < total_quadrants; i++){\n    bins[i] += recv_bins[i];\n  }\n\n  for(int i = 0; i < total_quadrants; i++){\n    std::cout << bins[i] << \" \";\n  }\n\n  std::cout << \"\\n\";\n}",
            "// TODO: Fill this in.\n   return;\n}",
            "int size;\n   int rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int xdim, ydim;\n   MPI_Dims_create(size, 2, &xdim, &ydim);\n   int nx, ny;\n   MPI_Cart_get(MPI_COMM_WORLD, 2, &xdim, &ny, &ny, &nx);\n\n   double xmin, xmax, ymin, ymax;\n   xmin = ymin = -1.0;\n   xmax = ymax = 1.0;\n\n   std::array<std::array<size_t, 2>, 4> counts;\n\n   auto quadrants = points | ranges::views::filter([&](auto&& p){\n      return (p.x >= xmin && p.x < xmax && p.y >= ymin && p.y < ymax);\n   }) | ranges::views::group_by([&](auto&& p){\n      int ix, iy;\n      MPI_Cart_rank(MPI_COMM_WORLD, &ix, &iy);\n      return (p.x < 0 && p.y < 0) + 2 * (p.x >= 0 && p.y < 0) + 4 * (p.x < 0 && p.y >= 0) + 8 * (p.x >= 0 && p.y >= 0);\n   });\n\n   // This is the critical section\n   quadrants |= ranges::action::sort | ranges::action::sort;\n\n   counts |= quadrants | ranges::action::transform([&](auto&& q){\n      size_t s = 0;\n      q |= ranges::action::for_each([&](auto&& p){\n         ++s;\n      });\n      return s;\n   });\n\n   MPI_Barrier(MPI_COMM_WORLD);\n\n   int recvcounts[4] = {};\n   int displs[4] = {};\n   for (int i = 0; i < 4; ++i) {\n      int rank;\n      MPI_Cart_rank(MPI_COMM_WORLD, &i, &rank);\n      int count;\n      MPI_Status status;\n      MPI_Recv(&count, 1, MPI_INT, rank, 0, MPI_COMM_WORLD, &status);\n      recvcounts[i] = count;\n      displs[i] = i == 0? 0 : displs[i-1] + recvcounts[i-1];\n   }\n\n   std::vector<size_t> recvbuf(displs[3] + recvcounts[3]);\n   MPI_Gatherv(&counts[0][0], displs[3], MPI_INT, &recvbuf[0], recvcounts, displs, MPI_INT, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      bins = {0, 0, 0, 0};\n      std::vector<size_t> s(4);\n      for (int i = 0; i < size; ++i) {\n         int x, y;\n         MPI_Cart_rank(MPI_COMM_WORLD, &i, &y);\n         MPI_Cart_coords(MPI_COMM_WORLD, &y, 2, &x);\n         s[x] += recvbuf[i];\n      }\n      bins = {s[0], s[1], s[2], s[3]};\n   }\n}",
            "}",
            "int rank, size;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\t\n\tint dims[2] = {2, 2};\n\tint periods[2] = {0, 0};\n\n\tMPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 0, &bins);\n\tMPI_Comm cart;\n\tMPI_Cart_dup(MPI_COMM_WORLD, 2, dims, periods, &cart);\n\n\tstd::array<size_t, 4> binsLocal;\n\tstd::vector<Point> pointsLocal;\n\t\n\tMPI_Cart_sub(cart, {0, 0}, &cart);\n\tif (rank == 0) {\n\t\tbinsLocal[0] = 0;\n\t\tbinsLocal[1] = 0;\n\t\tbinsLocal[2] = 0;\n\t\tbinsLocal[3] = 0;\n\t\t\n\t\tpointsLocal = points;\n\t\tMPI_Bcast(&pointsLocal, 1, MPI_VECTOR, 0, MPI_COMM_WORLD);\n\t}\n\telse {\n\t\tMPI_Bcast(&pointsLocal, 1, MPI_VECTOR, 0, MPI_COMM_WORLD);\n\t}\n\n\tint my_rank;\n\tint nb_ranks;\n\tMPI_Comm_rank(cart, &my_rank);\n\tMPI_Comm_size(cart, &nb_ranks);\n\t\n\t//printf(\"Rank: %d, my_rank: %d\\n\", rank, my_rank);\n\t\n\tsize_t nb_points = pointsLocal.size();\n\tsize_t nb_points_local = nb_points / nb_ranks;\n\n\tif (my_rank == nb_ranks - 1) {\n\t\tnb_points_local = nb_points - nb_points_local * (nb_ranks - 1);\n\t}\n\t\n\t//printf(\"Points: %d\\n\", nb_points_local);\n\n\t//omp_set_num_threads(2);\n\t#pragma omp parallel for shared(pointsLocal, nb_points_local, binsLocal) private(my_rank, nb_ranks)\n\tfor (int i = 0; i < nb_points_local; i++) {\n\t\tdouble x = pointsLocal[i].x;\n\t\tdouble y = pointsLocal[i].y;\n\t\tint quadrant;\n\t\tif (x >= 0 && y >= 0) {\n\t\t\tquadrant = 0;\n\t\t}\n\t\telse if (x < 0 && y >= 0) {\n\t\t\tquadrant = 1;\n\t\t}\n\t\telse if (x < 0 && y < 0) {\n\t\t\tquadrant = 2;\n\t\t}\n\t\telse if (x >= 0 && y < 0) {\n\t\t\tquadrant = 3;\n\t\t}\n\n\t\tif (my_rank == 0) {\n\t\t\tbinsLocal[quadrant] += 1;\n\t\t}\n\t}\n\n\tMPI_Reduce(&binsLocal, &bins, 4, MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // Compute local number of points in each quadrant\n   std::array<size_t, 4> local_bins = {};\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x > 0 && y > 0) {\n         ++local_bins[0];\n      } else if (x < 0 && y > 0) {\n         ++local_bins[1];\n      } else if (x < 0 && y < 0) {\n         ++local_bins[2];\n      } else if (x > 0 && y < 0) {\n         ++local_bins[3];\n      }\n   }\n\n   // Sum bins across ranks and store in rank 0\n   if (rank == 0) {\n      std::array<size_t, 4> tmp_bins = {};\n      MPI_Reduce(local_bins.data(), tmp_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n      bins = tmp_bins;\n   } else {\n      MPI_Reduce(local_bins.data(), NULL, 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n\n   // Set bins to 0 if any rank has 0 points\n   if (bins[0] == 0 || bins[1] == 0 || bins[2] == 0 || bins[3] == 0) {\n      std::fill(bins.begin(), bins.end(), 0);\n   }\n}",
            "bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n\n   // Calculate which bin each point belongs to.\n   for (const Point& point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         bins[0] += 1;\n      }\n      else if (point.x < 0 && point.y < 0) {\n         bins[3] += 1;\n      }\n      else if (point.x < 0 && point.y >= 0) {\n         bins[1] += 1;\n      }\n      else if (point.x >= 0 && point.y < 0) {\n         bins[2] += 1;\n      }\n   }\n\n   // Sum the counts in each bin.\n   MPI_Allreduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n   // Store the counts.\n   if (bins[0] > 0 && bins[1] > 0 && bins[2] > 0 && bins[3] > 0) {\n      bins[0] = bins[0] / bins[1];\n      bins[1] = bins[1] / bins[2];\n      bins[2] = bins[2] / bins[3];\n      bins[3] = bins[3] / bins[0];\n   }\n   else if (bins[0] > 0 && bins[1] > 0 && bins[2] > 0 && bins[3] == 0) {\n      bins[1] = bins[1] / bins[2];\n      bins[3] = 0;\n      bins[2] = 0;\n      bins[0] = 1;\n   }\n   else if (bins[0] > 0 && bins[1] > 0 && bins[2] == 0 && bins[3] > 0) {\n      bins[0] = bins[0] / bins[1];\n      bins[2] = 0;\n      bins[3] = bins[3] / bins[0];\n      bins[1] = 1;\n   }\n   else if (bins[0] > 0 && bins[1] == 0 && bins[2] > 0 && bins[3] > 0) {\n      bins[0] = bins[0] / bins[2];\n      bins[1] = 0;\n      bins[2] = bins[2] / bins[0];\n      bins[3] = 1;\n   }\n   else if (bins[0] == 0 && bins[1] > 0 && bins[2] > 0 && bins[3] > 0) {\n      bins[0] = 0;\n      bins[1] = bins[1] / bins[2];\n      bins[2] = bins[2] / bins[3];\n      bins[3] = 1;\n   }\n   else if (bins[0] > 0 && bins[1] == 0 && bins[2] > 0 && bins[3] == 0) {\n      bins[1] = 0;\n      bins[2] = bins[2] / bins[0];\n      bins[3] = 0;\n      bins[0] = 1;\n   }\n   else if (bins[0] > 0 && bins[1] == 0 && bins[2] == 0 && bins[3] > 0) {\n      bins[1] = 0;\n      bins[2] = 0;\n      bins[3] = bins[3] / bins[0];\n      bins[0] = 1;\n   }\n   else if (bins[0] == 0 && bins[1",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int xdim = 2, ydim = 2;\n\n    int x_dim_size, y_dim_size;\n    MPI_Dims_create(size, 2, &x_dim_size);\n    MPI_Dims_create(size, 2, &y_dim_size);\n\n    int x_rank, y_rank;\n    MPI_Cart_coords(MPI_COMM_WORLD, rank, 2, &x_rank, &y_rank);\n\n    int x_periods = 1, y_periods = 1;\n    int reorder = 0;\n\n    MPI_Cart_create(MPI_COMM_WORLD, 2, &x_dim_size, &x_periods, &reorder, &x_rank, &x_periods);\n    MPI_Cart_create(MPI_COMM_WORLD, 2, &y_dim_size, &y_periods, &reorder, &y_rank, &y_periods);\n\n    int coord[2] = {x_rank, y_rank};\n\n    MPI_Cart_shift(MPI_COMM_WORLD, 0, 1, &coord[0], &coord[1]);\n\n    double min_x = 0, min_y = 0, max_x = 0, max_y = 0;\n    if(x_rank == 0)\n    {\n        min_x = points[0].x;\n    }\n\n    MPI_Allreduce(&min_x, &max_x, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);\n\n    if(y_rank == 0)\n    {\n        min_y = points[0].y;\n    }\n\n    MPI_Allreduce(&min_y, &max_y, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);\n\n\n    double delta_x = (max_x - min_x)/xdim;\n    double delta_y = (max_y - min_y)/ydim;\n\n    std::vector<Point> quadrant_points;\n\n    if(x_rank == 0 && y_rank == 0)\n    {\n        for(int i = 0; i < points.size(); i++)\n        {\n            if(points[i].x >= min_x && points[i].x <= min_x + delta_x && points[i].y >= min_y && points[i].y <= min_y + delta_y)\n            {\n                quadrant_points.push_back(points[i]);\n            }\n        }\n    }\n\n    MPI_Bcast(&quadrant_points, quadrant_points.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n\n\n    bins.fill(0);\n    #pragma omp parallel for shared(quadrant_points)\n    for(int i = 0; i < quadrant_points.size(); i++)\n    {\n        if(quadrant_points[i].x >= min_x && quadrant_points[i].x < min_x + delta_x)\n        {\n            bins[0]++;\n        }\n        else if(quadrant_points[i].x >= min_x + delta_x && quadrant_points[i].x < min_x + 2*delta_x)\n        {\n            bins[1]++;\n        }\n        else if(quadrant_points[i].x >= min_x + 2*delta_x && quadrant_points[i].x < min_x + 3*delta_x)\n        {\n            bins[2]++;\n        }\n        else if(quadrant_points[i].x >= min_x + 3*delta_x && quadrant_points[i].x <= max_x)\n        {\n            bins[3]++;\n        }\n\n        if(quadrant_points[i].y",
            "const int nbins = 4;\n\n    std::array<size_t, 4> local{0, 0, 0, 0};\n\n    int mpi_rank, mpi_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\n    MPI_Status status;\n    MPI_Request request;\n\n    Point *data = new Point[points.size()];\n\n    for(int i = 0; i < points.size(); i++) {\n        data[i] = points[i];\n    }\n\n    MPI_Scatter(data, points.size(), MPI_INT, &data, points.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n    std::sort(data, data + points.size());\n\n#pragma omp parallel for\n    for(int i = 0; i < points.size(); i++) {\n        if(data[i].x > 0.0) {\n            if(data[i].y > 0.0) {\n                local[0]++;\n            } else if(data[i].y == 0.0) {\n                local[2]++;\n            } else {\n                local[3]++;\n            }\n        } else if(data[i].x == 0.0) {\n            if(data[i].y > 0.0) {\n                local[1]++;\n            } else if(data[i].y == 0.0) {\n                local[2]++;\n            } else {\n                local[3]++;\n            }\n        } else {\n            if(data[i].y > 0.0) {\n                local[0]++;\n            } else if(data[i].y == 0.0) {\n                local[2]++;\n            } else {\n                local[3]++;\n            }\n        }\n    }\n\n    MPI_Gather(&local, nbins, MPI_INT, bins.data(), nbins, MPI_INT, 0, MPI_COMM_WORLD);\n\n    delete[] data;\n}",
            "int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int np;\n   MPI_Comm_size(MPI_COMM_WORLD, &np);\n\n   int dims[2];\n   MPI_Dims_create(np, 2, dims);\n\n   int periods[2] = {false, false};\n\n   MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, true, MPI_INFO_NULL, &cart_comm);\n   MPI_Cart_coords(cart_comm, rank, 2, coords);\n\n   int x, y;\n   MPI_Cart_coords(cart_comm, rank, 2, coords);\n   x = coords[0];\n   y = coords[1];\n\n   int xdim = dims[0];\n   int ydim = dims[1];\n\n   std::array<size_t, 4> counts{0,0,0,0};\n\n   #pragma omp parallel for shared(points)\n   for (size_t i = 0; i < points.size(); i++) {\n      Point p = points[i];\n\n      int xbin, ybin;\n      xbin = ((p.x > 0) - (p.x < 0)) * ((p.x > 0) - (p.x < 0)) + 1;\n      ybin = ((p.y > 0) - (p.y < 0)) * ((p.y > 0) - (p.y < 0)) + 1;\n\n      int xloc = xbin + 2 * (xdim - x - 1);\n      int yloc = ybin + 2 * (ydim - y - 1);\n\n      int idx = xloc + 2 * yloc;\n      counts[idx]++;\n   }\n\n   int s = 2 * (xdim - 1);\n   int e = 2 * (xdim - x - 1);\n   int f = 2 * (ydim - 1);\n   int b = 2 * (ydim - y - 1);\n\n   int x_begin = std::min(s, e);\n   int x_end = std::max(s, e);\n\n   int y_begin = std::min(f, b);\n   int y_end = std::max(f, b);\n\n   for (int i = x_begin; i < x_end; i++) {\n      for (int j = y_begin; j < y_end; j++) {\n         int idx = i + 2 * j;\n         int target_rank = j * xdim + i;\n         if (rank == target_rank) {\n            continue;\n         }\n         int source_rank = rank;\n         MPI_Send(&counts[idx], 1, MPI_LONG_LONG_INT, target_rank, 0, MPI_COMM_WORLD);\n         MPI_Recv(&counts[idx], 1, MPI_LONG_LONG_INT, source_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n   }\n\n   if (rank == 0) {\n      for (int i = 0; i < 4; i++) {\n         bins[i] = 0;\n      }\n      for (int i = 0; i < points.size(); i++) {\n         Point p = points[i];\n         int xbin = ((p.x > 0) - (p.x < 0)) * ((p.x > 0) - (p.x < 0)) + 1;\n         int ybin = ((p.y > 0) - (p.y < 0)) * ((p.y > 0) - (p.y < 0)) + 1;\n         int idx = xbin + 2 * ybin;\n         bins[idx]++;\n      }\n      for (int i = 1; i < np; i++) {\n         int source_rank = i;\n         MPI_Send(&bins[0], 4, MPI_LONG_LONG_INT, source_rank,",
            "MPI_Comm comm = MPI_COMM_WORLD;\n   int rank, size;\n   MPI_Comm_rank(comm, &rank);\n   MPI_Comm_size(comm, &size);\n\n   // compute number of points to be processed by each thread\n   int npts = points.size();\n   int chunk = npts / size;\n   int extra = npts % size;\n   int start = chunk * rank + std::min(rank, extra);\n   int end = start + chunk;\n   if (extra > 0 && rank == size - 1)\n      end += extra;\n   if (start > end)\n      return;\n\n   // get the number of quadrants\n   int nx = size, ny = 2;\n   int px, py;\n   MPI_Dims_create(size, 2, &nx, &ny);\n   int qx = nx / 2;\n   int qy = ny / 2;\n   int qx_rank = rank % qx;\n   int qy_rank = rank / qx;\n\n   // for each point in the chunk, increment the corresponding quadrant\n   for (int i = start; i < end; i++) {\n      Point p = points[i];\n      px = (p.x >= 0? 0 : 1);\n      py = (p.y >= 0? 0 : 1);\n      bins[(qx_rank + px) * qy + (qy_rank + py)]++;\n   }\n}",
            "// TODO: implement\n}",
            "// Fill this in.\n}",
            "}",
            "MPI_Init(0, 0);\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int dims[1] = {2};\n    int periods[1] = {1};\n    int coords[1];\n    MPI_Dims_create(size, 1, dims);\n    MPI_Cart_create(MPI_COMM_WORLD, 1, dims, periods, 1, &mpi_comm_cart);\n    MPI_Cart_get(mpi_comm_cart, 1, dims, periods, coords);\n    int numPerProc = (points.size() + size - 1) / size;\n    int start = coords[0] * numPerProc;\n    int end = start + numPerProc;\n    if (end > points.size()) end = points.size();\n    std::array<size_t, 4> bins_local = { 0, 0, 0, 0 };\n    std::vector<Point> points_local;\n    points_local.assign(points.begin() + start, points.begin() + end);\n#pragma omp parallel shared(points_local, bins_local)\n    {\n        int nthreads = omp_get_num_threads();\n        int thread_id = omp_get_thread_num();\n        int chunk = points_local.size() / nthreads;\n        int start = chunk * thread_id;\n        int end = chunk * (thread_id + 1);\n        if (end > points_local.size()) end = points_local.size();\n        for (int i = start; i < end; ++i) {\n            Point p = points_local[i];\n            if (p.x > 0 && p.y > 0) bins_local[0]++;\n            else if (p.x < 0 && p.y > 0) bins_local[1]++;\n            else if (p.x < 0 && p.y < 0) bins_local[2]++;\n            else bins_local[3]++;\n        }\n    }\n    MPI_Allreduce(MPI_IN_PLACE, bins_local.data(), 4, MPI_LONG_LONG_INT, MPI_SUM, MPI_COMM_WORLD);\n    if (rank == 0) bins = bins_local;\n    MPI_Comm_free(&mpi_comm_cart);\n    MPI_Finalize();\n}",
            "// TODO: write your code here\n\n}",
            "int rank, num_ranks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n   std::array<size_t, 4> local_bins{0, 0, 0, 0};\n\n   std::vector<Point> image;\n   for (auto p : points) {\n      image.push_back(p);\n   }\n\n   for (int i = 0; i < image.size(); ++i) {\n      double x = image[i].x, y = image[i].y;\n      int quadrant = 0;\n      if (x >= 0) quadrant += 1;\n      if (x >= 0 && y >= 0) quadrant += 2;\n      if (y >= 0) quadrant += 4;\n      ++local_bins[quadrant];\n   }\n\n   // MPI_Gatherv is used to count in parallel\n   std::vector<int> counts(num_ranks, 0);\n   std::vector<int> displs(num_ranks, 0);\n   std::vector<size_t> all_bins(4, 0);\n\n   counts[rank] = local_bins.size();\n   displs[rank] = rank * local_bins.size();\n   MPI_Gatherv(local_bins.data(), counts[rank], MPI_LONG_LONG, all_bins.data(), counts.data(), displs.data(), MPI_LONG_LONG, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      for (int i = 0; i < num_ranks; ++i) {\n         for (int j = 0; j < all_bins.size(); ++j) {\n            bins[j] += all_bins[j];\n         }\n      }\n   }\n}",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   std::vector<Point> image;\n   for (int i = rank; i < points.size(); i += size) {\n      image.push_back(points[i]);\n   }\n   int xmin = -3;\n   int ymin = -3;\n   int xmax = 3;\n   int ymax = 3;\n   int x = image[0].x;\n   int y = image[0].y;\n   int x_bin = 0;\n   int y_bin = 0;\n   for (int i = 1; i < image.size(); i++) {\n      x = image[i].x;\n      y = image[i].y;\n      if (x < xmin)\n         x_bin = 0;\n      else if (x < xmax)\n         x_bin = 1;\n      else if (x < 2*xmax)\n         x_bin = 2;\n      else\n         x_bin = 3;\n      if (y < ymin)\n         y_bin = 0;\n      else if (y < ymax)\n         y_bin = 1;\n      else if (y < 2*ymax)\n         y_bin = 2;\n      else\n         y_bin = 3;\n      bins[x_bin]++;\n      bins[y_bin+4]++;\n      bins[y_bin+x_bin]++;\n   }\n   if (rank == 0) {\n      std::cout << x_bin << std::endl;\n      std::cout << y_bin << std::endl;\n      std::cout << x_bin+4 << std::endl;\n      std::cout << y_bin+4 << std::endl;\n      std::cout << x_bin+y_bin << std::endl;\n      std::cout << x_bin+y_bin+4 << std::endl;\n      std::cout << y_bin+x_bin << std::endl;\n      std::cout << y_bin+x_bin+4 << std::endl;\n      std::cout << y_bin+x_bin+y_bin << std::endl;\n      std::cout << y_bin+x_bin+y_bin+4 << std::endl;\n   }\n}",
            "}",
            "// get the number of ranks and rank id\n   int nRanks, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // number of processes in the x direction\n   int nx;\n   int mx = (rank == 0)? (points.size() / nRanks) : 0;\n   int mxy = mx * nRanks;\n   if (rank == 0) {\n      nx = mx / nRanks;\n      assert(nx * nRanks == mx);\n   }\n   else {\n      nx = mx;\n   }\n\n   // compute number of threads\n   int nThreads = omp_get_max_threads();\n\n   // get the extent of the data in x-direction\n   double xmin = points[0].x;\n   double xmax = points[0].x;\n   for (int i = 1; i < mxy; i++) {\n      if (points[i].x < xmin) {\n         xmin = points[i].x;\n      }\n      if (points[i].x > xmax) {\n         xmax = points[i].x;\n      }\n   }\n\n   // compute number of bins\n   int nbins = nx / (2 * nThreads);\n   if (nbins < 1) {\n      nbins = 1;\n   }\n\n   // compute the starting point in x-direction\n   int binStart = nx / 2 * rank;\n   int binEnd = binStart + nx / 2;\n\n   // count quadrants\n   std::array<int, 4> binCnt;\n#pragma omp parallel for default(none) shared(points, mx, binStart, binEnd, binCnt, xmin, xmax) reduction(+: binCnt)\n   for (int j = 0; j < mx; j++) {\n      int tid = omp_get_thread_num();\n      int bin = binStart + nThreads * tid + j / nbins;\n      if (points[mx * rank + j].x < 0.0) {\n         binCnt[0]++;\n      }\n      else if (points[mx * rank + j].x < xmax / 2.0) {\n         binCnt[1]++;\n      }\n      else if (points[mx * rank + j].x < xmax) {\n         binCnt[2]++;\n      }\n      else {\n         binCnt[3]++;\n      }\n   }\n\n   // reduce the counts of each thread\n   MPI_Allreduce(MPI_IN_PLACE, binCnt.data(), binCnt.size(), MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n   // store the bin counts\n   if (rank == 0) {\n      bins = binCnt;\n   }\n\n   // free memory\n   MPI_Barrier(MPI_COMM_WORLD);\n\n   // free memory\n   if (rank == 0) {\n      MPI_Type_free(&mpi_point_t);\n   }\n}",
            "const int nprocs = omp_get_num_procs();\n    const int rank = omp_get_thread_num();\n    const int root = 0;\n    const int dims[2] = {2, 2};\n    MPI_Datatype pointtype;\n    MPI_Type_contiguous(sizeof(Point), MPI_BYTE, &pointtype);\n    MPI_Type_commit(&pointtype);\n\n    // Count points in each quadrant and store in local bins.\n    std::array<size_t, 4> localbins{0,0,0,0};\n    for(const auto& p : points) {\n        if(p.x >= 0 && p.y >= 0)\n            localbins[0]++;\n        if(p.x < 0 && p.y >= 0)\n            localbins[1]++;\n        if(p.x < 0 && p.y < 0)\n            localbins[2]++;\n        if(p.x >= 0 && p.y < 0)\n            localbins[3]++;\n    }\n\n    // Create Cartesian communicator.\n    MPI_Comm comm;\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, 0, 0, &comm);\n\n    // Gather bins to rank 0.\n    if(rank == root) {\n        std::vector<size_t> allbins(4 * nprocs);\n        MPI_Gather(&localbins, 4, MPI_UNSIGNED_LONG_LONG, allbins.data(), 4, MPI_UNSIGNED_LONG_LONG, root, comm);\n        bins[0] = 0;\n        bins[1] = 0;\n        bins[2] = 0;\n        bins[3] = 0;\n        for(size_t i = 0; i < nprocs; ++i) {\n            bins[0] += allbins[i * 4];\n            bins[1] += allbins[i * 4 + 1];\n            bins[2] += allbins[i * 4 + 2];\n            bins[3] += allbins[i * 4 + 3];\n        }\n    }\n    else {\n        MPI_Gather(&localbins, 4, MPI_UNSIGNED_LONG_LONG, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, root, comm);\n    }\n\n    MPI_Type_free(&pointtype);\n    MPI_Comm_free(&comm);\n}",
            "// TODO: implement\n    return;\n}",
            "int my_rank = -1;\n    int num_ranks = -1;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    double min_x = std::numeric_limits<double>::infinity();\n    double max_x = -std::numeric_limits<double>::infinity();\n    double min_y = std::numeric_limits<double>::infinity();\n    double max_y = -std::numeric_limits<double>::infinity();\n    size_t image_size = points.size();\n\n    // compute min_x, min_y, max_x, max_y\n    // parallelize with OpenMP\n    #pragma omp parallel\n    {\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < image_size; ++i)\n        {\n            if (points[i].x < min_x)\n                min_x = points[i].x;\n            if (points[i].x > max_x)\n                max_x = points[i].x;\n            if (points[i].y < min_y)\n                min_y = points[i].y;\n            if (points[i].y > max_y)\n                max_y = points[i].y;\n        }\n    }\n\n    // get the rank of the first and last row of the image\n    size_t first_rank_row = 0, last_rank_row = 0;\n    if (num_ranks % 2 == 0)\n        first_rank_row = image_size / num_ranks / 2;\n    else\n        last_rank_row = image_size / num_ranks / 2 + 1;\n    last_rank_row = image_size - last_rank_row;\n\n    // get the rank of the first and last column of the image\n    size_t first_rank_col = 0, last_rank_col = 0;\n    if (num_ranks % 2 == 0)\n        first_rank_col = image_size / num_ranks / 2;\n    else\n        last_rank_col = image_size / num_ranks / 2 + 1;\n    last_rank_col = image_size - last_rank_col;\n\n    // calculate the number of points in each bin\n    // parallelize with OpenMP\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < image_size; ++i)\n    {\n        size_t row = i / num_ranks / 2;\n        size_t col = i % num_ranks / 2;\n\n        if (col < first_rank_col || col > last_rank_col || row < first_rank_row || row > last_rank_row)\n            continue;\n\n        // calculate which quadrant the point belongs to\n        if (points[i].x >= 0 && points[i].y >= 0)\n            ++bins[0];\n        else if (points[i].x < 0 && points[i].y >= 0)\n            ++bins[1];\n        else if (points[i].x < 0 && points[i].y < 0)\n            ++bins[2];\n        else\n            ++bins[3];\n    }\n\n    // gather the number of points in each bin on rank 0\n    std::array<size_t, 4> send_data;\n    std::array<size_t, 4> recv_data;\n    if (my_rank == 0)\n    {\n        for (int i = 1; i < num_ranks; ++i)\n        {\n            MPI_Recv(&recv_data, 4, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 4; ++j)\n            {\n                bins[j] += recv_data[j];\n            }\n        }\n    }\n    else\n    {\n        M",
            "// Your code here\n\n\n    return;\n}",
            "int mpi_rank, mpi_size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\n  int count = points.size();\n\n  std::array<size_t, 4> local_bins{0};\n  std::vector<double> x_values;\n  std::vector<double> y_values;\n  x_values.resize(count);\n  y_values.resize(count);\n\n  for (int i = 0; i < count; i++) {\n    x_values[i] = points[i].x;\n    y_values[i] = points[i].y;\n  }\n\n  std::sort(x_values.begin(), x_values.end());\n  std::sort(y_values.begin(), y_values.end());\n\n  double min_x = x_values[0];\n  double max_x = x_values[count - 1];\n  double min_y = y_values[0];\n  double max_y = y_values[count - 1];\n\n  int num_x_parts = ceil((max_x - min_x) / (1.0 / mpi_size));\n  int num_y_parts = ceil((max_y - min_y) / (1.0 / mpi_size));\n\n  int local_count = 0;\n\n  for (int i = 0; i < count; i++) {\n    int x_index = getIndex(x_values[i], num_x_parts);\n    int y_index = getIndex(y_values[i], num_y_parts);\n    local_bins[x_index + y_index * 2]++;\n    local_count++;\n  }\n\n  int global_count = local_count;\n  MPI_Allreduce(&local_count, &global_count, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n  std::vector<size_t> global_bins;\n\n  if (mpi_rank == 0) {\n    global_bins.resize(4);\n  }\n\n  MPI_Gather(&local_bins, 4, MPI_UNSIGNED_LONG, &global_bins[0], 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n  bins = global_bins;\n}",
            "// TODO: Implement me\n}",
            "// Initialization of bins and cartesian\n    int dims[2] = {2, 2};\n    int periods[2] = {0, 0};\n    int reorder = 1;\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, reorder, &cartesian);\n\n    int dims_size[2];\n    MPI_Cart_get(cartesian, 2, dims_size, periods, reorder);\n\n    // Initialize bins with zeros\n    for (int i = 0; i < bins.size(); i++) {\n        bins[i] = 0;\n    }\n\n    // Get the rank and number of processes\n    int rank;\n    int num_procs;\n    MPI_Comm_rank(cartesian, &rank);\n    MPI_Comm_size(cartesian, &num_procs);\n\n    // Set the limits of the quadrants\n    double x_min = 0;\n    double x_max = 6;\n    double y_min = -3;\n    double y_max = 3;\n    if (rank % 2 == 0) {\n        x_min = -6;\n    }\n\n    if (rank < num_procs - 1 && (rank + 1) % 2 == 0) {\n        x_max = 3;\n    }\n\n    if (rank / 2 == 0) {\n        y_min = -3;\n    }\n\n    if (rank > 0 && (rank - 1) % 2 == 0) {\n        y_max = 0;\n    }\n\n    // Count points in the quadrants\n    for (int i = 0; i < points.size(); i++) {\n        double x = points[i].x;\n        double y = points[i].y;\n\n        if (x > x_min && x < x_max && y > y_min && y < y_max) {\n            int quadrant = 0;\n\n            if (x < 0) {\n                quadrant += 1;\n            }\n\n            if (y < 0) {\n                quadrant += 2;\n            }\n\n            bins[quadrant]++;\n        }\n    }\n\n    // Reduce the bins\n    for (int i = 0; i < bins.size(); i++) {\n        MPI_Allreduce(&bins[i], &bins[i], 1, MPI_LONG_LONG, MPI_SUM, cartesian);\n    }\n\n    // Check if the reduction worked properly\n    if (rank == 0) {\n        for (int i = 0; i < bins.size(); i++) {\n            std::cout << bins[i] << \" \";\n        }\n\n        std::cout << \"\\n\";\n    }\n\n    // Finalize the cartesian communication\n    MPI_Cart_free(&cartesian);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // int xDim = 10, yDim = 10;\n    // int binSize = 10;\n\n    int xDim = 10;\n    int yDim = 10;\n    int binSize = 10;\n\n    int x = rank % xDim;\n    int y = rank / xDim;\n    int binX = x / binSize;\n    int binY = y / binSize;\n    size_t N = points.size();\n\n    std::vector<size_t> binCount(4);\n\n    int mpi_bins_per_dim = binSize * binSize;\n    int mpi_bin_x = x / mpi_bins_per_dim;\n    int mpi_bin_y = y / mpi_bins_per_dim;\n    int mpi_bin_width = xDim / mpi_bins_per_dim;\n    int mpi_bin_height = yDim / mpi_bins_per_dim;\n\n    // cout << \"x: \" << x << \" y: \" << y << \" binX: \" << binX << \" binY: \" << binY << endl;\n\n    std::vector<Point> mpi_points;\n    for(int i = x; i < x + mpi_bin_width; i++){\n        for(int j = y; j < y + mpi_bin_height; j++){\n            int mpi_bin_id = j * mpi_bins_per_dim + i;\n            int mpi_start_id = mpi_bin_id * N / size;\n            int mpi_end_id = (mpi_bin_id + 1) * N / size;\n            for(int i = mpi_start_id; i < mpi_end_id; i++){\n                mpi_points.push_back(points[i]);\n            }\n        }\n    }\n\n    for(size_t i = 0; i < mpi_points.size(); i++){\n        if(mpi_points[i].x >= binX * binSize && mpi_points[i].x < (binX + 1) * binSize && mpi_points[i].y >= binY * binSize && mpi_points[i].y < (binY + 1) * binSize){\n            binCount[0]++;\n        }\n        else if(mpi_points[i].x >= binX * binSize && mpi_points[i].x < (binX + 1) * binSize && mpi_points[i].y < binY * binSize){\n            binCount[1]++;\n        }\n        else if(mpi_points[i].x < binX * binSize && mpi_points[i].y >= binY * binSize && mpi_points[i].y < (binY + 1) * binSize){\n            binCount[2]++;\n        }\n        else if(mpi_points[i].x < binX * binSize && mpi_points[i].y < binY * binSize){\n            binCount[3]++;\n        }\n    }\n\n    if(rank == 0){\n        MPI_Reduce(MPI_IN_PLACE, binCount.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    } else{\n        MPI_Reduce(binCount.data(), binCount.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n\n    if(rank == 0){\n        for(int i = 0; i < 4; i++){\n            bins[i] = binCount[i];\n        }\n    }\n\n}",
            "int my_rank, num_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n    int* x_rank;\n    int* y_rank;\n    int* x_bin;\n    int* y_bin;\n    x_rank = (int*)malloc(num_ranks * sizeof(int));\n    y_rank = (int*)malloc(num_ranks * sizeof(int));\n    x_bin = (int*)malloc(num_ranks * sizeof(int));\n    y_bin = (int*)malloc(num_ranks * sizeof(int));\n\n    int x_count, y_count;\n    int max_x_rank = 0;\n    int max_y_rank = 0;\n\n    // compute x and y counts\n    for (int i = 0; i < points.size(); i++) {\n        Point p = points[i];\n        if (p.x > 0) {\n            x_rank[my_rank]++;\n            if (p.x > max_x_rank) {\n                max_x_rank = p.x;\n            }\n        } else {\n            x_rank[my_rank]--;\n        }\n\n        if (p.y > 0) {\n            y_rank[my_rank]++;\n            if (p.y > max_y_rank) {\n                max_y_rank = p.y;\n            }\n        } else {\n            y_rank[my_rank]--;\n        }\n    }\n\n    // broadcast x and y rank\n    MPI_Bcast(&x_rank, num_ranks, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Bcast(&y_rank, num_ranks, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // compute x and y bin\n    for (int i = 0; i < num_ranks; i++) {\n        if (x_rank[i] > 0) {\n            x_bin[i] = max_x_rank / x_rank[i];\n        } else {\n            x_bin[i] = -1;\n        }\n\n        if (y_rank[i] > 0) {\n            y_bin[i] = max_y_rank / y_rank[i];\n        } else {\n            y_bin[i] = -1;\n        }\n    }\n\n    // gather x and y bin\n    MPI_Gather(&x_bin, 1, MPI_INT, x_bin, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Gather(&y_bin, 1, MPI_INT, y_bin, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // compute bins and store on rank 0\n    if (my_rank == 0) {\n        for (int i = 0; i < num_ranks; i++) {\n            bins[0] += x_rank[i] * x_bin[i];\n            bins[1] += x_rank[i] * (max_x_rank - x_bin[i]);\n            bins[2] += y_rank[i] * y_bin[i];\n            bins[3] += y_rank[i] * (max_y_rank - y_bin[i]);\n        }\n    }\n}",
            "int rank, nranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n    int size = points.size();\n\n    int x_dim, y_dim;\n    MPI_Dims_create(nranks, 2, &y_dim);\n    MPI_Dims_create(nranks, 2, &x_dim);\n\n    int local_points = size / nranks;\n    int rest = size % nranks;\n\n    if (rank < rest) {\n        local_points++;\n    }\n\n    int local_point_start = rank * local_points;\n\n    int point_counts[4];\n\n#pragma omp parallel for\n    for (int i = 0; i < local_points; ++i) {\n        int local_point_index = local_point_start + i;\n        if (local_point_index >= size) {\n            break;\n        }\n\n        Point& p = points[local_point_index];\n        int quadrant_index = 0;\n        if (p.x > 0) {\n            if (p.y > 0) {\n                quadrant_index = 1;\n            } else {\n                quadrant_index = 2;\n            }\n        } else {\n            quadrant_index = 3;\n        }\n\n        int global_rank = (rank / x_dim) * y_dim + quadrant_index;\n        point_counts[global_rank]++;\n    }\n\n    MPI_Allreduce(point_counts, bins.data(), 4, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n}",
            "//TODO: Your code here\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n\tint size;\n\tMPI_Comm_size(comm, &size);\n\tint rank;\n\tMPI_Comm_rank(comm, &rank);\n\n\tstd::vector<Point> image;\n\timage = points;\n\n\t// each process get local copy of data\n\tstd::vector<Point> local_points;\n\tint npoints = image.size();\n\tint chunk = npoints / size;\n\tint rem = npoints % size;\n\tif (rank < rem) {\n\t\tlocal_points.insert(local_points.end(), image.begin() + rank * (chunk + 1), image.begin() + (rank + 1) * (chunk + 1));\n\t} else {\n\t\tlocal_points.insert(local_points.end(), image.begin() + rank * chunk + rem, image.begin() + rank * chunk + rem + chunk);\n\t}\n\n\t// compute number of point in each quadrant for local data\n\tstd::array<size_t, 4> local_bins = { 0, 0, 0, 0 };\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < local_points.size(); i++) {\n\t\tif (local_points[i].x >= 0) {\n\t\t\tif (local_points[i].y >= 0) {\n\t\t\t\tlocal_bins[0]++;\n\t\t\t} else {\n\t\t\t\tlocal_bins[1]++;\n\t\t\t}\n\t\t} else {\n\t\t\tif (local_points[i].y >= 0) {\n\t\t\t\tlocal_bins[2]++;\n\t\t\t} else {\n\t\t\t\tlocal_bins[3]++;\n\t\t\t}\n\t\t}\n\t}\n\n\t// add local bins to bins\n\tbins[0] = local_bins[0];\n\tbins[1] = local_bins[1];\n\tbins[2] = local_bins[2];\n\tbins[3] = local_bins[3];\n\n\t// sum bins\n\tMPI_Reduce(MPI_IN_PLACE, bins.data(), 4, MPI_LONG_LONG_INT, MPI_SUM, 0, comm);\n\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int dims[2] = {size, size};\n   int periods[2] = {1, 1};\n   int coords[2];\n   MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 1, &MPI_COMM_WORLD);\n   MPI_Cart_coords(MPI_COMM_WORLD, rank, 2, coords);\n   int neighbors[8];\n   MPI_Cart_shift(MPI_COMM_WORLD, 0, 1, &neighbors[0], &neighbors[4]);\n   MPI_Cart_shift(MPI_COMM_WORLD, 1, 1, &neighbors[1], &neighbors[5]);\n   MPI_Cart_shift(MPI_COMM_WORLD, 0, -1, &neighbors[2], &neighbors[6]);\n   MPI_Cart_shift(MPI_COMM_WORLD, 1, -1, &neighbors[3], &neighbors[7]);\n   auto mpi_sum = [](auto& a, auto& b) { return a+b; };\n   auto mpi_max = [](auto& a, auto& b) { return (a>b)?a:b; };\n   // compute local bins\n   std::array<size_t, 4> localBins = {0,0,0,0};\n   for(const Point& p : points) {\n      int x = coords[0];\n      int y = coords[1];\n      if(p.x >= 0) {\n         if(p.y >= 0) {\n            localBins[0]++;\n         } else {\n            localBins[1]++;\n         }\n      } else {\n         if(p.y >= 0) {\n            localBins[2]++;\n         } else {\n            localBins[3]++;\n         }\n      }\n   }\n   // communicate bins\n   MPI_Alltoall(localBins.data(), 1, MPI_UNSIGNED, bins.data(), 1, MPI_UNSIGNED, MPI_COMM_WORLD);\n   for(size_t i=0;i<4;i++) {\n      bins[i] = std::accumulate(bins.begin(),bins.end(),0,mpi_sum);\n   }\n}",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   if (rank == 0) {\n      std::vector<int> counts(size);\n\n      // #pragma omp parallel\n      // {\n      //    int thread_num = omp_get_thread_num();\n      //    printf(\"thread_num %d\\n\", thread_num);\n\n      //    int thread_count = omp_get_num_threads();\n      //    printf(\"thread_count %d\\n\", thread_count);\n      // }\n\n      for (int i = 0; i < size; i++) {\n         counts[i] = countQuadrants(points, bins);\n      }\n\n      // reduce\n      for (int i = 0; i < size; i++) {\n         if (i!= rank) {\n            bins[0] += counts[i];\n         }\n      }\n   } else {\n      // printf(\"[%d] countQuadrants\\n\", rank);\n      bins[countQuadrants(points, bins)]++;\n   }\n}",
            "int nprocs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (rank == 0) {\n        auto count = [](Point p) {\n            if (p.x > 0) {\n                if (p.y > 0) return 0;\n                else if (p.y < 0) return 3;\n                else return 1;\n            } else {\n                if (p.y > 0) return 2;\n                else if (p.y < 0) return 3;\n                else return 0;\n            }\n        };\n\n        #pragma omp parallel for\n        for (int i = 0; i < nprocs; i++) {\n            MPI_Status status;\n            MPI_Recv(bins.data(), 4, MPI_LONG_LONG, i, 0, MPI_COMM_WORLD, &status);\n            for (int j = 0; j < points.size(); j++) {\n                int tmp = count(points[j]);\n                if (tmp == 1) bins[0]++;\n                else if (tmp == 2) bins[1]++;\n                else if (tmp == 3) bins[2]++;\n                else bins[3]++;\n            }\n        }\n    } else {\n        std::vector<Point> local_points;\n        std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n        for (int i = rank; i < points.size(); i+=nprocs) {\n            local_points.push_back(points[i]);\n        }\n        #pragma omp parallel for\n        for (int i = 0; i < local_points.size(); i++) {\n            int tmp = count(local_points[i]);\n            if (tmp == 1) local_bins[0]++;\n            else if (tmp == 2) local_bins[1]++;\n            else if (tmp == 3) local_bins[2]++;\n            else local_bins[3]++;\n        }\n        MPI_Send(local_bins.data(), 4, MPI_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int n_ranks, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int n_proc_x = (int)std::sqrt(n_ranks);\n   int n_proc_y = n_ranks / n_proc_x;\n   std::vector<double> local_x(points.size());\n   std::vector<double> local_y(points.size());\n   std::vector<int> local_q(points.size());\n   std::vector<size_t> local_counts(4, 0);\n\n   #pragma omp parallel for\n   for (int i = 0; i < points.size(); i++) {\n      local_x[i] = points[i].x;\n      local_y[i] = points[i].y;\n   }\n\n   #pragma omp parallel for\n   for (int i = 0; i < local_x.size(); i++) {\n      if (local_x[i] > 0 && local_y[i] > 0)\n         local_q[i] = 0;\n      else if (local_x[i] < 0 && local_y[i] > 0)\n         local_q[i] = 1;\n      else if (local_x[i] < 0 && local_y[i] < 0)\n         local_q[i] = 2;\n      else if (local_x[i] > 0 && local_y[i] < 0)\n         local_q[i] = 3;\n   }\n\n   #pragma omp parallel for\n   for (int i = 0; i < local_x.size(); i++)\n      local_counts[local_q[i]]++;\n\n   std::vector<size_t> global_counts(4, 0);\n\n   MPI_Allreduce(local_counts.data(), global_counts.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n   bins[0] = global_counts[0];\n   bins[1] = global_counts[1];\n   bins[2] = global_counts[2];\n   bins[3] = global_counts[3];\n\n}",
            "// TODO: implement me\n\n}",
            "// FIXME: Your code here\n\n}",
            "bins = {0, 0, 0, 0};\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int nRanks;\n    MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n\n    //Create the cartesian communication topology. The process grid has dimensions 2 by 2.\n    int dims[2];\n    dims[0] = 2;\n    dims[1] = 2;\n    int periods[2];\n    periods[0] = 1;\n    periods[1] = 1;\n    int reorder = 1;\n    MPI_Comm cartesianComm;\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, reorder, &cartesianComm);\n\n    std::array<int, 2> myCoordinates;\n    MPI_Cart_coords(cartesianComm, rank, 2, myCoordinates.data());\n\n    std::array<int, 2> neighborsCoordinates;\n    //Gets coordinates of the neighboring ranks.\n    MPI_Cart_shift(cartesianComm, 0, 1, &neighborsCoordinates[0], &neighborsCoordinates[1]);\n\n    //Sends points to the neighboring ranks.\n    int nPointsInBin = points.size() / nRanks;\n    int pointsToSend = nPointsInBin;\n    int pointsInBuffer = 0;\n\n    if(rank == 0)\n        pointsToSend = points.size() - nPointsInBin * (nRanks - 1);\n    else if(rank == nRanks - 1)\n        pointsToSend = nPointsInBin;\n\n    std::vector<Point> buffer;\n    if(rank == 0){\n        buffer = std::vector<Point>(points.begin(), points.begin() + pointsToSend);\n        pointsInBuffer = pointsToSend;\n    }\n    else{\n        std::vector<Point> pointsToSendBuffer;\n        pointsToSendBuffer.resize(pointsToSend);\n        MPI_Send(&pointsToSendBuffer[0], pointsToSend, MPI_BYTE, neighborsCoordinates[0], 0, cartesianComm);\n        MPI_Recv(&buffer[0], pointsToSend, MPI_BYTE, neighborsCoordinates[0], 0, cartesianComm, MPI_STATUS_IGNORE);\n        pointsInBuffer = pointsToSend;\n    }\n\n    //Counting in parallel the number of points in each bin.\n    int quadrantIndex = myCoordinates[0] * 2 + myCoordinates[1];\n    int numberOfThreads = omp_get_max_threads();\n    int chunkSize = (int)pointsInBuffer / numberOfThreads;\n    #pragma omp parallel for\n    for(int i = 0; i < numberOfThreads; i++){\n        int start = i * chunkSize;\n        int end = start + chunkSize;\n        if(i == numberOfThreads - 1)\n            end = pointsInBuffer;\n\n        for(int j = start; j < end; j++){\n            if(buffer[j].x > 0 && buffer[j].y > 0){\n                bins[quadrantIndex] += 1;\n            }\n            else if(buffer[j].x < 0 && buffer[j].y > 0){\n                bins[quadrantIndex + 1] += 1;\n            }\n            else if(buffer[j].x < 0 && buffer[j].y < 0){\n                bins[quadrantIndex + 2] += 1;\n            }\n            else if(buffer[j].x > 0 && buffer[j].y < 0){\n                bins[quadrantIndex + 3] += 1;\n            }\n        }\n    }\n\n    //Deallocating memory.\n    if(rank == 0)\n        delete [] buffer;\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n   int rank, size;\n   MPI_Comm_rank(comm, &rank);\n   MPI_Comm_size(comm, &size);\n\n   int i, j;\n   int nbx = 10;\n   int nby = 10;\n   int x_off = rank%nbx;\n   int y_off = rank/nbx;\n\n   // Initialize bins array on each rank\n   for (i=0;i<4;i++) {\n      bins[i] = 0;\n   }\n\n   // Loop over the points and update the bins on each rank\n   for (i=0;i<points.size();i++) {\n      j = 0;\n      if (points[i].x >= 0) {\n         j = 1;\n      }\n      if (points[i].y >= 0) {\n         j = j+2;\n      }\n      bins[j] = bins[j]+1;\n   }\n\n   // Sum the bins on each rank to get total counts\n   // Store the sum in bins[0]\n   for (i=0;i<4;i++) {\n      MPI_Allreduce(&bins[i], &bins[0], 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n   }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    MPI_Info info;\n    int rank, size;\n    MPI_Comm_size(comm, &size);\n    MPI_Comm_rank(comm, &rank);\n\n    int n_per_rank = points.size()/size;\n    int extra = points.size()%size;\n\n    int *points_counts = new int[4];\n    for (int i=0; i<4; i++)\n        points_counts[i] = 0;\n\n    Point* points_rank = new Point[n_per_rank+extra];\n    for (int i=0; i<n_per_rank; i++)\n        points_rank[i] = points[rank*n_per_rank+i];\n    for (int i=0; i<extra; i++)\n        points_rank[n_per_rank+i] = points[rank*n_per_rank+n_per_rank+i];\n\n    std::vector<double> x;\n    std::vector<double> y;\n    std::vector<int> quadrant;\n\n    for (int i=0; i<n_per_rank+extra; i++) {\n        x.push_back(points_rank[i].x);\n        y.push_back(points_rank[i].y);\n        if (points_rank[i].x > 0 && points_rank[i].y > 0)\n            quadrant[i] = 0;\n        else if (points_rank[i].x > 0 && points_rank[i].y < 0)\n            quadrant[i] = 1;\n        else if (points_rank[i].x < 0 && points_rank[i].y > 0)\n            quadrant[i] = 2;\n        else if (points_rank[i].x < 0 && points_rank[i].y < 0)\n            quadrant[i] = 3;\n    }\n\n    int *bins_rank;\n    int *bins_temp;\n    MPI_Allreduce(&points_counts, &bins_rank, 4, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    bins_temp = new int[4];\n    for (int i=0; i<4; i++)\n        bins_temp[i] = 0;\n\n    int bins_size;\n    MPI_Datatype type;\n    MPI_Type_contiguous(1, MPI_INT, &type);\n    MPI_Type_commit(&type);\n    MPI_Allreduce(bins_rank, bins_temp, 4, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    MPI_Type_free(&type);\n\n    for (int i=0; i<4; i++)\n        bins[i] = bins_temp[i];\n\n    delete [] points_counts;\n    delete [] points_rank;\n    delete [] bins_rank;\n    delete [] bins_temp;\n    delete [] x;\n    delete [] y;\n    delete [] quadrant;\n    MPI_Finalize();\n}",
            "// TODO\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int nthreads = 1;\n   #pragma omp parallel\n   {\n      nthreads = omp_get_num_threads();\n   }\n\n   int maxThreads = 0;\n   MPI_Allreduce(&nthreads, &maxThreads, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);\n   int nQuads = 4;\n\n   std::array<size_t, 4> localBins{0};\n\n   #pragma omp parallel for schedule(dynamic) reduction(+:localBins)\n   for (size_t i = 0; i < points.size(); ++i) {\n      size_t quadrant = 0;\n\n      if (points[i].x >= 0)\n         quadrant += 1;\n      if (points[i].y >= 0)\n         quadrant += 2;\n\n      localBins[quadrant]++;\n   }\n\n   std::array<size_t, 4> globalBins{0};\n\n   MPI_Allreduce(&localBins, &globalBins, 4, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n   if (rank == 0) {\n      bins = globalBins;\n   }\n}",
            "const size_t n_points = points.size();\n    bins.fill(0);\n\n    const int mpi_rank = omp_get_thread_num();\n    const int mpi_size = omp_get_num_threads();\n\n    // get rank's x, y, z, and the number of procs along each dimension\n    int x, y, z;\n    int n_x, n_y, n_z;\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Dims_create(mpi_size, 3, &n_x);\n    MPI_Cart_coords(MPI_COMM_WORLD, mpi_rank, 3, &x, &y, &z);\n\n    // get the number of points in each quadrant and store the results in `bins`\n    // (1) each rank gets its own subsection of the points\n    // (2) each rank counts the points in its quadrant\n    // (3) each rank sums its counts in quadrant with the master rank\n    std::vector<Point> local_points;\n    std::array<size_t, 4> local_bins;\n    local_bins.fill(0);\n\n    // (1)\n    size_t start = n_points / mpi_size;\n    size_t end = start;\n    if (mpi_rank < n_points % mpi_size) {\n        start += mpi_rank;\n        end += mpi_rank + 1;\n    }\n    local_points.insert(local_points.end(), points.begin() + start, points.begin() + end);\n\n    // (2)\n    for (int i = 0; i < local_points.size(); i++) {\n        // get the coordinates of each point\n        double xi = local_points[i].x;\n        double yi = local_points[i].y;\n        if (xi < 0) {\n            if (yi < 0) {\n                local_bins[0] += 1;\n            } else if (yi > 0) {\n                local_bins[3] += 1;\n            } else {\n                local_bins[2] += 1;\n            }\n        } else if (xi > 0) {\n            if (yi < 0) {\n                local_bins[1] += 1;\n            } else if (yi > 0) {\n                local_bins[2] += 1;\n            } else {\n                local_bins[3] += 1;\n            }\n        } else {\n            if (yi < 0) {\n                local_bins[0] += 1;\n            } else if (yi > 0) {\n                local_bins[1] += 1;\n            } else {\n                local_bins[2] += 1;\n            }\n        }\n    }\n\n    // (3)\n    std::array<size_t, 4> global_bins;\n    if (mpi_rank == 0) {\n        global_bins.fill(0);\n        for (int i = 1; i < mpi_size; i++) {\n            MPI_Status status;\n            MPI_Recv(global_bins.data(), 4, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n            for (int j = 0; j < 4; j++) {\n                bins[j] += global_bins[j];\n            }\n        }\n        for (int i = 0; i < 4; i++) {\n            bins[i] += local_bins[i];\n        }\n    } else {\n        MPI_Send(local_bins.data(), 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO: Fill in this function\n    int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (size == 1) {\n        // single process\n        bins[0] = bins[1] = bins[2] = bins[3] = 0;\n        for (auto& p : points) {\n            if (p.x > 0.0 && p.y > 0.0) bins[0]++;\n            else if (p.x < 0.0 && p.y > 0.0) bins[1]++;\n            else if (p.x < 0.0 && p.y < 0.0) bins[2]++;\n            else if (p.x > 0.0 && p.y < 0.0) bins[3]++;\n        }\n    } else {\n        // multi process\n        std::vector<Point> local_points;\n        size_t local_size;\n        MPI_Scatter(&points[0], 1, MPI_INT, &local_points[0], 1, MPI_INT, 0, MPI_COMM_WORLD);\n        MPI_Scatter(&points[0], 1, MPI_INT, &local_size, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n        // omp parallel reduction(+:sum)\n        // {\n        //     size_t sum = 0;\n        //     for (size_t i = 0; i < local_size; i++) {\n        //         auto& p = local_points[i];\n        //         if (p.x > 0.0 && p.y > 0.0) sum++;\n        //         else if (p.x < 0.0 && p.y > 0.0) sum++;\n        //         else if (p.x < 0.0 && p.y < 0.0) sum++;\n        //         else if (p.x > 0.0 && p.y < 0.0) sum++;\n        //     }\n        //     bins[rank] = sum;\n        // }\n        // MPI_Reduce(MPI_IN_PLACE, &bins[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n        // omp parallel reduction(+:sum)\n        // {\n        //     size_t sum = 0;\n        //     #pragma omp parallel for reduction(+:sum)\n        //     for (size_t i = 0; i < local_size; i++) {\n        //         auto& p = local_points[i];\n        //         if (p.x > 0.0 && p.y > 0.0) sum++;\n        //         else if (p.x < 0.0 && p.y > 0.0) sum++;\n        //         else if (p.x < 0.0 && p.y < 0.0) sum++;\n        //         else if (p.x > 0.0 && p.y < 0.0) sum++;\n        //     }\n        //     bins[rank] = sum;\n        // }\n        // MPI_Reduce(MPI_IN_PLACE, &bins[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n        #pragma omp parallel reduction(+:sum)\n        {\n            size_t sum = 0;\n            #pragma omp for\n            for (size_t i = 0; i < local_size; i++) {\n                auto& p = local_points[i];\n                if (p.x > 0.0 && p.y > 0.0) sum++;\n                else if (p.x < 0.0 && p.y > 0.0) sum++;\n                else if (p.x < 0.0 && p.y < 0.0) sum++;\n                else if (p.x > 0.0 && p.y < 0.0) sum++;",
            "int n_mpi_tasks, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &n_mpi_tasks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // split the work\n    int n_mpi_tasks_per_row = (int) ceil(sqrt(n_mpi_tasks));\n    int row = rank / n_mpi_tasks_per_row;\n    int column = rank % n_mpi_tasks_per_row;\n\n    size_t n_points = points.size();\n    size_t block_size = n_points / n_mpi_tasks;\n    size_t leftovers = n_points % n_mpi_tasks;\n\n    size_t start = block_size * rank;\n    size_t end = start + block_size;\n    if (rank < leftovers)\n        end += 1;\n\n    std::array<size_t, 4> bin_counts;\n    bin_counts.fill(0);\n\n    omp_set_num_threads(4);\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        Point point = points[i];\n        size_t quadrant = 0;\n        if (point.x >= 0) {\n            if (point.y >= 0) quadrant = 0;\n            else quadrant = 3;\n        }\n        else {\n            if (point.y >= 0) quadrant = 1;\n            else quadrant = 2;\n        }\n        bin_counts[quadrant]++;\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    // merge\n    if (rank == 0) {\n        for (int i = 1; i < n_mpi_tasks; i++) {\n            int task = i * n_mpi_tasks_per_row + row;\n            if (task < n_mpi_tasks) {\n                int column = task % n_mpi_tasks_per_row;\n                MPI_Status status;\n                MPI_Recv(bins.data(), 4, MPI_UNSIGNED_LONG_LONG, task, 0, MPI_COMM_WORLD, &status);\n                for (int j = 0; j < 4; j++) {\n                    bin_counts[j] += bins[j];\n                }\n            }\n        }\n    }\n    else {\n        MPI_Send(bin_counts.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    // write back\n    if (rank == 0) {\n        bins = bin_counts;\n    }\n}",
            "// your code here\n}",
            "/*\n    YOUR CODE HERE\n  */\n}",
            "int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   if(rank == 0) std::cout << \"Rank \" << rank << \": \" << points.size() << \" points\" << std::endl;\n\n   // Initialize to 0\n   bins.fill(0);\n\n   // Calculate the total number of points on the current rank.\n   // The total number of points in the image is the number of points on the first rank times the number of ranks.\n   // This number of points is the same on every rank so we can use MPI_Allreduce.\n   size_t numPointsThisRank = points.size();\n   size_t numPointsGlobal = numPointsThisRank * size_t(MPI_Comm_size(MPI_COMM_WORLD));\n   MPI_Allreduce(MPI_IN_PLACE, &numPointsThisRank, 1, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n   // Create a vector containing the number of points in each quadrant\n   std::vector<size_t> count(4);\n\n   // Create a vector containing the points in each quadrant\n   std::vector<Point> quadrantPoints[4];\n   #pragma omp parallel for\n   for(size_t i = 0; i < points.size(); ++i) {\n      int quadrant = getQuadrant(points[i].x, points[i].y);\n      if(quadrant >= 0) {\n         count[quadrant] += 1;\n         quadrantPoints[quadrant].push_back(points[i]);\n      }\n   }\n   if(rank == 0) {\n      for(int i = 0; i < 4; ++i) {\n         std::cout << \"Count in quadrant \" << i << \": \" << count[i] << std::endl;\n      }\n   }\n\n   // Reduce the counts in each quadrant to rank 0\n   size_t numPointsReduced = 0;\n   for(int i = 0; i < 4; ++i) {\n      numPointsReduced += count[i];\n      if(rank == 0) {\n         bins[i] = count[i];\n      }\n   }\n   if(rank == 0) {\n      std::cout << \"Total number of points: \" << numPointsGlobal << std::endl;\n      std::cout << \"Total number of points reduced: \" << numPointsReduced << std::endl;\n   }\n\n   // Reduce the points in each quadrant to rank 0.\n   // We are using a reduce operation for this because the number of points in each quadrant is the same on every rank.\n   std::vector<Point> pointsReduced(numPointsReduced);\n   std::vector<size_t> indices(numPointsGlobal);\n   size_t index = 0;\n   for(int i = 0; i < 4; ++i) {\n      // This is the offset from the beginning of the global vector of points\n      size_t offset = index;\n      MPI_Allreduce(MPI_IN_PLACE, quadrantPoints[i].data(), quadrantPoints[i].size(), MPI_POINT, MPI_REDUCE_APPEND, MPI_COMM_WORLD);\n      index += quadrantPoints[i].size();\n      for(int j = 0; j < quadrantPoints[i].size(); ++j) {\n         indices[offset + j] = i;\n      }\n   }\n\n   // Copy the points from the reduced vector to the full vector\n   if(rank == 0) {\n      std::cout << \"Copying quadrant points\" << std::endl;\n   }\n   for(size_t i = 0; i < numPointsReduced; ++i) {\n      pointsReduced[i].x = quadrantPoints[indices[i]][i].x;\n      pointsReduced[i].y = quadrantPoints[indices[i]][i].y;\n   }\n   if(rank == 0) {\n      std::cout << \"Done copying quadrant points\" << std::endl;\n   }\n\n   if(rank == 0) {\n      std",
            "constexpr int dims = 2;\n    int nproc, rank, mpi_err;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    const int N = points.size();\n    size_t count = 0;\n    size_t part = N / nproc;\n    int start = part * rank;\n    int end = part * (rank + 1);\n\n    // count points in this rank\n    #pragma omp parallel for reduction(+:count)\n    for (int i = start; i < end; i++) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            count++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            count++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            count++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            count++;\n        }\n    }\n\n    // collect counts and store in bins\n    MPI_Allreduce(MPI_IN_PLACE, &count, 1, MPI_LONG, MPI_SUM, MPI_COMM_WORLD);\n    bins[0] = count;\n\n    // TODO: Implement remaining quadrants (1,2,3,4) using OpenMP and MPI\n\n\n    // check if all ranks have equal number of points\n    for (int i = 1; i < nproc; i++) {\n        MPI_Recv(&count, 1, MPI_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        if (bins[i]!= count) {\n            throw std::runtime_error(\"unequal number of points\");\n        }\n    }\n\n    // bins[0] = bins[1] + bins[2] + bins[3]\n    bins[0] = bins[1] + bins[2] + bins[3];\n\n    // sort bins\n    for (int i = 1; i < dims; i++) {\n        if (bins[i] < bins[i - 1]) {\n            throw std::runtime_error(\"not sorted\");\n        }\n    }\n}",
            "}",
            "int rank, nranks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n   // Get the extent of the image, the size of the image, and the coordinates of this process in the global grid\n   std::array<size_t, 2> image_size = { 0, 0 };\n   std::array<int, 2> coords = { 0, 0 };\n   MPI_Allreduce(MPI_IN_PLACE, image_size.data(), 2, MPI_UNSIGNED, MPI_MAX, MPI_COMM_WORLD);\n   MPI_Allreduce(MPI_IN_PLACE, coords.data(), 2, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n   int my_rank_row = rank / nranks;\n   int my_rank_col = rank % nranks;\n\n   // Get the extent of the current process's quadrant\n   std::array<size_t, 2> my_quadrant_size = { 0, 0 };\n   std::array<int, 2> my_quadrant_coords = { 0, 0 };\n\n   // Get the extent of the current process's quadrant\n   my_quadrant_size[0] = image_size[0] / nranks;\n   my_quadrant_size[1] = image_size[1] / nranks;\n   my_quadrant_coords[0] = coords[0] + my_rank_row * my_quadrant_size[0];\n   my_quadrant_coords[1] = coords[1] + my_rank_col * my_quadrant_size[1];\n\n   // Get the extent of the current process's quadrant\n   std::array<size_t, 2> my_quadrant_size_divisor = { 0, 0 };\n   std::array<int, 2> my_quadrant_coords_divisor = { 0, 0 };\n\n   my_quadrant_size_divisor[0] = my_quadrant_size[0] / nranks;\n   my_quadrant_size_divisor[1] = my_quadrant_size[1] / nranks;\n   my_quadrant_coords_divisor[0] = my_quadrant_coords[0] + my_rank_row * my_quadrant_size_divisor[0];\n   my_quadrant_coords_divisor[1] = my_quadrant_coords[1] + my_rank_col * my_quadrant_size_divisor[1];\n\n   int my_rank_row_divisor = rank / nranks;\n   int my_rank_col_divisor = rank % nranks;\n\n   bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n\n   // Check if point is inside the quadrant\n   #pragma omp parallel for reduction(+ : bins[0], bins[1], bins[2], bins[3])\n   for (auto& point : points) {\n      bool in_quadrant_x = point.x >= my_quadrant_coords[0] && point.x < my_quadrant_coords[0] + my_quadrant_size_divisor[0];\n      bool in_quadrant_y = point.y >= my_quadrant_coords[1] && point.y < my_quadrant_coords[1] + my_quadrant_size_divisor[1];\n      if (in_quadrant_x && in_quadrant_y) {\n         if (point.x >= 0 && point.y >= 0) {\n            bins[0]++;\n         } else if (point.x < 0 && point.y >= 0) {\n            bins[1]++;\n         } else if (point.x < 0 && point.y < 0) {\n            bins[2]++;",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        bins = {0, 0, 0, 0};\n    }\n\n    // Each rank computes the quadrant that its points belong in\n    std::vector<int> quadrants(points.size());\n    for (size_t i = 0; i < points.size(); ++i) {\n        quadrants[i] = (points[i].x >= 0.0? 1 : 2) + (points[i].y >= 0.0? 0 : 2);\n    }\n\n    // Each rank computes the sum of quadrants in each bin\n    std::vector<int> localBins(4);\n    for (size_t i = 0; i < points.size(); ++i) {\n        ++localBins[quadrants[i]];\n    }\n\n    // Merge the localBins into the global bins\n    std::vector<int> globalBins(4);\n    MPI_Allreduce(MPI_IN_PLACE, localBins.data(), 4, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    for (size_t i = 0; i < 4; ++i) {\n        globalBins[i] = localBins[i];\n    }\n    bins = globalBins;\n}",
            "// TODO\n}",
            "int nbins = 4;\n   int size, rank, x, y;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Status status;\n   std::array<size_t, 4> localBins;\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      x = (points[i].x > 0);\n      y = (points[i].y > 0);\n      localBins[x + y]++;\n   }\n   std::array<size_t, 4> globalBins;\n   MPI_Allreduce(localBins.data(), globalBins.data(), nbins, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n   for (int i = 0; i < size; i++) {\n      if (i!= rank) {\n         MPI_Recv(&bins, nbins, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, &status);\n         bins += globalBins;\n      }\n   }\n   if (rank == 0) {\n      MPI_Send(&bins, nbins, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n   }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n   int rank;\n   int nranks;\n\n   MPI_Comm_rank(comm, &rank);\n   MPI_Comm_size(comm, &nranks);\n\n   int n_points_per_rank = points.size() / nranks;\n   int start = rank * n_points_per_rank;\n   int end = start + n_points_per_rank;\n\n   std::array<size_t, 4> bins_loc;\n\n   size_t n_points = 0;\n   for (size_t i = start; i < end; ++i) {\n      Point p = points[i];\n      if (p.x >= 0 && p.y >= 0) {\n         ++n_points;\n      }\n      else if (p.x >= 0 && p.y < 0) {\n         ++bins_loc[1];\n      }\n      else if (p.x < 0 && p.y >= 0) {\n         ++bins_loc[2];\n      }\n      else if (p.x < 0 && p.y < 0) {\n         ++bins_loc[3];\n      }\n   }\n   bins_loc[0] = n_points;\n\n   bins = {0, 0, 0, 0};\n   MPI_Reduce(&bins_loc, bins.data(), 4, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, size;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &size);\n    // TODO:\n\n    // Each processor will count the number of points in each quadrant.\n    // To do this, we will take a cartesian grid and place the points in each grid cell.\n    // For example, if we have a grid of 2 by 2, then our grid looks like:\n    //\n    //     (-1, 1) - (-1, 0)\n    //          |          |\n    //     (0, 1) - (0, 0) - (+1, 0)\n    //          |          |\n    //     (+1, 1) - (+1, 0)\n    //\n    // So, in our case, we have 2 by 2 grid. So, the grid will look like:\n    //\n    //     (-2, -2) - (-2, -1) - (-2, 0) - (-2, 1) - (-2, 2)\n    //          |          |          |          |          |\n    //     (-1, -2) - (-1, -1) - (-1, 0) - (-1, 1) - (-1, 2)\n    //          |          |          |          |          |\n    //     (0, -2) - (0, -1) - (0, 0) - (0, 1) - (0, 2)\n    //          |          |          |          |          |\n    //     (+1, -2) - (+1, -1) - (+1, 0) - (+1, 1) - (+1, 2)\n    //          |          |          |          |          |\n    //     (+2, -2) - (+2, -1) - (+2, 0) - (+2, 1) - (+2, 2)\n    //\n    // In this grid, we can easily see which grid cell our point belongs to.\n    // So, first, we will create a cartesian grid.\n    // To create a cartesian grid, we need to specify the number of rows and columns.\n    // Also, we will need to specify how the rows and columns should be distributed among all ranks.\n    // Each process will have a cartesian grid. So, we will first create the grid,\n    // and then we will assign the points to the grid.\n    //\n    // We will use MPI_Cart_create to create a cartesian grid.\n    // In MPI_Cart_create, we will specify the number of rows and columns,\n    // the number of processes along each dimension, and the number of dimensions.\n    // We will also specify the number of processes along the x axis and y axis.\n    // MPI_Cart_create will take this information and create a cartesian grid.\n    //\n    // Now, we can assign each point to a grid cell.\n    // This will be done by using MPI_Cart_rank.\n    // MPI_Cart_rank will take the grid and a point and return the rank of the processor where the point belongs to.\n    // The rank of the processor will be returned in a variable `index`.\n    // Once we have the rank, we will use `MPI_Get_address` to obtain the address of each bin.\n    // This will be done as follows:\n    // 1. Define an integer variable `bins`.\n    // 2. Obtain the address of `bins`.\n    // 3. Use `MPI_Get_address` to obtain the address of each bin.\n    //\n    // We will use `MPI_Allreduce` to obtain the sum of bins from all the ranks.\n    //\n    // We will use the following code to obtain the sum of bins from all the ranks.\n    // 1. Initialize bins with 0.\n    // 2. Use MPI_Allreduce to obtain the sum of bins.\n    //\n    //\n    // Once we obtain the sum of bins from all the ranks, we will divide it by the number of ranks.\n    // This will be done as follows:\n    // 1. Use MPI_Allreduce to obtain the sum of bins from all the ranks.\n    // 2. Obtain the number of ranks.\n    // 3.",
            "int my_rank;\n    int n_proc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &n_proc);\n\n    // get local points in this rank\n    std::vector<Point> local_points;\n    if(my_rank == 0) {\n        local_points = points;\n    } else {\n        local_points.clear();\n    }\n\n    // get points from all ranks\n    std::vector<Point> all_points;\n    if(my_rank == 0) {\n        for(int i = 1; i < n_proc; i++) {\n            std::vector<Point> tmp;\n            MPI_Recv(&tmp, sizeof(tmp), MPI_BYTE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            all_points.insert(all_points.end(), tmp.begin(), tmp.end());\n        }\n        all_points.insert(all_points.end(), local_points.begin(), local_points.end());\n    } else {\n        MPI_Send(&local_points, sizeof(local_points), MPI_BYTE, 0, 0, MPI_COMM_WORLD);\n    }\n\n    // count quadrants\n    size_t quadrant_count[4];\n    quadrant_count[0] = 0;\n    quadrant_count[1] = 0;\n    quadrant_count[2] = 0;\n    quadrant_count[3] = 0;\n    for(auto p : all_points) {\n        if(p.x >= 0 && p.y >= 0) {\n            quadrant_count[0]++;\n        } else if(p.x <= 0 && p.y >= 0) {\n            quadrant_count[1]++;\n        } else if(p.x <= 0 && p.y <= 0) {\n            quadrant_count[2]++;\n        } else if(p.x >= 0 && p.y <= 0) {\n            quadrant_count[3]++;\n        }\n    }\n\n    // scatter counts to each rank\n    if(my_rank == 0) {\n        bins[0] = quadrant_count[0];\n        bins[1] = quadrant_count[1];\n        bins[2] = quadrant_count[2];\n        bins[3] = quadrant_count[3];\n        MPI_Scatter(bins.data(), 1, MPI_LONG_LONG_INT, bins.data(), 1, MPI_LONG_LONG_INT, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Scatter(bins.data(), 1, MPI_LONG_LONG_INT, bins.data(), 1, MPI_LONG_LONG_INT, 0, MPI_COMM_WORLD);\n    }\n}",
            "// Initialize MPI and OpenMP\n   //...\n\n   // Initialize bins to 0\n   for (size_t i = 0; i < 4; i++) {\n      bins[i] = 0;\n   }\n\n   //...\n\n   // Perform the computation in parallel\n   //...\n}",
            "// TODO\n}",
            "// TODO\n   return;\n}",
            "int comm_sz;\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    const int N = points.size();\n    const double x_min = -10;\n    const double x_max = 10;\n    const double y_min = -10;\n    const double y_max = 10;\n\n    const int q_size = 4;\n\n    const double x_width = (x_max - x_min) / 2.0;\n    const double y_width = (y_max - y_min) / 2.0;\n\n    std::vector<Point> local_points;\n\n    if (rank < N) {\n        local_points = points;\n    }\n    //std::cout << \"local points size:\" << local_points.size() << std::endl;\n\n    int local_count[4];\n    local_count[0] = 0;\n    local_count[1] = 0;\n    local_count[2] = 0;\n    local_count[3] = 0;\n\n#pragma omp parallel for\n    for (int i = 0; i < local_points.size(); i++) {\n        Point p = local_points[i];\n        int q = 0;\n        if (p.x > x_max)\n            q = 1;\n        else if (p.x < x_min)\n            q = 3;\n        else if (p.y > y_max)\n            q = 2;\n        else if (p.y < y_min)\n            q = 0;\n        else if (p.y > y_min && p.y <= y_max && p.x > x_max && p.x <= x_min)\n            q = 1;\n        else if (p.y > y_min && p.y <= y_max && p.x < x_min && p.x >= x_max)\n            q = 3;\n        else if (p.y < y_max && p.y >= y_min && p.x > x_max && p.x <= x_min)\n            q = 2;\n        else if (p.y < y_max && p.y >= y_min && p.x < x_min && p.x >= x_max)\n            q = 0;\n        local_count[q]++;\n    }\n\n    //std::cout << \"local count 0:\" << local_count[0] << std::endl;\n    //std::cout << \"local count 1:\" << local_count[1] << std::endl;\n    //std::cout << \"local count 2:\" << local_count[2] << std::endl;\n    //std::cout << \"local count 3:\" << local_count[3] << std::endl;\n\n    int global_count[4];\n\n    MPI_Gather(local_count, 4, MPI_INT, global_count, 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins[0] = global_count[0];\n        bins[1] = global_count[1];\n        bins[2] = global_count[2];\n        bins[3] = global_count[3];\n    }\n\n}",
            "int rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tstd::vector<Point> localPoints;\n\n\tif (rank == 0)\n\t{\n\t\tbins = std::array<size_t, 4>();\n\t}\n\n\tMPI_Bcast(&bins, sizeof(bins), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n\t// split work among MPI ranks\n\tfor (auto point : points)\n\t{\n\t\tif (point.x >= 0 && point.y >= 0)\n\t\t{\n\t\t\tlocalPoints.push_back(point);\n\t\t}\n\t}\n\n\tint pointsPerRank = localPoints.size() / size;\n\n\tstd::vector<Point> localPoints2;\n\tfor (int i = rank * pointsPerRank; i < (rank + 1) * pointsPerRank; i++)\n\t{\n\t\tlocalPoints2.push_back(localPoints[i]);\n\t}\n\n\tstd::vector<Point> localPoints3;\n\t#pragma omp parallel for\n\tfor (int i = 0; i < localPoints2.size(); i++)\n\t{\n\t\tlocalPoints3.push_back(localPoints2[i]);\n\t}\n\n\tint counts[4];\n\tcounts[0] = 0;\n\tcounts[1] = 0;\n\tcounts[2] = 0;\n\tcounts[3] = 0;\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < localPoints3.size(); i++)\n\t{\n\t\tPoint point = localPoints3[i];\n\t\tif (point.x >= 0 && point.y >= 0)\n\t\t{\n\t\t\tcounts[0]++;\n\t\t}\n\t\telse if (point.x >= 0 && point.y < 0)\n\t\t{\n\t\t\tcounts[1]++;\n\t\t}\n\t\telse if (point.x < 0 && point.y < 0)\n\t\t{\n\t\t\tcounts[2]++;\n\t\t}\n\t\telse if (point.x < 0 && point.y >= 0)\n\t\t{\n\t\t\tcounts[3]++;\n\t\t}\n\t}\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < size; i++)\n\t{\n\t\tMPI_Send(&counts, sizeof(counts), MPI_BYTE, i, 0, MPI_COMM_WORLD);\n\t}\n\n\tif (rank == 0)\n\t{\n\t\tfor (int i = 1; i < size; i++)\n\t\t{\n\t\t\tint *recvcounts = new int[4];\n\t\t\tMPI_Recv(recvcounts, sizeof(recvcounts), MPI_BYTE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t\tfor (int i = 0; i < 4; i++)\n\t\t\t{\n\t\t\t\tbins[i] += recvcounts[i];\n\t\t\t}\n\t\t}\n\t\tfor (int i = 1; i < size; i++)\n\t\t{\n\t\t\tMPI_Recv(&counts, sizeof(counts), MPI_BYTE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t}\n\t}\n\telse\n\t{\n\t\tMPI_Send(&counts, sizeof(counts), MPI_BYTE, 0, 0, MPI_COMM_WORLD);\n\t}\n\n\tif (rank == 0)\n\t{\n\t\tfor (int i = 0; i < size; i++)\n\t\t{\n\t\t\tMPI_Status status;\n\t\t\tMPI_Recv(&counts, sizeof(counts), MPI_BYTE, i, 0, MPI_COMM_WORLD, &status);\n\t\t\tbins[0] += counts[0];\n\t\t\tbins[1] += counts[1];",
            "// Your code here\n\n    return;\n}",
            "/*... */\n}",
            "bins.fill(0);\n   int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   #pragma omp parallel\n   {\n      int thread_id = omp_get_thread_num();\n      int nthreads = omp_get_num_threads();\n\n      // initialize bins\n      std::array<size_t, 4> local_bins;\n      local_bins.fill(0);\n\n      #pragma omp for\n      for (int i = 0; i < points.size(); ++i) {\n         Point const& point = points[i];\n         double x = point.x;\n         double y = point.y;\n\n         if (x > 0) {\n            if (y > 0) {\n               local_bins[0]++;\n            } else {\n               local_bins[3]++;\n            }\n         } else {\n            if (y > 0) {\n               local_bins[1]++;\n            } else {\n               local_bins[2]++;\n            }\n         }\n      }\n\n      // merge bins\n      #pragma omp critical\n      {\n         bins[0] += local_bins[0];\n         bins[1] += local_bins[1];\n         bins[2] += local_bins[2];\n         bins[3] += local_bins[3];\n      }\n   }\n}",
            "int xdim, ydim;\n    MPI_Comm_size(MPI_COMM_WORLD, &xdim);\n    MPI_Comm_size(MPI_COMM_WORLD, &ydim);\n    std::array<size_t, 4> local_counts = {0, 0, 0, 0};\n    size_t const size = points.size();\n    if(size == 0) return;\n\n    #pragma omp parallel for\n    for(size_t i = 0; i < size; ++i)\n    {\n        int xrank, yrank;\n        MPI_Cart_coords(MPI_COMM_WORLD, omp_get_thread_num(), 2, &xrank, &yrank);\n        Point& p = points[i];\n        if(p.x >= 0 && p.y >= 0)\n            ++local_counts[0];\n        else if(p.x < 0 && p.y >= 0)\n            ++local_counts[1];\n        else if(p.x < 0 && p.y < 0)\n            ++local_counts[2];\n        else\n            ++local_counts[3];\n    }\n\n    std::array<size_t, 4> global_counts;\n    MPI_Allreduce(local_counts.data(), global_counts.data(), 4, MPI_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n    bins = global_counts;\n}",
            "//TODO\n\n}",
            "int mpi_rank, mpi_size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\tdouble xmin, xmax, ymin, ymax;\n\txmin = -10;\n\txmax = 10;\n\tymin = -10;\n\tymax = 10;\n\n\tsize_t npoints = points.size();\n\tstd::array<double, 4> counts{};\n\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < npoints; ++i) {\n\t\tPoint p = points.at(i);\n\t\tint q = 0;\n\t\tif (p.x > 0 && p.y > 0) q = 0;\n\t\telse if (p.x < 0 && p.y > 0) q = 1;\n\t\telse if (p.x < 0 && p.y < 0) q = 2;\n\t\telse if (p.x > 0 && p.y < 0) q = 3;\n\t\t#pragma omp atomic\n\t\tcounts[q]++;\n\t}\n\n\tstd::array<double, 4> rcounts;\n\n\tMPI_Allreduce(&counts[0], &rcounts[0], 4, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n\tif (mpi_rank == 0) {\n\t\tbins[0] = rcounts[0];\n\t\tbins[1] = rcounts[1];\n\t\tbins[2] = rcounts[2];\n\t\tbins[3] = rcounts[3];\n\t}\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // The following code is incorrect because it does not consider the points that fall in the upper right quadrant\n   // for (int i = 0; i < points.size(); i++) {\n   //    int quadrant = 0;\n   //    if (points[i].x > 0 && points[i].y > 0) {\n   //       quadrant = 0;\n   //    } else if (points[i].x < 0 && points[i].y > 0) {\n   //       quadrant = 1;\n   //    } else if (points[i].x < 0 && points[i].y < 0) {\n   //       quadrant = 2;\n   //    } else if (points[i].x > 0 && points[i].y < 0) {\n   //       quadrant = 3;\n   //    }\n   //    bins[quadrant]++;\n   // }\n   //\n   // MPI_Reduce(bins.data(), bins.data(), 4, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n   //\n   // if (rank == 0) {\n   //    std::cout << \"quadrant 0: \" << bins[0] << std::endl;\n   //    std::cout << \"quadrant 1: \" << bins[1] << std::endl;\n   //    std::cout << \"quadrant 2: \" << bins[2] << std::endl;\n   //    std::cout << \"quadrant 3: \" << bins[3] << std::endl;\n   // }\n\n   int dims[2];\n   int coords[2];\n\n   dims[0] = dims[1] = (int) sqrt(size);\n\n   MPI_Dims_create(size, 2, dims);\n   MPI_Cart_get(MPI_COMM_WORLD, 2, dims, coords, true);\n\n   std::vector<Point> sub_points;\n\n   #pragma omp parallel for\n   for (int i = 0; i < points.size(); i++) {\n      int quadrant = 0;\n      if (points[i].x > 0 && points[i].y > 0) {\n         quadrant = 0;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         quadrant = 1;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         quadrant = 2;\n      } else if (points[i].x > 0 && points[i].y < 0) {\n         quadrant = 3;\n      }\n      if (rank == MPI_Cart_rank(MPI_COMM_WORLD, coords)) {\n         sub_points.push_back(points[i]);\n      }\n   }\n\n   int sub_size;\n   MPI_Comm comm;\n   MPI_Comm_split(MPI_COMM_WORLD, 0, rank, &comm);\n   MPI_Comm_size(comm, &sub_size);\n\n   std::array<size_t, 4> sub_bins = {{0, 0, 0, 0}};\n\n   for (int i = 0; i < sub_points.size(); i++) {\n      if (sub_points[i].x > 0 && sub_points[i].y > 0) {\n         sub_bins[0]++;\n      } else if (sub_points[i].x < 0 && sub_points[i].y > 0) {\n         sub_bins[1]++;\n      } else if (sub_points[i].x < 0 && sub_points[i].y < 0) {\n         sub_bins[2]++;\n      } else if (sub_points[i].x > 0 && sub_points[i].y < 0) {\n         sub_bins[3",
            "// TODO: fill in the gaps\n\n  // Hints:\n  //  - Every rank should have a complete copy of image\n  //  - Every rank should use its own MPI rank to calculate its own result in bins\n  //  - You can use MPI_Gather to get the results from all ranks.\n  //  - The example is to illustrate the idea, you don't need to use `std::array<size_t, 4>`.\n}",
            "int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // TODO: Fill in the code\n\n}",
            "int const rank = omp_get_thread_num();\n    int const size = omp_get_num_threads();\n    // TODO\n    //\n    // Use MPI and OpenMP to count in parallel.\n    //\n    // You will need to split the work between all processes, and then combine the results.\n    //\n    // Assume MPI has already been initialized.\n\n    // create cartesian topology\n    int dims[2] = {1, 1};\n    int periods[2] = {1, 1};\n    int coords[2] = {0, 0};\n    MPI_Dims_create(size, 2, dims);\n\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 0, &comm);\n    MPI_Cart_coords(comm, rank, 2, coords);\n\n    // initalize the bins\n    bins.fill(0);\n\n    // define the quadrants\n    std::array<Point, 4> quadrants = {{\n        {10, 10},\n        {-10, 10},\n        {-10, -10},\n        {10, -10}\n    }};\n\n    // create the work partitioning\n    int dims_max[2];\n    MPI_Cart_get(comm, 2, dims_max, periods, coords);\n\n    int n_quadrants = dims_max[0]*dims_max[1];\n\n    int n_work = points.size() / n_quadrants;\n\n    int start = coords[0]*dims_max[0] + coords[1];\n    int end = (coords[0] + 1)*dims_max[0] + coords[1];\n\n    // parallel for loop\n    for (int i = 0; i < n_work; i++) {\n        int idx = i*n_quadrants + start;\n\n        for (int j = 0; j < n_quadrants; j++) {\n            if (points[idx].x <= quadrants[j].x && points[idx].y <= quadrants[j].y) {\n                bins[j]++;\n                break;\n            }\n        }\n        idx++;\n    }\n\n    // find the sum of each bin\n    int sum[4];\n    MPI_Reduce(bins.data(), sum, 4, MPI_INT, MPI_SUM, 0, comm);\n\n    if (rank == 0) {\n        // store the sum\n        bins = sum;\n    }\n}",
            "size_t num_procs, proc_id;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &proc_id);\n\n    const size_t per_proc = (int)(points.size() / num_procs);\n\n    int x_id, y_id;\n\n    std::array<size_t, 4> x_bins{0, 0, 0, 0};\n    std::array<size_t, 4> y_bins{0, 0, 0, 0};\n\n#pragma omp parallel\n    {\n#pragma omp single\n        for (int i = 0; i < per_proc; i++) {\n            x_id = (points[i].x > 0) * 1 + (points[i].x == 0) * 2;\n            y_id = (points[i].y > 0) * 1 + (points[i].y == 0) * 2;\n\n            x_bins[x_id]++;\n            y_bins[y_id]++;\n        }\n    }\n\n    MPI_Reduce(x_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(y_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int world_rank;\n    MPI_Comm_rank(comm, &world_rank);\n    int world_size;\n    MPI_Comm_size(comm, &world_size);\n\n    const int dims = 2;\n    const int proc_size = world_size;\n    int proc_rank = world_rank;\n    int proc_coords[dims];\n    MPI_Dims_create(proc_size, dims, proc_coords);\n    MPI_Cart_create(comm, dims, proc_coords, false, false, &comm);\n\n    MPI_Comm_rank(comm, &proc_rank);\n    MPI_Comm_size(comm, &proc_size);\n\n    int dims_available;\n    MPI_Cartdim_get(comm, &dims_available);\n\n    int coords[dims];\n    MPI_Cart_coords(comm, proc_rank, dims, coords);\n\n    int neighbours[2 * dims];\n    MPI_Cart_shift(comm, 0, 1, &neighbours[0], &neighbours[1]);\n    MPI_Cart_shift(comm, 1, 1, &neighbours[2], &neighbours[3]);\n\n    // send the values to neighbors\n    MPI_Request req[2 * dims];\n    MPI_Status status[2 * dims];\n    int num_send = 0;\n    std::vector<std::pair<size_t, size_t>> send_buffer;\n    if (coords[0]!= 0) {\n        send_buffer.push_back({0, points.size()});\n        MPI_Isend(&send_buffer[num_send].first, 1, MPI_UNSIGNED_LONG_LONG, neighbours[0], 0, comm, &req[num_send]);\n        MPI_Isend(&send_buffer[num_send].second, 1, MPI_UNSIGNED_LONG_LONG, neighbours[0], 0, comm, &req[num_send + 1]);\n        num_send += 2;\n    }\n    if (coords[1]!= 0) {\n        send_buffer.push_back({0, points.size()});\n        MPI_Isend(&send_buffer[num_send].first, 1, MPI_UNSIGNED_LONG_LONG, neighbours[2], 0, comm, &req[num_send]);\n        MPI_Isend(&send_buffer[num_send].second, 1, MPI_UNSIGNED_LONG_LONG, neighbours[2], 0, comm, &req[num_send + 1]);\n        num_send += 2;\n    }\n    if (coords[0]!= dims_available - 1) {\n        send_buffer.push_back({0, points.size()});\n        MPI_Isend(&send_buffer[num_send].first, 1, MPI_UNSIGNED_LONG_LONG, neighbours[1], 0, comm, &req[num_send]);\n        MPI_Isend(&send_buffer[num_send].second, 1, MPI_UNSIGNED_LONG_LONG, neighbours[1], 0, comm, &req[num_send + 1]);\n        num_send += 2;\n    }\n    if (coords[1]!= dims_available - 1) {\n        send_buffer.push_back({0, points.size()});\n        MPI_Isend(&send_buffer[num_send].first, 1, MPI_UNSIGNED_LONG_LONG, neighbours[3], 0, comm, &req[num_send]);\n        MPI_Isend(&send_buffer[num_send].second, 1, MPI_UNSIGNED_LONG_LONG, neighbours[3], 0, comm, &req[num_send + 1]);\n        num_send += 2;\n    }\n\n    std::vector<std::pair<size_t, size_t>> rec",
            "// TODO: count the number of points in each quadrant\n   // Use MPI and OpenMP\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   const int N = points.size();\n   if (N == 0)\n      return;\n\n   const int n = size;\n   const int m = 2;\n   int dims[2] = {n, m};\n   int periods[2] = {0, 0};\n\n   int coords[2];\n   MPI_Cart_coords(MPI_COMM_WORLD, rank, 2, coords);\n\n   int x = coords[0];\n   int y = coords[1];\n\n   auto quadrant = [&](double x, double y) {\n      if (x >= 0 && y >= 0)\n         return 0;\n      if (x < 0 && y >= 0)\n         return 1;\n      if (x < 0 && y < 0)\n         return 2;\n      if (x >= 0 && y < 0)\n         return 3;\n   };\n\n   std::array<size_t, 4> local_bins;\n   local_bins.fill(0);\n\n   #pragma omp parallel for\n   for (int i = 0; i < N; i++) {\n      int b = quadrant(points[i].x, points[i].y);\n      local_bins[b] += 1;\n   }\n\n   int displs[4];\n   MPI_Allgather(local_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_COMM_WORLD);\n\n   size_t max_val = std::numeric_limits<unsigned long long>::max();\n   size_t min_val = std::numeric_limits<unsigned long long>::min();\n\n   for (auto &b : bins)\n      if (b > max_val) b = max_val;\n      else if (b < min_val) b = min_val;\n\n   return;\n}",
            "auto n = points.size();\n    auto const mpiSize = omp_get_num_procs();\n    auto const mpiRank = omp_get_thread_num();\n    bins.fill(0);\n\n    auto const width = 10;\n    auto const height = 10;\n    auto const mpiWidth = width / mpiSize;\n    auto const mpiHeight = height / mpiSize;\n\n    auto myPoints = std::vector<Point>();\n    myPoints.reserve(n / mpiSize);\n\n    for (auto i = 0; i < n; ++i) {\n        auto const &point = points[i];\n        auto const quadrant = getQuadrant(point, width, height);\n        if (quadrant >= 0 && quadrant < mpiSize) {\n            myPoints.push_back(point);\n        }\n    }\n\n    auto pointsPerRank = std::vector<size_t>(mpiSize);\n    MPI_Allreduce(MPI_IN_PLACE, pointsPerRank.data(), mpiSize, MPI_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    auto localBins = std::array<size_t, 4>();\n    localBins.fill(0);\n    std::sort(myPoints.begin(), myPoints.end());\n    auto const startX = mpiRank * mpiWidth;\n    auto const endX = startX + mpiWidth;\n    auto const startY = mpiRank * mpiHeight;\n    auto const endY = startY + mpiHeight;\n\n    for (auto i = 0; i < myPoints.size(); ++i) {\n        auto const &point = myPoints[i];\n        if (point.x < startX || point.x > endX) continue;\n        if (point.y < startY || point.y > endY) continue;\n        auto const quadrant = getQuadrant(point, width, height);\n        if (quadrant < 0 || quadrant >= mpiSize) continue;\n        ++localBins[quadrant];\n    }\n\n    auto binsOnRank = std::array<size_t, 4>();\n    binsOnRank.fill(0);\n\n    auto const binPerRank = pointsPerRank[mpiRank] / mpiSize;\n    for (auto i = 0; i < mpiSize; ++i) {\n        auto const offset = i * binPerRank;\n        for (auto j = 0; j < binPerRank; ++j) {\n            binsOnRank[j] += localBins[offset + j];\n        }\n    }\n\n    if (mpiRank == 0) {\n        for (auto i = 0; i < binsOnRank.size(); ++i) {\n            bins[i] += binsOnRank[i];\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    auto img = Image(points);\n\n    int xDim = img.getXDim();\n    int yDim = img.getYDim();\n\n    if (xDim % size!= 0) {\n        throw std::runtime_error(\"The image is not divisible by the number of processes\");\n    }\n\n    int xSlice = xDim / size;\n\n    auto bin1 = img.countPointsInQuadrant(xSlice, 0, yDim);\n    auto bin2 = img.countPointsInQuadrant(xSlice, xSlice, yDim);\n    auto bin3 = img.countPointsInQuadrant(xSlice, 2 * xSlice, yDim);\n    auto bin4 = img.countPointsInQuadrant(xSlice, 3 * xSlice, yDim);\n\n    bins[0] = bin1;\n    bins[1] = bin2;\n    bins[2] = bin3;\n    bins[3] = bin4;\n\n    if (rank == 0) {\n        std::vector<size_t> partialBins(bins.size());\n\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&partialBins[0], 4, MPI_UNSIGNED_LONG_LONG, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 4; j++) {\n                bins[j] += partialBins[j];\n            }\n        }\n\n    } else {\n        MPI_Send(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, 1, MPI_COMM_WORLD);\n    }\n\n}",
            "// your code here\n   int rank, num_procs;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n   int n_per_proc = (int)points.size() / num_procs;\n   int n_remainder = (int)points.size() - n_per_proc * num_procs;\n\n   int n_start = rank * n_per_proc + (rank < n_remainder? rank : n_remainder);\n   int n_end = n_start + n_per_proc + (rank < n_remainder? 1 : 0);\n\n   #pragma omp parallel\n   {\n      int tid = omp_get_thread_num();\n      int chunk = (n_end - n_start) / omp_get_num_threads();\n      int begin = n_start + tid * chunk;\n      int end = begin + chunk;\n      if (tid == omp_get_num_threads() - 1) end = n_end;\n\n      bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n      for (int i = begin; i < end; i++) {\n         if (points[i].x > 0 && points[i].y > 0) {\n            bins[0]++;\n         }\n         else if (points[i].x < 0 && points[i].y > 0) {\n            bins[1]++;\n         }\n         else if (points[i].x < 0 && points[i].y < 0) {\n            bins[2]++;\n         }\n         else {\n            bins[3]++;\n         }\n      }\n   }\n\n   if (rank == 0) {\n      int bin_sum = 0;\n      for (int i = 0; i < 4; i++) {\n         bin_sum += bins[i];\n      }\n      if (bin_sum!= (int)points.size()) {\n         std::cout << \"Error! The number of points does not match the sum of bins!\" << std::endl;\n         std::cout << \"Number of points: \" << points.size() << std::endl;\n         std::cout << \"Sum of bins: \" << bin_sum << std::endl;\n      }\n      for (int i = 0; i < 4; i++) {\n         std::cout << \"Quadrant \" << i << \": \" << bins[i] << std::endl;\n      }\n   }\n}",
            "}",
            "// TODO: replace this with your own code\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Get global size of the input vector.\n    long long npoints = points.size();\n    long long chunk = npoints / size;\n    long long extra = npoints % size;\n\n    // Calculate starting and ending index for this rank\n    long long start = chunk * rank;\n    long long end = chunk * rank + chunk;\n    if (rank < extra) {\n        end += 1;\n    }\n    end += start;\n\n    // Count the points in each quadrant for this rank.\n    int npoints_per_quadrant[4] = {0, 0, 0, 0};\n\n#pragma omp parallel\n{\n    // Loop over points in range and count the points in each quadrant.\n#pragma omp for\n    for (long long i = start; i < end; i++) {\n        double x = points[i].x;\n        double y = points[i].y;\n        if (x > 0 && y > 0) {\n            npoints_per_quadrant[0]++;\n        }\n        else if (x < 0 && y > 0) {\n            npoints_per_quadrant[1]++;\n        }\n        else if (x < 0 && y < 0) {\n            npoints_per_quadrant[2]++;\n        }\n        else {\n            npoints_per_quadrant[3]++;\n        }\n    }\n}\n\n    // Reduce the counts for each quadrant.\n    // Initialize the counts on rank 0 to zero.\n    MPI_Reduce(npoints_per_quadrant, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size, dim;\n    int ierr = MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int ierr = MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int ierr = MPI_Dims_create(size, 2, &dim);\n\n    // Create a 2D Cartesian communicator.\n    MPI_Comm comm2d;\n    MPI_Cart_create(MPI_COMM_WORLD, 2, &dim, NULL, true, &comm2d);\n\n    // Calculate the total number of points in each quadrant.\n    int dims[2];\n    MPI_Cartdim_get(comm2d, &dims[0]);\n    MPI_Cartdim_get(comm2d, &dims[1]);\n\n    std::vector<Point> local_points;\n    if (rank == 0)\n    {\n        local_points = points;\n    }\n\n    MPI_Scatter(points.data(), points.size(), MPI_BYTE, local_points.data(), local_points.size(), MPI_BYTE, 0, comm2d);\n\n    // Count the number of points in each quadrant.\n    for (auto &p : local_points)\n    {\n        int x_rank, y_rank;\n        MPI_Cart_coords(comm2d, rank, 2, &x_rank, &y_rank);\n\n        double x = p.x;\n        double y = p.y;\n\n        if (x >= 0.0 && y >= 0.0)\n        {\n            bins[0] += 1;\n        }\n        else if (x < 0.0 && y >= 0.0)\n        {\n            bins[1] += 1;\n        }\n        else if (x < 0.0 && y < 0.0)\n        {\n            bins[2] += 1;\n        }\n        else if (x >= 0.0 && y < 0.0)\n        {\n            bins[3] += 1;\n        }\n    }\n\n    MPI_Reduce(&bins[0], &bins[0], 4, MPI_LONG_LONG_INT, MPI_SUM, 0, comm2d);\n\n    MPI_Finalize();\n}",
            "int num_procs = 1;\n   int rank = 0;\n   MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   double xmax, xmin, ymax, ymin;\n   if (rank == 0) {\n      xmin = points[0].x;\n      xmax = points[0].x;\n      ymin = points[0].y;\n      ymax = points[0].y;\n      for (auto const& p : points) {\n         xmin = std::min(xmin, p.x);\n         xmax = std::max(xmax, p.x);\n         ymin = std::min(ymin, p.y);\n         ymax = std::max(ymax, p.y);\n      }\n      int xdim = num_procs * 2;\n      int ydim = num_procs * 2;\n      MPI_Comm cart_comm;\n      MPI_Dims_create(num_procs, 2, &ydim);\n      MPI_Cart_create(MPI_COMM_WORLD, 2, &ydim, 0, 0, &cart_comm);\n      MPI_Cart_coords(cart_comm, rank, 2, &rank);\n      MPI_Cart_shift(cart_comm, 0, 1, &rank, &rank + 1);\n      MPI_Cart_shift(cart_comm, 1, 1, &rank, &rank + 1);\n      MPI_Cart_shift(cart_comm, 0, -1, &rank, &rank - 1);\n      MPI_Cart_shift(cart_comm, 1, -1, &rank, &rank - 1);\n      MPI_Cart_coords(cart_comm, rank, 2, &rank);\n   }\n\n   MPI_Bcast(&xmin, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   MPI_Bcast(&xmax, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   MPI_Bcast(&ymin, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   MPI_Bcast(&ymax, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n   int x_dim = 2 * num_procs;\n   int y_dim = 2 * num_procs;\n\n   size_t num_points = points.size();\n\n   #pragma omp parallel for\n   for (size_t i = 0; i < num_points; i++) {\n      int x = (int) ((points[i].x - xmin) / (xmax - xmin) * (x_dim - 1));\n      int y = (int) ((points[i].y - ymin) / (ymax - ymin) * (y_dim - 1));\n\n      if (x < 0) {\n         x = 0;\n      }\n\n      if (y < 0) {\n         y = 0;\n      }\n\n      if (x > x_dim - 1) {\n         x = x_dim - 1;\n      }\n\n      if (y > y_dim - 1) {\n         y = y_dim - 1;\n      }\n\n      int quadrant = x + y * x_dim;\n\n      #pragma omp atomic\n      bins[quadrant] += 1;\n   }\n\n   MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size, numThreads;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Query_thread(&numThreads);\n\n  bins.fill(0);\n  const size_t N = points.size();\n\n  if (N == 0) {\n    return;\n  }\n\n  const size_t N_per_thread = N / numThreads;\n  const size_t N_extra = N % numThreads;\n\n  std::vector<Point> local_points(N_per_thread);\n  std::copy(points.begin() + rank * N_per_thread, points.begin() + (rank + 1) * N_per_thread, local_points.begin());\n  if (rank == numThreads - 1) {\n    local_points.insert(local_points.end(), points.begin() + (numThreads - 1) * N_per_thread, points.end());\n  }\n\n  std::array<size_t, 4> local_bins = { 0, 0, 0, 0 };\n\n  #pragma omp parallel\n  {\n    if (rank == 0) {\n      #pragma omp for schedule(static) nowait\n      for (size_t i = 0; i < N; i++) {\n        if (points[i].x >= 0 && points[i].y >= 0) {\n          local_bins[0] += 1;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n          local_bins[1] += 1;\n        } else if (points[i].x >= 0 && points[i].y < 0) {\n          local_bins[2] += 1;\n        } else if (points[i].x < 0 && points[i].y >= 0) {\n          local_bins[3] += 1;\n        }\n      }\n    } else {\n      #pragma omp for schedule(static) nowait\n      for (size_t i = 0; i < local_points.size(); i++) {\n        if (local_points[i].x >= 0 && local_points[i].y >= 0) {\n          local_bins[0] += 1;\n        } else if (local_points[i].x < 0 && local_points[i].y < 0) {\n          local_bins[1] += 1;\n        } else if (local_points[i].x >= 0 && local_points[i].y < 0) {\n          local_bins[2] += 1;\n        } else if (local_points[i].x < 0 && local_points[i].y >= 0) {\n          local_bins[3] += 1;\n        }\n      }\n    }\n  }\n\n  std::array<size_t, 4> global_bins = { 0, 0, 0, 0 };\n\n  MPI_Reduce(&local_bins[0], &global_bins[0], 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    bins = global_bins;\n  }\n}",
            "// TODO: Your code here\n    ///////////////////////////////////////////////////////////////////////////\n    // 1. initialize bins\n    // 2. create cartesian communicator\n    // 3. distribute points among the processors\n    // 4. compute bins locally\n    // 5. gatherm the bins from all the processors and merge them\n    // 6. write the result on bins[0]\n    ///////////////////////////////////////////////////////////////////////////\n    int rank;\n    int size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bins = {};\n\n    // init bins\n    int half_size = size / 2;\n    int reminder = size % 2;\n    if (reminder == 1 && rank == size - 1) {\n        bins = {points.size(), 0, 0, 0};\n        return;\n    }\n    if (rank < half_size) {\n        bins = {0, 0, 0, 0};\n        return;\n    }\n    if (rank == half_size) {\n        bins = {0, 0, 0, 0};\n        return;\n    }\n\n    // create cartesian communicator\n    int dims[2] = {half_size, half_size};\n    int period[2] = {1, 1};\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, period, 1, &comm);\n\n    // distribute points among the processors\n    int nb_points = points.size();\n    int nb_points_per_proc = nb_points / size;\n    int rest = nb_points % size;\n    int rest_proc = rank < rest? 1 : 0;\n    int shift = rank < rest? rank : rank - rest;\n    std::vector<Point> local_points;\n    for (int i = 0; i < nb_points_per_proc + rest_proc; i++) {\n        local_points.push_back(points[i + shift]);\n    }\n\n    // compute bins locally\n    std::array<int, 4> local_bins;\n    for (auto& p : local_points) {\n        double x = p.x, y = p.y;\n        if (x > 0 && y > 0) {\n            local_bins[0]++;\n        }\n        else if (x < 0 && y > 0) {\n            local_bins[1]++;\n        }\n        else if (x < 0 && y < 0) {\n            local_bins[2]++;\n        }\n        else {\n            local_bins[3]++;\n        }\n    }\n\n    // gatherm the bins from all the processors and merge them\n    MPI_Gather(local_bins.data(), 4, MPI_INT, bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            bins[0] += bins[i];\n        }\n    }\n\n    // write the result on bins[0]\n    return;\n}",
            "// TODO\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n  int rank, num_ranks;\n  MPI_Comm_rank(comm, &rank);\n  MPI_Comm_size(comm, &num_ranks);\n\n  int dim = 2;\n\n  int num_procs = num_ranks;\n  int x_dim = 1;\n  int y_dim = 1;\n\n  int reorder = 0;\n  MPI_Dims_create(num_procs, dim, &dims);\n\n  MPI_Cart_create(comm, dim, dims, periods, reorder, &cart_comm);\n  MPI_Comm_rank(cart_comm, &rank);\n  MPI_Comm_size(cart_comm, &num_ranks);\n\n  int coords[dim];\n  int periods[dim];\n  MPI_Cart_get(cart_comm, dim, dims, coords, periods);\n\n  if (rank == 0) {\n    for (size_t i = 0; i < points.size(); ++i) {\n      int x = (int) (points[i].x / 2);\n      int y = (int) (points[i].y / 2);\n      if (x >= 0 && y >= 0) {\n        ++bins[0];\n      } else if (x < 0 && y >= 0) {\n        ++bins[1];\n      } else if (x >= 0 && y < 0) {\n        ++bins[2];\n      } else {\n        ++bins[3];\n      }\n    }\n  }\n\n  MPI_Cart_shift(cart_comm, 0, 1, &s_left, &s_right);\n  MPI_Cart_shift(cart_comm, 1, 1, &s_down, &s_up);\n\n  MPI_Request req[4];\n  MPI_Status status[4];\n\n  if (rank == 0) {\n    MPI_Isend(bins.data(), 4, MPI_INT, s_left, 0, cart_comm, &req[0]);\n    MPI_Isend(bins.data(), 4, MPI_INT, s_down, 0, cart_comm, &req[1]);\n    MPI_Recv(bins.data(), 4, MPI_INT, s_left, 0, cart_comm, &status[0]);\n    MPI_Recv(bins.data(), 4, MPI_INT, s_down, 0, cart_comm, &status[1]);\n    MPI_Wait(&req[0], &status[0]);\n    MPI_Wait(&req[1], &status[1]);\n  } else if (rank == s_left) {\n    MPI_Recv(bins.data(), 4, MPI_INT, 0, 0, cart_comm, &status[2]);\n    MPI_Send(bins.data(), 4, MPI_INT, s_up, 0, cart_comm);\n  } else if (rank == s_down) {\n    MPI_Recv(bins.data(), 4, MPI_INT, 0, 0, cart_comm, &status[3]);\n    MPI_Send(bins.data(), 4, MPI_INT, s_left, 0, cart_comm);\n  } else if (rank == s_up) {\n    MPI_Recv(bins.data(), 4, MPI_INT, 0, 0, cart_comm, &status[0]);\n    MPI_Send(bins.data(), 4, MPI_INT, s_right, 0, cart_comm);\n  } else if (rank == s_right) {\n    MPI_Recv(bins.data(), 4, MPI_INT, 0, 0, cart_comm, &status[1]);\n    MPI_Send(bins.data(), 4, MPI_INT, s_down, 0, cart_comm);\n  }\n\n  MPI_Barrier(cart_comm);\n\n  if (rank == 0) {\n    MPI_Reduce(",
            "MPI_Comm comm = MPI_COMM_WORLD;\n\n    // Get the rank and number of processes\n    int rank, numprocs;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &numprocs);\n\n    // Get the number of points in each process\n    int num_points = points.size();\n\n    // Compute a 1D decomposition of the data\n    int nx = num_points / numprocs;\n    int mod = num_points % numprocs;\n    int nx1 = nx + 1;\n    int nx2 = nx + 2;\n    int ny = 4;\n    int ny2 = ny + 2;\n    int nxy = nx1 * ny2;\n\n    // Get a block of data assigned to this process\n    std::vector<Point> block;\n    int ix = rank * nx;\n    int iy = rank * ny;\n    for (int i = ix; i < ix + nx1; ++i) {\n        for (int j = iy; j < iy + ny2; ++j) {\n            block.push_back(points[j * nx + i]);\n        }\n    }\n\n    // Add extra points for last processes\n    if (rank < mod) {\n        int nadd = nx2 - nx;\n        for (int i = nx; i < nx + nadd; ++i) {\n            block.push_back(points[rank * nx + i]);\n        }\n    }\n\n    // Compute the number of points in each quadrant\n    size_t total = 0;\n    for (auto &p : block) {\n        if (p.x >= 0 && p.y >= 0) {\n            ++total;\n        }\n    }\n    for (auto &p : block) {\n        if (p.x < 0 && p.y >= 0) {\n            ++total;\n        }\n    }\n    for (auto &p : block) {\n        if (p.x < 0 && p.y < 0) {\n            ++total;\n        }\n    }\n    for (auto &p : block) {\n        if (p.x >= 0 && p.y < 0) {\n            ++total;\n        }\n    }\n\n    // Sum the quadrant counts\n    MPI_Allreduce(&total, &bins[0], 1, MPI_INT, MPI_SUM, comm);\n\n    // Copy the counts to rank 0\n    if (rank == 0) {\n        for (int i = 1; i < numprocs; ++i) {\n            int tmp = 0;\n            MPI_Recv(&tmp, 1, MPI_INT, i, 0, comm, MPI_STATUS_IGNORE);\n            bins[0] += tmp;\n        }\n    } else {\n        MPI_Send(&bins[0], 1, MPI_INT, 0, 0, comm);\n    }\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int npoints = points.size();\n    int chunk = npoints / size;\n    int rem = npoints % size;\n    int beg = rank * chunk;\n    if (rank < rem) {\n        beg += rank;\n    } else {\n        beg += rem;\n    }\n    int end = beg + chunk;\n    if (rank < rem) {\n        end += 1;\n    }\n\n    std::array<size_t, 4> localBins = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (int i = beg; i < end; ++i) {\n        double x = points[i].x;\n        double y = points[i].y;\n        if (x > 0) {\n            if (y > 0) {\n                ++localBins[0];\n            } else {\n                ++localBins[3];\n            }\n        } else {\n            if (y > 0) {\n                ++localBins[1];\n            } else {\n                ++localBins[2];\n            }\n        }\n    }\n\n    std::array<size_t, 4> globalBins;\n\n    MPI_Allreduce(MPI_IN_PLACE, localBins.data(), localBins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        globalBins = localBins;\n    }\n\n    MPI_Gather(localBins.data(), 4, MPI_UNSIGNED_LONG_LONG, globalBins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = globalBins;\n    }\n}",
            "int rank, nRanks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n\n   // find the range of y and x values\n   double minX = std::numeric_limits<double>::max();\n   double maxX = std::numeric_limits<double>::min();\n   double minY = std::numeric_limits<double>::max();\n   double maxY = std::numeric_limits<double>::min();\n\n   // find the max and min of x and y\n   for (auto const& p : points) {\n      if (p.x > maxX) maxX = p.x;\n      if (p.x < minX) minX = p.x;\n      if (p.y > maxY) maxY = p.y;\n      if (p.y < minY) minY = p.y;\n   }\n\n   // divide up the points based on the quadrants\n   std::vector<Point> localPoints;\n\n   // allocate bins\n   bins.fill(0);\n\n   // find the min and max for each quadrant\n   double maxXq = (maxX + minX) / 2;\n   double maxYq = (maxY + minY) / 2;\n   double minXq = (minX + maxX) / 2;\n   double minYq = (minY + maxY) / 2;\n\n   for (auto const& p : points) {\n      if (p.x >= minXq && p.x <= maxXq) {\n         if (p.y >= minYq && p.y <= maxYq) {\n            localPoints.push_back(p);\n         }\n      }\n   }\n\n   // assign the points to the respective bins\n#pragma omp parallel\n   {\n      int threadNum = omp_get_thread_num();\n      int numThreads = omp_get_num_threads();\n      int chunk = localPoints.size() / numThreads;\n\n      // find the chunk for the thread\n      int chunkStart = chunk * threadNum;\n      int chunkEnd = chunkStart + chunk;\n      if (threadNum == numThreads - 1) {\n         chunkEnd = localPoints.size();\n      }\n\n      for (int i = chunkStart; i < chunkEnd; i++) {\n         if (localPoints[i].x >= minX && localPoints[i].x <= maxX && localPoints[i].y >= minY && localPoints[i].y <= maxY) {\n            if (localPoints[i].x >= minXq && localPoints[i].x <= maxXq && localPoints[i].y >= minYq && localPoints[i].y <= maxYq) {\n               bins[0]++;\n            } else if (localPoints[i].x >= maxXq && localPoints[i].x <= maxX && localPoints[i].y >= minYq && localPoints[i].y <= maxYq) {\n               bins[1]++;\n            } else if (localPoints[i].x >= minXq && localPoints[i].x <= maxXq && localPoints[i].y >= maxYq && localPoints[i].y <= maxY) {\n               bins[2]++;\n            } else {\n               bins[3]++;\n            }\n         }\n      }\n   }\n}",
            "// TODO\n}",
            "// TODO: Your code goes here\n\n}",
            "}",
            "int nproc, procId;\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  MPI_Comm_rank(MPI_COMM_WORLD, &procId);\n  int nbins[4];\n  std::fill_n(nbins, 4, 0);\n  //count quadrants\n  for (Point& p : points) {\n    double qx, qy;\n    if (p.x > 0) {\n      qx = 1;\n    }\n    else {\n      qx = 0;\n    }\n    if (p.y > 0) {\n      qy = 1;\n    }\n    else {\n      qy = 0;\n    }\n    nbins[qx + qy * 2]++;\n  }\n  //add to bins\n  for (int i = 0; i < 4; i++) {\n    MPI_Allreduce(&nbins[i], &bins[i], 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n  }\n  return;\n}",
            "int Nprocs, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &Nprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   std::vector<Point> myPoints;\n\n   if (rank == 0) {\n      myPoints = points;\n   }\n   else {\n      MPI_Status status;\n      MPI_Recv(&myPoints, sizeof(myPoints), MPI_CHAR, rank - 1, rank, MPI_COMM_WORLD, &status);\n   }\n\n   for (int i = 0; i < myPoints.size(); i++) {\n      if (myPoints[i].x >= 0 && myPoints[i].y >= 0) {\n         bins[0]++;\n      }\n      else if (myPoints[i].x <= 0 && myPoints[i].y >= 0) {\n         bins[1]++;\n      }\n      else if (myPoints[i].x <= 0 && myPoints[i].y <= 0) {\n         bins[2]++;\n      }\n      else if (myPoints[i].x >= 0 && myPoints[i].y <= 0) {\n         bins[3]++;\n      }\n   }\n\n   MPI_Status status;\n   MPI_Request request;\n\n   if (rank!= 0) {\n      MPI_Isend(&bins, sizeof(bins), MPI_CHAR, rank - 1, rank, MPI_COMM_WORLD, &request);\n   }\n\n   if (rank!= Nprocs - 1) {\n      MPI_Recv(&bins, sizeof(bins), MPI_CHAR, rank + 1, rank, MPI_COMM_WORLD, &status);\n   }\n\n   MPI_Wait(&request, &status);\n\n   if (rank == 0) {\n      int i = 0;\n      MPI_Status status;\n      MPI_Request request;\n      for (int rank = 1; rank < Nprocs; rank++) {\n         MPI_Isend(&bins[i], sizeof(bins[0]), MPI_CHAR, rank - 1, rank, MPI_COMM_WORLD, &request);\n         MPI_Recv(&bins[i], sizeof(bins[0]), MPI_CHAR, rank + 1, rank, MPI_COMM_WORLD, &status);\n         MPI_Wait(&request, &status);\n         i++;\n      }\n   }\n\n   MPI_Finalize();\n}",
            "// your code here\n}",
            "// TODO\n}",
            "//TODO: Your code here.\n\n    size_t N = omp_get_num_threads();\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size_t Np = points.size();\n    int P = omp_get_num_threads();\n    size_t Nq = Np / P;\n\n    size_t s = Nq * rank;\n    size_t e = s + Nq;\n\n    std::vector<std::array<size_t, 4>> local_bins(P);\n\n    int dims[2] = {1, P};\n    MPI_Cart_sub(MPI_COMM_WORLD, &dims, &MPI_COMM_QUAD);\n\n    for(size_t i = s; i < e; i++) {\n        double x = points[i].x;\n        double y = points[i].y;\n        if(x < 0.0) {\n            if(y < 0.0) {\n                local_bins[rank][0] += 1;\n            }\n            else {\n                local_bins[rank][1] += 1;\n            }\n        }\n        else {\n            if(y < 0.0) {\n                local_bins[rank][2] += 1;\n            }\n            else {\n                local_bins[rank][3] += 1;\n            }\n        }\n    }\n    MPI_Allreduce(MPI_IN_PLACE, local_bins.data(), P, MPI_INT, MPI_SUM, MPI_COMM_QUAD);\n\n    for(int i = 0; i < P; i++) {\n        bins[i] = local_bins[i][rank];\n    }\n}",
            "int nproc, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    omp_set_num_threads(nproc);\n\n    std::array<size_t, 4> local_bins;\n    for (auto& p: local_bins) p = 0;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); i++) {\n        if (points[i].x > 0.0 && points[i].y > 0.0) {\n            local_bins[0]++;\n        }\n        else if (points[i].x > 0.0 && points[i].y < 0.0) {\n            local_bins[1]++;\n        }\n        else if (points[i].x < 0.0 && points[i].y > 0.0) {\n            local_bins[2]++;\n        }\n        else {\n            local_bins[3]++;\n        }\n    }\n\n    std::array<size_t, 4> global_bins = local_bins;\n\n    MPI_Allreduce(MPI_IN_PLACE, &global_bins[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n    if (rank == 0) bins = global_bins;\n}",
            "size_t N = points.size();\n    int n_ranks, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    double q_x[4], q_y[4];\n    q_x[0] = -1;\n    q_y[0] = -1;\n    q_x[1] = 1;\n    q_y[1] = -1;\n    q_x[2] = 1;\n    q_y[2] = 1;\n    q_x[3] = -1;\n    q_y[3] = 1;\n\n    double x_min = 100, x_max = -100;\n    double y_min = 100, y_max = -100;\n\n    for (int i = 0; i < N; ++i) {\n        x_min = x_min < points[i].x? x_min : points[i].x;\n        x_max = x_max > points[i].x? x_max : points[i].x;\n        y_min = y_min < points[i].y? y_min : points[i].y;\n        y_max = y_max > points[i].y? y_max : points[i].y;\n    }\n\n    double dx = (x_max - x_min) / 4;\n    double dy = (y_max - y_min) / 4;\n\n    double x_pos[4], y_pos[4];\n\n    x_pos[0] = x_min;\n    x_pos[1] = x_min + dx;\n    x_pos[2] = x_min + dx * 2;\n    x_pos[3] = x_min + dx * 3;\n\n    y_pos[0] = y_min;\n    y_pos[1] = y_min + dy;\n    y_pos[2] = y_min + dy * 2;\n    y_pos[3] = y_min + dy * 3;\n\n    std::vector<Point> local_points;\n\n    for (int i = 0; i < N; ++i) {\n        int flag = 0;\n        for (int j = 0; j < 4; ++j) {\n            if (points[i].x >= x_pos[j] && points[i].x <= x_pos[j + 1] && points[i].y >= y_pos[j] && points[i].y <= y_pos[j + 1]) {\n                flag = 1;\n                local_points.push_back(points[i]);\n            }\n        }\n        if (flag == 0) {\n            local_points.push_back(points[i]);\n        }\n    }\n\n    int n_threads = 1;\n\n    int count = 0;\n    omp_set_num_threads(n_threads);\n\n    int n_points = local_points.size();\n\n#pragma omp parallel for reduction(+:count)\n    for (int i = 0; i < n_points; ++i) {\n        int bin = 0;\n        for (int j = 0; j < 4; ++j) {\n            if (local_points[i].x >= q_x[j] && local_points[i].x <= q_x[j + 1] && local_points[i].y >= q_y[j] && local_points[i].y <= q_y[j + 1]) {\n                bin = j;\n            }\n        }\n        count++;\n        if (bin!= 0) {\n            count--;\n        }\n    }\n\n    if (rank == 0) {\n        bins[0] = count;\n    }\n\n    // Reduce the counts from all ranks\n    MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "MPI_Comm cart_comm;\n    MPI_Dims_create(MPI_COMM_WORLD.Get_size(), 2, bins.data());\n    MPI_Cart_create(MPI_COMM_WORLD, 2, bins.data(), false, nullptr, &cart_comm);\n    int rank, nranks;\n    MPI_Comm_rank(cart_comm, &rank);\n    MPI_Comm_size(cart_comm, &nranks);\n\n    int nx_proc, ny_proc, dims[2];\n    MPI_Cart_get(cart_comm, 2, dims, nullptr, &cart_comm);\n    nx_proc = dims[0];\n    ny_proc = dims[1];\n\n    std::vector<Point> local_points;\n    std::vector<size_t> local_bins(4, 0);\n    for (auto const& p : points) {\n        double x = p.x, y = p.y;\n        double xmin = -2.0, xmax = 2.0, ymin = -2.0, ymax = 2.0;\n        int x_quadrant = 0, y_quadrant = 0;\n        if (x < 0.0)\n            x_quadrant = 1;\n        if (y < 0.0)\n            y_quadrant = 1;\n        if (x_quadrant > 0 && y_quadrant > 0)\n            x_quadrant = 2;\n        if (x_quadrant > 0 && y_quadrant < 0)\n            y_quadrant = 2;\n        if (x_quadrant < 0 && y_quadrant > 0)\n            x_quadrant = 3;\n        if (x_quadrant < 0 && y_quadrant < 0)\n            y_quadrant = 3;\n\n        if (x < xmin + (xmax - xmin) / nx_proc * rank)\n            x_quadrant = 0;\n        if (x > xmin + (xmax - xmin) / nx_proc * (rank + 1))\n            x_quadrant = 0;\n        if (y < ymin + (ymax - ymin) / ny_proc * rank)\n            y_quadrant = 0;\n        if (y > ymin + (ymax - ymin) / ny_proc * (rank + 1))\n            y_quadrant = 0;\n\n        if (x_quadrant > 0 && y_quadrant > 0)\n            local_bins[0]++;\n        if (x_quadrant > 0 && y_quadrant < 0)\n            local_bins[1]++;\n        if (x_quadrant < 0 && y_quadrant > 0)\n            local_bins[2]++;\n        if (x_quadrant < 0 && y_quadrant < 0)\n            local_bins[3]++;\n    }\n    //TODO\n    // count points in each quadrant for each rank\n\n    //TODO\n    // MPI_Gather to count points in each quadrant for each rank\n    //      in bins\n\n    if (rank == 0) {\n        MPI_Gather(local_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, cart_comm);\n    } else {\n        MPI_Gather(local_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, nullptr, 4, MPI_UNSIGNED_LONG_LONG, 0, cart_comm);\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "int rank, num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    MPI_Datatype MPI_POINT;\n    MPI_Type_contiguous(2, MPI_DOUBLE, &MPI_POINT);\n    MPI_Type_commit(&MPI_POINT);\n\n    int root = 0;\n    int left = rank % 2;\n    int right = (rank + 1) % 2;\n\n    std::vector<Point> points_copy = points;\n\n    std::array<size_t, 4> counts;\n    counts.fill(0);\n\n    // MPI_Gatherv doesn't work correctly here because we are gathering more than\n    // the number of elements in the local vector.\n    // So we are using MPI_Allgather instead\n    // MPI_Gatherv(points.data(), points.size(), MPI_POINT,\n    //             counts.data(), counts.data(),\n    //             counts.data(), MPI_POINT, root, MPI_COMM_WORLD);\n\n    MPI_Allgather(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL,\n                  counts.data(), counts.data(), MPI_POINT, MPI_COMM_WORLD);\n\n    // std::array<size_t, 4> points_per_rank = {points.size()/num_ranks, points.size()/num_ranks,\n    //                                          points.size()/num_ranks, points.size()/num_ranks};\n    // MPI_Gatherv(points.data(), points_per_rank[rank], MPI_POINT,\n    //             counts.data(), counts.data(),\n    //             counts.data(), MPI_POINT, root, MPI_COMM_WORLD);\n\n    // int offset = 0;\n    // if (rank!= root) {\n    //     offset = (rank + 1) * points.size() / num_ranks;\n    // }\n\n    // std::array<int, 4> displacements = {offset, offset+points.size()/num_ranks, offset+points.size()/num_ranks*2, offset+points.size()/num_ranks*3};\n\n    // MPI_Gatherv(points.data(), points.size()/num_ranks, MPI_POINT,\n    //             counts.data(), counts.data(),\n    //             displacements.data(), MPI_POINT, root, MPI_COMM_WORLD);\n\n\n    if (rank == root) {\n        std::array<size_t, 4> counts_per_rank = {points.size() / num_ranks, points.size() / num_ranks,\n                                                 points.size() / num_ranks, points.size() / num_ranks};\n        std::array<int, 4> displacements = {points.size() / num_ranks, points.size() / num_ranks * 2,\n                                            points.size() / num_ranks * 3, points.size() / num_ranks * 4};\n        MPI_Gatherv(points.data(), counts_per_rank[rank], MPI_POINT,\n                    bins.data(), counts.data(),\n                    displacements.data(), MPI_POINT, root, MPI_COMM_WORLD);\n    } else {\n        std::array<size_t, 4> counts_per_rank = {points_copy.size() / num_ranks, points_copy.size() / num_ranks,\n                                                 points_copy.size() / num_ranks, points_copy.size() / num_ranks};\n        std::array<int, 4> displacements = {points_copy.size() / num_ranks, points_copy.size() / num_ranks * 2,\n                                            points_copy.size() / num_ranks * 3, points_copy.",
            "// TODO: Fill in this function\n}",
            "assert(bins.size() == 4);\n\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int color = 0;\n    MPI_Comm cart_comm;\n    MPI_Cart_create(MPI_COMM_WORLD, 2, &size, NULL, 1, &color, &cart_comm);\n\n    int dims[2];\n    MPI_Cart_get(cart_comm, 2, dims, NULL, NULL);\n\n    std::vector<Point> my_points(points);\n    std::vector<Point> tmp;\n\n    std::vector<int> counts(4, 0);\n    int global_counts[4] = {0};\n\n    MPI_Allreduce(&counts[0], global_counts, 4, MPI_INT, MPI_SUM, cart_comm);\n\n    for (auto &p: my_points) {\n        if (p.x < 0) {\n            if (p.y > 0) {\n                tmp.push_back(p);\n            } else {\n                p.x *= -1;\n                p.y *= -1;\n                tmp.push_back(p);\n            }\n        } else if (p.y > 0) {\n            tmp.push_back(p);\n        } else {\n            p.x *= -1;\n            p.y *= -1;\n            tmp.push_back(p);\n        }\n    }\n\n    std::vector<Point> all_points;\n    MPI_Allreduce(&tmp[0], &all_points[0], tmp.size(), PointMPIType, MPI_SUM, cart_comm);\n\n    int x_pos, y_pos;\n    MPI_Cart_coords(cart_comm, rank, 2, &x_pos, &y_pos);\n\n    for (auto p: all_points) {\n        if (p.x > 0 && p.y > 0) {\n            if (x_pos > 0 && y_pos > 0) {\n                global_counts[0]++;\n            } else if (x_pos > 0 && y_pos == 0) {\n                global_counts[1]++;\n            } else if (x_pos == 0 && y_pos > 0) {\n                global_counts[2]++;\n            } else {\n                global_counts[3]++;\n            }\n        } else if (p.x > 0 && p.y < 0) {\n            if (x_pos > 0 && y_pos > 0) {\n                global_counts[3]++;\n            } else if (x_pos > 0 && y_pos == 0) {\n                global_counts[2]++;\n            } else if (x_pos == 0 && y_pos > 0) {\n                global_counts[1]++;\n            } else {\n                global_counts[0]++;\n            }\n        } else if (p.x < 0 && p.y > 0) {\n            if (x_pos > 0 && y_pos > 0) {\n                global_counts[2]++;\n            } else if (x_pos > 0 && y_pos == 0) {\n                global_counts[3]++;\n            } else if (x_pos == 0 && y_pos > 0) {\n                global_counts[0]++;\n            } else {\n                global_counts[1]++;\n            }\n        } else if (p.x < 0 && p.y < 0) {\n            if (x_pos > 0 && y_pos > 0) {\n                global_counts[1]++;\n            } else if (x_pos > 0 && y_pos == 0) {\n                global_counts[0]++;\n            } else if (x_pos == 0 && y_pos > 0) {\n                global_counts[3]++;\n            } else {\n                global_counts[2]++;\n            }\n        }\n    }\n    bins",
            "int nranks, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    std::vector<Point> image;\n    if (rank == 0)\n        image = points;\n    MPI_Bcast(&image, image.size(), MPI_BYTE, 0, MPI_COMM_WORLD);\n    int nthreads;\n    #pragma omp parallel\n    #pragma omp single\n    nthreads = omp_get_num_threads();\n    size_t n = image.size();\n    size_t q = n / nranks;\n    size_t r = n % nranks;\n    size_t k = q + (rank < r);\n    std::vector<size_t> counts(4);\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < k; ++i) {\n        if (image[i].x >= 0.0) {\n            if (image[i].y >= 0.0)\n                counts[0]++;\n            else\n                counts[1]++;\n        } else {\n            if (image[i].y >= 0.0)\n                counts[2]++;\n            else\n                counts[3]++;\n        }\n    }\n    if (rank == 0) {\n        for (int i = 1; i < nranks; i++) {\n            MPI_Status status;\n            MPI_Recv(bins.data(), 4, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, &status);\n        }\n        MPI_Send(counts.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Send(counts.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "MPI_Comm comm;\n    MPI_Comm_dup(MPI_COMM_WORLD, &comm);\n    int rank, size;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &size);\n    size_t localSize = points.size();\n    size_t globalSize;\n    MPI_Allreduce(&localSize, &globalSize, 1, MPI_INT, MPI_SUM, comm);\n    if (rank == 0) {\n        bins.fill(0);\n    }\n    // local points\n    std::vector<Point> localPoints;\n    // copy the points to localPoints if I am in the correct rank\n    if (rank == 0) {\n        // localPoints = points;\n        for (size_t i = 0; i < points.size(); i++) {\n            if (rank == 0) {\n                localPoints.push_back(points[i]);\n            }\n        }\n    }\n    // get the quadrant bins for points\n    std::vector<size_t> quadBins(localPoints.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < localPoints.size(); i++) {\n        if (localPoints[i].x >= 0 && localPoints[i].y >= 0) {\n            quadBins[i] = 0;\n        }\n        if (localPoints[i].x >= 0 && localPoints[i].y < 0) {\n            quadBins[i] = 1;\n        }\n        if (localPoints[i].x < 0 && localPoints[i].y >= 0) {\n            quadBins[i] = 2;\n        }\n        if (localPoints[i].x < 0 && localPoints[i].y < 0) {\n            quadBins[i] = 3;\n        }\n    }\n    // bins = quadBins;\n    MPI_Allreduce(quadBins.data(), bins.data(), 4, MPI_LONG_LONG_INT, MPI_SUM, comm);\n    MPI_Finalize();\n}",
            "// TODO: Your code here\n\n\n}",
            "// TODO: your code here\n    return;\n}",
            "// Create a new communicator which divides ranks into 2 ranks per MPI dimension\n\tint np = omp_get_num_threads()*MPI_COMM_WORLD.Get_size();\n\tstd::vector<int> periods(2, false);\n\tint rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm nthreads_comm = MPI_COMM_WORLD;\n\tMPI_Comm_split(MPI_COMM_WORLD, 2*rank, 2*rank, &nthreads_comm);\n\n\t// Determine which rank each point belongs to in the cartesian grid\n\tstd::vector<Point> local_points(points);\n\tstd::vector<int> ranks(points.size());\n\tfor (size_t i = 0; i < local_points.size(); i++) {\n\t\t// Assume MPI rank 0 starts at (0, 0)\n\t\tPoint pt = local_points[i];\n\t\tranks[i] = 4 * rank + (pt.y > 0) * 2 + (pt.x > 0);\n\t\tif (pt.y > 0) {\n\t\t\tlocal_points[i].y = pt.y - 1;\n\t\t}\n\t\tif (pt.x > 0) {\n\t\t\tlocal_points[i].x = pt.x - 1;\n\t\t}\n\t}\n\n\t// Count the number of points in each rank\n\tstd::vector<size_t> counts(nthreads_comm.Get_size(), 0);\n\tstd::vector<int> recvcounts(nthreads_comm.Get_size(), 0);\n\tstd::vector<int> displs(nthreads_comm.Get_size(), 0);\n\tMPI_Allgather(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, ranks.data(), ranks.size(), MPI_INT, nthreads_comm);\n\tMPI_Alltoall(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, counts.data(), 1, MPI_INT, nthreads_comm);\n\n\t// Calculate recvcounts, displs\n\tint disp = 0;\n\tfor (int i = 0; i < nthreads_comm.Get_size(); i++) {\n\t\tdispls[i] = disp;\n\t\tdisp += counts[i];\n\t}\n\trecvcounts = counts;\n\n\t// Count the number of points in each rank\n\tstd::vector<size_t> local_counts(nthreads_comm.Get_size(), 0);\n\tMPI_Alltoallv(local_points.data(), counts.data(), displs.data(), MPI_POINT_TYPE, local_counts.data(), recvcounts.data(), displs.data(), MPI_POINT_TYPE, nthreads_comm);\n\t#pragma omp parallel for num_threads(nthreads_comm.Get_size())\n\tfor (int i = 0; i < nthreads_comm.Get_size(); i++) {\n\t\tbins[i] += local_counts[i];\n\t}\n}",
            "size_t count = points.size();\n    int rank, nproc;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if(rank == 0) std::fill(bins.begin(), bins.end(), 0);\n\n    std::vector<Point> my_points;\n    if(rank == 0) {\n        for(int i = 0; i < nproc; i++)\n            my_points.insert(my_points.end(), points.begin() + i * count / nproc, points.begin() + (i + 1) * count / nproc);\n    } else {\n        my_points.insert(my_points.end(), points.begin() + rank * count / nproc, points.begin() + (rank + 1) * count / nproc);\n    }\n\n    size_t nquadrants = 0;\n    if(rank == 0)\n        for(Point p : my_points) {\n            double x = p.x, y = p.y;\n            if(x > 0)\n                if(y > 0)\n                    nquadrants++;\n                else\n                    nquadrants++;\n            else\n                if(y > 0)\n                    nquadrants++;\n                else\n                    nquadrants++;\n        }\n\n    int bins_per_proc = nquadrants / nproc + (nquadrants % nproc > 0);\n    int bins_per_proc_remainder = nquadrants % nproc;\n\n    //std::cout << \"my_points: \" << rank << \" \" << my_points.size() << std::endl;\n    //std::cout << \"nquadrants: \" << rank << \" \" << nquadrants << std::endl;\n    //std::cout << \"bins_per_proc: \" << rank << \" \" << bins_per_proc << std::endl;\n    //std::cout << \"bins_per_proc_remainder: \" << rank << \" \" << bins_per_proc_remainder << std::endl;\n\n    int bin = 0;\n    if(rank == 0) {\n        for(int i = 0; i < nproc; i++)\n            MPI_Send(&bin, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n    }\n\n#pragma omp parallel for\n    for(int i = 0; i < nproc; i++)\n        for(Point p : my_points) {\n            double x = p.x, y = p.y;\n            if(x > 0)\n                if(y > 0)\n                    bin++;\n                else\n                    bin++;\n            else\n                if(y > 0)\n                    bin++;\n                else\n                    bin++;\n\n            if(bin == bins_per_proc) {\n                bin = 0;\n                MPI_Recv(&bin, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n                bin++;\n            }\n\n            if(i == rank && bin == bins_per_proc + bins_per_proc_remainder)\n                bin = 0;\n        }\n\n    if(rank == 0) {\n        for(int i = 0; i < nproc; i++)\n            MPI_Send(&bin, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n        for(int i = 0; i < nproc; i++)\n            MPI_Recv(&bin, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n        for(int i = 0; i < nproc; i++)\n            MPI_Recv(&bin, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n        for(int i = 0; i < nproc; i++)\n            if(i!= rank)\n                MPI_"
        ]
    }
]