[
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "12_geometry_smallest_triangle",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(Kokkos::View<const Point*> &points) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  Kokkos::parallel_reduce(points.extent(0), KOKKOS_LAMBDA(int i, double &localMin) {\n    for (int j = i + 1; j < points.extent(0); j++) {\n      for (int k = j + 1; k < points.extent(0); k++) {\n        double area = triangleArea(points(i), points(j), points(k));\n        if (area < localMin) {\n          localMin = area;\n        }\n      }\n    }\n  }, Kokkos::Min<double>(smallestArea));\n\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  Kokkos::parallel_reduce(points.extent(0), KOKKOS_LAMBDA(int i, double& localMin) {\n    for (int j = i + 1; j < points.extent(0); j++) {\n      for (int k = j + 1; k < points.extent(0); k++) {\n        double area = triangleArea(points(i), points(j), points(k));\n        if (area < localMin) {\n          localMin = area;\n        }\n      }\n    }\n  }, Kokkos::Min<double>(smallestArea));\n\n  return smallestArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n  Kokkos::parallel_reduce(points.extent(0), KOKKOS_LAMBDA(int i, double& localMinArea) {\n    for (int j = i + 1; j < points.extent(0); j++) {\n      for (int k = j + 1; k < points.extent(0); k++) {\n        double area = triangleArea(points(i), points(j), points(k));\n        if (area < localMinArea) {\n          localMinArea = area;\n        }\n      }\n    }\n  }, Kokkos::Min<double>(minArea));\n  return minArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  Kokkos::parallel_reduce(points.extent(0), KOKKOS_LAMBDA(int i, double& localMin) {\n    for (int j = i + 1; j < points.extent(0); j++) {\n      for (int k = j + 1; k < points.extent(0); k++) {\n        double area = triangleArea(points(i), points(j), points(k));\n        if (area < localMin) {\n          localMin = area;\n        }\n      }\n    }\n  }, Kokkos::Min<double>(smallestArea));\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  Kokkos::parallel_reduce(points.size(), KOKKOS_LAMBDA(int i, double& localMinArea) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points(i), points(j), points(k));\n        if (area < localMinArea) {\n          localMinArea = area;\n        }\n      }\n    }\n  }, Kokkos::Min<double>(smallestArea));\n\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  Kokkos::parallel_for(points.extent(0), [&](const int i) {\n    for (int j = i + 1; j < points.extent(0); j++) {\n      for (int k = j + 1; k < points.extent(0); k++) {\n        double area = triangleArea(points(i), points(j), points(k));\n        if (area < smallestArea) {\n          smallestArea = area;\n        }\n      }\n    }\n  });\n\n  return smallestArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n\n  Kokkos::parallel_reduce(points.extent(0), KOKKOS_LAMBDA(int i, double &localMinArea) {\n    for (int j = i + 1; j < points.extent(0); ++j) {\n      for (int k = j + 1; k < points.extent(0); ++k) {\n        double area = triangleArea(points(i), points(j), points(k));\n        if (area < localMinArea) {\n          localMinArea = area;\n        }\n      }\n    }\n  }, Kokkos::Min<double>(minArea));\n\n  return minArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  Kokkos::parallel_reduce(points.extent(0), KOKKOS_LAMBDA(int i, double &localMin) {\n    for (int j = i + 1; j < points.extent(0); j++) {\n      for (int k = j + 1; k < points.extent(0); k++) {\n        double area = triangleArea(points(i), points(j), points(k));\n        if (area < localMin) {\n          localMin = area;\n        }\n      }\n    }\n  }, Kokkos::Min<double>(smallestArea));\n\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  Kokkos::parallel_reduce(points.extent(0), KOKKOS_LAMBDA(int i, double& localMin) {\n    for (int j = i + 1; j < points.extent(0); j++) {\n      for (int k = j + 1; k < points.extent(0); k++) {\n        double area = triangleArea(points(i), points(j), points(k));\n        if (area < localMin) {\n          localMin = area;\n        }\n      }\n    }\n  }, Kokkos::Min<double>(smallestArea));\n\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  Kokkos::parallel_reduce(points.size(), KOKKOS_LAMBDA(int i, double& localMinArea) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points(i), points(j), points(k));\n        if (area < localMinArea) {\n          localMinArea = area;\n        }\n      }\n    }\n  }, Kokkos::Min<double>(smallestArea));\n\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  Kokkos::parallel_reduce(points.extent(0), KOKKOS_LAMBDA(int i, double& localMin) {\n    for (int j = i + 1; j < points.extent(0); j++) {\n      for (int k = j + 1; k < points.extent(0); k++) {\n        double area = triangleArea(points(i), points(j), points(k));\n        if (area < localMin) {\n          localMin = area;\n        }\n      }\n    }\n  }, Kokkos::Min<double>(smallestArea));\n\n  return smallestArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n\n  Kokkos::parallel_reduce(points.extent(0), KOKKOS_LAMBDA(int i, double& localMinArea) {\n    for (int j = i + 1; j < points.extent(0); j++) {\n      for (int k = j + 1; k < points.extent(0); k++) {\n        double area = triangleArea(points(i), points(j), points(k));\n        if (area < localMinArea) {\n          localMinArea = area;\n        }\n      }\n    }\n  }, Kokkos::Min<double>(minArea));\n\n  return minArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  Kokkos::parallel_reduce(points.size(), KOKKOS_LAMBDA(int i, double& localMinArea) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points(i), points(j), points(k));\n        if (area < localMinArea) {\n          localMinArea = area;\n        }\n      }\n    }\n  }, Kokkos::Min<double>(smallestArea));\n\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  Kokkos::parallel_reduce(points.extent(0), KOKKOS_LAMBDA(int i, double& update) {\n    for (int j = i + 1; j < points.extent(0); ++j) {\n      for (int k = j + 1; k < points.extent(0); ++k) {\n        double area = triangleArea(points(i), points(j), points(k));\n        if (area < update) {\n          update = area;\n        }\n      }\n    }\n  }, Kokkos::Min<double>(smallestArea));\n\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  Kokkos::parallel_reduce(points.size(), KOKKOS_LAMBDA(int i, double& localMin) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points(i), points(j), points(k));\n        if (area < localMin) {\n          localMin = area;\n        }\n      }\n    }\n  }, Kokkos::Min<double>(smallestArea));\n  return smallestArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n\n  Kokkos::parallel_reduce(points.extent(0), KOKKOS_LAMBDA(int i, double& localMinArea) {\n    for (int j = i + 1; j < points.extent(0); j++) {\n      for (int k = j + 1; k < points.extent(0); k++) {\n        double area = triangleArea(points(i), points(j), points(k));\n        if (area < localMinArea) {\n          localMinArea = area;\n        }\n      }\n    }\n  }, Kokkos::Min<double>(minArea));\n\n  return minArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  Kokkos::parallel_reduce(points.size(), KOKKOS_LAMBDA(int i, double& localMinArea) {\n    for (int j = i + 1; j < points.size(); j++) {\n      for (int k = j + 1; k < points.size(); k++) {\n        double area = triangleArea(points(i), points(j), points(k));\n        if (area < localMinArea) {\n          localMinArea = area;\n        }\n      }\n    }\n  }, Kokkos::Min<double>(smallestArea));\n\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  Kokkos::parallel_reduce(points.extent(0), KOKKOS_LAMBDA(int i, double &local_min) {\n    for (int j = i + 1; j < points.extent(0); ++j) {\n      for (int k = j + 1; k < points.extent(0); ++k) {\n        double area = triangleArea(points(i), points(j), points(k));\n        if (area < local_min) {\n          local_min = area;\n        }\n      }\n    }\n  }, Kokkos::Min<double>(smallestArea));\n\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  Kokkos::parallel_reduce(points.extent(0), KOKKOS_LAMBDA(int i, double& localMin) {\n    for (int j = i + 1; j < points.extent(0); j++) {\n      for (int k = j + 1; k < points.extent(0); k++) {\n        double area = triangleArea(points(i), points(j), points(k));\n        if (area < localMin) {\n          localMin = area;\n        }\n      }\n    }\n  }, Kokkos::Min<double>(smallestArea));\n\n  return smallestArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n\n  Kokkos::parallel_reduce(points.extent(0), KOKKOS_LAMBDA(int i, double& localMinArea) {\n    for (int j = i + 1; j < points.extent(0); ++j) {\n      for (int k = j + 1; k < points.extent(0); ++k) {\n        double area = triangleArea(points(i), points(j), points(k));\n        if (area < localMinArea) {\n          localMinArea = area;\n        }\n      }\n    }\n  }, Kokkos::Min<double>(minArea));\n\n  return minArea;\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "12_geometry_smallest_triangle",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  return smallestArea;\n}\n",
      "\n    double smallestArea = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            for (int k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                #pragma omp critical\n                {\n                    if (area < smallestArea) {\n                        smallestArea = area;\n                    }\n                }\n            }\n        }\n    }\n\n    return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  return smallestArea;\n}\n",
      "\n  double smallestArea = DBL_MAX;\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  return smallestArea;\n}\n",
      "\n    double smallestArea = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            for (int k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                #pragma omp critical\n                {\n                    if (area < smallestArea) {\n                        smallestArea = area;\n                    }\n                }\n            }\n        }\n    }\n\n    return smallestArea;\n}\n",
      "\n    double smallestArea = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            for (int k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                #pragma omp critical\n                {\n                    if (area < smallestArea) {\n                        smallestArea = area;\n                    }\n                }\n            }\n        }\n    }\n    return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  return smallestArea;\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "12_geometry_smallest_triangle",
    "parallelism_model": "serial",
    "prompt": "struct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double minArea = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            for (size_t k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n    return minArea;\n}",
      "\n    double minArea = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            for (size_t k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n    return minArea;\n}",
      "\n    double minArea = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            for (size_t k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n    return minArea;\n}",
      "\n    double minArea = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            for (size_t k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n    return minArea;\n}",
      "\n    double minArea = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            for (size_t k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n    return minArea;\n}",
      "\n    double minArea = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            for (size_t k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n    return minArea;\n}",
      "\n    double minArea = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            for (size_t k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n    return minArea;\n}",
      "\n    double minArea = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            for (size_t k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n    return minArea;\n}",
      "\n    double minArea = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            for (size_t k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n    return minArea;\n}",
      "\n    double minArea = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            for (size_t k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n    return minArea;\n}",
      "\n  double minArea = std::numeric_limits<double>::max();\n  for (size_t i = 0; i < points.size(); ++i) {\n    for (size_t j = i + 1; j < points.size(); ++j) {\n      for (size_t k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n  return minArea;\n}\n",
      "\n    double minArea = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            for (size_t k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n    return minArea;\n}",
      "\n\n    double minArea = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            for (size_t k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n    return minArea;\n}\n",
      "\n    double minArea = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            for (size_t k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n    return minArea;\n}",
      "\n    double minArea = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            for (size_t k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n    return minArea;\n}",
      "\n    double minArea = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            for (size_t k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n    return minArea;\n}",
      "\n    double minArea = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            for (size_t k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n    return minArea;\n}",
      "\n    double minArea = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            for (size_t k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n    return minArea;\n}",
      "\n    double minArea = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            for (size_t k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n    return minArea;\n}",
      "\n    double minArea = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            for (size_t k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                if (area < minArea) {\n                    minArea = area;\n                }\n            }\n        }\n    }\n    return minArea;\n}"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "12_geometry_smallest_triangle",
    "parallelism_model": "hip",
    "prompt": "struct Point {\n\tdouble x, y;\n};\n\n__device__ double triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * fabs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Compute the area of the smallest triangle that can be formed by any 3 points.\n   Return the result in area.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\n__global__ void smallestArea(const Point *points, size_t N, double *area) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        area[tid] = minArea;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        area[tid] = minArea;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n\n    double minArea = DBL_MAX;\n    for (int i = 0; i < N; ++i) {\n        for (int j = i + 1; j < N; ++j) {\n            for (int k = j + 1; k < N; ++k) {\n                double currentArea = triangleArea(points[i], points[j], points[k]);\n                if (currentArea < minArea) {\n                    minArea = currentArea;\n                }\n            }\n        }\n    }\n    atomicMin(area, minArea);\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        area[tid] = minArea;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        area[tid] = minArea;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= N) return;\n\n    double minArea = DBL_MAX;\n    for (int i = 0; i < N; ++i) {\n        for (int j = i + 1; j < N; ++j) {\n            for (int k = j + 1; k < N; ++k) {\n                double currentArea = triangleArea(points[i], points[j], points[k]);\n                if (currentArea < minArea) {\n                    minArea = currentArea;\n                }\n            }\n        }\n    }\n    atomicMin(area, minArea);\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        area[tid] = minArea;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        area[tid] = minArea;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        area[tid] = minArea;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        area[tid] = minArea;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        area[tid] = minArea;\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "12_geometry_smallest_triangle",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < smallestArea) {\n          smallestArea = area;\n        }\n      }\n    }\n  }\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < smallestArea) {\n          smallestArea = area;\n        }\n      }\n    }\n  }\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < smallestArea) {\n          smallestArea = area;\n        }\n      }\n    }\n  }\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < smallestArea) {\n          smallestArea = area;\n        }\n      }\n    }\n  }\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < smallestArea) {\n          smallestArea = area;\n        }\n      }\n    }\n  }\n\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < smallestArea) {\n          smallestArea = area;\n        }\n      }\n    }\n  }\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < smallestArea) {\n          smallestArea = area;\n        }\n      }\n    }\n  }\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < smallestArea) {\n          smallestArea = area;\n        }\n      }\n    }\n  }\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < smallestArea) {\n          smallestArea = area;\n        }\n      }\n    }\n  }\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < smallestArea) {\n          smallestArea = area;\n        }\n      }\n    }\n  }\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < smallestArea) {\n          smallestArea = area;\n        }\n      }\n    }\n  }\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < smallestArea) {\n          smallestArea = area;\n        }\n      }\n    }\n  }\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < smallestArea) {\n          smallestArea = area;\n        }\n      }\n    }\n  }\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < smallestArea) {\n          smallestArea = area;\n        }\n      }\n    }\n  }\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < smallestArea) {\n          smallestArea = area;\n        }\n      }\n    }\n  }\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < smallestArea) {\n          smallestArea = area;\n        }\n      }\n    }\n  }\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < smallestArea) {\n          smallestArea = area;\n        }\n      }\n    }\n  }\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < smallestArea) {\n          smallestArea = area;\n        }\n      }\n    }\n  }\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < smallestArea) {\n          smallestArea = area;\n        }\n      }\n    }\n  }\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < smallestArea) {\n          smallestArea = area;\n        }\n      }\n    }\n  }\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  return globalSmallestArea;\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "12_geometry_smallest_triangle",
    "parallelism_model": "cuda",
    "prompt": "struct Point {\n\tdouble x, y;\n};\n\n__device__ double triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * fabs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Compute the area of the smallest triangle that can be formed by any 3 points.\n   Return the result in area.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\n__global__ void smallestArea(const Point *points, size_t N, double *area) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n\n    double minArea = DBL_MAX;\n    for (int i = 0; i < N; ++i) {\n        for (int j = i + 1; j < N; ++j) {\n            for (int k = j + 1; k < N; ++k) {\n                double currentArea = triangleArea(points[i], points[j], points[k]);\n                if (currentArea < minArea) {\n                    minArea = currentArea;\n                }\n            }\n        }\n    }\n    atomicMin(area, minArea);\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            for (int j = i + 1; j < N; j++) {\n                for (int k = j + 1; k < N; k++) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        area[tid] = minArea;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            for (int j = i + 1; j < N; j++) {\n                for (int k = j + 1; k < N; k++) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            for (int j = i + 1; j < N; j++) {\n                for (int k = j + 1; k < N; k++) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        area[tid] = minArea;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minArea = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            for (int j = i + 1; j < N; ++j) {\n                for (int k = j + 1; k < N; ++k) {\n                    double currentArea = triangleArea(points[i], points[j], points[k]);\n                    if (currentArea < minArea) {\n                        minArea = currentArea;\n                    }\n                }\n            }\n        }\n        atomicMin(area, minArea);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "12_geometry_smallest_triangle",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n  return globalSmallestArea;\n}\n",
      "\n    double smallestArea = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            for (int k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                #pragma omp critical\n                {\n                    if (area < smallestArea) {\n                        smallestArea = area;\n                    }\n                }\n            }\n        }\n    }\n\n    double globalSmallestArea;\n    MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n  return globalSmallestArea;\n}\n",
      "\n    double smallestArea = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            for (int k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                #pragma omp critical\n                {\n                    if (area < smallestArea) {\n                        smallestArea = area;\n                    }\n                }\n            }\n        }\n    }\n\n    double globalSmallestArea;\n    MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n  return globalSmallestArea;\n}\n",
      "\n    double smallestArea = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            for (int k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                #pragma omp critical\n                {\n                    if (area < smallestArea) {\n                        smallestArea = area;\n                    }\n                }\n            }\n        }\n    }\n\n    double globalSmallestArea;\n    MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalSmallestArea;\n}\n",
      "\n    double smallestArea = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            for (int k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                #pragma omp critical\n                {\n                    if (area < smallestArea) {\n                        smallestArea = area;\n                    }\n                }\n            }\n        }\n    }\n\n    double globalSmallestArea;\n    MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n  return globalSmallestArea;\n}\n",
      "\n    double smallestArea = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            for (int k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                #pragma omp critical\n                {\n                    if (area < smallestArea) {\n                        smallestArea = area;\n                    }\n                }\n            }\n        }\n    }\n    double globalSmallestArea;\n    MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n  return smallestArea;\n}\n",
      "\n    double smallestArea = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            for (int k = j + 1; k < points.size(); ++k) {\n                double area = triangleArea(points[i], points[j], points[k]);\n                #pragma omp critical\n                {\n                    if (area < smallestArea) {\n                        smallestArea = area;\n                    }\n                }\n            }\n        }\n    }\n    return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n  return smallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n  return globalSmallestArea;\n}\n",
      "\n  double smallestArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < smallestArea) {\n            smallestArea = area;\n          }\n        }\n      }\n    }\n  }\n\n  double globalSmallestArea;\n  MPI_Allreduce(&smallestArea, &globalSmallestArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n  return globalSmallestArea;\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "14_geometry_closest_pair_1d",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\ndouble distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(Kokkos::View<const double*> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMin) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMin) {\n                localMin = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDistance));\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMin) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMin) {\n                localMin = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDistance));\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &minDist) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < minDist) {\n                minDist = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDistance));\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMin) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMin) {\n                localMin = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDistance));\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMin) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMin) {\n                localMin = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDistance));\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMin) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMin) {\n                localMin = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDistance));\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMin) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMin) {\n                localMin = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDistance));\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMin) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMin) {\n                localMin = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDistance));\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMin) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMin) {\n                localMin = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDistance));\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMin) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMin) {\n                localMin = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDistance));\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMin) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMin) {\n                localMin = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDistance));\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMin) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMin) {\n                localMin = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDistance));\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMin) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMin) {\n                localMin = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDistance));\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMin) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMin) {\n                localMin = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDistance));\n    return minDistance;\n}",
      "\ndouble minDistance = std::numeric_limits<double>::max();\nKokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMin) {\n    for (int j = i + 1; j < x.size(); j++) {\n        double dist = distance(x(i), x(j));\n        if (dist < localMin) {\n            localMin = dist;\n        }\n    }\n}, Kokkos::Min<double>(minDistance));\nreturn minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMin) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMin) {\n                localMin = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDistance));\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMin) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMin) {\n                localMin = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDistance));\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMin) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMin) {\n                localMin = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDistance));\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMin) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMin) {\n                localMin = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDistance));\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMin) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMin) {\n                localMin = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDistance));\n    return minDistance;\n}"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "14_geometry_closest_pair_1d",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\ndouble distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Use OpenMP to compute in parallel.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        for (int j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        for (int j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        for (int j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        for (int j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        for (int j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        for (int j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        for (int j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        #pragma omp parallel for\n        for (int j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        for (int j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n    return minDistance;\n}"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "14_geometry_closest_pair_1d",
    "parallelism_model": "serial",
    "prompt": "double distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double currentDistance = distance(x[i], x[j]);\n            if (currentDistance < minDistance) {\n                minDistance = currentDistance;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double currentDistance = distance(x[i], x[j]);\n            if (currentDistance < minDistance) {\n                minDistance = currentDistance;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double currentDistance = distance(x[i], x[j]);\n            if (currentDistance < minDistance) {\n                minDistance = currentDistance;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double currentDistance = distance(x[i], x[j]);\n            if (currentDistance < minDistance) {\n                minDistance = currentDistance;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    std::vector<double> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    \n    double min_distance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < sorted_x.size() - 1; ++i) {\n        double current_distance = distance(sorted_x[i], sorted_x[i + 1]);\n        if (current_distance < min_distance) {\n            min_distance = current_distance;\n        }\n    }\n    \n    return min_distance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    std::vector<double> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    \n    double minDistance = std::numeric_limits<double>::max();\n    for (int i = 0; i < sortedX.size() - 1; ++i) {\n        double currentDistance = distance(sortedX[i], sortedX[i + 1]);\n        if (currentDistance < minDistance) {\n            minDistance = currentDistance;\n        }\n    }\n    \n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    std::vector<double> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    \n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 1; i < sortedX.size(); ++i) {\n        double currentDistance = distance(sortedX[i], sortedX[i - 1]);\n        if (currentDistance < minDistance) {\n            minDistance = currentDistance;\n        }\n    }\n    \n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double currentDistance = distance(x[i], x[j]);\n            if (currentDistance < minDistance) {\n                minDistance = currentDistance;\n            }\n        }\n    }\n    return minDistance;\n}"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "14_geometry_closest_pair_1d",
    "parallelism_model": "hip",
    "prompt": "__device__ double distance(double x1, double x2) {\n\treturn fabs(x1 - x2);\n}\n\n/* Compute the distance between the closest two elements in the vector x.\n   Store the result in distance.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\n__global__ void closestPair(const double *x, size_t N, double *distance) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double minDist = DBL_MAX;\n    \n    for (int i = 0; i < N; i++) {\n        for (int j = i + 1; j < N; j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDist) {\n                minDist = dist;\n            }\n        }\n    }\n    \n    distance[tid] = minDist;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double minDist = DBL_MAX;\n\n    for (int i = 0; i < N; i++) {\n        for (int j = i + 1; j < N; j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDist) {\n                minDist = dist;\n            }\n        }\n    }\n\n    distance[tid] = minDist;\n}"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "14_geometry_closest_pair_1d",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\ndouble distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    std::vector<double> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_DOUBLE, localX.data(), localSize, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; ++i) {\n        for (int j = 0; j < x.size(); ++j) {\n            if (i != j) {\n                double dist = distance(localX[i], x[j]);\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    \n    int localSize = x.size() / size;\n    int start = rank * localSize;\n    int end = (rank == size - 1) ? x.size() : start + localSize;\n    \n    for (int i = start; i < end; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    \n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n    \n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    int start = rank * localSize;\n    int end = (rank == size - 1) ? x.size() : start + localSize;\n\n    for (int i = start; i < end; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    int start = rank * localSize;\n    int end = (rank == size - 1) ? x.size() : start + localSize;\n\n    for (int i = start; i < end; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    int start = rank * localSize;\n    int end = (rank == size - 1) ? x.size() : start + localSize;\n\n    for (int i = start; i < end; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    int start = rank * localSize;\n    int end = (rank == size - 1) ? x.size() : start + localSize;\n\n    for (int i = start; i < end; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    int start = rank * localSize;\n    int end = (rank == size - 1) ? x.size() : start + localSize;\n\n    for (int i = start; i < end; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalMinDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    int start = rank * localSize;\n    int end = (rank == size - 1) ? x.size() : start + localSize;\n\n    for (int i = start; i < end; ++i) {\n        for (int j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    int start = rank * localSize;\n    int end = (rank == size - 1) ? x.size() : start + localSize;\n\n    for (int i = start; i < end; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalMinDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    int start = rank * localSize;\n    int end = (rank == size - 1) ? x.size() : start + localSize;\n\n    for (int i = start; i < end; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalMinDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    \n    int localSize = x.size() / size;\n    int start = rank * localSize;\n    int end = (rank == size - 1) ? x.size() : start + localSize;\n    \n    for (int i = start; i < end; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    \n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n    \n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    int start = rank * localSize;\n    int end = (rank == size - 1) ? x.size() : start + localSize;\n\n    for (int i = start; i < end; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalMinDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    int start = rank * localSize;\n    int end = (rank == size - 1) ? x.size() : start + localSize;\n\n    for (int i = start; i < end; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalMinDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    int start = rank * localSize;\n    int end = (rank == size - 1) ? x.size() : start + localSize;\n\n    for (int i = start; i < end; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalMinDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    int localStart = rank * localSize;\n    int localEnd = (rank == size - 1) ? x.size() : localStart + localSize;\n\n    for (int i = localStart; i < localEnd; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    int start = rank * localSize;\n    int end = (rank == size - 1) ? x.size() : start + localSize;\n\n    for (int i = start; i < end; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    int localStart = rank * localSize;\n    int localEnd = (rank == size - 1) ? x.size() : localStart + localSize;\n\n    for (int i = localStart; i < localEnd; ++i) {\n        for (int j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalMinDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    int start = rank * localSize;\n    int end = (rank == size - 1) ? x.size() : start + localSize;\n\n    for (int i = start; i < end; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalMinDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    int start = rank * localSize;\n    int end = (rank == size - 1) ? x.size() : start + localSize;\n\n    for (int i = start; i < end; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    \n    int localSize = x.size() / size;\n    int start = rank * localSize;\n    int end = (rank == size - 1) ? x.size() : start + localSize;\n    \n    for (int i = start; i < end; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    \n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n    \n    return globalMinDistance;\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "14_geometry_closest_pair_1d",
    "parallelism_model": "cuda",
    "prompt": "__device__ double distance(double x1, double x2) {\n\treturn fabs(x1 - x2);\n}\n\n/* Compute the distance between the closest two elements in the vector x.\n   Store the result in distance.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\n__global__ void closestPair(const double *x, size_t N, double *distance) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        for (int i = tid + 1; i < N; i++) {\n            double dist = distance(x[tid], x[i]);\n            atomicMin(reinterpret_cast<unsigned long long int*>(distance), __double_as_longlong(dist));\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double minDist = DBL_MAX;\n\n    for (int i = 0; i < N; i++) {\n        for (int j = i + 1; j < N; j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDist) {\n                minDist = dist;\n            }\n        }\n    }\n\n    distance[tid] = minDist;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double minDist = DBL_MAX;\n    \n    for (int i = 0; i < N; i++) {\n        for (int j = i + 1; j < N; j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDist) {\n                minDist = dist;\n            }\n        }\n    }\n    \n    distance[tid] = minDist;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        for (int i = tid + 1; i < N; i++) {\n            double dist = distance(x[tid], x[i]);\n            atomicMin(reinterpret_cast<unsigned long long int*>(distance), __double_as_longlong(dist));\n        }\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        for (int i = tid + 1; i < N; i++) {\n            double dist = distance(x[tid], x[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double minDist = DBL_MAX;\n\n    for (int i = 0; i < N; i++) {\n        for (int j = i + 1; j < N; j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDist) {\n                minDist = dist;\n            }\n        }\n    }\n\n    distance[tid] = minDist;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        for (int i = tid + 1; i < N; i++) {\n            double dist = distance(x[tid], x[i]);\n            atomicMin(reinterpret_cast<unsigned long long int*>(distance), __double_as_longlong(dist));\n        }\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        for (int i = tid + 1; i < N; i++) {\n            double dist = distance(x[tid], x[i]);\n            atomicMin(reinterpret_cast<unsigned long long int*>(distance), __double_as_longlong(dist));\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double minDist = DBL_MAX;\n    \n    for (int i = tid; i < N; i += blockDim.x * gridDim.x) {\n        for (int j = i + 1; j < N; j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDist) {\n                minDist = dist;\n            }\n        }\n    }\n    \n    atomicMin(reinterpret_cast<unsigned long long int*>(distance), __double_as_longlong(minDist));\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; ++i) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        for (int i = tid + 1; i < N; i++) {\n            double dist = distance(x[tid], x[i]);\n            atomicMin(reinterpret_cast<unsigned long long int*>(distance), __double_as_longlong(dist));\n        }\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        for (int i = tid + 1; i < N; i++) {\n            double dist = distance(x[tid], x[i]);\n            atomicMin(reinterpret_cast<unsigned long long int*>(distance), __double_as_longlong(dist));\n        }\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    double minDist = DBL_MAX;\n    \n    for (int i = tid; i < N; i += blockDim.x) {\n        for (int j = i + 1; j < N; j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDist) {\n                minDist = dist;\n            }\n        }\n    }\n    \n    atomicMin(distance, minDist);\n}",
      "\n    int tid = threadIdx.x;\n    double minDist = DBL_MAX;\n\n    for (int i = tid; i < N; i += blockDim.x) {\n        for (int j = i + 1; j < N; j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDist) {\n                minDist = dist;\n            }\n        }\n    }\n\n    atomicMin(distance, minDist);\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDist = DBL_MAX;\n        for (int i = 0; i < N; i++) {\n            if (i != tid) {\n                double dist = distance(x[tid], x[i]);\n                if (dist < minDist) {\n                    minDist = dist;\n                }\n            }\n        }\n        distance[tid] = minDist;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = tid + 1; i < N; i++) {\n            double dist = distance(x[tid], x[i]);\n            atomicMin(reinterpret_cast<unsigned long long int*>(distance), __double_as_longlong(dist));\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "14_geometry_closest_pair_1d",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\ndouble distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                #pragma omp critical\n                {\n                    if (dist < minDistance) {\n                        minDistance = dist;\n                    }\n                }\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        for (int j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                #pragma omp critical\n                {\n                    if (dist < minDistance) {\n                        minDistance = dist;\n                    }\n                }\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n    return globalMinDistance;\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    double min_distance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < min_distance) {\n                    min_distance = dist;\n                }\n            }\n        }\n    }\n\n    double global_min_distance;\n    MPI_Allreduce(&min_distance, &global_min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_distance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                #pragma omp critical\n                {\n                    if (dist < minDistance) {\n                        minDistance = dist;\n                    }\n                }\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n    return globalMinDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                #pragma omp critical\n                {\n                    if (dist < minDistance) {\n                        minDistance = dist;\n                    }\n                }\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n    return globalMinDistance;\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    double min_distance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < min_distance) {\n                #pragma omp critical\n                {\n                    if (dist < min_distance) {\n                        min_distance = dist;\n                    }\n                }\n            }\n        }\n    }\n\n    double global_min_distance;\n    MPI_Allreduce(&min_distance, &global_min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_distance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                #pragma omp critical\n                {\n                    if (dist < minDistance) {\n                        minDistance = dist;\n                    }\n                }\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                #pragma omp critical\n                {\n                    if (dist < minDistance) {\n                        minDistance = dist;\n                    }\n                }\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n    return globalMinDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                #pragma omp critical\n                {\n                    if (dist < minDistance) {\n                        minDistance = dist;\n                    }\n                }\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n    return globalMinDistance;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    double min_distance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < min_distance) {\n                    min_distance = dist;\n                }\n            }\n        }\n    }\n\n    double global_min_distance;\n    MPI_Allreduce(&min_distance, &global_min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_distance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        for (int j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                #pragma omp critical\n                {\n                    if (dist < minDistance) {\n                        minDistance = dist;\n                    }\n                }\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    #pragma omp parallel for\n    for (int i = rank; i < x.size(); i += size) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        #pragma omp parallel for\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            if (dist < minDistance) {\n                #pragma omp critical\n                {\n                    if (dist < minDistance) {\n                        minDistance = dist;\n                    }\n                }\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Allreduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n    return globalMinDistance;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    double min_distance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < min_distance) {\n                    min_distance = dist;\n                }\n            }\n        }\n    }\n\n    double global_min_distance;\n    MPI_Allreduce(&min_distance, &global_min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_distance;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    double min_distance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < min_distance) {\n                    min_distance = dist;\n                }\n            }\n        }\n    }\n\n    double global_min_distance;\n    MPI_Allreduce(&min_distance, &global_min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_distance;\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    double min_distance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < min_distance) {\n                    min_distance = dist;\n                }\n            }\n        }\n    }\n\n    double global_min_distance;\n    MPI_Allreduce(&min_distance, &global_min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_distance;\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "13_geometry_closest_pair_2d",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(Kokkos::View<const Point*> &points) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double minDistance = std::numeric_limits<double>::max();\n    const int numPoints = points.extent(0);\n    \n    Kokkos::parallel_for(numPoints, KOKKOS_LAMBDA(int i) {\n        for (int j = i + 1; j < numPoints; j++) {\n            double dist = distance(points(i), points(j));\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    });\n    \n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    const int numPoints = points.extent(0);\n    Kokkos::parallel_for(numPoints, KOKKOS_LAMBDA(const int i) {\n        for (int j = i + 1; j < numPoints; ++j) {\n            double dist = distance(points(i), points(j));\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    });\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        for (int j = i + 1; j < points.extent(0); j++) {\n            double dist = distance(points(i), points(j));\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    });\n\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        for (int j = i + 1; j < points.extent(0); j++) {\n            double dist = distance(points(i), points(j));\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    });\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    const int numPoints = points.extent(0);\n    \n    Kokkos::parallel_for(numPoints, KOKKOS_LAMBDA(int i) {\n        for (int j = i + 1; j < numPoints; j++) {\n            double dist = distance(points(i), points(j));\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    });\n\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        for (int j = i + 1; j < points.extent(0); j++) {\n            double dist = distance(points(i), points(j));\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    });\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        for (int j = i + 1; j < points.extent(0); j++) {\n            double dist = distance(points(i), points(j));\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    });\n\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    const int numPoints = points.extent(0);\n    Kokkos::parallel_for(numPoints, KOKKOS_LAMBDA(const int i) {\n        for (int j = i + 1; j < numPoints; j++) {\n            double dist = distance(points(i), points(j));\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    });\n    Kokkos::fence();\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    const int numPoints = points.extent(0);\n\n    Kokkos::parallel_for(numPoints, KOKKOS_LAMBDA(const int i) {\n        for (int j = i + 1; j < numPoints; ++j) {\n            double dist = distance(points(i), points(j));\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    });\n\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        for (int j = i + 1; j < points.extent(0); j++) {\n            double dist = distance(points(i), points(j));\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    });\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    const int numPoints = points.extent(0);\n    \n    Kokkos::parallel_for(numPoints, KOKKOS_LAMBDA(int i) {\n        for (int j = i + 1; j < numPoints; j++) {\n            double dist = distance(points(i), points(j));\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    });\n    \n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        for (int j = i + 1; j < points.extent(0); j++) {\n            double dist = distance(points(i), points(j));\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    });\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    const int numPoints = points.extent(0);\n    \n    Kokkos::parallel_for(\"closest_pair\", numPoints, KOKKOS_LAMBDA(const int i) {\n        for (int j = i + 1; j < numPoints; ++j) {\n            double dist = distance(points(i), points(j));\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    });\n    \n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    Kokkos::parallel_for(points.size(), KOKKOS_LAMBDA(int i) {\n        for (int j = i + 1; j < points.size(); j++) {\n            double dist = distance(points(i), points(j));\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    });\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    const int numPoints = points.extent(0);\n    \n    Kokkos::parallel_for(\"closest_pair\", numPoints, KOKKOS_LAMBDA(const int i) {\n        for (int j = i + 1; j < numPoints; ++j) {\n            double dist = distance(points(i), points(j));\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    });\n\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    const int numPoints = points.extent(0);\n    \n    Kokkos::parallel_for(\"closest_pair\", numPoints, KOKKOS_LAMBDA(const int i) {\n        for (int j = i + 1; j < numPoints; ++j) {\n            double dist = distance(points(i), points(j));\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    });\n\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    const int numPoints = points.extent(0);\n    Kokkos::parallel_for(numPoints, KOKKOS_LAMBDA(const int i) {\n        for (int j = i + 1; j < numPoints; j++) {\n            double dist = distance(points(i), points(j));\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    });\n    Kokkos::fence();\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    const int numPoints = points.extent(0);\n    Kokkos::parallel_for(\"closest_pair\", numPoints, KOKKOS_LAMBDA(const int i) {\n        for (int j = i + 1; j < numPoints; j++) {\n            double dist = distance(points(i), points(j));\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    });\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    const int numPoints = points.extent(0);\n    Kokkos::parallel_for(numPoints, KOKKOS_LAMBDA(const int i) {\n        for (int j = i + 1; j < numPoints; j++) {\n            double dist = distance(points(i), points(j));\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    });\n    Kokkos::fence();\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    const int numPoints = points.extent(0);\n    \n    Kokkos::parallel_for(numPoints, KOKKOS_LAMBDA(const int i) {\n        for (int j = i + 1; j < numPoints; j++) {\n            double dist = distance(points(i), points(j));\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    });\n    \n    return minDistance;\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "13_geometry_closest_pair_2d",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Use OpenMP to compute in parallel.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n    return minDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    return minDistance;\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "13_geometry_closest_pair_2d",
    "parallelism_model": "serial",
    "prompt": "struct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    return minDistance;\n}"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "13_geometry_closest_pair_2d",
    "parallelism_model": "hip",
    "prompt": "struct Point {\n\tdouble x, y;\n};\n\n__device__ double distance(Point const& p1, Point const& p2) {\n\treturn sqrt(pow(p2.x-p1.x, 2) + pow(p2.y-p1.y, 2));\n}\n\n/* Compute the distance between the closest two points in the vector points.\n   Store the result in distance.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as points.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\n__global__ void closestPair(const Point *points, size_t numPoints, double *distance) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    if (tid >= numPoints) {\n        return;\n    }\n\n    double minDist = DBL_MAX;\n    for (int i = 0; i < numPoints; ++i) {\n        if (i != tid) {\n            double dist = distance(points[tid], points[i]);\n            if (dist < minDist) {\n                minDist = dist;\n            }\n        }\n    }\n\n    distance[tid] = minDist;\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "13_geometry_closest_pair_2d",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The final result is returned on rank 0.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size = points.size();\n    for (int i = 0; i < size; ++i) {\n        for (int j = i + 1; j < size; ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    }\n    return 0.0;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = points.size() / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? points.size() : local_start + local_n;\n\n    double min_distance = std::numeric_limits<double>::max();\n    for (int i = local_start; i < local_end; ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < min_distance) {\n                min_distance = dist;\n            }\n        }\n    }\n\n    double global_min_distance;\n    MPI_Reduce(&min_distance, &global_min_distance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_distance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    }\n    return 0.0;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    return globalMinDistance;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    for (size_t i = 0; i < points.size(); ++i) {\n        for (size_t j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < minDistance) {\n                minDistance = dist;\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    return globalMinDistance;\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "13_geometry_closest_pair_2d",
    "parallelism_model": "cuda",
    "prompt": "struct Point {\n\tdouble x, y;\n};\n\n__device__ double distance(Point const& p1, Point const& p2) {\n\treturn sqrt(pow(p2.x-p1.x, 2) + pow(p2.y-p1.y, 2));\n}\n\n/* Compute the distance between the closest two points in the vector points.\n   Store the result in distance.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as points.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\n__global__ void closestPair(const Point *points, size_t numPoints, double *distance) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; ++i) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < numPoints) {\n        for (int i = tid + 1; i < numPoints; i++) {\n            double dist = distance(points[tid], points[i]);\n            atomicMin(distance, dist);\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "13_geometry_closest_pair_2d",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The final result is returned on rank 0.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = rank; i < points.size(); i += size) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size = points.size();\n\n    #pragma omp parallel for\n    for (int i = 0; i < size; ++i) {\n        for (int j = i + 1; j < size; ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size = points.size();\n    #pragma omp parallel for\n    for (int i = 0; i < size; ++i) {\n        for (int j = i + 1; j < size; ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    }\n    return 0.0;\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = points.size() / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? points.size() : local_start + local_n;\n\n    double min_distance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; ++i) {\n        for (int j = 0; j < points.size(); ++j) {\n            if (i != j) {\n                double dist = distance(points[i], points[j]);\n                if (dist < min_distance) {\n                    #pragma omp critical\n                    {\n                        if (dist < min_distance) {\n                            min_distance = dist;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    double global_min_distance;\n    MPI_Reduce(&min_distance, &global_min_distance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_distance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    int size = points.size();\n\n    #pragma omp parallel for\n    for (int i = 0; i < size; ++i) {\n        for (int j = i + 1; j < size; ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    double minDistance = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < minDistance) {\n                    minDistance = dist;\n                }\n            }\n        }\n    }\n\n    double globalMinDistance;\n    MPI_Reduce(&minDistance, &globalMinDistance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMinDistance;\n    } else {\n        return 0.0;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "10_geometry_convex_hull",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\nvoid convexHull(Kokkos::View<const Point*> &points, Kokkos::View<Point*> &hull) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // Your code goes here\n    // Implement the convex hull algorithm to find the smallest convex polygon that contains all the points\n}",
      "\n    // Your code goes here\n}\n",
      "\n    // Implement the convex hull algorithm here\n}",
      "\n    // Your code here\n    // Implement the convex hull algorithm to find the smallest convex polygon that contains all the points\n    // Store the result in the `hull` view\n}",
      "\n    // Your code goes here\n    // Implement the convex hull algorithm to find the smallest convex polygon that contains all the points\n}",
      "\n    // Your code here\n    // This is where you should implement the algorithm to find the convex hull of the given set of points\n}",
      "\n    // Your code here\n    // Implement the convex hull algorithm to find the smallest convex polygon that contains all the points\n    // Store the result in the `hull` view\n}",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n    // This is where you should write the implementation of the convex hull algorithm using Kokkos for parallel computation\n}",
      "\n    // Your code here\n}\n",
      "\n    // Your code goes here\n}\n",
      "\n    // Your code goes here\n    // Implement the convex hull algorithm to find the smallest convex polygon that contains all the points\n    // Store the result in the `hull` view\n}",
      "\n    // Your code goes here\n    // Implement the convex hull algorithm to find the smallest convex polygon that contains all the points\n}",
      "\n    // Your code goes here\n    // Implement the convex hull algorithm to find the smallest convex polygon that contains all the points\n}",
      "\n    // Your code goes here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code goes here\n    // Implement the convex hull algorithm to find the smallest convex polygon that contains all the points\n}",
      "\n    // Your code here\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "10_geometry_convex_hull",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\nvoid convexHull(std::vector<Point> const& points, std::vector<Point> &hull) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // Your code here\n    // Implement the convex hull algorithm using OpenMP for parallel computation\n    // Store the result in the `hull` vector\n}",
      "\n    // Your code here\n    // Implement the convex hull algorithm using OpenMP for parallel computation\n    // Store the result in the `hull` vector\n}",
      "\n    // Function body\n    // Your code here\n}\n",
      "\n    // Your code here\n    // Implement the convex hull algorithm using OpenMP for parallel computation\n    // Store the result in the `hull` vector\n}",
      "\n    // Your code here\n    // Implement the convex hull algorithm using OpenMP for parallel computation\n}",
      "\n    // Your code here\n    // Implement the convex hull algorithm using OpenMP for parallel computation\n    // Store the result in the `hull` vector\n}",
      "\n    // Your code here\n    // Implement the convex hull algorithm using OpenMP for parallel computation\n}",
      "\n    // Your code here\n    // Implement the convex hull algorithm using OpenMP for parallel computation\n    // Store the result in the `hull` vector\n}",
      "\n    // Your code here\n    // Implement the convex hull algorithm using OpenMP for parallel computation\n    // Store the result in the `hull` vector\n}",
      "\n    // Your code here\n    // Implement the convex hull algorithm using OpenMP for parallel computation\n    // Store the result in the `hull` vector\n}",
      "\n    // Implement the body of the function here\n    // Your code goes here\n}\n",
      "\n    // Your code here\n    // Implement the convex hull algorithm using OpenMP for parallel computation\n    // Store the result in the `hull` vector\n}",
      "\n    // Your code here\n    // Implement the convex hull algorithm using OpenMP for parallel computation\n}",
      "\n    // Your code here\n    // Implement the convex hull algorithm using OpenMP for parallel computation\n    // Store the result in the `hull` vector\n}",
      "\n    // Your code here\n    // Implement the convex hull algorithm using OpenMP for parallel computation\n    // Store the result in the `hull` vector\n}",
      "\n    // Implement the body of the convexHull function here\n    // ...\n}\n",
      "\n    // Your code here\n    // Implement the convex hull algorithm using OpenMP for parallel computation\n    // Store the result in the `hull` vector\n}",
      "\n    // Your code here\n    // Implement the convex hull algorithm using OpenMP for parallel computation\n    // Store the result in the `hull` vector\n}",
      "\n    // Your code goes here\n    // Implement the convex hull algorithm using OpenMP for parallel computation\n    // Store the result in the `hull` vector\n}",
      "\n    // Your code goes here\n    // Implement the convex hull algorithm using OpenMP for parallel computation\n    // Store the result in the `hull` vector\n}"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "10_geometry_convex_hull",
    "parallelism_model": "serial",
    "prompt": "struct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\nvoid convexHull(std::vector<Point> const& points, std::vector<Point> &hull) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int n = points.size();\n    if (n < 3) {\n        hull = points;\n        return;\n    }\n    \n    auto orientation = [](Point p, Point q, Point r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // collinear\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n    \n    auto comparePoints = [](Point p1, Point p2) {\n        if (p1.x == p2.x) {\n            return p1.y < p2.y;\n        }\n        return p1.x < p2.x;\n    };\n    \n    std::sort(points.begin(), points.end(), comparePoints);\n    \n    std::vector<Point> upperHull, lowerHull;\n    \n    for (int i = 0; i < n; ++i) {\n        while (upperHull.size() >= 2 && orientation(upperHull[upperHull.size() - 2], upperHull[upperHull.size() - 1], points[i]) != 2) {\n            upperHull.pop_back();\n        }\n        upperHull.push_back(points[i]);\n    }\n    \n    for (int i = n - 1; i >= 0; --i) {\n        while (lowerHull.size() >= 2 && orientation(lowerHull[lowerHull.size() - 2], lowerHull[lowerHull.size() - 1], points[i]) != 2) {\n            lowerHull.pop_back();\n        }\n        lowerHull.push_back(points[i]);\n    }\n    \n    hull.clear();\n    for (int i = 0; i < upperHull.size(); ++i) {\n        hull.push_back(upperHull[i]);\n    }\n    for (int i = 1; i < lowerHull.size() - 1; ++i) {\n        hull.push_back(lowerHull[i]);\n    }\n}\n",
      "\n    int n = points.size();\n    if (n < 3) {\n        hull = points;\n        return;\n    }\n    \n    auto cmp = [](Point const& a, Point const& b) {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    };\n    std::sort(points.begin(), points.end(), cmp);\n    \n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // colinear\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n    \n    std::vector<Point> upper, lower;\n    for (int i = 0; i < n; ++i) {\n        while (upper.size() >= 2 && orientation(upper[upper.size() - 2], upper[upper.size() - 1], points[i]) != 2) {\n            upper.pop_back();\n        }\n        upper.push_back(points[i]);\n    }\n    \n    for (int i = n - 1; i >= 0; --i) {\n        while (lower.size() >= 2 && orientation(lower[lower.size() - 2], lower[lower.size() - 1], points[i]) != 2) {\n            lower.pop_back();\n        }\n        lower.push_back(points[i]);\n    }\n    \n    hull.clear();\n    for (size_t i = 0; i < upper.size(); ++i) {\n        hull.push_back(upper[i]);\n    }\n    for (size_t i = 1; i < lower.size() - 1; ++i) {\n        hull.push_back(lower[i]);\n    }\n}\n",
      "\n    int n = points.size();\n    if (n < 3) {\n        hull = points;\n        return;\n    }\n    \n    auto cmp = [](Point const& p1, Point const& p2) {\n        return p1.x < p2.x || (p1.x == p2.x && p1.y < p2.y);\n    };\n    \n    std::sort(points.begin(), points.end(), cmp);\n    \n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;\n        return (val > 0) ? 1 : 2;\n    };\n    \n    std::vector<Point> upper, lower;\n    \n    for (int i = 0; i < n; ++i) {\n        while (upper.size() >= 2 && orientation(upper[upper.size() - 2], upper[upper.size() - 1], points[i]) != 2) {\n            upper.pop_back();\n        }\n        upper.push_back(points[i]);\n    }\n    \n    for (int i = n - 1; i >= 0; --i) {\n        while (lower.size() >= 2 && orientation(lower[lower.size() - 2], lower[lower.size() - 1], points[i]) != 2) {\n            lower.pop_back();\n        }\n        lower.push_back(points[i]);\n    }\n    \n    hull.clear();\n    for (size_t i = 0; i < upper.size(); ++i) {\n        hull.push_back(upper[i]);\n    }\n    for (size_t i = 1; i < lower.size() - 1; ++i) {\n        hull.push_back(lower[i]);\n    }\n}\n",
      "\n    int n = points.size();\n    if (n <= 3) {\n        hull = points;\n        return;\n    }\n\n    auto cmp = [](Point const& a, Point const& b) {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    };\n\n    std::sort(points.begin(), points.end(), cmp);\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;\n        return (val > 0) ? 1 : 2;\n    };\n\n    std::vector<Point> upper, lower;\n\n    for (int i = 0; i < n; ++i) {\n        while (upper.size() >= 2 && orientation(upper[upper.size() - 2], upper[upper.size() - 1], points[i]) != 2) {\n            upper.pop_back();\n        }\n        upper.push_back(points[i]);\n    }\n\n    for (int i = n - 1; i >= 0; --i) {\n        while (lower.size() >= 2 && orientation(lower[lower.size() - 2], lower[lower.size() - 1], points[i]) != 2) {\n            lower.pop_back();\n        }\n        lower.push_back(points[i]);\n    }\n\n    hull.clear();\n    for (size_t i = 0; i < upper.size(); ++i) {\n        hull.push_back(upper[i]);\n    }\n    for (size_t i = 1; i < lower.size() - 1; ++i) {\n        hull.push_back(lower[i]);\n    }\n}\n",
      "\n    int n = points.size();\n    if (n <= 3) {\n        hull = points;\n        return;\n    }\n\n    auto cmp = [](Point const& p1, Point const& p2) {\n        return p1.x < p2.x || (p1.x == p2.x && p1.y < p2.y);\n    };\n\n    std::sort(points.begin(), points.end(), cmp);\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // collinear\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n\n    std::vector<Point> upper, lower;\n\n    for (int i = 0; i < n; ++i) {\n        while (upper.size() >= 2 && orientation(upper[upper.size() - 2], upper[upper.size() - 1], points[i]) != 2) {\n            upper.pop_back();\n        }\n        upper.push_back(points[i]);\n    }\n\n    for (int i = n - 1; i >= 0; --i) {\n        while (lower.size() >= 2 && orientation(lower[lower.size() - 2], lower[lower.size() - 1], points[i]) != 2) {\n            lower.pop_back();\n        }\n        lower.push_back(points[i]);\n    }\n\n    hull.clear();\n    for (size_t i = 0; i < upper.size(); ++i) {\n        hull.push_back(upper[i]);\n    }\n    for (size_t i = 1; i < lower.size() - 1; ++i) {\n        hull.push_back(lower[i]);\n    }\n}\n",
      "\n    int n = points.size();\n    if (n < 3) {\n        hull = points;\n        return;\n    }\n\n    auto cmp = [](Point const& a, Point const& b) {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    };\n\n    std::sort(points.begin(), points.end(), cmp);\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // colinear\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n\n    std::vector<Point> upper, lower;\n    for (int i = 0; i < n; ++i) {\n        while (upper.size() >= 2 && orientation(upper[upper.size() - 2], upper[upper.size() - 1], points[i]) != 2) {\n            upper.pop_back();\n        }\n        upper.push_back(points[i]);\n    }\n\n    for (int i = n - 1; i >= 0; --i) {\n        while (lower.size() >= 2 && orientation(lower[lower.size() - 2], lower[lower.size() - 1], points[i]) != 2) {\n            lower.pop_back();\n        }\n        lower.push_back(points[i]);\n    }\n\n    hull.clear();\n    for (size_t i = 0; i < upper.size(); ++i) {\n        hull.push_back(upper[i]);\n    }\n    for (size_t i = 1; i + 1 < lower.size(); ++i) {\n        hull.push_back(lower[i]);\n    }\n}\n",
      "\n    int n = points.size();\n    if (n < 3) {\n        hull = points;\n        return;\n    }\n\n    auto cmp = [](Point const& a, Point const& b) {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    };\n\n    std::sort(points.begin(), points.end(), cmp);\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // collinear\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n\n    std::vector<Point> upper, lower;\n    for (int i = 0; i < n; ++i) {\n        while (upper.size() >= 2 && orientation(upper[upper.size() - 2], upper[upper.size() - 1], points[i]) != 2) {\n            upper.pop_back();\n        }\n        upper.push_back(points[i]);\n    }\n\n    for (int i = n - 1; i >= 0; --i) {\n        while (lower.size() >= 2 && orientation(lower[lower.size() - 2], lower[lower.size() - 1], points[i]) != 2) {\n            lower.pop_back();\n        }\n        lower.push_back(points[i]);\n    }\n\n    hull.clear();\n    for (size_t i = 0; i < upper.size(); ++i) {\n        hull.push_back(upper[i]);\n    }\n    for (size_t i = 1; i < lower.size() - 1; ++i) {\n        hull.push_back(lower[i]);\n    }\n}\n",
      "\n    int n = points.size();\n    if (n < 3) {\n        hull = points;\n        return;\n    }\n\n    auto cmp = [](Point const& a, Point const& b) {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    };\n\n    sort(points.begin(), points.end(), cmp);\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // colinear\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n\n    std::vector<Point> upper, lower;\n\n    for (int i = 0; i < n; ++i) {\n        while (upper.size() >= 2 && orientation(upper[upper.size() - 2], upper[upper.size() - 1], points[i]) != 2) {\n            upper.pop_back();\n        }\n        upper.push_back(points[i]);\n    }\n\n    for (int i = n - 1; i >= 0; --i) {\n        while (lower.size() >= 2 && orientation(lower[lower.size() - 2], lower[lower.size() - 1], points[i]) != 2) {\n            lower.pop_back();\n        }\n        lower.push_back(points[i]);\n    }\n\n    hull.clear();\n    for (int i = 0; i < upper.size(); ++i) {\n        hull.push_back(upper[i]);\n    }\n    for (int i = 1; i < lower.size() - 1; ++i) {\n        hull.push_back(lower[i]);\n    }\n}\n",
      "\n    // Sort the points lexicographically\n    std::vector<Point> sortedPoints = points;\n    std::sort(sortedPoints.begin(), sortedPoints.end(), [](Point const& p1, Point const& p2) {\n        return p1.x < p2.x || (p1.x == p2.x && p1.y < p2.y);\n    });\n\n    // Build the lower hull\n    for (int i = 0; i < sortedPoints.size(); ++i) {\n        while (hull.size() >= 2 && orientation(hull[hull.size() - 2], hull[hull.size() - 1], sortedPoints[i]) <= 0) {\n            hull.pop_back();\n        }\n        hull.push_back(sortedPoints[i]);\n    }\n\n    // Build the upper hull\n    int lowerHullSize = hull.size();\n    for (int i = sortedPoints.size() - 2; i >= 0; --i) {\n        while (hull.size() > lowerHullSize && orientation(hull[hull.size() - 2], hull[hull.size() - 1], sortedPoints[i]) <= 0) {\n            hull.pop_back();\n        }\n        hull.push_back(sortedPoints[i]);\n    }\n\n    // Remove the last point, which is a duplicate of the first point\n    hull.pop_back();\n}\n",
      "\n    int n = points.size();\n    if (n < 3) {\n        hull = points;\n        return;\n    }\n    \n    auto cmp = [](Point const& a, Point const& b) {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    };\n    std::sort(points.begin(), points.end(), cmp);\n    \n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // collinear\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n    \n    std::vector<Point> upper, lower;\n    for (int i = 0; i < n; ++i) {\n        while (upper.size() >= 2 && orientation(upper[upper.size() - 2], upper[upper.size() - 1], points[i]) != 2) {\n            upper.pop_back();\n        }\n        upper.push_back(points[i]);\n    }\n    \n    for (int i = n - 1; i >= 0; --i) {\n        while (lower.size() >= 2 && orientation(lower[lower.size() - 2], lower[lower.size() - 1], points[i]) != 2) {\n            lower.pop_back();\n        }\n        lower.push_back(points[i]);\n    }\n    \n    hull.clear();\n    for (int i = 0; i < upper.size(); ++i) {\n        hull.push_back(upper[i]);\n    }\n    for (int i = 1; i < lower.size() - 1; ++i) {\n        hull.push_back(lower[i]);\n    }\n}\n",
      "\n    int n = points.size();\n    if (n < 3) {\n        hull = points;\n        return;\n    }\n    \n    auto cmp = [](Point const& a, Point const& b) {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    };\n    std::sort(points.begin(), points.end(), cmp);\n    \n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // colinear\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n    \n    std::vector<Point> upper, lower;\n    for (int i = 0; i < n; ++i) {\n        while (upper.size() >= 2 && orientation(upper[upper.size() - 2], upper[upper.size() - 1], points[i]) != 2) {\n            upper.pop_back();\n        }\n        upper.push_back(points[i]);\n    }\n    \n    for (int i = n - 1; i >= 0; --i) {\n        while (lower.size() >= 2 && orientation(lower[lower.size() - 2], lower[lower.size() - 1], points[i]) != 2) {\n            lower.pop_back();\n        }\n        lower.push_back(points[i]);\n    }\n    \n    hull.clear();\n    for (size_t i = 0; i < upper.size(); ++i) {\n        hull.push_back(upper[i]);\n    }\n    for (size_t i = 1; i < lower.size() - 1; ++i) {\n        hull.push_back(lower[i]);\n    }\n}\n",
      "\n    int n = points.size();\n    if (n < 3) {\n        hull = points;\n        return;\n    }\n    \n    auto cmp = [](Point const& a, Point const& b) {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    };\n    sort(points.begin(), points.end(), cmp);\n    \n    auto orientation = [](Point const& a, Point const& b, Point const& c) {\n        return (b.y - a.y) * (c.x - b.x) - (b.x - a.x) * (c.y - b.y);\n    };\n    \n    auto start = 0;\n    for (int i = 0; i < n; ++i) {\n        while (start >= 2 && orientation(hull[start - 2], hull[start - 1], points[i]) <= 0) {\n            hull.pop_back();\n            --start;\n        }\n        hull.push_back(points[i]);\n        ++start;\n    }\n    \n    for (int i = n - 2, t = hull.size() + 1; i >= 0; --i) {\n        while (start >= t && orientation(hull[start - 2], hull[start - 1], points[i]) <= 0) {\n            hull.pop_back();\n            --start;\n        }\n        hull.push_back(points[i]);\n        ++start;\n    }\n    hull.pop_back();\n}\n",
      "\n    int n = points.size();\n    if (n < 3) {\n        hull = points;\n        return;\n    }\n    \n    auto cmp = [](Point const& a, Point const& b) {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    };\n    sort(points.begin(), points.end(), cmp);\n    \n    auto orientation = [](Point const& a, Point const& b, Point const& c) {\n        return (b.y - a.y) * (c.x - b.x) - (b.x - a.x) * (c.y - b.y);\n    };\n    \n    std::vector<Point> upper, lower;\n    for (int i = 0; i < n; ++i) {\n        while (upper.size() >= 2 && orientation(upper[upper.size() - 2], upper.back(), points[i]) <= 0) {\n            upper.pop_back();\n        }\n        upper.push_back(points[i]);\n    }\n    \n    for (int i = n - 1; i >= 0; --i) {\n        while (lower.size() >= 2 && orientation(lower[lower.size() - 2], lower.back(), points[i]) <= 0) {\n            lower.pop_back();\n        }\n        lower.push_back(points[i]);\n    }\n    \n    hull.clear();\n    for (int i = 0; i < upper.size() - 1; ++i) {\n        hull.push_back(upper[i]);\n    }\n    for (int i = 0; i < lower.size() - 1; ++i) {\n        hull.push_back(lower[i]);\n    }\n}\n",
      "\n    int n = points.size();\n    if (n < 3) {\n        hull = points;\n        return;\n    }\n\n    auto cmp = [](Point const& a, Point const& b) {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    };\n\n    std::sort(points.begin(), points.end(), cmp);\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // collinear\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n\n    std::vector<Point> upper, lower;\n\n    for (int i = 0; i < n; ++i) {\n        while (upper.size() >= 2 && orientation(upper[upper.size() - 2], upper[upper.size() - 1], points[i]) != 2) {\n            upper.pop_back();\n        }\n        upper.push_back(points[i]);\n    }\n\n    for (int i = n - 1; i >= 0; --i) {\n        while (lower.size() >= 2 && orientation(lower[lower.size() - 2], lower[lower.size() - 1], points[i]) != 2) {\n            lower.pop_back();\n        }\n        lower.push_back(points[i]);\n    }\n\n    hull.clear();\n    for (size_t i = 0; i < upper.size(); ++i) {\n        hull.push_back(upper[i]);\n    }\n    for (size_t i = 1; i < lower.size() - 1; ++i) {\n        hull.push_back(lower[i]);\n    }\n}\n",
      "\n    int n = points.size();\n    if (n < 3) {\n        hull = points;\n        return;\n    }\n    \n    auto cmp = [](Point const& a, Point const& b) {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    };\n    sort(points.begin(), points.end(), cmp);\n    \n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // collinear\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n    \n    std::vector<Point> upper, lower;\n    for (int i = 0; i < n; ++i) {\n        while (upper.size() >= 2 && orientation(upper[upper.size() - 2], upper[upper.size() - 1], points[i]) != 2) {\n            upper.pop_back();\n        }\n        upper.push_back(points[i]);\n    }\n    \n    for (int i = n - 1; i >= 0; --i) {\n        while (lower.size() >= 2 && orientation(lower[lower.size() - 2], lower[lower.size() - 1], points[i]) != 2) {\n            lower.pop_back();\n        }\n        lower.push_back(points[i]);\n    }\n    \n    hull.clear();\n    for (int i = 0; i < upper.size(); ++i) {\n        hull.push_back(upper[i]);\n    }\n    for (int i = 1; i < lower.size() - 1; ++i) {\n        hull.push_back(lower[i]);\n    }\n}\n",
      "\n    int n = points.size();\n    if (n < 3) {\n        hull = points;\n        return;\n    }\n\n    auto cmp = [](Point const& a, Point const& b) {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    };\n\n    std::sort(points.begin(), points.end(), cmp);\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // collinear\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n\n    std::vector<Point> upper, lower;\n    for (int i = 0; i < n; ++i) {\n        while (upper.size() >= 2 && orientation(upper[upper.size() - 2], upper[upper.size() - 1], points[i]) != 2) {\n            upper.pop_back();\n        }\n        upper.push_back(points[i]);\n    }\n\n    for (int i = n - 1; i >= 0; --i) {\n        while (lower.size() >= 2 && orientation(lower[lower.size() - 2], lower[lower.size() - 1], points[i]) != 2) {\n            lower.pop_back();\n        }\n        lower.push_back(points[i]);\n    }\n\n    hull.clear();\n    for (size_t i = 0; i < upper.size(); ++i) {\n        hull.push_back(upper[i]);\n    }\n    for (size_t i = 1; i < lower.size() - 1; ++i) {\n        hull.push_back(lower[i]);\n    }\n}\n",
      "\n    // Sort the points lexicographically\n    std::vector<Point> sortedPoints = points;\n    std::sort(sortedPoints.begin(), sortedPoints.end(), [](Point const& p1, Point const& p2) {\n        return p1.x < p2.x || (p1.x == p2.x && p1.y < p2.y);\n    });\n\n    // Build the lower hull\n    for (int i = 0; i < sortedPoints.size(); ++i) {\n        while (hull.size() >= 2 && orientation(hull[hull.size() - 2], hull[hull.size() - 1], sortedPoints[i]) <= 0) {\n            hull.pop_back();\n        }\n        hull.push_back(sortedPoints[i]);\n    }\n\n    // Build the upper hull\n    int lowerHullSize = hull.size();\n    for (int i = sortedPoints.size() - 2; i >= 0; --i) {\n        while (hull.size() > lowerHullSize && orientation(hull[hull.size() - 2], hull[hull.size() - 1], sortedPoints[i]) <= 0) {\n            hull.pop_back();\n        }\n        hull.push_back(sortedPoints[i]);\n    }\n\n    // Remove the last point, which is a duplicate of the first point\n    hull.pop_back();\n}\n",
      "\n    int n = points.size();\n    if (n < 3) {\n        hull = points;\n        return;\n    }\n\n    auto cmp = [](Point const& a, Point const& b) {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    };\n\n    sort(points.begin(), points.end(), cmp);\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // collinear\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n\n    std::vector<Point> upper, lower;\n\n    for (int i = 0; i < n; ++i) {\n        while (upper.size() >= 2 && orientation(upper[upper.size() - 2], upper[upper.size() - 1], points[i]) != 2) {\n            upper.pop_back();\n        }\n        upper.push_back(points[i]);\n    }\n\n    for (int i = n - 1; i >= 0; --i) {\n        while (lower.size() >= 2 && orientation(lower[lower.size() - 2], lower[lower.size() - 1], points[i]) != 2) {\n            lower.pop_back();\n        }\n        lower.push_back(points[i]);\n    }\n\n    hull.clear();\n    for (size_t i = 0; i < upper.size(); ++i) {\n        hull.push_back(upper[i]);\n    }\n    for (size_t i = 1; i < lower.size() - 1; ++i) {\n        hull.push_back(lower[i]);\n    }\n}\n",
      "\n    // Sort the points lexicographically\n    std::vector<Point> sortedPoints = points;\n    std::sort(sortedPoints.begin(), sortedPoints.end(), [](const Point &a, const Point &b) {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    });\n\n    // Build lower hull\n    std::vector<Point> lowerHull;\n    for (const Point &p : sortedPoints) {\n        while (lowerHull.size() >= 2 && crossProduct(lowerHull[lowerHull.size() - 2], lowerHull[lowerHull.size() - 1], p) <= 0) {\n            lowerHull.pop_back();\n        }\n        lowerHull.push_back(p);\n    }\n\n    // Build upper hull\n    std::vector<Point> upperHull;\n    for (int i = sortedPoints.size() - 1; i >= 0; --i) {\n        const Point &p = sortedPoints[i];\n        while (upperHull.size() >= 2 && crossProduct(upperHull[upperHull.size() - 2], upperHull[upperHull.size() - 1], p) <= 0) {\n            upperHull.pop_back();\n        }\n        upperHull.push_back(p);\n    }\n\n    // Combine the lower and upper hulls to form the convex hull\n    hull.clear();\n    for (int i = 0; i < lowerHull.size(); ++i) {\n        hull.push_back(lowerHull[i]);\n    }\n    for (int i = 1; i < upperHull.size() - 1; ++i) {\n        hull.push_back(upperHull[i]);\n    }\n}\n",
      "\n    int n = points.size();\n    if (n < 3) {\n        hull = points;\n        return;\n    }\n\n    auto cmp = [](Point const& a, Point const& b) {\n        return a.x < b.x || (a.x == b.x && a.y < b.y);\n    };\n\n    std::sort(points.begin(), points.end(), cmp);\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;\n        return (val > 0) ? 1 : 2;\n    };\n\n    std::vector<Point> upper, lower;\n\n    for (int i = 0; i < n; ++i) {\n        while (upper.size() >= 2 && orientation(upper[upper.size() - 2], upper[upper.size() - 1], points[i]) != 2) {\n            upper.pop_back();\n        }\n        upper.push_back(points[i]);\n    }\n\n    for (int i = n - 1; i >= 0; --i) {\n        while (lower.size() >= 2 && orientation(lower[lower.size() - 2], lower[lower.size() - 1], points[i]) != 2) {\n            lower.pop_back();\n        }\n        lower.push_back(points[i]);\n    }\n\n    hull.clear();\n    for (size_t i = 0; i < upper.size(); ++i) {\n        hull.push_back(upper[i]);\n    }\n    for (size_t i = 1; i < lower.size() - 1; ++i) {\n        hull.push_back(lower[i]);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "10_geometry_convex_hull",
    "parallelism_model": "hip",
    "prompt": "struct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as points.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\n__global__ void convexHull(const Point *points, size_t numPoints, Point *hull, size_t hullSize) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code goes here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code goes here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code goes here\n}\n",
      "\n    // Your code here\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "10_geometry_convex_hull",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of points. The final hull is returned on rank 0.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\nvoid convexHull(std::vector<Point> const& points, std::vector<Point> &hull) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "10_geometry_convex_hull",
    "parallelism_model": "cuda",
    "prompt": "struct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as points.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\n__global__ void convexHull(const Point *points, size_t numPoints, Point *hull, size_t hullSize) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // Your code goes here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code goes here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code goes here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code goes here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code goes here\n}\n",
      "\n    // Your code goes here\n}\n",
      "\n    // Your code goes here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code goes here\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "10_geometry_convex_hull",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of points. The final hull is returned on rank 0.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\nvoid convexHull(std::vector<Point> const& points, std::vector<Point> &hull) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code goes here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n   // Your code here\n}",
      "\n    // Your code here\n}"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "11_geometry_convex_hull_perimeter",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(Kokkos::View<const Point*> &points) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int n = points.extent(0);\n    double perimeter = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        Point p1 = points(i);\n        Point p2 = points((i + 1) % n);\n        perimeter += distance(p1, p2);\n    }\n\n    return perimeter;\n}\n",
      "\n    int n = points.extent(0);\n    double perimeter = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        Point p1 = points(i);\n        Point p2 = points((i + 1) % n);\n        perimeter += distance(p1, p2);\n    }\n\n    return perimeter;\n}\n",
      "\n    int n = points.extent(0);\n    std::vector<int> hull;\n    std::sort(points.data(), points.data() + n, [](const Point &a, const Point &b) {\n        return a.x != b.x ? a.x < b.x : a.y < b.y;\n    });\n\n    for (int rep = 0; rep < 2; rep++) {\n        int s = hull.size();\n        for (int i = 0; i < n; i++) {\n            while (hull.size() >= s + 2 && (hull[hull.size() - 1] - hull[hull.size() - 2]) * (points[i] - points[hull[hull.size() - 2]]) <= 0) {\n                hull.pop_back();\n            }\n            hull.push_back(i);\n        }\n        hull.pop_back();\n        std::reverse(points.data(), points.data() + n);\n    }\n\n    double perimeter = 0;\n    for (int i = 0; i < hull.size(); i++) {\n        perimeter += distance(points[hull[i]], points[hull[(i + 1) % hull.size()]]);\n    }\n    return perimeter;\n}\n",
      "\n    // Sort the points by x-coordinate\n    std::vector<Point> sortedPoints(points.data(), points.data() + points.size());\n    std::sort(sortedPoints.begin(), sortedPoints.end(), [](const Point& p1, const Point& p2) {\n        return p1.x < p2.x || (p1.x == p2.x && p1.y < p2.y);\n    });\n\n    // Compute the upper hull\n    std::vector<Point> upperHull;\n    for (const Point& p : sortedPoints) {\n        while (upperHull.size() >= 2 && \n               (p.x - upperHull[upperHull.size() - 2].x) * (upperHull.back().y - upperHull[upperHull.size() - 2].y) - \n               (p.y - upperHull[upperHull.size() - 2].y) * (upperHull.back().x - upperHull[upperHull.size() - 2].x) <= 0) {\n            upperHull.pop_back();\n        }\n        upperHull.push_back(p);\n    }\n\n    // Compute the lower hull\n    std::vector<Point> lowerHull;\n    for (int i = sortedPoints.size() - 1; i >= 0; --i) {\n        const Point& p = sortedPoints[i];\n        while (lowerHull.size() >= 2 && \n               (p.x - lowerHull[lowerHull.size() - 2].x) * (lowerHull.back().y - lowerHull[lowerHull.size() - 2].y) - \n               (p.y - lowerHull[lowerHull.size() - 2].y) * (lowerHull.back().x - lowerHull[lowerHull.size() - 2].x) <= 0) {\n            lowerHull.pop_back();\n        }\n        lowerHull.push_back(p);\n    }\n\n    // Combine the upper and lower hulls\n    std::vector<Point> convexHull;\n    convexHull.insert(convexHull.end(), upperHull.begin(), upperHull.end() - 1);\n    convexHull.insert(convexHull.end(), lowerHull.begin() + 1, lowerHull.end() - 1);\n\n    // Compute the perimeter of the convex hull\n    double perimeter = 0.0;\n    for (size_t i = 0; i < convexHull.size(); ++i) {\n        perimeter += distance(convexHull[i], convexHull[(i + 1) % convexHull.size()]);\n    }\n\n    return perimeter;\n}\n",
      "\n    int n = points.extent(0);\n    double perimeter = 0.0;\n    for (int i = 0; i < n; i++) {\n        Point p1 = points(i);\n        Point p2 = points((i + 1) % n);\n        perimeter += distance(p1, p2);\n    }\n    return perimeter;\n}\n",
      "\n    int n = points.extent(0);\n    double perimeter = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        Point p1 = points(i);\n        Point p2 = points((i + 1) % n);\n        perimeter += distance(p1, p2);\n    }\n\n    return perimeter;\n}\n",
      "\n\t// Your code here\n\t// Implement the convex hull algorithm to find the perimeter of the smallest convex polygon that contains all the points\n\t// You can use the distance function to calculate the distance between two points\n\t// Return the perimeter of the convex hull\n}\n",
      "\n    int n = points.extent(0);\n    double perimeter = 0.0;\n    for (int i = 0; i < n; i++) {\n        Point p1 = points(i);\n        Point p2 = points((i + 1) % n);\n        perimeter += distance(p1, p2);\n    }\n    return perimeter;\n}\n",
      "\n    int n = points.extent(0);\n    double perimeter = 0.0;\n    for (int i = 0; i < n; i++) {\n        Point p1 = points(i);\n        Point p2 = points((i + 1) % n);\n        perimeter += distance(p1, p2);\n    }\n    return perimeter;\n}\n",
      "\n    int n = points.extent(0);\n    double perimeter = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        Point p1 = points(i);\n        Point p2 = points((i + 1) % n);\n        perimeter += distance(p1, p2);\n    }\n\n    return perimeter;\n}\n",
      "\n    // Sort the points based on their x-coordinates\n    std::vector<Point> sortedPoints(points.data(), points.data() + points.size());\n    std::sort(sortedPoints.begin(), sortedPoints.end(), [](const Point& p1, const Point& p2) {\n        return p1.x < p2.x || (p1.x == p2.x && p1.y < p2.y);\n    });\n\n    // Compute the upper hull\n    std::vector<Point> upperHull;\n    for (const Point& p : sortedPoints) {\n        while (upperHull.size() >= 2 && \n               (p.x - upperHull[upperHull.size() - 2].x) * (upperHull[upperHull.size() - 1].y - upperHull[upperHull.size() - 2].y) - \n               (p.y - upperHull[upperHull.size() - 2].y) * (upperHull[upperHull.size() - 1].x - upperHull[upperHull.size() - 2].x) <= 0) {\n            upperHull.pop_back();\n        }\n        upperHull.push_back(p);\n    }\n\n    // Compute the lower hull\n    std::vector<Point> lowerHull;\n    for (int i = sortedPoints.size() - 1; i >= 0; --i) {\n        const Point& p = sortedPoints[i];\n        while (lowerHull.size() >= 2 && \n               (p.x - lowerHull[lowerHull.size() - 2].x) * (lowerHull[lowerHull.size() - 1].y - lowerHull[lowerHull.size() - 2].y) - \n               (p.y - lowerHull[lowerHull.size() - 2].y) * (lowerHull[lowerHull.size() - 1].x - lowerHull[lowerHull.size() - 2].x) <= 0) {\n            lowerHull.pop_back();\n        }\n        lowerHull.push_back(p);\n    }\n\n    // Combine the upper and lower hulls\n    std::vector<Point> convexHull;\n    convexHull.insert(convexHull.end(), upperHull.begin(), upperHull.end() - 1);\n    convexHull.insert(convexHull.end(), lowerHull.begin(), lowerHull.end() - 1);\n\n    // Compute the perimeter of the convex hull\n    double perimeter = 0.0;\n    for (int i = 0; i < convexHull.size(); ++i) {\n        perimeter += distance(convexHull[i], convexHull[(i + 1) % convexHull.size()]);\n    }\n\n    return perimeter;\n}\n",
      "\n    int n = points.extent(0);\n    double perimeter = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        Point p1 = points(i);\n        Point p2 = points((i + 1) % n);\n        perimeter += distance(p1, p2);\n    }\n\n    return perimeter;\n}\n",
      "\n    double perimeter = 0.0;\n    int n = points.extent(0);\n    for (int i = 0; i < n; i++) {\n        Point p1 = points(i);\n        Point p2 = points((i + 1) % n);\n        perimeter += distance(p1, p2);\n    }\n    return perimeter;\n}\n",
      "\n    int n = points.extent(0);\n    double perimeter = 0.0;\n    for (int i = 0; i < n; ++i) {\n        Point p1 = points(i);\n        Point p2 = points((i + 1) % n);\n        perimeter += distance(p1, p2);\n    }\n    return perimeter;\n}\n",
      "\n    int n = points.extent(0);\n    double perimeter = 0.0;\n\n    for (int i = 0; i < n; ++i) {\n        Point p1 = points(i);\n        Point p2 = points((i + 1) % n);\n        perimeter += distance(p1, p2);\n    }\n\n    return perimeter;\n}\n",
      "\n    int n = points.extent(0);\n    double perimeter = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        Point p1 = points(i);\n        Point p2 = points((i + 1) % n);\n        perimeter += distance(p1, p2);\n    }\n\n    return perimeter;\n}\n",
      "\n    // Your code here\n    // Implement the algorithm to compute the convex hull perimeter using the input points\n    // You can use any suitable algorithm such as Graham's scan or Jarvis march\n    // Remember to use Kokkos to compute in parallel\n    // Return the perimeter of the smallest convex polygon that contains all the points\n}\n",
      "\n    // Your code here\n    // Implement the convex hull algorithm to find the perimeter of the smallest convex polygon that contains all the points\n    // Return the perimeter\n}\n",
      "\n    // Your code here\n    // Implement the algorithm to compute the convex hull of the given points and calculate the perimeter of the convex polygon.\n    // You can use any suitable algorithm, such as Graham's scan or Jarvis march.\n    // Remember to use Kokkos to compute in parallel if possible.\n    // Return the perimeter of the convex hull.\n}\n",
      "\n    // Your code here\n    // Implement the convex hull algorithm to find the perimeter of the smallest convex polygon\n    // that contains all the points in the vector points\n    // Use Kokkos to compute in parallel\n    // Return the perimeter\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "11_geometry_convex_hull_perimeter",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(std::vector<Point> const& points) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double perimeter = 0.0;\n    int n = points.size();\n    #pragma omp parallel for reduction(+:perimeter)\n    for (int i = 0; i < n; i++) {\n        int next = (i + 1) % n;\n        perimeter += distance(points[i], points[next]);\n    }\n    return perimeter;\n}\n",
      "\n    int n = points.size();\n    if (n < 3) {\n        return 0.0;\n    }\n\n    double perimeter = 0.0;\n    #pragma omp parallel for reduction(+:perimeter)\n    for (int i = 0; i < n; ++i) {\n        int next = (i + 1) % n;\n        perimeter += distance(points[i], points[next]);\n    }\n\n    return perimeter;\n}\n",
      "\n    // Your code here\n    // (You can use the given distance function and OpenMP for parallel computation)\n    // Remember to return the computed perimeter\n}\n",
      "\n    // Your code here\n    // (Implement the convex hull algorithm to find the perimeter of the smallest convex polygon containing all the points)\n    // Use OpenMP to compute in parallel\n}",
      "\n    // Your code here\n    // (You can use the provided distance function and OpenMP for parallel computation)\n    // ...\n}\n",
      "\n    int n = points.size();\n    if (n < 3) {\n        return 0.0;\n    }\n\n    double perimeter = 0.0;\n    #pragma omp parallel for reduction(+:perimeter)\n    for (int i = 0; i < n; ++i) {\n        int next = (i + 1) % n;\n        perimeter += distance(points[i], points[next]);\n    }\n\n    return perimeter;\n}\n",
      "\n    int n = points.size();\n    if (n < 3) {\n        return 0.0;\n    }\n\n    double perimeter = 0.0;\n    #pragma omp parallel for reduction(+:perimeter)\n    for (int i = 0; i < n; ++i) {\n        int next = (i + 1) % n;\n        perimeter += distance(points[i], points[next]);\n    }\n\n    return perimeter;\n}\n",
      "\n    // Your code here\n    // (You can use the provided distance function and OpenMP for parallel computation)\n    // Remember to return the computed perimeter\n}\n",
      "\n    // Your code here\n    // (You can use the provided distance function and OpenMP for parallel computation)\n    // Remember to return the computed perimeter\n}\n",
      "\n    // Your code here\n    // Implement the convex hull algorithm to find the perimeter of the smallest convex polygon that contains all the points\n    // Use OpenMP to compute in parallel\n}",
      "\n    double perimeter = 0.0;\n    int n = points.size();\n    #pragma omp parallel for reduction(+:perimeter)\n    for (int i = 0; i < n; i++) {\n        int next = (i + 1) % n;\n        perimeter += distance(points[i], points[next]);\n    }\n    return perimeter;\n}\n",
      "\n    // Your code here\n    // Implement the algorithm to compute the convex hull perimeter using OpenMP for parallel computation\n    // ...\n}\n",
      "\n    // Your code here\n    // (You can use the given distance function and OpenMP for parallel computation)\n    // Remember to return the computed perimeter\n}\n",
      "\n    // Your code here\n    // Implement the convex hull algorithm to find the perimeter of the smallest convex polygon that contains all the points\n    // Use OpenMP to compute in parallel\n    // Return the perimeter\n}\n",
      "\n    int n = points.size();\n    if (n < 3) {\n        return 0.0;\n    }\n\n    double perimeter = 0.0;\n    #pragma omp parallel for reduction(+:perimeter)\n    for (int i = 0; i < n; ++i) {\n        int next = (i + 1) % n;\n        perimeter += distance(points[i], points[next]);\n    }\n\n    return perimeter;\n}\n",
      "\n    // Your code here\n    // Implement the convex hull algorithm to find the perimeter of the smallest convex polygon that contains all the points\n    // Use OpenMP to compute in parallel\n    // Return the perimeter\n}\n",
      "\n    // Your code here\n    // Implement the algorithm to compute the convex hull perimeter using OpenMP for parallel computation\n    // ...\n    // Return the computed perimeter\n}\n",
      "\n    // Your code here\n    // (Implement the convex hull algorithm to find the perimeter of the smallest convex polygon containing all the points)\n    // You can use OpenMP to parallelize the computation\n    // Return the perimeter of the convex hull\n}\n",
      "\n    // Your code here\n    // (You can use the given distance function and OpenMP for parallel computation)\n    // Remember to return the computed perimeter\n}\n",
      "\n    // Your code here\n    // (You can use the provided distance function and the Point struct)\n    // Remember to use OpenMP for parallel computation\n}"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "11_geometry_convex_hull_perimeter",
    "parallelism_model": "serial",
    "prompt": "struct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(std::vector<Point> const& points) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int n = points.size();\n    if (n <= 2) {\n        return 0.0;\n    }\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // collinear\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n\n    auto findNextPoint = [&](int p, std::vector<Point> const& hull) {\n        int q = (p + 1) % n;\n        for (int i = 0; i < n; i++) {\n            if (orientation(points[p], points[i], points[q]) == 2) {\n                q = i;\n            }\n        }\n        return q;\n    };\n\n    std::vector<bool> used(n, false);\n    int p = 0;\n    double perimeter = 0.0;\n    do {\n        used[p] = true;\n        int q = findNextPoint(p, points);\n        perimeter += distance(points[p], points[q]);\n        p = q;\n    } while (p != 0);\n\n    return perimeter;\n}\n",
      "\n    int n = points.size();\n    if (n <= 2) {\n        return 0.0;\n    }\n    \n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // collinear\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n    \n    auto findNextPoint = [&](int p, std::vector<Point> const& hull) {\n        int q = (p + 1) % n;\n        for (int i = 0; i < n; i++) {\n            if (orientation(points[p], points[i], points[q]) == 2) {\n                q = i;\n            }\n        }\n        return q;\n    };\n    \n    std::vector<bool> used(n, false);\n    int p = 0, q;\n    std::vector<Point> hull;\n    do {\n        hull.push_back(points[p]);\n        q = findNextPoint(p, hull);\n        p = q;\n    } while (p != 0);\n    \n    double perimeter = 0.0;\n    for (int i = 0; i < hull.size(); i++) {\n        perimeter += distance(hull[i], hull[(i + 1) % hull.size()]);\n    }\n    \n    return perimeter;\n}\n",
      "\n    int n = points.size();\n    if (n <= 2) {\n        return 0.0;\n    }\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // collinear\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n\n    auto findNextPoint = [&](std::vector<Point> const& hull, Point const& p) {\n        int m = hull.size();\n        int q = (m - 1) % m;\n        while (orientation(hull[q], hull[m % m], p) != 2) {\n            m = q;\n            q = (m - 1) % m;\n        }\n        return m;\n    };\n\n    std::vector<Point> hull;\n    int l = 0;\n    for (int i = 1; i < n; i++) {\n        if (points[i].x < points[l].x) {\n            l = i;\n        }\n    }\n    int p = l, q;\n    do {\n        hull.push_back(points[p]);\n        q = (p + 1) % n;\n        for (int i = 0; i < n; i++) {\n            if (orientation(points[p], points[i], points[q]) == 2) {\n                q = i;\n            }\n        }\n        p = q;\n    } while (p != l);\n\n    double perimeter = 0.0;\n    for (int i = 0; i < hull.size(); i++) {\n        perimeter += distance(hull[i], hull[(i + 1) % hull.size()]);\n    }\n    return perimeter;\n}\n",
      "\n    int n = points.size();\n    if (n <= 2) {\n        return 0;\n    }\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // collinear\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n\n    std::vector<Point> hull;\n\n    int l = 0;\n    for (int i = 1; i < n; i++) {\n        if (points[i].x < points[l].x) {\n            l = i;\n        }\n    }\n\n    int p = l, q;\n    do {\n        hull.push_back(points[p]);\n        q = (p + 1) % n;\n        for (int i = 0; i < n; i++) {\n            if (orientation(points[p], points[i], points[q]) == 2) {\n                q = i;\n            }\n        }\n        p = q;\n    } while (p != l);\n\n    double perimeter = 0;\n    for (int i = 0; i < hull.size(); i++) {\n        perimeter += distance(hull[i], hull[(i + 1) % hull.size()]);\n    }\n\n    return perimeter;\n}\n",
      "\n    int n = points.size();\n    if (n <= 2) {\n        return 0;\n    }\n    \n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) {\n            return 0; // collinear\n        }\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n    \n    auto findNextPoint = [&](int p, std::vector<Point> const& hull) {\n        int q = (p + 1) % n;\n        for (int i = 0; i < n; ++i) {\n            if (orientation(points[p], points[i], points[q]) == 2) {\n                q = i;\n            }\n        }\n        return q;\n    };\n    \n    std::vector<bool> used(n, false);\n    int p = 0;\n    double perimeter = 0;\n    do {\n        used[p] = true;\n        int q = findNextPoint(p, points);\n        perimeter += distance(points[p], points[q]);\n        p = q;\n    } while (p != 0);\n    \n    return perimeter;\n}\n",
      "\n    int n = points.size();\n    if (n <= 2) {\n        return 0.0;\n    }\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // collinear\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n\n    std::vector<Point> hull;\n\n    int l = 0;\n    for (int i = 1; i < n; i++) {\n        if (points[i].x < points[l].x) {\n            l = i;\n        }\n    }\n\n    int p = l, q;\n    do {\n        hull.push_back(points[p]);\n        q = (p + 1) % n;\n        for (int i = 0; i < n; i++) {\n            if (orientation(points[p], points[i], points[q]) == 2) {\n                q = i;\n            }\n        }\n        p = q;\n    } while (p != l);\n\n    double perimeter = 0.0;\n    for (int i = 0; i < hull.size(); i++) {\n        perimeter += distance(hull[i], hull[(i + 1) % hull.size()]);\n    }\n\n    return perimeter;\n}\n",
      "\n    int n = points.size();\n    if (n <= 2) {\n        return 0.0;\n    }\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // collinear\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n\n    std::vector<Point> hull;\n\n    int l = 0;\n    for (int i = 1; i < n; i++) {\n        if (points[i].x < points[l].x) {\n            l = i;\n        }\n    }\n\n    int p = l, q;\n    do {\n        hull.push_back(points[p]);\n        q = (p + 1) % n;\n        for (int i = 0; i < n; i++) {\n            if (orientation(points[p], points[i], points[q]) == 2) {\n                q = i;\n            }\n        }\n        p = q;\n    } while (p != l);\n\n    double perimeter = 0.0;\n    for (int i = 0; i < hull.size(); i++) {\n        perimeter += distance(hull[i], hull[(i + 1) % hull.size()]);\n    }\n\n    return perimeter;\n}\n",
      "\n    int n = points.size();\n    if (n <= 2) {\n        return 0.0;\n    }\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0.0) {\n            return 0;  // collinear\n        }\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n\n    std::vector<Point> hull;\n\n    int l = 0;\n    for (int i = 1; i < n; i++) {\n        if (points[i].x < points[l].x) {\n            l = i;\n        }\n    }\n\n    int p = l, q;\n    do {\n        hull.push_back(points[p]);\n        q = (p + 1) % n;\n        for (int i = 0; i < n; i++) {\n            if (orientation(points[p], points[i], points[q]) == 2) {\n                q = i;\n            }\n        }\n        p = q;\n    } while (p != l);\n\n    double perimeter = 0.0;\n    for (int i = 0; i < hull.size(); i++) {\n        perimeter += distance(hull[i], hull[(i + 1) % hull.size()]);\n    }\n\n    return perimeter;\n}\n",
      "\n    int n = points.size();\n    if (n <= 2) {\n        return 0.0;\n    }\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) {\n            return 0;  // collinear\n        }\n        return (val > 0) ? 1 : 2; // clockwise or counterclockwise\n    };\n\n    auto findNextPoint = [&](std::vector<Point> const& hull, int i) {\n        int next = (i + 1) % n;\n        for (int j = 0; j < n; j++) {\n            if (orientation(hull[i], points[j], hull[next]) == 2) {\n                next = j;\n            }\n        }\n        return next;\n    };\n\n    std::vector<Point> hull;\n    int l = 0;\n    for (int i = 1; i < n; i++) {\n        if (points[i].x < points[l].x) {\n            l = i;\n        }\n    }\n    int p = l, q;\n    do {\n        hull.push_back(points[p]);\n        q = findNextPoint(hull, p);\n        p = q;\n    } while (p != l);\n\n    double perimeter = 0.0;\n    for (int i = 0; i < hull.size(); i++) {\n        perimeter += distance(hull[i], hull[(i + 1) % hull.size()]);\n    }\n    return perimeter;\n}\n",
      "\n    int n = points.size();\n    if (n <= 2) {\n        return 0.0;\n    }\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // collinear\n        return (val > 0) ? 1 : 2; // clockwise or counterclockwise\n    };\n\n    auto findNextPoint = [&](int p, std::vector<Point> const& hull) {\n        int q = (p + 1) % n;\n        for (int r = 0; r < n; r++) {\n            if (orientation(hull[p], points[r], hull[q]) == 2) {\n                q = r;\n            }\n        }\n        return q;\n    };\n\n    std::vector<Point> hull;\n    int l = 0;\n    for (int i = 1; i < n; i++) {\n        if (points[i].x < points[l].x) {\n            l = i;\n        }\n    }\n    int p = l, q;\n    do {\n        hull.push_back(points[p]);\n        q = findNextPoint(p, hull);\n        p = q;\n    } while (p != l);\n\n    double perimeter = 0.0;\n    for (int i = 0; i < hull.size(); i++) {\n        perimeter += distance(hull[i], hull[(i + 1) % hull.size()]);\n    }\n    return perimeter;\n}\n",
      "\n    int n = points.size();\n    if (n <= 2) {\n        return 0;\n    }\n    \n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // collinear\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n    \n    auto findNextPoint = [&](std::vector<Point> const& hull, Point const& p) {\n        int m = hull.size();\n        int q = (p.x == hull[m-1].x && p.y == hull[m-1].y) ? m-2 : m-1;\n        while (q >= 0 && orientation(hull[q], hull[m-1], p) != 2) {\n            --m;\n            q = (p.x == hull[m-1].x && p.y == hull[m-1].y) ? m-2 : m-1;\n        }\n        return m;\n    };\n    \n    std::vector<Point> hull;\n    std::vector<bool> used(n, false);\n    int p = 0;\n    do {\n        hull.push_back(points[p]);\n        used[p] = true;\n        int q = (p + 1) % n;\n        for (int i = 0; i < n; ++i) {\n            if (orientation(points[p], points[i], points[q]) == 2) {\n                q = i;\n            }\n        }\n        p = q;\n    } while (p != 0);\n    \n    double perimeter = 0;\n    for (int i = 0; i < hull.size(); ++i) {\n        perimeter += distance(hull[i], hull[(i+1) % hull.size()]);\n    }\n    return perimeter;\n}\n",
      "\n    int n = points.size();\n    if (n <= 2) {\n        return 0.0;\n    }\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0.0) {\n            return 0;  // collinear\n        }\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n\n    auto findNextPoint = [&](int p, std::vector<Point> const& hull) {\n        int q = (p + 1) % n;\n        for (int i = 0; i < n; ++i) {\n            if (orientation(points[p], points[i], points[q]) == 2) {\n                q = i;\n            }\n        }\n        return q;\n    };\n\n    std::vector<bool> used(n, false);\n    int p = 0;\n    double perimeter = 0.0;\n    do {\n        used[p] = true;\n        int q = findNextPoint(p, points);\n        perimeter += distance(points[p], points[q]);\n        p = q;\n    } while (p != 0);\n\n    return perimeter;\n}\n",
      "\n    int n = points.size();\n    if (n <= 2) {\n        return 0.0;\n    }\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0.0) {\n            return 0;  // collinear\n        }\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n\n    std::vector<Point> hull;\n\n    int l = 0;\n    for (int i = 1; i < n; i++) {\n        if (points[i].x < points[l].x) {\n            l = i;\n        }\n    }\n\n    int p = l, q;\n    do {\n        hull.push_back(points[p]);\n        q = (p + 1) % n;\n        for (int i = 0; i < n; i++) {\n            if (orientation(points[p], points[i], points[q]) == 2) {\n                q = i;\n            }\n        }\n        p = q;\n    } while (p != l);\n\n    double perimeter = 0.0;\n    for (int i = 0; i < hull.size() - 1; i++) {\n        perimeter += distance(hull[i], hull[i + 1]);\n    }\n    perimeter += distance(hull.back(), hull.front());\n\n    return perimeter;\n}\n",
      "\n    int n = points.size();\n    if (n <= 2) {\n        return 0.0;\n    }\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // colinear\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n\n    auto findNextPoint = [&](std::vector<Point> const& hull, Point const& p) {\n        int m = hull.size();\n        int q = (m - 1) % m;\n        while (orientation(hull[q], hull[m % m], p) != 2) {\n            m = q;\n            q = (m - 1 + m) % m;\n        }\n        return m;\n    };\n\n    std::vector<Point> hull;\n    int l = 0;\n    for (int i = 1; i < n; i++) {\n        if (points[i].x < points[l].x) {\n            l = i;\n        }\n    }\n\n    int p = l, q;\n    do {\n        hull.push_back(points[p]);\n        q = (p + 1) % n;\n        for (int i = 0; i < n; i++) {\n            if (orientation(points[p], points[i], points[q]) == 2) {\n                q = i;\n            }\n        }\n        p = q;\n    } while (p != l);\n\n    double perimeter = 0.0;\n    for (int i = 0; i < hull.size(); i++) {\n        perimeter += distance(hull[i], hull[(i + 1) % hull.size()]);\n    }\n\n    return perimeter;\n}\n",
      "\n    int n = points.size();\n    if (n <= 2) {\n        return 0;\n    }\n    \n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) {\n            return 0; // collinear\n        }\n        return (val > 0) ? 1 : 2; // clockwise or counterclockwise\n    };\n    \n    auto findNextPoint = [&](std::vector<Point> const& hull, Point const& p) {\n        int m = hull.size();\n        int q = (m - 1) % m;\n        while (orientation(hull[q], hull[m % m], p) != 2) {\n            m = q;\n            q = (m - 1) % m;\n        }\n        return m;\n    };\n    \n    std::vector<Point> hull;\n    int l = 0;\n    for (int i = 1; i < n; i++) {\n        if (points[i].x < points[l].x) {\n            l = i;\n        }\n    }\n    int p = l, q;\n    do {\n        hull.push_back(points[p]);\n        q = (p + 1) % n;\n        for (int i = 0; i < n; i++) {\n            if (orientation(points[p], points[i], points[q]) == 2) {\n                q = i;\n            }\n        }\n        p = q;\n    } while (p != l);\n    \n    double perimeter = 0;\n    for (int i = 0; i < hull.size(); i++) {\n        perimeter += distance(hull[i], hull[(i + 1) % hull.size()]);\n    }\n    \n    return perimeter;\n}\n",
      "\n    int n = points.size();\n    if (n <= 2) {\n        return 0.0;\n    }\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0.0) {\n            return 0; // collinear\n        }\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n\n    auto findNextPoint = [&](int p, std::vector<Point> const& hull) {\n        int q = (p + 1) % n;\n        for (int i = 0; i < n; i++) {\n            if (orientation(points[p], points[i], points[q]) == 2) {\n                q = i;\n            }\n        }\n        return q;\n    };\n\n    std::vector<bool> used(n, false);\n    int p = 0;\n    double perimeter = 0.0;\n    do {\n        used[p] = true;\n        int q = findNextPoint(p, points);\n        perimeter += distance(points[p], points[q]);\n        p = q;\n    } while (p != 0);\n\n    return perimeter;\n}\n",
      "\n    int n = points.size();\n    if (n <= 2) {\n        return 0.0;\n    }\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // collinear\n        return (val > 0) ? 1 : 2; // clockwise or counterclockwise\n    };\n\n    auto findNextPoint = [&](std::vector<Point> const& hull, Point const& p) {\n        int m = hull.size();\n        int q = (p.x == hull[m-1].x && p.y == hull[m-1].y) ? m-2 : m-1;\n        while (q >= 0 && orientation(hull[q], hull[m-1], p) != 2) {\n            --m;\n            q = (p.x == hull[m-1].x && p.y == hull[m-1].y) ? m-2 : m-1;\n        }\n        return m;\n    };\n\n    std::vector<Point> hull;\n    int l = 0;\n    for (int i = 1; i < n; ++i) {\n        if (points[i].x < points[l].x) {\n            l = i;\n        }\n    }\n\n    int p = l, q;\n    do {\n        hull.push_back(points[p]);\n        q = (p + 1) % n;\n        for (int i = 0; i < n; ++i) {\n            if (orientation(points[p], points[i], points[q]) == 2) {\n                q = i;\n            }\n        }\n        p = q;\n    } while (p != l);\n\n    double perimeter = 0.0;\n    for (int i = 0; i < hull.size(); ++i) {\n        perimeter += distance(hull[i], hull[(i+1) % hull.size()]);\n    }\n\n    return perimeter;\n}\n",
      "\n    int n = points.size();\n    if (n < 3) {\n        return 0.0;\n    }\n\n    std::vector<Point> hull;\n    std::sort(points.begin(), points.end(), [](Point const& p1, Point const& p2) {\n        return p1.x < p2.x || (p1.x == p2.x && p1.y < p2.y);\n    });\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) {\n            return 0;  // collinear\n        }\n        return (val > 0) ? 1 : 2;  // clock or counterclock wise\n    };\n\n    auto buildHull = [&](std::vector<Point>& hull, int& idx, Point const& p, Point const& q) {\n        while (idx > 0 && orientation(hull[idx - 1], hull[idx], points[i]) != 2) {\n            hull.pop_back();\n            idx--;\n        }\n        hull.push_back(points[i]);\n        idx++;\n    };\n\n    int idx = 0;\n    for (int i = 0; i < n; i++) {\n        buildHull(hull, idx, points[i], points[i]);\n    }\n\n    for (int i = n - 2, t = idx + 1; i >= 0; i--) {\n        buildHull(hull, idx, points[i], points[i]);\n    }\n\n    double perimeter = 0.0;\n    for (int i = 0; i < idx - 1; i++) {\n        perimeter += distance(hull[i], hull[i + 1]);\n    }\n    perimeter += distance(hull[idx - 1], hull[0]);\n\n    return perimeter;\n}\n",
      "\n    int n = points.size();\n    if (n <= 2) {\n        return 0.0;\n    }\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0.0) {\n            return 0;  // collinear\n        }\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n\n    std::vector<Point> hull;\n\n    int l = 0;\n    for (int i = 1; i < n; i++) {\n        if (points[i].x < points[l].x) {\n            l = i;\n        }\n    }\n\n    int p = l, q;\n    do {\n        hull.push_back(points[p]);\n        q = (p + 1) % n;\n        for (int i = 0; i < n; i++) {\n            if (orientation(points[p], points[i], points[q]) == 2) {\n                q = i;\n            }\n        }\n        p = q;\n    } while (p != l);\n\n    double perimeter = 0.0;\n    for (int i = 0; i < hull.size(); i++) {\n        perimeter += distance(hull[i], hull[(i + 1) % hull.size()]);\n    }\n\n    return perimeter;\n}\n",
      "\n    int n = points.size();\n    if (n <= 2) {\n        return 0.0;\n    }\n\n    auto orientation = [](Point const& p, Point const& q, Point const& r) {\n        double val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n        if (val == 0) return 0;  // colinear\n        return (val > 0) ? 1 : 2; // clock or counterclock wise\n    };\n\n    auto findNextPoint = [&](int p, std::vector<int> const& hull) {\n        int q = (p + 1) % n;\n        for (int i = 0; i < n; i++) {\n            if (orientation(points[p], points[i], points[q]) == 2) {\n                q = i;\n            }\n        }\n        return q;\n    };\n\n    std::vector<int> hull;\n    int l = 0;\n    for (int i = 1; i < n; i++) {\n        if (points[i].x < points[l].x) {\n            l = i;\n        }\n    }\n    int p = l, q;\n    do {\n        hull.push_back(p);\n        q = findNextPoint(p, hull);\n        p = q;\n    } while (p != l);\n\n    double perimeter = 0.0;\n    for (int i = 0; i < hull.size(); i++) {\n        perimeter += distance(points[hull[i]], points[hull[(i + 1) % hull.size()]]);\n    }\n    return perimeter;\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "11_geometry_convex_hull_perimeter",
    "parallelism_model": "hip",
    "prompt": "struct Point {\n\tdouble x, y;\n};\n\n__device__ double distance(Point const& p1, Point const& p2) {\n\treturn sqrt(pow(p2.x-p1.x, 2) + pow(p2.y-p1.y, 2));\n}\n\n/* Compute the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Store the result in perimeter.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as points.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\n__global__ void convexHullPerimeter(const Point *points, size_t numPoints, double *perimeter) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n\t// Gift wrapping algorithm to find convex hull\n\tint leftmost = 0;\n\tfor (int i = 1; i < numPoints; ++i) {\n\t\tif (points[i].x < points[leftmost].x) {\n\t\t\tleftmost = i;\n\t\t}\n\t}\n\n\tint current = leftmost;\n\tint next;\n\tdouble totalPerimeter = 0;\n\n\tdo {\n\t\tnext = (current + 1) % numPoints;\n\t\tfor (int i = 0; i < numPoints; ++i) {\n\t\t\tif (i != current && i != next) {\n\t\t\t\tdouble crossProduct = (points[next].x - points[current].x) * (points[i].y - points[current].y) - (points[next].y - points[current].y) * (points[i].x - points[current].x);\n\t\t\t\tif (crossProduct > 0) {\n\t\t\t\t\tnext = i;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\ttotalPerimeter += distance(points[current], points[next]);\n\t\tcurrent = next;\n\t} while (current != leftmost);\n\n\t*perimeter = totalPerimeter;\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int next = (idx + 1) % numPoints;\n\n    if (idx < numPoints) {\n        double totalPerimeter = 0.0;\n        for (int i = 0; i < numPoints; i++) {\n            totalPerimeter += distance(points[i], points[(i + 1) % numPoints]);\n        }\n        *perimeter = totalPerimeter;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < numPoints) {\n        double result = 0.0;\n        for (int i = 0; i < numPoints; ++i) {\n            result += distance(points[i], points[(i + 1) % numPoints]);\n        }\n        *perimeter = result;\n    }\n}\n",
      "\n\t// Graham's scan algorithm to compute the convex hull\n\t// Sort the points based on polar angle with the lowest point\n\t// Then iterate through the sorted points to find the convex hull\n\t// Finally, compute the perimeter of the convex hull\n\n\t// Sort the points based on polar angle with the lowest point\n\t// (You can use a sorting algorithm like quicksort or std::sort)\n\n\t// Iterate through the sorted points to find the convex hull\n\t// (You can use a stack to keep track of the convex hull points)\n\n\t// Compute the perimeter of the convex hull\n\t// (You can use the distance function to compute the distance between consecutive points on the convex hull)\n\n\t// Store the result in perimeter\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int next = (idx + 1) % numPoints;\n    double totalPerimeter = 0.0;\n\n    if (idx < numPoints) {\n        for (int i = 0; i < numPoints; i++) {\n            totalPerimeter += distance(points[i], points[(i + 1) % numPoints]);\n        }\n    }\n\n    perimeter[idx] = totalPerimeter;\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < numPoints) {\n        double result = 0.0;\n        for (int i = 0; i < numPoints; ++i) {\n            result += distance(points[i], points[(i + 1) % numPoints]);\n        }\n        *perimeter = result;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < numPoints) {\n        double result = 0.0;\n        for (int i = 0; i < numPoints; ++i) {\n            result += distance(points[i], points[(i + 1) % numPoints]);\n        }\n        *perimeter = result;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int next = (idx + 1) % numPoints;\n    double totalPerimeter = 0.0;\n\n    if (idx < numPoints) {\n        for (int i = 0; i < numPoints; i++) {\n            totalPerimeter += distance(points[i], points[(i + 1) % numPoints]);\n        }\n    }\n\n    perimeter[idx] = totalPerimeter;\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < numPoints) {\n        double result = 0.0;\n        for (int i = 0; i < numPoints; i++) {\n            if (i != idx) {\n                double maxDistance = 0.0;\n                for (int j = 0; j < numPoints; j++) {\n                    if (j != i && j != idx) {\n                        double crossProduct = (points[j].x - points[i].x) * (points[idx].y - points[i].y) - (points[j].y - points[i].y) * (points[idx].x - points[i].x);\n                        if (crossProduct < 0) {\n                            double dist = distance(points[i], points[j]);\n                            if (dist > maxDistance) {\n                                maxDistance = dist;\n                            }\n                        }\n                    }\n                }\n                result += maxDistance;\n            }\n        }\n        perimeter[idx] = result;\n    }\n}\n",
      "\n    // Gift wrapping algorithm (Jarvis march)\n    int leftmost = 0;\n    for (int i = 1; i < numPoints; ++i) {\n        if (points[i].x < points[leftmost].x) {\n            leftmost = i;\n        }\n    }\n\n    int current = leftmost;\n    double totalPerimeter = 0.0;\n    do {\n        int next = (current + 1) % numPoints;\n        for (int i = 0; i < numPoints; ++i) {\n            if (i != current && i != next &&\n                ((points[next].x - points[current].x) * (points[i].y - points[current].y) -\n                 (points[next].y - points[current].y) * (points[i].x - points[current].x)) > 0) {\n                next = i;\n            }\n        }\n        totalPerimeter += distance(points[current], points[next]);\n        current = next;\n    } while (current != leftmost);\n\n    *perimeter = totalPerimeter;\n}\n",
      "\n\t// Gift wrapping algorithm to find the convex hull\n\tint leftmost = 0;\n\tfor (int i = 1; i < numPoints; ++i) {\n\t\tif (points[i].x < points[leftmost].x) {\n\t\t\tleftmost = i;\n\t\t}\n\t}\n\n\tint current = leftmost;\n\tint next;\n\tdouble totalPerimeter = 0;\n\n\tdo {\n\t\tnext = (current + 1) % numPoints;\n\t\tfor (int i = 0; i < numPoints; ++i) {\n\t\t\tif (i != current && i != next) {\n\t\t\t\tdouble crossProduct = (points[next].x - points[current].x) * (points[i].y - points[current].y) - (points[next].y - points[current].y) * (points[i].x - points[current].x);\n\t\t\t\tif (crossProduct > 0) {\n\t\t\t\t\tnext = i;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\ttotalPerimeter += distance(points[current], points[next]);\n\t\tcurrent = next;\n\t} while (current != leftmost);\n\n\t*perimeter = totalPerimeter;\n}\n",
      "\n    // Gift wrapping algorithm (Jarvis march)\n    int leftmost = 0;\n    for (int i = 1; i < numPoints; ++i) {\n        if (points[i].x < points[leftmost].x) {\n            leftmost = i;\n        }\n    }\n\n    int current = leftmost;\n    double totalPerimeter = 0.0;\n    do {\n        int next = (current + 1) % numPoints;\n        for (int i = 0; i < numPoints; ++i) {\n            if (i != current && i != next &&\n                ((points[next].x - points[current].x) * (points[i].y - points[current].y) -\n                 (points[i].x - points[current].x) * (points[next].y - points[current].y)) > 0) {\n                next = i;\n            }\n        }\n        totalPerimeter += distance(points[current], points[next]);\n        current = next;\n    } while (current != leftmost);\n\n    *perimeter = totalPerimeter;\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < numPoints) {\n        double result = 0.0;\n        for (int i = 0; i < numPoints; ++i) {\n            result += distance(points[i], points[(i + 1) % numPoints]);\n        }\n        *perimeter = result;\n    }\n}\n",
      "\n\t// Gift wrapping algorithm to find the convex hull\n\tint leftmost = 0;\n\tfor (int i = 1; i < numPoints; ++i) {\n\t\tif (points[i].x < points[leftmost].x) {\n\t\t\tleftmost = i;\n\t\t}\n\t}\n\n\tint current = leftmost;\n\tint next;\n\tdouble totalPerimeter = 0;\n\n\tdo {\n\t\tnext = (current + 1) % numPoints;\n\t\tfor (int i = 0; i < numPoints; ++i) {\n\t\t\tif (i != current && i != next) {\n\t\t\t\tdouble crossProduct = (points[next].x - points[current].x) * (points[i].y - points[current].y) - (points[next].y - points[current].y) * (points[i].x - points[current].x);\n\t\t\t\tif (crossProduct > 0) {\n\t\t\t\t\tnext = i;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\ttotalPerimeter += distance(points[current], points[next]);\n\t\tcurrent = next;\n\t} while (current != leftmost);\n\n\t*perimeter = totalPerimeter;\n}\n",
      "\n\t// Gift wrapping algorithm to find the convex hull\n\tint leftmost = 0;\n\tfor (int i = 1; i < numPoints; ++i) {\n\t\tif (points[i].x < points[leftmost].x) {\n\t\t\tleftmost = i;\n\t\t}\n\t}\n\n\tint current = leftmost;\n\tint next;\n\tdouble totalPerimeter = 0;\n\n\tdo {\n\t\tnext = (current + 1) % numPoints;\n\t\tfor (int i = 0; i < numPoints; ++i) {\n\t\t\tif (i != current && i != next) {\n\t\t\t\tdouble crossProduct = (points[next].x - points[current].x) * (points[i].y - points[current].y) - (points[next].y - points[current].y) * (points[i].x - points[current].x);\n\t\t\t\tif (crossProduct > 0) {\n\t\t\t\t\tnext = i;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\ttotalPerimeter += distance(points[current], points[next]);\n\t\tcurrent = next;\n\t} while (current != leftmost);\n\n\t*perimeter = totalPerimeter;\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < numPoints) {\n        double totalPerimeter = 0.0;\n        for (int i = 0; i < numPoints; i++) {\n            if (i != idx) {\n                double maxDistance = 0.0;\n                for (int j = 0; j < numPoints; j++) {\n                    if (j != i && j != idx) {\n                        double crossProduct = (points[j].x - points[i].x) * (points[idx].y - points[i].y) - (points[j].y - points[i].y) * (points[idx].x - points[i].x);\n                        if (crossProduct < 0) {\n                            double dist = distance(points[i], points[j]);\n                            if (dist > maxDistance) {\n                                maxDistance = dist;\n                            }\n                        }\n                    }\n                }\n                totalPerimeter += maxDistance;\n            }\n        }\n        perimeter[idx] = totalPerimeter;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int next = (idx + 1) % numPoints;\n    double totalPerimeter = 0.0;\n\n    if (idx < numPoints) {\n        for (int i = 0; i < numPoints; i++) {\n            totalPerimeter += distance(points[i], points[next]);\n            next = (next + 1) % numPoints;\n        }\n    }\n\n    perimeter[idx] = totalPerimeter;\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int next = (idx + 1) % numPoints;\n    double totalPerimeter = 0.0;\n\n    if (idx < numPoints) {\n        for (int i = 0; i < numPoints; i++) {\n            totalPerimeter += distance(points[i], points[next]);\n            next = (next + 1) % numPoints;\n        }\n    }\n\n    perimeter[idx] = totalPerimeter;\n}\n",
      "\n\t// Gift wrapping algorithm to find the convex hull\n\tint leftmost = 0;\n\tfor (int i = 1; i < numPoints; ++i) {\n\t\tif (points[i].x < points[leftmost].x) {\n\t\t\tleftmost = i;\n\t\t}\n\t}\n\n\tint current = leftmost;\n\tdouble totalPerimeter = 0.0;\n\tdo {\n\t\tint next = (current + 1) % numPoints;\n\t\tfor (int i = 0; i < numPoints; ++i) {\n\t\t\tif (i != current && i != next &&\n\t\t\t\t((points[next].x - points[current].x) * (points[i].y - points[current].y) -\n\t\t\t\t(points[next].y - points[current].y) * (points[i].x - points[current].x)) > 0) {\n\t\t\t\tnext = i;\n\t\t\t}\n\t\t}\n\t\ttotalPerimeter += distance(points[current], points[next]);\n\t\tcurrent = next;\n\t} while (current != leftmost);\n\n\t*perimeter = totalPerimeter;\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int next = (idx + 1) % numPoints;\n\n    double totalPerimeter = 0.0;\n    for (int i = 0; i < numPoints; i++) {\n        totalPerimeter += distance(points[i], points[next]);\n        next = (next + 1) % numPoints;\n    }\n\n    if (idx == 0) {\n        *perimeter = totalPerimeter;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "11_geometry_convex_hull_perimeter",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The final result is returned on rank 0.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(std::vector<Point> const& points) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n// Your code goes here\n// Implement the convex hull algorithm to calculate the perimeter of the smallest convex polygon containing all the points\n// Use MPI to compute in parallel and gather the results on rank 0\n// Return the final result\n}",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // TODO: Implement the convex hull algorithm to calculate the perimeter of the smallest convex polygon containing all the points\n}\n",
      "\n    // Implement the convex hull algorithm to find the perimeter of the smallest convex polygon\n    // that contains all the points in the vector points\n    // Use MPI to compute in parallel\n    // Assume MPI has already been initialized\n    // Every rank has a complete copy of points\n    // The final result is returned on rank 0\n\n    // Your implementation here\n    // ...\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Implement the convex hull algorithm to find the perimeter of the smallest convex polygon\n    // that contains all the points in the vector points\n    // Use MPI to compute in parallel and return the final result on rank 0\n}\n",
      "\n    // Your code here\n}\n",
      "\n// Your code goes here\n// Implement the convex hull algorithm to find the perimeter of the smallest convex polygon\n// that contains all the points in the vector points\n// Use MPI to compute in parallel and return the final result on rank 0\n// You can use any convex hull algorithm such as Graham Scan or Jarvis March\n// Remember to handle the MPI communication and synchronization appropriately\n}",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n    // (You can use the provided distance function and any other necessary code)\n}\n",
      "\n    // Your code here\n    // You can use MPI functions to distribute the work among ranks and gather the results on rank 0\n}\n",
      "\n    // Your code here\n    // Implement the convex hull algorithm to find the perimeter of the smallest convex polygon that contains all the points\n    // Use MPI to compute in parallel\n    // Return the final result on rank 0\n}\n",
      "\n    // Your code here\n    // (You can use the provided distance function and any other necessary functions or libraries)\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "11_geometry_convex_hull_perimeter",
    "parallelism_model": "cuda",
    "prompt": "struct Point {\n\tdouble x, y;\n};\n\n__device__ double distance(Point const& p1, Point const& p2) {\n\treturn sqrt(pow(p2.x-p1.x, 2) + pow(p2.y-p1.y, 2));\n}\n\n/* Compute the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Store the result in perimeter.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as points.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\n__global__ void convexHullPerimeter(const Point *points, size_t numPoints, double *perimeter) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n\t// Implement the Graham scan algorithm to find the convex hull of the points\n\t// Sort the points based on their polar angle with the lowest y-coordinate point\n\t// Find the convex hull by iterating through the sorted points and removing any points that create a clockwise turn\n\t// Calculate the perimeter of the convex hull by summing the distances between consecutive points\n\t// Store the result in the perimeter variable\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < numPoints) {\n        double result = 0.0;\n        for (int i = 0; i < numPoints; ++i) {\n            if (i != idx) {\n                double maxDistance = 0.0;\n                for (int j = 0; j < numPoints; ++j) {\n                    if (j != i && j != idx) {\n                        double dist = distance(points[i], points[j]) * distance(points[j], points[idx]) * distance(points[idx], points[i]);\n                        if (dist > maxDistance) {\n                            maxDistance = dist;\n                        }\n                    }\n                }\n                result += sqrt(maxDistance);\n            }\n        }\n        perimeter[idx] = result;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int next = (idx + 1) % numPoints;\n\n    if (idx < numPoints) {\n        double totalPerimeter = 0.0;\n        for (int i = 0; i < numPoints; i++) {\n            totalPerimeter += distance(points[i], points[next]);\n            next = (next + 1) % numPoints;\n        }\n        perimeter[idx] = totalPerimeter;\n    }\n}\n",
      "\n\t// Implement the Graham scan algorithm to find the convex hull of the points\n\t// Sort the points based on polar angle with the lowest y-coordinate point as the reference\n\t// Then iterate through the sorted points and keep track of the convex hull\n\t// Finally, calculate the perimeter of the convex hull and store the result in perimeter\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < numPoints) {\n        double result = 0.0;\n        for (int i = 0; i < numPoints; ++i) {\n            if (i != idx) {\n                double maxDistance = 0.0;\n                for (int j = 0; j < numPoints; ++j) {\n                    if (j != i && j != idx) {\n                        double dist = distance(points[i], points[j]) * distance(points[i], points[idx]) * distance(points[j], points[idx]);\n                        if (dist > maxDistance) {\n                            maxDistance = dist;\n                        }\n                    }\n                }\n                result += sqrt(maxDistance);\n            }\n        }\n        perimeter[idx] = result;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < numPoints) {\n        double result = 0.0;\n        for (int i = 0; i < numPoints; ++i) {\n            if (i != idx) {\n                double maxDist = 0.0;\n                for (int j = 0; j < numPoints; ++j) {\n                    if (j != i && j != idx) {\n                        double dist = distance(points[i], points[j]);\n                        if (dist > maxDist && (points[j].x - points[i].x) * (points[idx].y - points[i].y) - (points[j].y - points[i].y) * (points[idx].x - points[i].x) > 0) {\n                            maxDist = dist;\n                        }\n                    }\n                }\n                result += maxDist;\n            }\n        }\n        perimeter[idx] = result;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < numPoints) {\n        double result = 0.0;\n        for (int i = 0; i < numPoints; ++i) {\n            if (i != idx) {\n                double maxDistance = 0.0;\n                for (int j = 0; j < numPoints; ++j) {\n                    if (j != i && j != idx) {\n                        double d = distance(points[i], points[j]);\n                        if (d > maxDistance && (points[j].x - points[i].x) * (points[idx].y - points[i].y) - (points[j].y - points[i].y) * (points[idx].x - points[i].x) > 0) {\n                            maxDistance = d;\n                        }\n                    }\n                }\n                result += maxDistance;\n            }\n        }\n        perimeter[idx] = result;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int next = (tid + 1) % numPoints;\n    double totalPerimeter = 0.0;\n\n    if (tid < numPoints) {\n        Point currentPoint = points[tid];\n        Point nextPoint = points[next];\n        while (next != 0) {\n            totalPerimeter += distance(currentPoint, nextPoint);\n            currentPoint = nextPoint;\n            next = (next + 1) % numPoints;\n            nextPoint = points[next];\n        }\n    }\n\n    perimeter[tid] = totalPerimeter;\n}\n",
      "\n\t// Graham's scan algorithm to find convex hull\n\t// Sort points based on polar angle with the lowest point\n\t// Find the convex hull using the sorted points\n\t// Compute the perimeter of the convex hull\n}\n",
      "\n\t// Graham's scan algorithm to find convex hull\n\t// Sort points based on polar angle with the lowest point\n\t// Find the convex hull using the sorted points\n\t// Compute the perimeter of the convex hull\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int next = (idx + 1) % numPoints;\n\n    if (idx < numPoints) {\n        double totalPerimeter = 0.0;\n        for (int i = 0; i < numPoints; i++) {\n            totalPerimeter += distance(points[i], points[next]);\n            next = (next + 1) % numPoints;\n        }\n        perimeter[idx] = totalPerimeter;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < numPoints) {\n        double result = 0.0;\n        for (int i = 0; i < numPoints; ++i) {\n            if (i != idx) {\n                double maxDistance = 0.0;\n                for (int j = 0; j < numPoints; ++j) {\n                    if (j != i && j != idx) {\n                        double crossProduct = (points[j].x - points[i].x) * (points[idx].y - points[i].y) - (points[j].y - points[i].y) * (points[idx].x - points[i].x);\n                        if (crossProduct < 0) {\n                            double currentDistance = distance(points[i], points[j]);\n                            if (currentDistance > maxDistance) {\n                                maxDistance = currentDistance;\n                            }\n                        }\n                    }\n                }\n                result += maxDistance;\n            }\n        }\n        perimeter[idx] = result;\n    }\n}\n",
      "\n\t// Graham's scan algorithm to find convex hull\n\t// Sort points based on polar angle with the lowest point\n\t// Find the convex hull using the sorted points\n\t// Compute the perimeter of the convex hull\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < numPoints) {\n        double result = 0.0;\n        for (int i = 0; i < numPoints; ++i) {\n            result += distance(points[i], points[(i + 1) % numPoints]);\n        }\n        *perimeter = result;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int next = (idx + 1) % numPoints;\n\n    if (idx < numPoints) {\n        double totalPerimeter = 0.0;\n        for (int i = 0; i < numPoints; i++) {\n            totalPerimeter += distance(points[i], points[(i + 1) % numPoints]);\n        }\n        *perimeter = totalPerimeter;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int next = (idx + 1) % numPoints;\n\n    if (idx < numPoints) {\n        double totalPerimeter = 0.0;\n        for (int i = 0; i < numPoints; i++) {\n            totalPerimeter += distance(points[i], points[(i + 1) % numPoints]);\n        }\n        *perimeter = totalPerimeter;\n    }\n}\n",
      "\n    // Implement the Graham scan algorithm to find the convex hull of the points\n    // Sort the points based on polar angle with the lowest y-coordinate point as the reference\n    // Then iterate through the sorted points to find the convex hull\n    // Finally, calculate the perimeter of the convex hull and store the result in perimeter\n}\n",
      "\n\t// Your code here\n}\n",
      "\n    // Implement the Graham scan algorithm to find the convex hull of the points\n    // Sort the points based on their polar angle with the lowest y-coordinate point as the reference\n    // Then iterate through the sorted points and use a stack to find the convex hull\n\n    // Calculate the distance between two points using the distance function\n    // Add up the distances between consecutive points to get the perimeter\n\n    // Note: The implementation of the Graham scan algorithm and sorting of points based on polar angle is not provided here\n}\n",
      "\n\t// Your code here\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "11_geometry_convex_hull_perimeter",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The final result is returned on rank 0.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(std::vector<Point> const& points) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // Your code goes here\n    // You can use MPI and OpenMP to compute in parallel\n    // The final result should be returned on rank 0\n}\n",
      "\n    // Your code here\n    // (You can use the provided distance function and the Point struct)\n}\n",
      "\n    // Your code here\n    // Implement the algorithm to compute the convex hull perimeter using MPI and OpenMP\n    // Remember to handle the parallelization and communication between MPI ranks\n    // Return the final result on rank 0\n}\n",
      "\n    // Your code goes here\n    // Implement the convex hull algorithm to calculate the perimeter of the smallest convex polygon that contains all the points\n    // Use MPI and OpenMP to compute in parallel\n    // Return the final result on rank 0\n}\n",
      "\n    // Your code goes here\n    // Implement the convex hull algorithm to find the perimeter of the smallest convex polygon that contains all the points\n    // Use MPI and OpenMP to compute in parallel\n    // Return the final result on rank 0\n    // You may use the distance function provided\n}",
      "\n    // Your code here\n    // Implement the convex hull algorithm to find the perimeter of the smallest convex polygon that contains all the points\n    // Use MPI and OpenMP to compute in parallel\n    // Return the final result on rank 0\n}",
      "\n    // Your code goes here\n    // Implement the algorithm to compute the convex hull perimeter using MPI and OpenMP\n    // Remember to handle the parallelization and communication between MPI ranks\n    // Return the final result on rank 0\n}\n",
      "\n    // Your code goes here\n    // Implement the algorithm to compute the convex hull perimeter using MPI and OpenMP\n    // Remember to handle the parallelization and communication between MPI ranks\n    // Return the final result on rank 0\n}\n",
      "\n    // Your code goes here\n    // Implement the algorithm to compute the convex hull perimeter using MPI and OpenMP\n    // Make sure to handle the parallelization and communication among MPI ranks\n    // Return the final result on rank 0\n}\n",
      "\n    // Your code here\n    // Implement the algorithm to compute the convex hull perimeter using MPI and OpenMP\n    // Remember to handle the case when the number of points is less than 3\n    // Return the final result on rank 0\n}\n",
      "\n    // Your code goes here\n    // Implement the algorithm to compute the convex hull perimeter using MPI and OpenMP\n    // Return the result\n}\n",
      "\n    // Your code goes here\n    // Implement the algorithm to compute the convex hull perimeter using MPI and OpenMP\n    // Remember to handle the parallelization and communication between MPI ranks\n    // Return the final result on rank 0\n}\n",
      "\n    // Your code goes here\n    // Implement the algorithm to compute the convex hull perimeter using MPI and OpenMP\n    // Remember to handle the parallelization and communication among MPI ranks\n    // Return the final result on rank 0\n}\n",
      "\n    // Your code goes here\n    // Implement the algorithm to compute the convex hull perimeter using MPI and OpenMP\n    // Remember to handle the parallelization and communication between MPI ranks\n    // Return the final result on rank 0\n}\n",
      "\n    // Your code here\n    // Implement the algorithm to compute the convex hull perimeter using MPI and OpenMP\n    // Return the final result on rank 0\n    // You may use MPI functions like MPI_Comm_rank, MPI_Comm_size, MPI_Bcast, MPI_Reduce, etc.\n    // You may also use OpenMP parallel for loop to parallelize the computation\n}\n",
      "\n    // Your code here\n    // Implement the algorithm to compute the convex hull perimeter using MPI and OpenMP\n    // Return the final result on rank 0\n    // Remember to handle the case when the number of points is less than 3\n}\n",
      "\n    // Your code goes here\n    // Implement the algorithm to compute the convex hull perimeter using MPI and OpenMP\n    // Remember to handle the parallelization and communication between MPI ranks\n    // The final result should be returned on rank 0\n    // You may use any necessary helper functions or data structures\n    // Make sure to handle the edge cases and boundary conditions\n    // Return the computed perimeter\n}\n",
      "\n    // Your code here\n    // Implement the algorithm to compute the convex hull perimeter using MPI and OpenMP\n    // Return the final result on rank 0\n    // Use MPI functions like MPI_Send, MPI_Recv, MPI_Reduce, etc. for communication and synchronization\n    // Use OpenMP for parallelizing the computation within each rank\n    // You may use any suitable algorithm for computing the convex hull, such as Graham Scan or Jarvis March\n    // Remember to handle the case when the input vector is empty or has only one point\n    // You can also use any necessary helper functions or data structures\n}\n",
      "\n    // Implement the body of the function here\n    // (Note: I'm assuming the existence of a function to compute the convex hull of the points)\n    // (and a function to compute the perimeter of the convex hull)\n\n    // Compute the convex hull of the points\n\n    // Compute the perimeter of the convex hull\n\n    // Return the perimeter\n}\n",
      "\n    // Your code here\n    // Implement the algorithm to compute the convex hull perimeter using MPI and OpenMP\n    // Return the final result on rank 0\n    // You may use MPI functions like MPI_Bcast, MPI_Gather, MPI_Reduce, etc. and OpenMP for parallelization\n    // Remember to handle the case when the input vector is empty\n    // You can use the distance function provided to calculate the distance between two points\n    // The final result should be returned on rank 0\n}"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "16_graph_largest_component",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix. A is an undirected graph.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(Kokkos::View<const int**> &A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::View<int*> component(\"component\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        component(i) = i;\n    });\n    Kokkos::fence();\n\n    for (size_t k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n            if (A(k, i) == 1) {\n                int root_k = component(k);\n                int root_i = component(i);\n                for (size_t j = 0; j < N; ++j) {\n                    if (component(j) == root_i) {\n                        component(j) = root_k;\n                    }\n                }\n            }\n        });\n        Kokkos::fence();\n    }\n\n    Kokkos::View<int> count(\"count\");\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& lcount) {\n        if (component(i) == i) {\n            lcount++;\n        }\n    }, count);\n    Kokkos::fence();\n\n    return count();\n}\n",
      "\n    Kokkos::View<int*> visited(\"visited\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        visited(i) = 0;\n    });\n    Kokkos::fence();\n\n    int maxComponentSize = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (visited(i) == 0) {\n            int componentSize = 0;\n            Kokkos::View<int*> stack(\"stack\", N);\n            int stackTop = -1;\n            stack(++stackTop) = i;\n            while (stackTop >= 0) {\n                int node = stack(stackTop--);\n                if (visited(node) == 0) {\n                    visited(node) = 1;\n                    componentSize++;\n                    for (size_t j = 0; j < N; ++j) {\n                        if (A(node, j) == 1 && visited(j) == 0) {\n                            stack(++stackTop) = j;\n                        }\n                    }\n                }\n            }\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n\n    return maxComponentSize;\n}",
      "\n    Kokkos::View<int*> visited(\"visited\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        visited(i) = 0;\n    });\n    Kokkos::fence();\n\n    int maxComponentSize = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_max) {\n        if (visited(i) == 0) {\n            local_max = std::max(local_max, dfs(A, visited, i, N));\n        }\n    }, Kokkos::Max<int>(maxComponentSize));\n    Kokkos::fence();\n\n    return maxComponentSize;\n}\n",
      "\n    Kokkos::View<int*> visited(\"visited\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        visited(i) = 0;\n    });\n    Kokkos::fence();\n\n    int maxComponentSize = 0;\n    for (size_t i = 0; i < N; ++i) {\n        if (visited(i) == 0) {\n            int componentSize = 1;\n            visited(i) = 1;\n            Kokkos::View<int*> queue(\"queue\", N);\n            int front = 0, rear = 0;\n            queue(rear++) = i;\n            while (front != rear) {\n                int u = queue(front++);\n                for (size_t v = 0; v < N; ++v) {\n                    if (A(u, v) && visited(v) == 0) {\n                        visited(v) = 1;\n                        queue(rear++) = v;\n                        componentSize++;\n                    }\n                }\n            }\n            maxComponentSize = std::max(maxComponentSize, componentSize);\n        }\n    }\n    return maxComponentSize;\n}",
      "\n    Kokkos::View<int*> component(\"component\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n        component(i) = i;\n    });\n    Kokkos::fence();\n\n    for (size_t k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n            for (size_t j = 0; j < N; ++j) {\n                if (A(i, j) == 1) {\n                    int root_i = i;\n                    while (component(root_i) != root_i) {\n                        root_i = component(root_i);\n                    }\n                    int root_j = j;\n                    while (component(root_j) != root_j) {\n                        root_j = component(root_j);\n                    }\n                    if (root_i != root_j) {\n                        component(root_j) = root_i;\n                    }\n                }\n            }\n        });\n        Kokkos::fence();\n    }\n\n    int max_count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int count = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (component(j) == i) {\n                count++;\n            }\n        }\n        if (count > max_count) {\n            max_count = count;\n        }\n    }\n\n    return max_count;\n}",
      "\n    Kokkos::View<int*> visited(\"visited\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        visited(i) = 0;\n    });\n    Kokkos::fence();\n\n    int maxComponentSize = 0;\n    for (size_t i = 0; i < N; ++i) {\n        if (visited(i) == 0) {\n            int componentSize = 1;\n            visited(i) = 1;\n            Kokkos::View<int*> queue(\"queue\", N);\n            int front = 0, rear = 0;\n            queue(rear++) = i;\n\n            while (front != rear) {\n                int v = queue(front++);\n                for (size_t j = 0; j < N; ++j) {\n                    if (A(v, j) == 1 && visited(j) == 0) {\n                        visited(j) = 1;\n                        queue(rear++) = j;\n                        componentSize++;\n                    }\n                }\n            }\n            maxComponentSize = std::max(maxComponentSize, componentSize);\n        }\n    }\n    return maxComponentSize;\n}",
      "\n    Kokkos::View<int*> component(\"component\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n        component(i) = i;\n    });\n    Kokkos::fence();\n\n    for (size_t k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n            if (A(k, i) == 1) {\n                int root_k = component(k);\n                int root_i = component(i);\n                for (size_t j = 0; j < N; ++j) {\n                    if (component(j) == root_i) {\n                        component(j) = root_k;\n                    }\n                }\n            }\n        });\n        Kokkos::fence();\n    }\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n        int root = component(i);\n        while (root != component(root)) {\n            root = component(root);\n        }\n        component(i) = root;\n    });\n    Kokkos::fence();\n\n    Kokkos::View<int> count(\"count\");\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(int i, int &local_count) {\n        if (component(i) == i) {\n            local_count++;\n        }\n    }, count);\n    Kokkos::fence();\n\n    return count();\n}\n",
      "\n    Kokkos::View<int*> component(\"component\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n        component(i) = i;\n    });\n    Kokkos::fence();\n\n    for (size_t k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n            for (size_t j = 0; j < N; ++j) {\n                if (A(i, j) == 1) {\n                    int root_i = i;\n                    while (component(root_i) != root_i) {\n                        root_i = component(root_i);\n                    }\n                    int root_j = j;\n                    while (component(root_j) != root_j) {\n                        root_j = component(root_j);\n                    }\n                    if (root_i != root_j) {\n                        component(root_j) = root_i;\n                    }\n                }\n            }\n        });\n        Kokkos::fence();\n    }\n\n    int max_count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int count = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (component(j) == i) {\n                count++;\n            }\n        }\n        if (count > max_count) {\n            max_count = count;\n        }\n    }\n\n    return max_count;\n}",
      "\n    Kokkos::View<int*> component(\"component\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        component(i) = i;\n    });\n    Kokkos::fence();\n\n    for (size_t k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n            for (size_t j = 0; j < N; ++j) {\n                if (A(i, j) == 1) {\n                    int root_i = i;\n                    while (component(root_i) != root_i) {\n                        root_i = component(root_i);\n                    }\n                    int root_j = j;\n                    while (component(root_j) != root_j) {\n                        root_j = component(root_j);\n                    }\n                    if (root_i != root_j) {\n                        component(root_j) = root_i;\n                    }\n                }\n            }\n        });\n        Kokkos::fence();\n    }\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        int root_i = i;\n        while (component(root_i) != root_i) {\n            root_i = component(root_i);\n        }\n        component(i) = root_i;\n    });\n    Kokkos::fence();\n\n    Kokkos::View<int> component_sizes(\"component_sizes\");\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_max) {\n        if (component(i) == i) {\n            int size = 0;\n            for (size_t j = 0; j < N; ++j) {\n                if (component(j) == i) {\n                    size++;\n                }\n            }\n            if (size > local_max) {\n                local_max = size;\n            }\n        }\n    }, Kokkos::Max<int>(component_sizes));\n\n    return component_sizes();\n}",
      "\n    Kokkos::View<int*> component(\"component\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        component(i) = i;\n    });\n    Kokkos::fence();\n\n    for (size_t k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n            for (size_t j = 0; j < N; ++j) {\n                if (A(i, j) == 1) {\n                    int root_i = i;\n                    while (component(root_i) != root_i) {\n                        root_i = component(root_i);\n                    }\n                    int root_j = j;\n                    while (component(root_j) != root_j) {\n                        root_j = component(root_j);\n                    }\n                    if (root_i != root_j) {\n                        component(root_j) = root_i;\n                    }\n                }\n            }\n        });\n        Kokkos::fence();\n    }\n\n    int max_size = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int root_i = i;\n        while (component(root_i) != root_i) {\n            root_i = component(root_i);\n        }\n        int size = 1;\n        for (size_t j = i + 1; j < N; ++j) {\n            if (component(j) == root_i) {\n                size++;\n            }\n        }\n        if (size > max_size) {\n            max_size = size;\n        }\n    }\n\n    return max_size;\n}",
      "\n    Kokkos::View<int*> component(\"component\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        component(i) = i;\n    });\n    Kokkos::fence();\n\n    for (size_t k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n            for (size_t j = 0; j < N; ++j) {\n                if (A(i, j) == 1) {\n                    int root_i = i;\n                    while (component(root_i) != root_i) {\n                        root_i = component(root_i);\n                    }\n                    int root_j = j;\n                    while (component(root_j) != root_j) {\n                        root_j = component(root_j);\n                    }\n                    if (root_i != root_j) {\n                        component(root_j) = root_i;\n                    }\n                }\n            }\n        });\n        Kokkos::fence();\n    }\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        int root_i = i;\n        while (component(root_i) != root_i) {\n            root_i = component(root_i);\n        }\n        component(i) = root_i;\n    });\n    Kokkos::fence();\n\n    Kokkos::View<int> count(\"count\");\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        if (component(i) == i) {\n            local_count++;\n        }\n    }, count);\n    Kokkos::fence();\n\n    return count();\n}",
      "\n    Kokkos::View<int*> component(\"component\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        component(i) = i;\n    });\n    Kokkos::fence();\n\n    for (size_t k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n            for (size_t j = 0; j < N; ++j) {\n                if (A(i, j) == 1) {\n                    int root_i = i;\n                    while (component(root_i) != root_i) {\n                        root_i = component(root_i);\n                    }\n                    int root_j = j;\n                    while (component(root_j) != root_j) {\n                        root_j = component(root_j);\n                    }\n                    if (root_i != root_j) {\n                        component(root_j) = root_i;\n                    }\n                }\n            }\n        });\n        Kokkos::fence();\n    }\n\n    int maxCount = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int count = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (component(j) == i) {\n                count++;\n            }\n        }\n        if (count > maxCount) {\n            maxCount = count;\n        }\n    }\n\n    return maxCount;\n}",
      "\n    Kokkos::View<int*> component(\"component\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        component(i) = i;\n    });\n    Kokkos::fence();\n\n    for (size_t k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n            for (size_t j = 0; j < N; ++j) {\n                if (A(i, j) == 1) {\n                    int root_i = i;\n                    while (component(root_i) != root_i) {\n                        root_i = component(root_i);\n                    }\n                    int root_j = j;\n                    while (component(root_j) != root_j) {\n                        root_j = component(root_j);\n                    }\n                    if (root_i != root_j) {\n                        component(root_j) = root_i;\n                    }\n                }\n            }\n        });\n        Kokkos::fence();\n    }\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        int root_i = i;\n        while (component(root_i) != root_i) {\n            root_i = component(root_i);\n        }\n        component(i) = root_i;\n    });\n    Kokkos::fence();\n\n    Kokkos::View<int> count(\"count\");\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& lcount) {\n        if (component(i) == i) {\n            lcount++;\n        }\n    }, count);\n    Kokkos::fence();\n\n    return count();\n}",
      "\n    Kokkos::View<int*> component(\"component\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        component(i) = i;\n    });\n    Kokkos::fence();\n\n    for (size_t k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n            if (A(k, i) == 1) {\n                int root_k = component(k);\n                int root_i = component(i);\n                for (size_t j = 0; j < N; ++j) {\n                    if (component(j) == root_i) {\n                        component(j) = root_k;\n                    }\n                }\n            }\n        });\n        Kokkos::fence();\n    }\n\n    Kokkos::View<int> count(\"count\");\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        if (component(i) == i) {\n            local_count++;\n        }\n    }, count);\n    Kokkos::fence();\n\n    return count();\n}\n",
      "\n    Kokkos::View<int*> component(\"component\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        component(i) = i;\n    });\n    Kokkos::fence();\n\n    for (size_t k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n            if (A(k, i) == 1) {\n                int root_k = component(k);\n                int root_i = component(i);\n                for (size_t j = 0; j < N; ++j) {\n                    if (component(j) == root_i) {\n                        component(j) = root_k;\n                    }\n                }\n            }\n        });\n        Kokkos::fence();\n    }\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        int root = component(i);\n        while (root != component(root)) {\n            root = component(root);\n        }\n        component(i) = root;\n    });\n    Kokkos::fence();\n\n    Kokkos::View<int> count(\"count\");\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        if (component(i) == i) {\n            local_count++;\n        }\n    }, count);\n    Kokkos::fence();\n\n    return count();\n}\n",
      "\n    Kokkos::View<int*> component(\"component\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n        component(i) = i;\n    });\n    Kokkos::fence();\n\n    for (size_t k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n            for (size_t j = 0; j < N; ++j) {\n                if (A(i, j) == 1) {\n                    int root_i = i;\n                    while (component(root_i) != root_i) {\n                        root_i = component(root_i);\n                    }\n                    int root_j = j;\n                    while (component(root_j) != root_j) {\n                        root_j = component(root_j);\n                    }\n                    if (root_i != root_j) {\n                        component(root_j) = root_i;\n                    }\n                }\n            }\n        });\n        Kokkos::fence();\n    }\n\n    int max_count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int count = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (component(j) == i) {\n                count++;\n            }\n        }\n        if (count > max_count) {\n            max_count = count;\n        }\n    }\n\n    return max_count;\n}",
      "\n    Kokkos::View<int*> component(\"component\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        component(i) = i;\n    });\n    Kokkos::fence();\n\n    for (size_t k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n            if (A(k, i) == 1) {\n                int root_k = k;\n                while (component(root_k) != root_k) {\n                    root_k = component(root_k);\n                }\n                int root_i = i;\n                while (component(root_i) != root_i) {\n                    root_i = component(root_i);\n                }\n                if (root_k != root_i) {\n                    component(root_i) = root_k;\n                }\n            }\n        });\n        Kokkos::fence();\n    }\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        int root_i = i;\n        while (component(root_i) != root_i) {\n            root_i = component(root_i);\n        }\n        component(i) = root_i;\n    });\n    Kokkos::fence();\n\n    Kokkos::View<int> count(\"count\");\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& l) {\n        if (component(i) == i) {\n            l++;\n        }\n    }, count);\n    Kokkos::fence();\n\n    return count();\n}",
      "\n    Kokkos::View<int*> component(\"component\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        component(i) = i;\n    });\n    Kokkos::fence();\n\n    for (size_t k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n            for (size_t j = 0; j < N; ++j) {\n                if (A(i, j) == 1) {\n                    int root_i = i;\n                    while (component(root_i) != root_i) {\n                        root_i = component(root_i);\n                    }\n                    int root_j = j;\n                    while (component(root_j) != root_j) {\n                        root_j = component(root_j);\n                    }\n                    if (root_i != root_j) {\n                        component(root_j) = root_i;\n                    }\n                }\n            }\n        });\n        Kokkos::fence();\n    }\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        int root_i = i;\n        while (component(root_i) != root_i) {\n            root_i = component(root_i);\n        }\n        component(i) = root_i;\n    });\n    Kokkos::fence();\n\n    Kokkos::View<int> component_sizes(\"component_sizes\");\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        if (i == component(i)) {\n            Kokkos::atomic_fetch_add(&component_sizes(), 1);\n        }\n    });\n    Kokkos::fence();\n\n    int max_size = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int &local_max) {\n        local_max = Kokkos::max(local_max, component_sizes());\n    }, Kokkos::Max<int>(max_size));\n\n    return max_size;\n}\n",
      "\n    Kokkos::View<int*> component(\"component\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        component(i) = i;\n    });\n    Kokkos::fence();\n\n    for (size_t k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n            if (A(k, i) == 1) {\n                int root_k = k;\n                while (component(root_k) != root_k) {\n                    root_k = component(root_k);\n                }\n                int root_i = i;\n                while (component(root_i) != root_i) {\n                    root_i = component(root_i);\n                }\n                if (root_k != root_i) {\n                    component(root_i) = root_k;\n                }\n            }\n        });\n        Kokkos::fence();\n    }\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        int root = i;\n        while (component(root) != root) {\n            root = component(root);\n        }\n        component(i) = root;\n    });\n    Kokkos::fence();\n\n    Kokkos::View<int> count(\"count\");\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& lcount) {\n        if (component(i) == i) {\n            lcount++;\n        }\n    }, count);\n    Kokkos::fence();\n\n    return count();\n}",
      "\n    Kokkos::View<int*> component(\"component\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        component(i) = i;\n    });\n    Kokkos::fence();\n\n    for (size_t k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n            for (size_t j = 0; j < N; ++j) {\n                if (A(i, j) == 1) {\n                    int root_i = i;\n                    while (component(root_i) != root_i) {\n                        root_i = component(root_i);\n                    }\n                    int root_j = j;\n                    while (component(root_j) != root_j) {\n                        root_j = component(root_j);\n                    }\n                    if (root_i != root_j) {\n                        component(root_j) = root_i;\n                    }\n                }\n            }\n        });\n        Kokkos::fence();\n    }\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        int root_i = i;\n        while (component(root_i) != root_i) {\n            root_i = component(root_i);\n        }\n        component(i) = root_i;\n    });\n    Kokkos::fence();\n\n    Kokkos::View<int> component_sizes(\"component_sizes\");\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_max) {\n        if (component(i) == i) {\n            local_max++;\n        }\n    }, Kokkos::Max<int>(component_sizes));\n\n    return component_sizes();\n}"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "16_graph_largest_component",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int maxComponentSize = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < N; ++i) {\n        int componentSize = 0;\n        for (int j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                #pragma omp atomic\n                componentSize++;\n            }\n        }\n        #pragma omp critical\n        {\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n    return maxComponentSize;\n}\n",
      "\n    int largest = 0;\n    #pragma omp parallel for reduction(max:largest)\n    for (size_t i = 0; i < N; ++i) {\n        int count = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > largest) {\n            largest = count;\n        }\n    }\n    return largest;\n}\n",
      "\n    int maxComponentSize = 0;\n    #pragma omp parallel for reduction(max:maxComponentSize)\n    for (size_t i = 0; i < N; ++i) {\n        int componentSize = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                #pragma omp atomic\n                componentSize++;\n            }\n        }\n        if (componentSize > maxComponentSize) {\n            maxComponentSize = componentSize;\n        }\n    }\n    return maxComponentSize;\n}\n",
      "\n    int maxComponentSize = 0;\n    #pragma omp parallel for reduction(max:maxComponentSize)\n    for (size_t i = 0; i < N; ++i) {\n        int componentSize = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                #pragma omp atomic\n                componentSize++;\n            }\n        }\n        if (componentSize > maxComponentSize) {\n            maxComponentSize = componentSize;\n        }\n    }\n    return maxComponentSize;\n}\n",
      "\n    int largest = 0;\n    #pragma omp parallel for reduction(max:largest)\n    for (size_t i = 0; i < N; ++i) {\n        int count = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > largest) {\n            largest = count;\n        }\n    }\n    return largest;\n}\n",
      "\n    int maxComponentSize = 0;\n    #pragma omp parallel for reduction(max:maxComponentSize)\n    for (size_t i = 0; i < N; ++i) {\n        int componentSize = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                #pragma omp atomic\n                componentSize++;\n            }\n        }\n        if (componentSize > maxComponentSize) {\n            maxComponentSize = componentSize;\n        }\n    }\n    return maxComponentSize;\n}\n",
      "\n    int largest = 0;\n    #pragma omp parallel for reduction(max:largest)\n    for (size_t i = 0; i < N; ++i) {\n        int count = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > largest) {\n            largest = count;\n        }\n    }\n    return largest;\n}\n",
      "\n    int maxComponentSize = 0;\n    #pragma omp parallel for reduction(max:maxComponentSize)\n    for (size_t i = 0; i < N; ++i) {\n        int componentSize = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                #pragma omp atomic\n                componentSize++;\n            }\n        }\n        if (componentSize > maxComponentSize) {\n            maxComponentSize = componentSize;\n        }\n    }\n    return maxComponentSize;\n}\n",
      "\n    int maxComponentSize = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        int componentSize = 0;\n        for (int j = 0; j < N; j++) {\n            if (A[i * N + j] == 1) {\n                componentSize++;\n            }\n        }\n        #pragma omp critical\n        {\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n    return maxComponentSize;\n}\n",
      "\n    int maxComponentSize = 0;\n    #pragma omp parallel for reduction(max:maxComponentSize)\n    for (int i = 0; i < N; ++i) {\n        int componentSize = 0;\n        for (int j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                #pragma omp atomic\n                componentSize++;\n            }\n        }\n        if (componentSize > maxComponentSize) {\n            maxComponentSize = componentSize;\n        }\n    }\n    return maxComponentSize;\n}\n",
      "\n    int largest = 0;\n    #pragma omp parallel for reduction(max:largest)\n    for (size_t i = 0; i < N; ++i) {\n        int count = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > largest) {\n            largest = count;\n        }\n    }\n    return largest;\n}\n",
      "\n    int largest = 0;\n    #pragma omp parallel for reduction(max:largest)\n    for (size_t i = 0; i < N; ++i) {\n        int count = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > largest) {\n            largest = count;\n        }\n    }\n    return largest;\n}\n",
      "\n    int maxComponentSize = 0;\n    #pragma omp parallel for reduction(max:maxComponentSize)\n    for (size_t i = 0; i < N; ++i) {\n        int componentSize = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                componentSize++;\n            }\n        }\n        if (componentSize > maxComponentSize) {\n            maxComponentSize = componentSize;\n        }\n    }\n    return maxComponentSize;\n}\n",
      "\n    int largest = 0;\n    #pragma omp parallel for reduction(max:largest)\n    for (size_t i = 0; i < N; ++i) {\n        int count = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > largest) {\n            largest = count;\n        }\n    }\n    return largest;\n}\n",
      "\n    int maxComponentSize = 0;\n    #pragma omp parallel for reduction(max:maxComponentSize)\n    for (size_t i = 0; i < N; ++i) {\n        int componentSize = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                componentSize++;\n            }\n        }\n        if (componentSize > maxComponentSize) {\n            maxComponentSize = componentSize;\n        }\n    }\n    return maxComponentSize;\n}\n",
      "\n    int maxComponentSize = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < N; ++i) {\n        int componentSize = 0;\n        for (int j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                componentSize++;\n            }\n        }\n        #pragma omp critical\n        {\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n    return maxComponentSize;\n}\n",
      "\n    int maxComponentSize = 0;\n    #pragma omp parallel for reduction(max:maxComponentSize)\n    for (size_t i = 0; i < N; ++i) {\n        int componentSize = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                #pragma omp atomic\n                componentSize++;\n            }\n        }\n        if (componentSize > maxComponentSize) {\n            maxComponentSize = componentSize;\n        }\n    }\n    return maxComponentSize;\n}\n",
      "\n    int largest = 0;\n    #pragma omp parallel for reduction(max:largest)\n    for (size_t i = 0; i < N; ++i) {\n        int count = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > largest) {\n            largest = count;\n        }\n    }\n    return largest;\n}\n",
      "\n    int maxComponentSize = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < N; ++i) {\n        int componentSize = 0;\n        for (int j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                #pragma omp atomic\n                componentSize++;\n            }\n        }\n        #pragma omp critical\n        {\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n    return maxComponentSize;\n}\n",
      "\n    int maxComponentSize = 0;\n    #pragma omp parallel for reduction(max:maxComponentSize)\n    for (size_t i = 0; i < N; ++i) {\n        int componentSize = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                #pragma omp atomic\n                componentSize++;\n            }\n        }\n        if (componentSize > maxComponentSize) {\n            maxComponentSize = componentSize;\n        }\n    }\n    return maxComponentSize;\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "16_graph_largest_component",
    "parallelism_model": "serial",
    "prompt": "/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    std::vector<bool> visited(N, false);\n    int maxComponentSize = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int componentSize = 0;\n            std::queue<int> q;\n            q.push(i);\n            visited[i] = true;\n\n            while (!q.empty()) {\n                int curr = q.front();\n                q.pop();\n                componentSize++;\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[curr * N + j] && !visited[j]) {\n                        q.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n\n    return maxComponentSize;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int maxComponentSize = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int componentSize = 0;\n            std::queue<int> q;\n            q.push(i);\n            visited[i] = true;\n\n            while (!q.empty()) {\n                int current = q.front();\n                q.pop();\n                ++componentSize;\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[current * N + j] && !visited[j]) {\n                        q.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n\n    return maxComponentSize;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int maxComponentSize = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int componentSize = 0;\n            std::queue<int> q;\n            q.push(i);\n            visited[i] = true;\n\n            while (!q.empty()) {\n                int node = q.front();\n                q.pop();\n                componentSize++;\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        q.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n\n    return maxComponentSize;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int maxComponentSize = 0;\n\n    for (int i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int componentSize = 0;\n            std::queue<int> q;\n            q.push(i);\n            visited[i] = true;\n\n            while (!q.empty()) {\n                int node = q.front();\n                q.pop();\n                componentSize++;\n\n                for (int j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        q.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n\n    return maxComponentSize;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int maxComponentSize = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int componentSize = 0;\n            std::queue<int> q;\n            q.push(i);\n            visited[i] = true;\n\n            while (!q.empty()) {\n                int current = q.front();\n                q.pop();\n                componentSize++;\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[current * N + j] && !visited[j]) {\n                        q.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n\n    return maxComponentSize;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int maxComponentSize = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int componentSize = 0;\n            std::queue<int> q;\n            q.push(i);\n            visited[i] = true;\n\n            while (!q.empty()) {\n                int current = q.front();\n                q.pop();\n                componentSize++;\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[current * N + j] && !visited[j]) {\n                        q.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n\n            maxComponentSize = std::max(maxComponentSize, componentSize);\n        }\n    }\n\n    return maxComponentSize;\n}",
      "\n    std::vector<bool> visited(N, false);\n    int maxComponentSize = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int componentSize = 0;\n            std::queue<int> q;\n            q.push(i);\n            visited[i] = true;\n\n            while (!q.empty()) {\n                int current = q.front();\n                q.pop();\n                componentSize++;\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[current * N + j] && !visited[j]) {\n                        q.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n\n    return maxComponentSize;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int maxComponentSize = 0;\n    \n    for (int i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int componentSize = 0;\n            std::queue<int> q;\n            q.push(i);\n            visited[i] = true;\n            \n            while (!q.empty()) {\n                int node = q.front();\n                q.pop();\n                componentSize++;\n                \n                for (int j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        q.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n            \n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n    \n    return maxComponentSize;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int maxComponentSize = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int componentSize = 0;\n            std::queue<int> q;\n            q.push(i);\n            visited[i] = true;\n\n            while (!q.empty()) {\n                int node = q.front();\n                q.pop();\n                componentSize++;\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        q.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n\n    return maxComponentSize;\n}",
      "\n    std::vector<bool> visited(N, false);\n    int maxComponentSize = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int componentSize = 0;\n            std::queue<int> q;\n            q.push(i);\n            visited[i] = true;\n\n            while (!q.empty()) {\n                int current = q.front();\n                q.pop();\n                ++componentSize;\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[current * N + j] && !visited[j]) {\n                        q.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n\n    return maxComponentSize;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int maxComponentSize = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int componentSize = 0;\n            std::queue<int> q;\n            q.push(i);\n            visited[i] = true;\n\n            while (!q.empty()) {\n                int node = q.front();\n                q.pop();\n                componentSize++;\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        q.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n\n    return maxComponentSize;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int maxComponentSize = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int componentSize = 0;\n            std::queue<int> q;\n            q.push(i);\n            visited[i] = true;\n\n            while (!q.empty()) {\n                int current = q.front();\n                q.pop();\n                componentSize++;\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[current * N + j] && !visited[j]) {\n                        q.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n\n    return maxComponentSize;\n}",
      "\n    std::vector<bool> visited(N, false);\n    int maxComponentSize = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int componentSize = 0;\n            std::queue<int> q;\n            q.push(i);\n            visited[i] = true;\n\n            while (!q.empty()) {\n                int current = q.front();\n                q.pop();\n                componentSize++;\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[current * N + j] && !visited[j]) {\n                        q.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n\n    return maxComponentSize;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int maxComponentSize = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int componentSize = 0;\n            std::queue<int> q;\n            q.push(i);\n            visited[i] = true;\n\n            while (!q.empty()) {\n                int current = q.front();\n                q.pop();\n                ++componentSize;\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[current * N + j] && !visited[j]) {\n                        q.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n\n    return maxComponentSize;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int maxComponentSize = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int componentSize = 0;\n            std::queue<int> q;\n            q.push(i);\n            visited[i] = true;\n\n            while (!q.empty()) {\n                int node = q.front();\n                q.pop();\n                componentSize++;\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        q.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n\n    return maxComponentSize;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int maxComponentSize = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int componentSize = 0;\n            std::queue<int> q;\n            q.push(i);\n            visited[i] = true;\n\n            while (!q.empty()) {\n                int currentVertex = q.front();\n                q.pop();\n                componentSize++;\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[currentVertex * N + j] && !visited[j]) {\n                        q.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n\n    return maxComponentSize;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int maxComponentSize = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int componentSize = 0;\n            std::queue<int> q;\n            q.push(i);\n            visited[i] = true;\n\n            while (!q.empty()) {\n                int node = q.front();\n                q.pop();\n                componentSize++;\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        q.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n\n    return maxComponentSize;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int maxComponentSize = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int componentSize = 0;\n            std::queue<int> q;\n            q.push(i);\n            visited[i] = true;\n\n            while (!q.empty()) {\n                int current = q.front();\n                q.pop();\n                componentSize++;\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[current * N + j] && !visited[j]) {\n                        q.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n\n    return maxComponentSize;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int maxComponentSize = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int componentSize = 0;\n            std::queue<int> q;\n            q.push(i);\n            visited[i] = true;\n\n            while (!q.empty()) {\n                int node = q.front();\n                q.pop();\n                ++componentSize;\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        q.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n\n    return maxComponentSize;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int maxComponentSize = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            int componentSize = 0;\n            std::queue<int> q;\n            q.push(i);\n            visited[i] = true;\n\n            while (!q.empty()) {\n                int node = q.front();\n                q.pop();\n                ++componentSize;\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        q.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n\n            if (componentSize > maxComponentSize) {\n                maxComponentSize = componentSize;\n            }\n        }\n    }\n\n    return maxComponentSize;\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "16_graph_largest_component",
    "parallelism_model": "hip",
    "prompt": "/* Compute the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   Store the result in largestComponentSize.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\n__global__ void largestComponent(const int *A, size_t N, int *largestComponentSize) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        \n        // Depth-first search to find the size of the component\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        \n        delete[] visited;\n        \n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n\n        for (size_t i = 0; i < N; ++i) {\n            if (!visited[i]) {\n                componentSize++;\n                visited[i] = true;\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[i * N + j] && !visited[j]) {\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n\n        delete[] visited;\n\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] == 1 && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(idx);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n\n        for (size_t j = 0; j < N; ++j) {\n            if (A[tid * N + j] == 1 && !visited[j]) {\n                componentSize++;\n                visited[j] = true;\n                for (size_t k = 0; k < N; ++k) {\n                    if (A[j * N + k] == 1 && !visited[k]) {\n                        componentSize++;\n                        visited[k] = true;\n                    }\n                }\n            }\n        }\n\n        delete[] visited;\n\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        \n        for (size_t i = 0; i < N; ++i) {\n            if (!visited[i]) {\n                componentSize++;\n                visited[i] = true;\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[i * N + j] == 1 && !visited[j]) {\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n        \n        delete[] visited;\n        \n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        \n        for (size_t j = 0; j < N; ++j) {\n            if (A[tid * N + j] == 1 && !visited[j]) {\n                componentSize++;\n                visited[j] = true;\n                for (size_t k = 0; k < N; ++k) {\n                    if (A[j * N + k] == 1 && !visited[k]) {\n                        visited[k] = true;\n                    }\n                }\n            }\n        }\n        \n        delete[] visited;\n        \n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        \n        for (size_t i = 0; i < N; ++i) {\n            if (!visited[i]) {\n                componentSize++;\n                visited[i] = true;\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[i * N + j] && !visited[j]) {\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n        \n        delete[] visited;\n        \n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "16_graph_largest_component",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int maxComponent = 0;\n    int localMaxComponent = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < N; i += size) {\n        std::vector<bool> visited(N, false);\n        std::queue<int> queue;\n        int componentSize = 0;\n\n        queue.push(i);\n        visited[i] = true;\n\n        while (!queue.empty()) {\n            int current = queue.front();\n            queue.pop();\n            componentSize++;\n\n            for (int j = 0; j < N; ++j) {\n                if (A[current * N + j] && !visited[j]) {\n                    queue.push(j);\n                    visited[j] = true;\n                }\n            }\n        }\n\n        localMaxComponent = std::max(localMaxComponent, componentSize);\n    }\n\n    MPI_Reduce(&localMaxComponent, &maxComponent, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return maxComponent;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localMax = 0;\n    int globalMax = 0;\n\n    // Calculate the number of vertices in the largest component for the local portion of the adjacency matrix\n    for (size_t i = 0; i < N; ++i) {\n        int count = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > localMax) {\n            localMax = count;\n        }\n    }\n\n    // Reduce the localMax values from all ranks to find the global maximum\n    MPI_Reduce(&localMax, &globalMax, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    // Return the global maximum from rank 0\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMax;\n    } else {\n        return 0;\n    }\n}",
      "\n    int largestComponent(const std::vector<int>& A, size_t N) {\n        int localMax = 0;\n        // Calculate localMax using A and N\n        // Use MPI to find the maximum localMax across all ranks\n        // Return the maximum localMax on rank 0\n    }\n}",
      "\n    \n        int localMax = 0;\n        // Calculate localMax using A and N\n\n        int globalMax;\n        MPI_Reduce(&localMax, &globalMax, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n        if (MPI::COMM_WORLD.Get_rank() == 0) {\n            return globalMax;\n        } else {\n            return 0;\n        }\n    }\n",
      "\n    \n        // TODO: Implement the body of the function\n    }\n",
      "\n    int largestComponent(const std::vector<int>& A, size_t N) {\n        int maxComponent = 0;\n        // Your implementation using MPI to compute the largest component goes here\n        return maxComponent;\n    }\n}",
      "\n    int largestComponent(const std::vector<int>& A, size_t N) {\n        // TODO: Implement the body of the function\n    }\n}",
      "\n    int localMax = 0;\n    int globalMax = 0;\n\n    // Calculate localMax using A and N\n\n    // Use MPI_Reduce to find the globalMax on rank 0\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMax;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int maxComponent = 0;\n    // Your code here\n    // Implement the parallel algorithm to find the largest component using MPI\n    // Each rank should work on a part of the graph and then the results should be combined on rank 0\n    // Use MPI functions such as MPI_Bcast, MPI_Scatter, MPI_Gather, and MPI_Reduce as needed\n    // Remember to handle the case where the graph is disconnected\n    // The final result should be stored in maxComponent\n    return maxComponent;\n}\n",
      "\n    int localMax = 0;\n    int globalMax = 0;\n\n    for (int i = 0; i < N; ++i) {\n        int count = 0;\n        for (int j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > localMax) {\n            localMax = count;\n        }\n    }\n\n    MPI_Reduce(&localMax, &globalMax, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return globalMax;\n}\n",
      "\n    int largestComponent(const std::vector<int>& A, size_t N) {\n        int localMax = 0;\n        // Calculate localMax using A and N\n        // Use MPI to find the global maximum and store it in localMax\n        // Return localMax on rank 0\n    }\n}",
      "\n    int localMax = 0;\n    int globalMax = 0;\n\n    // Calculate localMax for each rank\n    for (int i = 0; i < N; ++i) {\n        int count = 0;\n        for (int j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > localMax) {\n            localMax = count;\n        }\n    }\n\n    // Reduce localMax values to get globalMax on rank 0\n    MPI_Reduce(&localMax, &globalMax, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    // Return globalMax on rank 0\n    if (rank == 0) {\n        return globalMax;\n    } else {\n        return 0;\n    }\n}",
      "\n    int localMax = 0;\n    int globalMax = 0;\n\n    // Calculate localMax for each rank\n    for (int i = 0; i < N; ++i) {\n        int count = 0;\n        for (int j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > localMax) {\n            localMax = count;\n        }\n    }\n\n    // Reduce localMax values to get globalMax on rank 0\n    MPI_Reduce(&localMax, &globalMax, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMax;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int largestComponent(const std::vector<int>& A, size_t N) {\n        int localMax = 0;\n        // Calculate localMax for each rank\n        for (size_t i = 0; i < N; ++i) {\n            int count = 0;\n            for (size_t j = 0; j < N; ++j) {\n                if (A[i * N + j] == 1) {\n                    count++;\n                }\n            }\n            if (count > localMax) {\n                localMax = count;\n            }\n        }\n\n        // Reduce localMax values to find the global maximum\n        int globalMax;\n        MPI_Reduce(&localMax, &globalMax, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n        // Return the global maximum on rank 0\n        if (rank == 0) {\n            return globalMax;\n        } else {\n            return 0;\n        }\n    }\n}",
      "\n    int localMax = 0;\n    int globalMax = 0;\n\n    // Calculate localMax for each rank\n    for (int i = 0; i < N; ++i) {\n        int count = 0;\n        for (int j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > localMax) {\n            localMax = count;\n        }\n    }\n\n    // Reduce localMax values to get globalMax on rank 0\n    MPI_Reduce(&localMax, &globalMax, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    // Return globalMax on rank 0\n    if (rank == 0) {\n        return globalMax;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    \n        // Your code here\n    }\n",
      "\n    int localMax = 0;\n    // Calculate localMax using A and N\n\n    int globalMax;\n    MPI_Reduce(&localMax, &globalMax, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMax;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localMax = 0;\n    // Calculate localMax using A and N\n\n    int globalMax;\n    MPI_Reduce(&localMax, &globalMax, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMax;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "16_graph_largest_component",
    "parallelism_model": "cuda",
    "prompt": "/* Compute the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   Store the result in largestComponentSize.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\n__global__ void largestComponent(const int *A, size_t N, int *largestComponentSize) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n\n        // Depth-first search to find the size of the component\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n\n        delete[] visited;\n\n        // Update largestComponentSize using atomicMax to handle concurrent updates\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] == 1 && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n\n        int maxSize = 0;\n        for (size_t i = 0; i < N; ++i) {\n            if (!visited[i]) {\n                int size = 1;\n                visited[i] = true;\n                int stack[N];\n                int top = -1;\n                stack[++top] = i;\n\n                while (top >= 0) {\n                    int v = stack[top--];\n                    for (size_t j = 0; j < N; ++j) {\n                        if (A[v * N + j] && !visited[j]) {\n                            visited[j] = true;\n                            stack[++top] = j;\n                            size++;\n                        }\n                    }\n                }\n                if (size > maxSize) {\n                    maxSize = size;\n                }\n            }\n        }\n        *largestComponentSize = maxSize;\n        delete[] visited;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n\n        for (size_t j = 0; j < N; ++j) {\n            if (A[tid * N + j] == 1 && !visited[j]) {\n                componentSize++;\n                visited[j] = true;\n                for (size_t k = 0; k < N; ++k) {\n                    if (A[j * N + k] == 1 && !visited[k]) {\n                        visited[k] = true;\n                    }\n                }\n            }\n        }\n\n        delete[] visited;\n\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int vertex = stack.top();\n            stack.pop();\n            if (!visited[vertex]) {\n                visited[vertex] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[vertex * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; i++) {\n            visited[i] = false;\n        }\n        \n        // Depth-first search to find the size of the connected component\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; i++) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        \n        delete[] visited;\n        \n        // Update the largest component size if necessary\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] == 1 && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        \n        // Depth-first search to find the size of the component\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        \n        delete[] visited;\n        \n        // Update the largestComponentSize using atomic operation\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n\n        for (size_t j = 0; j < N; ++j) {\n            if (A[tid * N + j] == 1 && !visited[j]) {\n                componentSize++;\n                visited[j] = true;\n                int stack[N];\n                int top = -1;\n                stack[++top] = j;\n                while (top >= 0) {\n                    int current = stack[top--];\n                    for (size_t k = 0; k < N; ++k) {\n                        if (A[current * N + k] == 1 && !visited[k]) {\n                            componentSize++;\n                            visited[k] = true;\n                            stack[++top] = k;\n                        }\n                    }\n                }\n            }\n        }\n        delete[] visited;\n\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (int i = 0; i < N; i++) {\n            visited[i] = false;\n        }\n        \n        for (int j = 0; j < N; j++) {\n            if (A[tid * N + j] == 1 && !visited[j]) {\n                componentSize++;\n                visited[j] = true;\n                int stack[N];\n                int top = -1;\n                stack[++top] = j;\n                while (top >= 0) {\n                    int vertex = stack[top--];\n                    for (int k = 0; k < N; k++) {\n                        if (A[vertex * N + k] == 1 && !visited[k]) {\n                            componentSize++;\n                            visited[k] = true;\n                            stack[++top] = k;\n                        }\n                    }\n                }\n            }\n        }\n        \n        delete[] visited;\n        \n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        \n        // Depth-first search to find the size of the component\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        \n        delete[] visited;\n        \n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] == 1 && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] == 1 && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int componentSize = 0;\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n        std::stack<int> stack;\n        stack.push(tid);\n        while (!stack.empty()) {\n            int current = stack.top();\n            stack.pop();\n            if (!visited[current]) {\n                visited[current] = true;\n                componentSize++;\n                for (size_t i = 0; i < N; ++i) {\n                    if (A[current * N + i] && !visited[i]) {\n                        stack.push(i);\n                    }\n                }\n            }\n        }\n        delete[] visited;\n        atomicMax(largestComponentSize, componentSize);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "16_graph_largest_component",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int largest = 0;\n    #pragma omp parallel for reduction(max:largest)\n    for (int i = 0; i < N; ++i) {\n        int count = 0;\n        for (int j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > largest) {\n            largest = count;\n        }\n    }\n    int global_largest;\n    MPI_Reduce(&largest, &global_largest, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return global_largest;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int largest = 0;\n    int local_largest = 0;\n\n    #pragma omp parallel for reduction(max:local_largest)\n    for (size_t i = 0; i < N; ++i) {\n        int count = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > local_largest) {\n            local_largest = count;\n        }\n    }\n\n    MPI_Reduce(&local_largest, &largest, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return largest;\n}\n",
      "\n    int largest = 0;\n    int local_largest = 0;\n\n    #pragma omp parallel for reduction(max:local_largest)\n    for (int i = 0; i < N; ++i) {\n        int count = 0;\n        for (int j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > local_largest) {\n            local_largest = count;\n        }\n    }\n\n    MPI_Reduce(&local_largest, &largest, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return largest;\n}\n",
      "\n    int largest = 0;\n    int local_largest = 0;\n\n    #pragma omp parallel for reduction(max:local_largest)\n    for (size_t i = 0; i < N; ++i) {\n        int count = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > local_largest) {\n            local_largest = count;\n        }\n    }\n\n    MPI_Reduce(&local_largest, &largest, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return largest;\n}\n",
      "\n    int largest = 0;\n    int localLargest = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for reduction(max : localLargest)\n    for (int i = rank; i < N; i += size) {\n        int componentSize = 0;\n        std::vector<bool> visited(N, false);\n        std::queue<int> q;\n        q.push(i);\n        while (!q.empty()) {\n            int node = q.front();\n            q.pop();\n            if (!visited[node]) {\n                visited[node] = true;\n                componentSize++;\n                for (int j = 0; j < N; ++j) {\n                    if (A[i * N + j] && !visited[j]) {\n                        q.push(j);\n                    }\n                }\n            }\n        }\n        localLargest = std::max(localLargest, componentSize);\n    }\n\n    MPI_Reduce(&localLargest, &largest, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return largest;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int largest = 0;\n    int local_largest = 0;\n\n    #pragma omp parallel for reduction(max:local_largest)\n    for (size_t i = 0; i < N; ++i) {\n        int count = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > local_largest) {\n            local_largest = count;\n        }\n    }\n\n    MPI_Reduce(&local_largest, &largest, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return largest;\n}\n",
      "\n    \n        int largest = 0;\n        // Your parallel computation code using MPI and OpenMP goes here\n        // Update the value of 'largest' with the number of vertices in the largest component\n        return largest;\n    }\n",
      "\n    \n        int largest = 0;\n        // Your parallel computation code using MPI and OpenMP goes here\n\n        return largest;\n    }\n",
      "\n    int largest = 0;\n    int local_largest = 0;\n\n    #pragma omp parallel for reduction(max:local_largest)\n    for (size_t i = 0; i < N; ++i) {\n        int count = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > local_largest) {\n            local_largest = count;\n        }\n    }\n\n    MPI_Reduce(&local_largest, &largest, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return largest;\n}\n",
      "\n    int largest = 0;\n    #pragma omp parallel for reduction(max:largest)\n    for (int i = 0; i < N; ++i) {\n        int count = 0;\n        for (int j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > largest) {\n            largest = count;\n        }\n    }\n\n    int global_largest;\n    MPI_Reduce(&largest, &global_largest, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return global_largest;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int largest = 0;\n    #pragma omp parallel for reduction(max:largest)\n    for (int i = 0; i < N; i++) {\n        int count = 0;\n        for (int j = 0; j < N; j++) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > largest) {\n            largest = count;\n        }\n    }\n    int global_largest;\n    MPI_Reduce(&largest, &global_largest, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return global_largest;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int largest = 0;\n    int localLargest = 0;\n\n    #pragma omp parallel for reduction(+:localLargest)\n    for (int i = 0; i < N; ++i) {\n        int count = 0;\n        for (int j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > localLargest) {\n            localLargest = count;\n        }\n    }\n\n    MPI_Reduce(&localLargest, &largest, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return largest;\n}\n",
      "\n    int largest = 0;\n    int local_largest = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for reduction(max:local_largest)\n    for (int i = rank; i < N; i += size) {\n        int count = 0;\n        for (int j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > local_largest) {\n            local_largest = count;\n        }\n    }\n\n    MPI_Reduce(&local_largest, &largest, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return largest;\n    } else {\n        return 0;\n    }\n}",
      "\n    int largest = 0;\n    #pragma omp parallel for reduction(max:largest)\n    for (int i = 0; i < N; ++i) {\n        int count = 0;\n        for (int j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > largest) {\n            largest = count;\n        }\n    }\n\n    int global_largest;\n    MPI_Reduce(&largest, &global_largest, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return global_largest;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int largest = 0;\n    int local_largest = 0;\n\n    #pragma omp parallel for reduction(max:local_largest)\n    for (size_t i = 0; i < N; ++i) {\n        int count = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n        if (count > local_largest) {\n            local_largest = count;\n        }\n    }\n\n    MPI_Reduce(&local_largest, &largest, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return largest;\n}\n",
      "\n    \n        int largest = 0;\n        // Your parallel computation code using MPI and OpenMP goes here\n\n        return largest;\n    }\n",
      "\n    int largest = 0;\n    // Your parallel computation code using MPI and OpenMP goes here\n    // Update the value of 'largest' with the number of vertices in the largest component\n    return largest;\n}\n",
      "\n    // TODO: Implement the body of the function\n    // Your code here\n}\n",
      "\n    int largest = 0;\n    // Your parallel computation code using MPI and OpenMP goes here\n    // Update the 'largest' variable with the size of the largest component found\n    // Return the 'largest' variable on rank 0\n}\n",
      "\n    int largest = 0;\n    // Your parallel computation code using MPI and OpenMP goes here\n\n    return largest;\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "18_graph_count_components",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix. A is an undirected graph.\n\t Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(Kokkos::View<const int**> &A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::View<int*> visited(\"visited\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        visited(i) = 0;\n    });\n    Kokkos::fence();\n\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        if (visited(i) == 0) {\n            dfs(A, visited, N, i);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::View<int*> visited(\"visited\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        if (visited(i) == 0) {\n            count++;\n            Kokkos::View<int*> queue(\"queue\", N);\n            int qFront = 0, qBack = 0;\n            queue(qBack++) = i;\n            visited(i) = 1;\n            while (qFront < qBack) {\n                int node = queue(qFront++);\n                for (int j = 0; j < N; j++) {\n                    if (A(node, j) && visited(j) == 0) {\n                        queue(qBack++) = j;\n                        visited(j) = 1;\n                    }\n                }\n            }\n        }\n    });\n    return count;\n}\n",
      "\nint count = 0;\nKokkos::View<int*> visited(\"visited\", N);\nKokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    if (visited(i) == 0) {\n        count++;\n        Kokkos::View<int*> queue(\"queue\", N);\n        int q_front = 0, q_rear = 0;\n        queue(q_rear++) = i;\n        visited(i) = 1;\n        while (q_front < q_rear) {\n            int u = queue(q_front++);\n            for (int v = 0; v < N; v++) {\n                if (A(u, v) && !visited(v)) {\n                    queue(q_rear++) = v;\n                    visited(v) = 1;\n                }\n            }\n        }\n    }\n});\nreturn count;\n```\n}",
      "\n    Kokkos::View<int*> visited(\"visited\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        visited(i) = 0;\n    });\n    Kokkos::fence();\n\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        if (visited(i) == 0) {\n            Kokkos::parallel_for(Kokkos::RangePolicy<>(0, N), KOKKOS_LAMBDA(const int j) {\n                if (A(i, j) == 1 && visited(j) == 0) {\n                    visited(j) = 1;\n                }\n            });\n            Kokkos::fence();\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n  Kokkos::View<int*> labels(\"labels\", N);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    labels(i) = i;\n  });\n  Kokkos::fence();\n\n  for (size_t k = 0; k < N; ++k) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n      if (A(k, i) == 1) {\n        int root_k = k;\n        while (labels(root_k) != root_k) {\n          root_k = labels(root_k);\n        }\n        int root_i = i;\n        while (labels(root_i) != root_i) {\n          root_i = labels(root_i);\n        }\n        if (root_k != root_i) {\n          labels(root_i) = root_k;\n        }\n      }\n    });\n    Kokkos::fence();\n  }\n\n  int count = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int &local_count) {\n    if (labels(i) == i) {\n      local_count++;\n    }\n  }, count);\n  Kokkos::fence();\n\n  return count;\n}\n",
      "\n    Kokkos::View<int*> visited(\"visited\", N);\n    Kokkos::deep_copy(visited, 0);\n\n    int count = 0;\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        if (visited(i) == 0) {\n            Kokkos::View<int*> queue(\"queue\", N);\n            int front = 0, rear = 0;\n            queue(rear++) = i;\n            visited(i) = 1;\n\n            while (front != rear) {\n                int u = queue(front++);\n                for (int v = 0; v < N; ++v) {\n                    if (A(u, v) && visited(v) == 0) {\n                        queue(rear++) = v;\n                        visited(v) = 1;\n                    }\n                }\n            }\n            count++;\n        }\n    });\n\n    return count;\n}\n",
      "\n    Kokkos::View<int*> visited(\"visited\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        visited(i) = 0;\n    });\n    Kokkos::fence();\n\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        if (visited(i) == 0) {\n            count++;\n            Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n                if (A(i, j) == 1 && visited(j) == 0) {\n                    visited(j) = 1;\n                }\n            });\n            Kokkos::fence();\n        }\n    }\n\n    return count;\n}\n",
      "\n    Kokkos::View<int*> visited(\"visited\", N);\n    Kokkos::deep_copy(visited, 0);\n\n    int count = 0;\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        if (visited(i) == 0) {\n            count++;\n            Kokkos::View<int*> queue(\"queue\", N);\n            int q_front = 0, q_rear = 0;\n            queue(q_rear++) = i;\n            visited(i) = 1;\n\n            while (q_front < q_rear) {\n                int u = queue(q_front++);\n                for (int v = 0; v < N; v++) {\n                    if (A(u, v) && !visited(v)) {\n                        queue(q_rear++) = v;\n                        visited(v) = 1;\n                    }\n                }\n            }\n        }\n    });\n\n    return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::View<int*> visited(\"visited\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        if (visited(i) == 0) {\n            count++;\n            Kokkos::View<int*> queue(\"queue\", N);\n            int front = 0, rear = 0;\n            queue(rear++) = i;\n            visited(i) = 1;\n            while (front != rear) {\n                int u = queue(front++);\n                for (int v = 0; v < N; v++) {\n                    if (A(u, v) && visited(v) == 0) {\n                        queue(rear++) = v;\n                        visited(v) = 1;\n                    }\n                }\n            }\n        }\n    });\n    return count;\n}\n",
      "\n    Kokkos::View<int*> visited(\"visited\", N);\n    Kokkos::deep_copy(visited, 0);\n\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (visited(i) == 0) {\n            dfs(A, visited, i, N);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    Kokkos::View<int*> visited(\"visited\", N);\n    Kokkos::deep_copy(visited, 0);\n\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (visited(i) == 0) {\n            dfs(A, visited, i, N);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    Kokkos::View<int*> visited(\"visited\", N);\n    Kokkos::deep_copy(visited, 0);\n\n    int count = 0;\n\n    for (size_t i = 0; i < N; i++) {\n        if (visited(i) == 0) {\n            count++;\n            Kokkos::parallel_for(\"dfs\", N, KOKKOS_LAMBDA(const int j) {\n                if (A(i, j) == 1 && visited(j) == 0) {\n                    visited(j) = 1;\n                }\n            });\n        }\n    }\n\n    return count;\n}\n",
      "\nint count = 0;\nKokkos::View<int*> visited(\"visited\", N);\nKokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    if (visited(i) == 0) {\n        count++;\n        Kokkos::View<int*> queue(\"queue\", N);\n        int q_front = 0, q_rear = 0;\n        queue(q_rear++) = i;\n        visited(i) = 1;\n        while (q_front < q_rear) {\n            int u = queue(q_front++);\n            for (int v = 0; v < N; v++) {\n                if (A(u, v) && !visited(v)) {\n                    queue(q_rear++) = v;\n                    visited(v) = 1;\n                }\n            }\n        }\n    }\n});\nreturn count;\n```\n}",
      "\n    Kokkos::View<int*> visited(\"visited\", N);\n    Kokkos::deep_copy(visited, 0);\n\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        if (visited(i) == 0) {\n            dfs(A, visited, i, N);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    Kokkos::View<int*> visited(\"visited\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        visited(i) = 0;\n    });\n    Kokkos::fence();\n\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        if (visited(i) == 0) {\n            dfs(A, visited, i, N);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    Kokkos::View<int*> visited(\"visited\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        visited(i) = 0;\n    });\n    Kokkos::fence();\n\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        if (visited(i) == 0) {\n            count++;\n            Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n                if (A(i, j) == 1 && visited(j) == 0) {\n                    visited(j) = 1;\n                }\n            });\n            Kokkos::fence();\n        }\n    }\n    return count;\n}\n",
      "\nint count = 0;\nKokkos::View<int*> visited(\"visited\", N);\nKokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    if (visited(i) == 0) {\n        count++;\n        Kokkos::View<int*> queue(\"queue\", N);\n        int qstart = 0, qend = 0;\n        queue(qend++) = i;\n        visited(i) = 1;\n        while (qstart < qend) {\n            int node = queue(qstart++);\n            for (int j = 0; j < N; j++) {\n                if (A(node, j) && visited(j) == 0) {\n                    queue(qend++) = j;\n                    visited(j) = 1;\n                }\n            }\n        }\n    }\n});\nreturn count;\n```\n}",
      "\nint count = 0;\nKokkos::View<int*> visited(\"visited\", N);\nKokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    if (visited(i) == 0) {\n        count++;\n        Kokkos::View<int*> queue(\"queue\", N);\n        int front = 0, rear = 0;\n        queue(rear++) = i;\n        visited(i) = 1;\n        while (front != rear) {\n            int u = queue(front++);\n            for (int v = 0; v < N; v++) {\n                if (A(u, v) && !visited(v)) {\n                    queue(rear++) = v;\n                    visited(v) = 1;\n                }\n            }\n        }\n    }\n});\nreturn count;\n```\n}",
      "\n    Kokkos::View<int*> visited(\"visited\", N);\n    Kokkos::deep_copy(visited, 0);\n\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (visited(i) == 0) {\n            dfs(A, visited, N, i);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    Kokkos::View<int*> visited(\"visited\", N);\n    Kokkos::deep_copy(visited, 0);\n\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (visited(i) == 0) {\n            dfs(A, visited, N, i);\n            count++;\n        }\n    }\n\n    return count;\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "18_graph_count_components",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n\t Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int count = 0;\n    std::vector<bool> visited(N, false);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                if (!visited[i]) {\n                    dfs(A, N, i, visited);\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    int count = 0;\n    std::vector<bool> visited(N, false);\n\n    #pragma omp parallel for\n    for (int i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                if (!visited[i]) {\n                    dfs(A, N, i, visited);\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    int count = 0;\n    std::vector<bool> visited(N, false);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                if (!visited[i]) {\n                    dfs(A, N, i, visited);\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    int count = 0;\n    std::vector<bool> visited(N, false);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                if (!visited[i]) {\n                    dfs(A, N, i, visited);\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    int count = 0;\n    std::vector<bool> visited(N, false);\n\n    #pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                count++;\n            }\n            dfs(A, N, i, visited);\n        }\n    }\n\n    return count;\n}\n",
      "\n    int count = 0;\n    std::vector<bool> visited(N, false);\n\n    #pragma omp parallel for\n    for (int i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                if (!visited[i]) {\n                    count++;\n                    visited[i] = true;\n                    for (int j = 0; j < N; ++j) {\n                        if (A[i * N + j] && !visited[j]) {\n                            visited[j] = true;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    int count = 0;\n    std::vector<bool> visited(N, false);\n\n    #pragma omp parallel for\n    for (int i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                if (!visited[i]) {\n                    count++;\n                    visited[i] = true;\n                    for (int j = 0; j < N; ++j) {\n                        if (A[i * N + j] && !visited[j]) {\n                            visited[j] = true;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    int count = 0;\n    std::vector<bool> visited(N, false);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                if (!visited[i]) {\n                    dfs(A, N, i, visited);\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    int count = 0;\n    std::vector<bool> visited(N, false);\n\n    #pragma omp parallel for\n    for (int i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                if (!visited[i]) {\n                    dfs(A, N, i, visited);\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    int count = 0;\n    std::vector<bool> visited(N, false);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                if (!visited[i]) {\n                    dfs(A, N, i, visited);\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    int count = 0;\n    std::vector<bool> visited(N, false);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                if (!visited[i]) {\n                    dfs(A, N, i, visited);\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    int count = 0;\n    std::vector<bool> visited(N, false);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                if (!visited[i]) {\n                    dfs(A, N, i, visited);\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    int count = 0;\n    std::vector<bool> visited(N, false);\n\n    #pragma omp parallel for\n    for (int i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                if (!visited[i]) {\n                    dfs(A, N, i, visited);\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    int count = 0;\n    std::vector<bool> visited(N, false);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                if (!visited[i]) {\n                    dfs(A, N, i, visited);\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    int count = 0;\n    std::vector<bool> visited(N, false);\n\n    #pragma omp parallel for\n    for (int i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                count++;\n            }\n            dfs(A, N, i, visited);\n        }\n    }\n\n    return count;\n}\n",
      "\n    int count = 0;\n    std::vector<bool> visited(N, false);\n\n    #pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                count++;\n            }\n            dfs(A, N, i, visited);\n        }\n    }\n\n    return count;\n}\n",
      "\n    int count = 0;\n    std::vector<bool> visited(N, false);\n\n    #pragma omp parallel for\n    for (int i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                if (!visited[i]) {\n                    dfs(A, N, i, visited);\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    int count = 0;\n    std::vector<bool> visited(N, false);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                if (!visited[i]) {\n                    dfs(A, N, i, visited);\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    int count = 0;\n    std::vector<bool> visited(N, false);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                if (!visited[i]) {\n                    dfs(A, N, i, visited);\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    int count = 0;\n    std::vector<bool> visited(N, false);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                if (!visited[i]) {\n                    dfs(A, N, i, visited);\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "18_graph_count_components",
    "parallelism_model": "serial",
    "prompt": "/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            dfs(A, N, i, visited);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            dfs(A, N, i, visited);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            dfs(A, visited, i, N);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            dfs(A, N, i, visited);\n            ++count;\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            dfs(A, N, i, visited);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            dfs(A, N, visited, i);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            dfs(A, N, i, visited);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            dfs(A, visited, i, N);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            dfs(A, N, i, visited);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            dfs(A, visited, i, N);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            dfs(A, N, i, visited);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            dfs(A, visited, i, N);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            dfs(A, N, i, visited);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            dfs(A, N, i, visited);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            dfs(A, visited, i, N);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            dfs(A, visited, i, N);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n    \n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            dfs(A, N, i, visited);\n            ++count;\n        }\n    }\n    \n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            dfs(A, N, i, visited);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            dfs(A, visited, i, N);\n            count++;\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            dfs(A, N, i, visited);\n            count++;\n        }\n    }\n\n    return count;\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "18_graph_count_components",
    "parallelism_model": "hip",
    "prompt": "/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n\t Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\n__global__ void componentCount(const int *A, size_t N, int *numComponents) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (A[tid * N + tid] == 1) {\n            atomicAdd(numComponents, 1);\n            A[tid * N + tid] = 0;\n        }\n        __syncthreads();\n\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                A[tid * N + i] = 0;\n                A[i * N + tid] = 0;\n                componentCount<<<1, N>>>(A, N, numComponents);\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (A[tid * N + tid] == 1) {\n            atomicAdd(numComponents, 1);\n            A[tid * N + tid] = 0;\n        }\n        for (int i = tid + 1; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                A[tid * N + i] = 0;\n                A[i * N + tid] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (A[tid * N + tid] == 1) {\n            atomicAdd(numComponents, 1);\n            A[tid * N + tid] = 0;\n        }\n        for (int i = tid + 1; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                A[tid * N + i] = 0;\n                A[i * N + tid] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + j] == 1 && A[tid * N + j] == 0) {\n                        A[tid * N + j] = 1;\n                        A[j * N + tid] = 1;\n                    }\n                }\n            }\n        }\n    }\n\n    __syncthreads();\n\n    if (tid == 0) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[i * N + i] == 1) {\n                count++;\n            }\n        }\n        *numComponents = count;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool *visited = new bool[N];\n        for (size_t i = 0; i < N; ++i) {\n            visited[i] = false;\n        }\n\n        int count = 0;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (!visited[i]) {\n                dfs(A, N, i, visited);\n                count++;\n            }\n        }\n\n        atomicAdd(numComponents, count);\n        delete[] visited;\n    }\n}\n\n__device__ void dfs(const int *A, size_t N, int v, bool *visited) {\n    visited[v] = true;\n    for (size_t i = 0; i < N; ++i) {\n        if (A[v * N + i] && !visited[i]) {\n            dfs(A, N, i, visited);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + j] == 1 && A[tid * N + j] == 0) {\n                        A[tid * N + j] = 1;\n                        A[j * N + tid] = 1;\n                        i = -1;\n                        break;\n                    }\n                }\n            }\n        }\n    }\n    __syncthreads();\n\n    if (tid == 0) {\n        *numComponents = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[i * N + i] == 1) {\n                (*numComponents)++;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + j] == 1 && A[tid * N + j] == 0) {\n                        A[tid * N + j] = 1;\n                    }\n                }\n            }\n        }\n    }\n\n    if (tid == 0) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[i * N + i] == 1) {\n                count++;\n            }\n        }\n        *numComponents = count;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (A[tid * N + tid] == 1) {\n            atomicAdd(numComponents, 1);\n            A[tid * N + tid] = 0;\n        }\n        for (int i = tid + 1; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                A[tid * N + i] = 0;\n                A[i * N + tid] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                for (int j = 0; j < N; ++j) {\n                    if (A[i * N + j] == 1 && A[tid * N + j] == 0) {\n                        A[tid * N + j] = 1;\n                    }\n                }\n            }\n        }\n    }\n\n    __syncthreads();\n\n    if (tid == 0) {\n        int count = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[i * N + i] == 1) {\n                count++;\n            }\n        }\n        *numComponents = count;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (A[tid * N + tid] == 1) {\n            atomicAdd(numComponents, 1);\n            for (int i = tid + 1; i < N; i++) {\n                if (A[tid * N + i] == 1) {\n                    A[i * N + i] = 0;\n                    A[i * N + tid] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (A[tid * N + tid] == 1) {\n            atomicAdd(numComponents, 1);\n            for (int j = tid + 1; j < N; j++) {\n                if (A[tid * N + j] == 1) {\n                    A[j * N + j] = 0;\n                    A[j * N + tid] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (A[tid * N + tid] == 1) {\n            atomicAdd(numComponents, 1);\n            A[tid * N + tid] = 0;\n        }\n        for (int i = tid + 1; i < N; i++) {\n            if (A[tid * N + i] == 1 && A[i * N + i] == 1) {\n                A[i * N + i] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (A[tid * N + tid] == 1) {\n            atomicAdd(numComponents, 1);\n            for (int i = tid + 1; i < N; i++) {\n                if (A[tid * N + i] == 1) {\n                    A[i * N + i] = 0;\n                    A[i * N + tid] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + j] == 1 && A[tid * N + j] == 0) {\n                        A[tid * N + j] = 1;\n                    }\n                }\n            }\n        }\n    }\n\n    if (tid == 0) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[i * N + i] == 1) {\n                count++;\n            }\n        }\n        *numComponents = count;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (A[tid * N + tid] == 1) {\n            atomicAdd(numComponents, 1);\n            for (int i = tid + 1; i < N; i++) {\n                if (A[tid * N + i] == 1) {\n                    A[i * N + i] = 0;\n                    A[i * N + tid] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + j] == 1 && A[tid * N + j] == 0) {\n                        A[tid * N + j] = 1;\n                        A[j * N + tid] = 1;\n                    }\n                }\n            }\n        }\n    }\n\n    __syncthreads();\n\n    if (tid == 0) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[i * N + i] == 1) {\n                count++;\n            }\n        }\n        *numComponents = count;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (A[tid * N + tid] == 1) {\n            atomicAdd(numComponents, 1);\n            A[tid * N + tid] = 0;\n        }\n        for (int i = tid + 1; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                A[tid * N + i] = 0;\n                A[i * N + tid] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (A[tid * N + tid] == 1) {\n            atomicAdd(numComponents, 1);\n            for (int j = tid + 1; j < N; j++) {\n                if (A[tid * N + j] == 1) {\n                    A[tid * N + j] = 0;\n                    A[j * N + tid] = 0;\n                    componentCount<<<1, N>>>(A, N, numComponents);\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (A[tid * N + tid] == 1) {\n            atomicAdd(numComponents, 1);\n            A[tid * N + tid] = 0;\n        }\n        for (int i = tid + 1; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                A[tid * N + i] = 0;\n                A[i * N + tid] = 0;\n                componentCount<<<1, N>>>(A, N, numComponents);\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (A[tid * N + tid] == 1) {\n            atomicAdd(numComponents, 1);\n            for (int i = tid + 1; i < N; i++) {\n                if (A[tid * N + i] == 1) {\n                    A[tid * N + i] = 0;\n                    A[i * N + tid] = 0;\n                    componentCount<<<1, N>>>(A, N, numComponents);\n                }\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "18_graph_count_components",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n\t Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The component count is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    \n        int numComponents = 0;\n        // TODO: Implement the function to count the number of connected components in the graph defined by the adjacency matrix A\n        // Use MPI to compute in parallel\n        // Return the component count on rank 0\n        return numComponents;\n    }\n",
      "\n    \n        int count = 0;\n        std::vector<int> visited(N, 0);\n\n        for (int i = 0; i < N; ++i) {\n            if (!visited[i]) {\n                ++count;\n                std::queue<int> q;\n                q.push(i);\n                visited[i] = 1;\n\n                while (!q.empty()) {\n                    int node = q.front();\n                    q.pop();\n\n                    for (int j = 0; j < N; ++j) {\n                        if (A[node * N + j] && !visited[j]) {\n                            q.push(j);\n                            visited[j] = 1;\n                        }\n                    }\n                }\n            }\n        }\n\n        return count;\n    }\n",
      "\n    \n        int numComponents = 0;\n        std::vector<bool> visited(N, false);\n\n        for (int i = 0; i < N; ++i) {\n            if (!visited[i]) {\n                ++numComponents;\n                std::queue<int> q;\n                q.push(i);\n                visited[i] = true;\n\n                while (!q.empty()) {\n                    int node = q.front();\n                    q.pop();\n\n                    for (int j = 0; j < N; ++j) {\n                        if (A[node * N + j] && !visited[j]) {\n                            q.push(j);\n                            visited[j] = true;\n                        }\n                    }\n                }\n            }\n        }\n\n        return numComponents;\n    }\n",
      "\n    \n        int rank, size;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n        int localCount = 0;\n        for (int i = rank; i < N; i += size) {\n            if (A[i * N + i] == 1) {\n                localCount++;\n                for (int j = i + 1; j < N; j++) {\n                    if (A[i * N + j] == 1) {\n                        A[i * N + j] = 0;\n                        A[j * N + i] = 0;\n                    }\n                }\n            }\n        }\n\n        int globalCount;\n        MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n        if (rank == 0) {\n            return globalCount;\n        } else {\n            return 0;\n        }\n    }\n",
      "\n    \n        int rank, size;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n        int localCount = 0;\n        for (int i = rank; i < N; i += size) {\n            if (A[i * N + i] == 1) {\n                localCount++;\n                for (int j = i + 1; j < N; j++) {\n                    if (A[i * N + j] == 1) {\n                        A[i * N + j] = 0;\n                        A[j * N + i] = 0;\n                    }\n                }\n            }\n        }\n\n        int globalCount;\n        MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n        if (rank == 0) {\n            return globalCount;\n        } else {\n            return 0;\n        }\n    }\n",
      "\n    \n        int componentCount = 0;\n        std::vector<int> visited(N, 0);\n\n        for (int i = 0; i < N; ++i) {\n            if (!visited[i]) {\n                ++componentCount;\n                std::queue<int> q;\n                q.push(i);\n                visited[i] = 1;\n\n                while (!q.empty()) {\n                    int node = q.front();\n                    q.pop();\n\n                    for (int j = 0; j < N; ++j) {\n                        if (A[node * N + j] && !visited[j]) {\n                            q.push(j);\n                            visited[j] = 1;\n                        }\n                    }\n                }\n            }\n        }\n\n        return componentCount;\n    }\n",
      "\n    \n        int rank, size;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n        int localCount = 0;\n        for (int i = rank; i < N; i += size) {\n            if (A[i * N + i] == 1) {\n                localCount++;\n                for (int j = i + 1; j < N; j++) {\n                    if (A[i * N + j] == 1) {\n                        A[i * N + j] = 0;\n                        A[j * N + i] = 0;\n                    }\n                }\n            }\n        }\n\n        int globalCount;\n        MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n        if (rank == 0) {\n            return globalCount;\n        } else {\n            return 0;\n        }\n    }\n",
      "\n    \n        int rank, size;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n        int localCount = 0;\n        for (int i = rank; i < N; i += size) {\n            if (A[i * N + i] == 1) {\n                localCount++;\n                for (int j = i + 1; j < N; j++) {\n                    if (A[i * N + j] == 1) {\n                        A[i * N + j] = 0;\n                        A[j * N + i] = 0;\n                    }\n                }\n            }\n        }\n\n        int globalCount;\n        MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n        if (rank == 0) {\n            return globalCount;\n        } else {\n            return 0;\n        }\n    }\n",
      "\n    \n        int rank, size;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n        int localCount = 0;\n        for (int i = rank; i < N; i += size) {\n            if (A[i * N + i] == 1) {\n                localCount++;\n                for (int j = i + 1; j < N; j++) {\n                    if (A[i * N + j] == 1) {\n                        A[i * N + j] = 0;\n                        A[j * N + i] = 0;\n                    }\n                }\n            }\n        }\n\n        int globalCount;\n        MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n        if (rank == 0) {\n            return globalCount;\n        } else {\n            return 0;\n        }\n    }\n",
      "\n    \n        int count = 0;\n        std::vector<int> visited(N, 0);\n\n        for (int i = 0; i < N; ++i) {\n            if (!visited[i]) {\n                ++count;\n                std::queue<int> q;\n                q.push(i);\n                visited[i] = 1;\n\n                while (!q.empty()) {\n                    int node = q.front();\n                    q.pop();\n\n                    for (int j = 0; j < N; ++j) {\n                        if (A[node * N + j] && !visited[j]) {\n                            q.push(j);\n                            visited[j] = 1;\n                        }\n                    }\n                }\n            }\n        }\n\n        return count;\n    }\n",
      "\n    \n        int rank, size;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n        int localCount = 0;\n        std::vector<int> visited(N, 0);\n\n        for (int i = rank; i < N; i += size) {\n            if (visited[i] == 0) {\n                localCount++;\n                std::queue<int> q;\n                q.push(i);\n                visited[i] = 1;\n\n                while (!q.empty()) {\n                    int node = q.front();\n                    q.pop();\n\n                    for (int j = 0; j < N; ++j) {\n                        if (A[node * N + j] && !visited[j]) {\n                            q.push(j);\n                            visited[j] = 1;\n                        }\n                    }\n                }\n            }\n        }\n\n        int globalCount;\n        MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n        if (rank == 0) {\n            return globalCount;\n        } else {\n            return 0;\n        }\n    }\n",
      "\n    \n        int rank, size;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n        int localCount = 0;\n        std::vector<int> visited(N, 0);\n\n        for (int i = rank; i < N; i += size) {\n            if (!visited[i]) {\n                localCount++;\n                std::queue<int> q;\n                q.push(i);\n                visited[i] = 1;\n\n                while (!q.empty()) {\n                    int node = q.front();\n                    q.pop();\n\n                    for (int j = 0; j < N; j++) {\n                        if (A[node * N + j] && !visited[j]) {\n                            q.push(j);\n                            visited[j] = 1;\n                        }\n                    }\n                }\n            }\n        }\n\n        int globalCount;\n        MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n        if (rank == 0) {\n            return globalCount;\n        } else {\n            return 0;\n        }\n    }\n",
      "\n    \n        int rank, size;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n        int localCount = 0;\n        for (int i = rank; i < N; i += size) {\n            if (A[i * N + i] == 1) {\n                localCount++;\n                for (int j = i + 1; j < N; j++) {\n                    if (A[i * N + j] == 1) {\n                        A[i * N + j] = 0;\n                        A[j * N + i] = 0;\n                    }\n                }\n            }\n        }\n\n        int globalCount;\n        MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n        if (rank == 0) {\n            return globalCount;\n        } else {\n            return 0;\n        }\n    }\n",
      "\n    \n        int rank, size;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n        int localCount = 0;\n        for (int i = rank; i < N; i += size) {\n            if (A[i * N + i] == 1) {\n                localCount++;\n                for (int j = i + 1; j < N; j++) {\n                    if (A[i * N + j] == 1) {\n                        A[i * N + j] = 0;\n                        A[j * N + i] = 0;\n                    }\n                }\n            }\n        }\n\n        int globalCount;\n        MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n        if (rank == 0) {\n            return globalCount;\n        } else {\n            return 0;\n        }\n    }\n",
      "\n    \n        int rank, size;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n        int localCount = 0;\n        std::vector<int> visited(N, 0);\n\n        for (int i = rank; i < N; i += size) {\n            if (!visited[i]) {\n                localCount++;\n                std::queue<int> q;\n                q.push(i);\n                visited[i] = 1;\n\n                while (!q.empty()) {\n                    int node = q.front();\n                    q.pop();\n\n                    for (int j = 0; j < N; ++j) {\n                        if (A[node * N + j] && !visited[j]) {\n                            q.push(j);\n                            visited[j] = 1;\n                        }\n                    }\n                }\n            }\n        }\n\n        int globalCount;\n        MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n        if (rank == 0) {\n            return globalCount;\n        } else {\n            return 0;\n        }\n    }\n",
      "\n    \n        int rank, size;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n        int localCount = 0;\n        for (int i = rank; i < N; i += size) {\n            if (A[i * N + i] == 1) {\n                localCount++;\n                for (int j = i + 1; j < N; j++) {\n                    if (A[i * N + j] == 1) {\n                        A[i * N + j] = 0;\n                        A[j * N + i] = 0;\n                    }\n                }\n            }\n        }\n\n        int globalCount;\n        MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n        if (rank == 0) {\n            return globalCount;\n        } else {\n            return 0;\n        }\n    }\n",
      "\n    \n        int componentCount = 0;\n        // Your code to count the number of connected components using MPI goes here\n\n        return componentCount;\n    }\n",
      "\n    \n        int rank, size;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n        int localCount = 0;\n        for (int i = rank; i < N; i += size) {\n            if (A[i * N + i] == 1) {\n                localCount++;\n                for (int j = i + 1; j < N; j++) {\n                    if (A[i * N + j] == 1) {\n                        A[i * N + j] = 0;\n                        A[j * N + i] = 0;\n                    }\n                }\n            }\n        }\n\n        int globalCount;\n        MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n        if (rank == 0) {\n            return globalCount;\n        } else {\n            return 0;\n        }\n    }\n",
      "\n    \n        int count = 0;\n        // Your code to count the number of connected components using MPI goes here\n\n        return count;\n    }\n",
      "\n    \n        int rank, size;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n        int localCount = 0;\n        for (int i = rank; i < N; i += size) {\n            if (A[i * N + i] == 1) {\n                localCount++;\n                for (int j = i + 1; j < N; j++) {\n                    if (A[i * N + j] == 1) {\n                        A[j * N + i] = 0;\n                        A[i * N + j] = 0;\n                    }\n                }\n            }\n        }\n\n        int globalCount;\n        MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n        if (rank == 0) {\n            return globalCount;\n        } else {\n            return 0;\n        }\n    }\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "18_graph_count_components",
    "parallelism_model": "cuda",
    "prompt": "/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n\t Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\n__global__ void componentCount(const int *A, size_t N, int *numComponents) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (A[tid * N + tid] == 1) {\n            atomicAdd(numComponents, 1);\n            A[tid * N + tid] = 0;\n        }\n        for (int i = tid + 1; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                A[tid * N + i] = 0;\n                A[i * N + tid] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + j] == 1 && A[tid * N + j] == 0) {\n                        A[tid * N + j] = 1;\n                        A[j * N + tid] = 1;\n                    }\n                }\n            }\n        }\n    }\n    __syncthreads();\n\n    if (threadIdx.x == 0) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[blockIdx.x * N + i] == 1) {\n                count++;\n            }\n        }\n        atomicAdd(numComponents, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + j] == 1) {\n                        A[tid * N + j] = 1;\n                    }\n                }\n            }\n        }\n    }\n\n    if (threadIdx.x == 0) {\n        atomicAdd(numComponents, 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + j] == 1 && A[tid * N + j] == 0) {\n                        A[tid * N + j] = 1;\n                    }\n                }\n            }\n        }\n    }\n\n    if (tid == 0) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[i * N + i] == 1) {\n                count++;\n            }\n        }\n        *numComponents = count;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + j] == 1) {\n                        A[tid * N + j] = 1;\n                    }\n                }\n            }\n        }\n    }\n\n    if (threadIdx.x == 0) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[blockIdx.x * N + i] == 1) {\n                count++;\n            }\n        }\n        numComponents[blockIdx.x] = count;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int j = 0; j < N; ++j) {\n            if (A[tid * N + j] == 1) {\n                for (int k = 0; k < N; ++k) {\n                    if (A[j * N + k] == 1 && A[tid * N + k] == 0) {\n                        A[tid * N + k] = 1;\n                        A[k * N + tid] = 1;\n                    }\n                }\n            }\n        }\n    }\n    __syncthreads();\n\n    if (tid == 0) {\n        *numComponents = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[i * N + i] == 1) {\n                *numComponents += 1;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (A[tid * N + tid] == 1) {\n            atomicAdd(numComponents, 1);\n            A[tid * N + tid] = 0;\n        }\n        for (int j = tid + 1; j < N; j++) {\n            if (A[tid * N + j] == 1) {\n                A[tid * N + j] = 0;\n                A[j * N + tid] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (A[tid * N + tid] == 1) {\n            atomicAdd(numComponents, 1);\n            A[tid * N + tid] = 0;\n        }\n        for (int i = tid + 1; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                A[tid * N + i] = 0;\n                A[i * N + tid] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (A[tid * N + tid] == 1) {\n            atomicAdd(numComponents, 1);\n            A[tid * N + tid] = 0;\n        }\n        for (int j = tid + 1; j < N; j++) {\n            if (A[tid * N + j] == 1) {\n                A[tid * N + j] = 0;\n                A[j * N + tid] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (A[tid * N + tid] == 1) {\n            atomicAdd(numComponents, 1);\n            A[tid * N + tid] = 0;\n        }\n        for (int i = tid + 1; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                A[tid * N + i] = 0;\n                A[i * N + tid] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                for (int j = 0; j < N; ++j) {\n                    if (A[i * N + j] == 1) {\n                        A[tid * N + j] = 1;\n                    }\n                }\n            }\n        }\n    }\n    __syncthreads();\n\n    if (threadIdx.x == 0) {\n        int count = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[blockIdx.x * N + i] == 1) {\n                count++;\n            }\n        }\n        atomicAdd(numComponents, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (A[tid * N + tid] == 1) {\n            atomicAdd(numComponents, 1);\n            A[tid * N + tid] = 0;\n        }\n        for (int i = tid + 1; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                A[tid * N + i] = 0;\n                A[i * N + tid] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (A[tid * N + tid] == 1) {\n            atomicAdd(numComponents, 1);\n            A[tid * N + tid] = 0;\n        }\n        for (int j = tid + 1; j < N; j++) {\n            if (A[tid * N + j] == 1) {\n                A[tid * N + j] = 0;\n                A[j * N + tid] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + j] == 1 && A[tid * N + j] == 0) {\n                        A[tid * N + j] = 1;\n                    }\n                }\n            }\n        }\n    }\n\n    if (tid == 0) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[i * N + i] == 1) {\n                count++;\n            }\n        }\n        *numComponents = count;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                for (int j = 0; j < N; ++j) {\n                    if (A[i * N + j] == 1 && A[tid * N + j] == 0) {\n                        A[tid * N + j] = 1;\n                        A[j * N + tid] = 1;\n                        i = -1;\n                        break;\n                    }\n                }\n            }\n        }\n    }\n\n    __syncthreads();\n\n    if (tid == 0) {\n        int count = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[i * N + i] == 1) {\n                ++count;\n            }\n        }\n        *numComponents = count;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                for (int j = 0; j < N; ++j) {\n                    if (A[i * N + j] == 1 && A[tid * N + j] == 0) {\n                        A[tid * N + j] = 1;\n                        A[j * N + tid] = 1;\n                    }\n                }\n            }\n        }\n    }\n\n    __syncthreads();\n\n    if (tid < N) {\n        if (A[tid * N + tid] == 1) {\n            atomicAdd(numComponents, 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + j] == 1 && A[tid * N + j] == 0) {\n                        A[tid * N + j] = 1;\n                    }\n                }\n            }\n        }\n    }\n    __syncthreads();\n\n    if (threadIdx.x == 0) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[blockIdx.x * N + i] == 1) {\n                count++;\n            }\n        }\n        atomicAdd(numComponents, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                for (int j = 0; j < N; ++j) {\n                    if (A[i * N + j] == 1) {\n                        A[tid * N + j] = 1;\n                    }\n                }\n            }\n        }\n    }\n\n    if (threadIdx.x == 0) {\n        atomicAdd(numComponents, 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                for (int j = 0; j < N; ++j) {\n                    if (A[i * N + j] == 1 && A[tid * N + j] == 0) {\n                        A[tid * N + j] = 1;\n                        A[j * N + tid] = 1;\n                        i = -1;\n                        break;\n                    }\n                }\n            }\n        }\n    }\n\n    __syncthreads();\n\n    if (tid == 0) {\n        int count = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[i * N + i] == 1) {\n                ++count;\n            }\n        }\n        *numComponents = count;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + j] == 1 && A[tid * N + j] == 0) {\n                        A[tid * N + j] = 1;\n                    }\n                }\n            }\n        }\n    }\n\n    if (threadIdx.x == 0) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[blockIdx.x * N + i] == 1) {\n                count++;\n            }\n        }\n        atomicAdd(numComponents, count);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "18_graph_count_components",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n\t Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The component count is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int componentCount = 0;\n    int localCount = 0;\n\n    #pragma omp parallel for reduction(+:localCount)\n    for (int i = 0; i < N; ++i) {\n        if (A[i * N + i] == 1) {\n            localCount++;\n            for (int j = i + 1; j < N; ++j) {\n                if (A[i * N + j] == 1) {\n                    A[j * N + i] = 0;  // Mark as visited\n                }\n            }\n        }\n    }\n\n    MPI_Reduce(&localCount, &componentCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return componentCount;\n    } else {\n        return 0;\n    }\n}",
      "\n    int componentCount = 0;\n    // Implement the function using MPI and OpenMP to compute in parallel\n    // Each rank will work on a subset of the graph and then reduce the results to rank 0\n    // Use appropriate MPI and OpenMP functions to achieve parallelism\n    // Return the component count on rank 0\n    return componentCount;\n}\n",
      "\n    int componentCount = 0;\n    // Your code here\n    // Use MPI to distribute the work among ranks\n    // Use OpenMP to parallelize the computation within each rank\n    // Implement the algorithm to count the connected components in the graph\n    // Update the componentCount variable with the final count\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        // Gather the component counts from all ranks and sum them to get the final count\n    }\n    return componentCount;\n}\n",
      "\n    int componentCount = 0;\n    // Your parallel implementation using MPI and OpenMP goes here\n\n    return componentCount;\n}\n",
      "\n    int componentCount = 0;\n    // Your code here\n    // Implement the parallel algorithm to count the number of connected components in the graph\n    // Use MPI and OpenMP to compute in parallel\n    // Return the component count on rank 0\n    return componentCount;\n}\n",
      "\n    int componentCount = 0;\n    #pragma omp parallel for reduction(+:componentCount)\n    for (int i = 0; i < N; i++) {\n        if (A[i*N + i] == 1) {\n            componentCount++;\n            for (int j = i + 1; j < N; j++) {\n                if (A[i*N + j] == 1) {\n                    A[i*N + j] = 0;\n                    A[j*N + i] = 0;\n                }\n            }\n        }\n    }\n    MPI_Reduce(MPI_IN_PLACE, &componentCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    return componentCount;\n}\n",
      "\n    int componentCount = 0;\n    // Your code here\n    // Implement the algorithm to count the number of connected components in the graph\n    // Use MPI and OpenMP to compute in parallel\n    // Return the component count on rank 0\n    return componentCount;\n}\n",
      "\n    int componentCount = 0;\n    // Your code here\n    // Implement the parallel algorithm to count the number of connected components in the graph\n    // Use MPI and OpenMP to compute in parallel\n    // Make sure to return the component count on rank 0\n    return componentCount;\n}\n",
      "\n    int componentCount = 0;\n    // Your code here\n    // Implement the parallel algorithm to count the number of connected components in the graph\n    // Use MPI and OpenMP to compute in parallel\n    // Return the component count on rank 0\n    return componentCount;\n}\n",
      "\n    int componentCount = 0;\n    // Your parallel computation code using MPI and OpenMP goes here\n\n    return componentCount;\n}\n",
      "\n    int componentCount = 0;\n    // Your code here\n    // Implement the parallel algorithm to count the number of connected components in the graph\n    // Use MPI and OpenMP to distribute the work among the ranks and threads\n    // Update the componentCount variable with the final count of connected components\n    // Return the componentCount on rank 0\n    return componentCount;\n}\n",
      "\n    int componentCount = 0;\n    // Your code here\n    // Implement the parallel algorithm to count the number of connected components in the graph\n    // Use MPI and OpenMP to compute in parallel\n    // Ensure that the component count is returned on rank 0\n    // You may need to use MPI functions like MPI_Bcast, MPI_Reduce, MPI_Comm_rank, MPI_Comm_size, etc.\n    // You may also need to use OpenMP parallel for loops to parallelize the computation\n    // Remember that A is an undirected graph stored in row-major\n    // Update the componentCount variable with the final count of connected components\n    // Return the componentCount\n    return componentCount;\n}\n",
      "\n    int componentCount = 0;\n    // Your code here\n    // Implement the parallel algorithm to count the number of connected components in the graph\n    // Use MPI and OpenMP to compute in parallel\n    // Ensure that the component count is returned on rank 0\n    // You may need to use MPI_Bcast and MPI_Reduce to communicate the results\n    // Remember to handle the case when the graph is not connected\n    // Update the componentCount variable with the final count\n    // Return the componentCount on rank 0\n    return componentCount;\n}\n",
      "\n    int componentCount = 0;\n    // Your code here\n    // Implement the parallel algorithm to count the number of connected components in the graph\n    // Use MPI and OpenMP to compute in parallel\n    // Ensure that the component count is returned on rank 0\n    // You may need to use MPI functions like MPI_Bcast and MPI_Reduce\n    // You may also need to use OpenMP for parallelizing the computation\n    // Remember to handle the case when the graph is not connected\n    // Update the componentCount variable with the final count\n    // Return the componentCount\n    return componentCount;\n}\n",
      "\n    int componentCount = 0;\n    // Your code here\n    // Implement the parallel algorithm to count the number of connected components in the graph\n    // Use MPI and OpenMP to compute in parallel\n    // The component count should be returned on rank 0\n    // You may need to use MPI_Bcast and MPI_Reduce to communicate the results\n    // Remember to handle the case when N is smaller than the number of MPI ranks\n    // You may also need to handle the case when N is not divisible by the number of MPI ranks\n    // Use OpenMP to parallelize the computation within each MPI rank\n    // Remember to handle the case when N is smaller than the number of OpenMP threads\n    // You may need to use a disjoint set data structure or a depth-first search algorithm to find connected components\n    // Update the componentCount variable with the final count of connected components\n    return componentCount;\n}\n",
      "\n    int componentCount = 0;\n    // Your parallel computation code using MPI and OpenMP goes here\n\n    return componentCount;\n}\n",
      "\n    \n        int componentCount = 0;\n        int rank, size;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n        #pragma omp parallel for reduction(+:componentCount)\n        for (int i = rank; i < N; i += size) {\n            if (A[i * N + i] == 1) {\n                componentCount++;\n                for (int j = i + 1; j < N; j++) {\n                    if (A[i * N + j] == 1) {\n                        A[i * N + j] = 0;\n                        A[j * N + i] = 0;\n                    }\n                }\n            }\n        }\n\n        MPI_Reduce(&componentCount, &componentCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n        return componentCount;\n    }\n",
      "\n    \n        int componentCount = 0;\n        int rank, size;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n        #pragma omp parallel for reduction(+:componentCount)\n        for (int i = rank; i < N; i += size) {\n            if (A[i * N + i] == 1) {\n                componentCount++;\n                A[i * N + i] = 0;\n            }\n            for (int j = i + 1; j < N; j++) {\n                if (A[i * N + j] == 1) {\n                    A[i * N + j] = 0;\n                    A[j * N + i] = 0;\n                }\n            }\n        }\n\n        MPI_Reduce(&componentCount, &componentCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n        return componentCount;\n    }\n",
      "\n    \n        int componentCount = 0;\n        int rank, size;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n        #pragma omp parallel for reduction(+:componentCount)\n        for (int i = rank; i < N; i += size) {\n            if (A[i * N + i] == 1) {\n                componentCount++;\n                A[i * N + i] = 0;\n            }\n            for (int j = i + 1; j < N; j++) {\n                if (A[i * N + j] == 1) {\n                    A[i * N + j] = 0;\n                    A[j * N + i] = 0;\n                }\n            }\n        }\n\n        MPI_Reduce(&componentCount, &componentCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n        return componentCount;\n    }\n",
      "\n    \n        int componentCount = 0;\n        // TODO: Implement the parallel computation of connected components using MPI and OpenMP\n        // The result should be stored in the 'componentCount' variable\n        return componentCount;\n    }\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "15_graph_edge_count",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix. A represents a directed graph.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(Kokkos::View<const int**> &A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int count = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        for (size_t j = 0; j < N; j++) {\n            if (A(i, j) != 0) {\n                local_count++;\n            }\n        }\n    }, count);\n    return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        for (size_t j = 0; j < N; j++) {\n            if (A(i, j) != 0) {\n                local_count++;\n            }\n        }\n    }, count);\n    return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        for (size_t j = 0; j < N; j++) {\n            if (A(i, j) != 0) {\n                local_count++;\n            }\n        }\n    }, count);\n    return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        for (size_t j = 0; j < N; j++) {\n            if (A(i, j) == 1) {\n                local_count++;\n            }\n        }\n    }, count);\n    return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        for (size_t j = 0; j < N; j++) {\n            if (A(i, j) != 0) {\n                local_count++;\n            }\n        }\n    }, count);\n    return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        for (size_t j = 0; j < N; j++) {\n            if (A(i, j) != 0) {\n                local_count++;\n            }\n        }\n    }, count);\n    return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        for (size_t j = 0; j < N; j++) {\n            if (A(i, j) != 0) {\n                local_count++;\n            }\n        }\n    }, count);\n    return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        for (size_t j = 0; j < N; j++) {\n            if (A(i, j) == 1) {\n                local_count++;\n            }\n        }\n    }, count);\n    return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        for (size_t j = 0; j < N; j++) {\n            if (A(i, j) != 0) {\n                local_count++;\n            }\n        }\n    }, count);\n    return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        for (size_t j = 0; j < N; j++) {\n            if (A(i, j) == 1) {\n                local_count++;\n            }\n        }\n    }, count);\n    return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        for (size_t j = 0; j < N; j++) {\n            if (A(i, j) == 1) {\n                local_count++;\n            }\n        }\n    }, count);\n    return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        for (size_t j = 0; j < N; j++) {\n            if (A(i, j) != 0) {\n                local_count++;\n            }\n        }\n    }, count);\n    return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        for (size_t j = 0; j < N; j++) {\n            if (A(i, j) != 0) {\n                local_count++;\n            }\n        }\n    }, count);\n    return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        for (size_t j = 0; j < N; j++) {\n            if (A(i, j) != 0) {\n                local_count++;\n            }\n        }\n    }, count);\n    return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        for (size_t j = 0; j < N; j++) {\n            if (A(i, j) != 0) {\n                local_count++;\n            }\n        }\n    }, count);\n    return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        for (size_t j = 0; j < N; j++) {\n            if (A(i, j) == 1) {\n                local_count++;\n            }\n        }\n    }, count);\n    return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        for (size_t j = 0; j < N; j++) {\n            if (A(i, j) == 1) {\n                local_count++;\n            }\n        }\n    }, count);\n    return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        for (size_t j = 0; j < N; j++) {\n            if (A(i, j) != 0) {\n                local_count++;\n            }\n        }\n    }, count);\n    return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        for (size_t j = 0; j < N; j++) {\n            if (A(i, j) != 0) {\n                local_count++;\n            }\n        }\n    }, count);\n    return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        for (size_t j = 0; j < N; j++) {\n            if (A(i, j) == 1) {\n                local_count++;\n            }\n        }\n    }, count);\n    return count;\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "15_graph_edge_count",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n    return count;\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n    return count;\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n    return count;\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n    return count;\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n    return count;\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n    return count;\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n    return count;\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n    return count;\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n    return count;\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n    return count;\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n    return count;\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n    return count;\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n    return count;\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n    return count;\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n    return count;\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n    return count;\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n    return count;\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n    return count;\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n    return count;\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n    return count;\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "15_graph_edge_count",
    "parallelism_model": "serial",
    "prompt": "/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}",
      "\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}",
      "\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}",
      "\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}",
      "\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}",
      "\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}",
      "\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n    return count;\n}",
      "\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}",
      "\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}",
      "\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}",
      "\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}",
      "\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}",
      "\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}",
      "\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}",
      "\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}",
      "\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}",
      "\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}",
      "\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}",
      "\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}",
      "\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                ++count;\n            }\n        }\n    }\n    return count;\n}"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "15_graph_edge_count",
    "parallelism_model": "hip",
    "prompt": "/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   Store the result in numEdges. A represents a directed graph.\n   A is an NxN adjacency matrix stored in row-major.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\n__global__ void edgeCount(const int *A, size_t N, int *numEdges) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int edges = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                edges++;\n            }\n        }\n        atomicAdd(numEdges, edges);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "15_graph_edge_count",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int localCount = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n    int total_count;\n    MPI_Reduce(&count, &total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        return total_count;\n    }\n    return 0;\n}\n",
      "\n    int localCount = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "15_graph_edge_count",
    "parallelism_model": "cuda",
    "prompt": "/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   Store the result in numEdges. A represents a directed graph.\n   A is an NxN adjacency matrix stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\n__global__ void edgeCount(const int *A, size_t N, int *numEdges) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int edges = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                edges++;\n            }\n        }\n        atomicAdd(numEdges, edges);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] != 0) {\n                count++;\n            }\n        }\n        atomicAdd(numEdges, count);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "15_graph_edge_count",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n\n    int global_count;\n    MPI_Reduce(&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return global_count;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    #pragma omp parallel for reduction(+:localCount)\n    for (size_t i = 0; i < N * N; ++i) {\n        if (A[i] == 1) {\n            localCount++;\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    #pragma omp parallel for reduction(+:localCount)\n    for (size_t i = 0; i < N * N; ++i) {\n        if (A[i] == 1) {\n            localCount++;\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n\n    int global_count;\n    MPI_Reduce(&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return global_count;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    #pragma omp parallel for reduction(+:localCount)\n    for (size_t i = 0; i < N * N; ++i) {\n        if (A[i] == 1) {\n            localCount++;\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    #pragma omp parallel for reduction(+:localCount)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    #pragma omp parallel for reduction(+:localCount)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&count, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    #pragma omp parallel for reduction(+:localCount)\n    for (size_t i = 0; i < N * N; ++i) {\n        if (A[i] == 1) {\n            localCount++;\n        }\n    }\n\n    int globalCount = 0;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    #pragma omp parallel for reduction(+:localCount)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}",
      "\n    int localCount = 0;\n    #pragma omp parallel for reduction(+:localCount)\n    for (size_t i = 0; i < N * N; ++i) {\n        if (A[i] == 1) {\n            localCount++;\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    #pragma omp parallel for reduction(+:localCount)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    #pragma omp parallel for reduction(+:localCount)\n    for (size_t i = 0; i < N * N; ++i) {\n        if (A[i] == 1) {\n            localCount++;\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    #pragma omp parallel for reduction(+:localCount)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n\n    int global_count;\n    MPI_Reduce(&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return global_count;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int count = 0;\n    #pragma omp parallel for reduction(+:count)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                count++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&count, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    #pragma omp parallel for reduction(+:localCount)\n    for (size_t i = 0; i < N * N; ++i) {\n        if (A[i] == 1) {\n            localCount++;\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    #pragma omp parallel for reduction(+:localCount)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                localCount++;\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    #pragma omp parallel for reduction(+:localCount)\n    for (size_t i = 0; i < N * N; ++i) {\n        if (A[i] == 1) {\n            localCount++;\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localCount = 0;\n    #pragma omp parallel for reduction(+:localCount)\n    for (size_t i = 0; i < N * N; ++i) {\n        if (A[i] == 1) {\n            localCount++;\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "17_graph_highest_degree",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix. A is an undirected graph.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(Kokkos::View<const int**> &A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int maxDegree = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& localMax) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            degree += A(i, j);\n        }\n        localMax = degree > localMax ? degree : localMax;\n    }, Kokkos::Max<int>(maxDegree));\n    return maxDegree;\n}\n",
      "\n\n    int maxDegree = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int &localMax) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A(i, j) == 1) {\n                degree++;\n            }\n        }\n        if (degree > localMax) {\n            localMax = degree;\n        }\n    }, Kokkos::Max<int>(maxDegree));\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(int i, int &localMax) {\n        int degree = 0;\n        for (int j = 0; j < N; j++) {\n            if (A(i, j) == 1) {\n                degree++;\n            }\n        }\n        if (degree > localMax) {\n            localMax = degree;\n        }\n    }, Kokkos::Max<int>(maxDegree));\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& localMax) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            degree += A(i, j);\n        }\n        if (degree > localMax) {\n            localMax = degree;\n        }\n    }, Kokkos::Max<int>(maxDegree));\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_max) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            degree += A(i, j);\n        }\n        local_max = degree > local_max ? degree : local_max;\n    }, Kokkos::Max<int>(maxDegree));\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(int i, int &local_max) {\n        int degree = 0;\n        for (int j = 0; j < N; j++) {\n            if (A(i, j) == 1) {\n                degree++;\n            }\n        }\n        if (degree > local_max) {\n            local_max = degree;\n        }\n    }, Kokkos::Max<int>(maxDegree));\n    return maxDegree;\n}\n",
      "\n\n    int maxDegree = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& localMax) {\n        int degree = 0;\n        for (size_t j = 0; j < N; j++) {\n            degree += A(i, j);\n        }\n        localMax = degree > localMax ? degree : localMax;\n    }, Kokkos::Max<int>(maxDegree));\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int &localMax) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            degree += A(i, j);\n        }\n        if (degree > localMax) {\n            localMax = degree;\n        }\n    }, Kokkos::Max<int>(maxDegree));\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& localMax) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            degree += A(i, j);\n        }\n        localMax = degree > localMax ? degree : localMax;\n    }, Kokkos::Max<int>(maxDegree));\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(int i, int &localMax) {\n        int degree = 0;\n        for (int j = 0; j < N; j++) {\n            if (A(i, j) == 1) {\n                degree++;\n            }\n        }\n        if (degree > localMax) {\n            localMax = degree;\n        }\n    }, Kokkos::Max<int>(maxDegree));\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& localMax) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            degree += A(i, j);\n        }\n        localMax = degree > localMax ? degree : localMax;\n    }, Kokkos::Max<int>(maxDegree));\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& localMax) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            degree += A(i, j);\n        }\n        if (degree > localMax) {\n            localMax = degree;\n        }\n    }, Kokkos::Max<int>(maxDegree));\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(int i, int &localMax) {\n        int degree = 0;\n        for (int j = 0; j < N; j++) {\n            if (A(i, j) == 1) {\n                degree++;\n            }\n        }\n        if (degree > localMax) {\n            localMax = degree;\n        }\n    }, Kokkos::Max<int>(maxDegree));\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(int i, int &local_max) {\n        int degree = 0;\n        for (int j = 0; j < N; j++) {\n            if (A(i, j) == 1) {\n                degree++;\n            }\n        }\n        if (degree > local_max) {\n            local_max = degree;\n        }\n    }, Kokkos::Max<int>(maxDegree));\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(int i, int& localMax) {\n        int degree = 0;\n        for (int j = 0; j < N; j++) {\n            degree += A(i, j);\n        }\n        if (degree > localMax) {\n            localMax = degree;\n        }\n    }, Kokkos::Max<int>(maxDegree));\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int &localMax) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            degree += A(i, j);\n        }\n        localMax = (degree > localMax) ? degree : localMax;\n    }, Kokkos::Max<int>(maxDegree));\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(int i, int& localMax) {\n        int degree = 0;\n        for (int j = 0; j < N; j++) {\n            if (A(i, j) == 1) {\n                degree++;\n            }\n        }\n        if (degree > localMax) {\n            localMax = degree;\n        }\n    }, Kokkos::Max<int>(maxDegree));\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(int i, int &localMax) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            degree += A(i, j);\n        }\n        localMax = degree > localMax ? degree : localMax;\n    }, Kokkos::Max<int>(maxDegree));\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(int i, int &localMax) {\n        int degree = 0;\n        for (int j = 0; j < N; j++) {\n            degree += A(i, j);\n        }\n        if (degree > localMax) {\n            localMax = degree;\n        }\n    }, Kokkos::Max<int>(maxDegree));\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(int i, int &localMax) {\n        int degree = 0;\n        for (int j = 0; j < N; j++) {\n            degree += A(i, j);\n        }\n        localMax = degree > localMax ? degree : localMax;\n    }, Kokkos::Max<int>(maxDegree));\n    return maxDegree;\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "17_graph_highest_degree",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "17_graph_highest_degree",
    "parallelism_model": "serial",
    "prompt": "/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}",
      "\n    \n        int max = 0;\n        for (size_t i = 0; i < N; ++i) {\n            int degree = 0;\n            for (size_t j = 0; j < N; ++j) {\n                if (A[i * N + j] == 1) {\n                    degree++;\n                }\n            }\n            if (degree > max) {\n                max = degree;\n            }\n        }\n        return max;\n    }\n",
      "\n    \n        int max = 0;\n        for (size_t i = 0; i < N; ++i) {\n            int degree = 0;\n            for (size_t j = 0; j < N; ++j) {\n                if (A[i * N + j] == 1) {\n                    degree++;\n                }\n            }\n            if (degree > max) {\n                max = degree;\n            }\n        }\n        return max;\n    }\n",
      "\n    \n        int max = 0;\n        for (size_t i = 0; i < N; ++i) {\n            int degree = 0;\n            for (size_t j = 0; j < N; ++j) {\n                if (A[i * N + j] == 1) {\n                    degree++;\n                }\n            }\n            if (degree > max) {\n                max = degree;\n            }\n        }\n        return max;\n    }\n",
      "\n    \n        int max = 0;\n        for (size_t i = 0; i < N; ++i) {\n            int degree = 0;\n            for (size_t j = 0; j < N; ++j) {\n                if (A[i * N + j] == 1) {\n                    degree++;\n                }\n            }\n            if (degree > max) {\n                max = degree;\n            }\n        }\n        return max;\n    }\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "17_graph_highest_degree",
    "parallelism_model": "hip",
    "prompt": "/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Store the result in maxDegree.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\n__global__ void maxDegree(const int *A, size_t N, int *maxDegree) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; i++) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "17_graph_highest_degree",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    int localMaxDegree = 0;\n\n    for (int i = 0; i < N; ++i) {\n        int degree = 0;\n        for (int j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > localMaxDegree) {\n            localMaxDegree = degree;\n        }\n    }\n\n    MPI_Reduce(&localMaxDegree, &maxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    int localMaxDegree = 0;\n\n    for (int i = 0; i < N; ++i) {\n        int degree = 0;\n        for (int j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > localMaxDegree) {\n            localMaxDegree = degree;\n        }\n    }\n\n    MPI_Reduce(&localMaxDegree, &maxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    int localMaxDegree = 0;\n    for (int i = 0; i < N; ++i) {\n        int degree = 0;\n        for (int j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > localMaxDegree) {\n            localMaxDegree = degree;\n        }\n    }\n    MPI_Reduce(&localMaxDegree, &maxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    int localMaxDegree = 0;\n    for (int i = 0; i < N; ++i) {\n        int degree = 0;\n        for (int j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > localMaxDegree) {\n            localMaxDegree = degree;\n        }\n    }\n\n    MPI_Reduce(&localMaxDegree, &maxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    int localMaxDegree = 0;\n\n    for (int i = 0; i < N; ++i) {\n        int degree = 0;\n        for (int j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > localMaxDegree) {\n            localMaxDegree = degree;\n        }\n    }\n\n    MPI_Reduce(&localMaxDegree, &maxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    int localMaxDegree = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > localMaxDegree) {\n            localMaxDegree = degree;\n        }\n    }\n\n    MPI_Reduce(&localMaxDegree, &maxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    int localMaxDegree = 0;\n\n    for (int i = 0; i < N; ++i) {\n        int degree = 0;\n        for (int j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > localMaxDegree) {\n            localMaxDegree = degree;\n        }\n    }\n\n    MPI_Reduce(&localMaxDegree, &maxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n",
      "\n    int maxDegree = 0;\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    return maxDegree;\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "17_graph_highest_degree",
    "parallelism_model": "cuda",
    "prompt": "/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph. \n   Store the result in maxDegree.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\n__global__ void maxDegree(const int *A, size_t N, int *maxDegree) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n    if (threadId < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[threadId * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int degree = 0;\n        for (int i = 0; i < N; ++i) {\n            if (A[tid * N + i] == 1) {\n                degree++;\n            }\n        }\n        atomicMax(maxDegree, degree);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "17_graph_highest_degree",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    int globalMaxDegree;\n    MPI_Reduce(&maxDegree, &globalMaxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxDegree;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    int globalMaxDegree;\n    MPI_Reduce(&maxDegree, &globalMaxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxDegree;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    int globalMaxDegree;\n    MPI_Reduce(&maxDegree, &globalMaxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxDegree;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    int globalMaxDegree;\n    MPI_Reduce(&maxDegree, &globalMaxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxDegree;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    int globalMaxDegree;\n    MPI_Reduce(&maxDegree, &globalMaxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxDegree;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    int globalMaxDegree;\n    MPI_Reduce(&maxDegree, &globalMaxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxDegree;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    int globalMaxDegree;\n    MPI_Reduce(&maxDegree, &globalMaxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxDegree;\n    } else {\n        return 0;\n    }\n}",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    int globalMaxDegree;\n    MPI_Reduce(&maxDegree, &globalMaxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxDegree;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    int globalMaxDegree;\n    MPI_Reduce(&maxDegree, &globalMaxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxDegree;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    int globalMaxDegree;\n    MPI_Reduce(&maxDegree, &globalMaxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxDegree;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    int globalMaxDegree;\n    MPI_Reduce(&maxDegree, &globalMaxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxDegree;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    int globalMaxDegree;\n    MPI_Reduce(&maxDegree, &globalMaxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxDegree;\n    } else {\n        return 0;\n    }\n}",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (int i = 0; i < N; ++i) {\n        int degree = 0;\n        for (int j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    int globalMaxDegree;\n    MPI_Reduce(&maxDegree, &globalMaxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxDegree;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    int globalMaxDegree;\n    MPI_Reduce(&maxDegree, &globalMaxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxDegree;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n\n    int globalMaxDegree;\n    MPI_Reduce(&maxDegree, &globalMaxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxDegree;\n    } else {\n        return 0;\n    }\n}",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    int globalMaxDegree;\n    MPI_Reduce(&maxDegree, &globalMaxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxDegree;\n    } else {\n        return 0;\n    }\n}",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (int i = 0; i < N; i++) {\n        int degree = 0;\n        for (int j = 0; j < N; j++) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    int globalMaxDegree;\n    MPI_Reduce(&maxDegree, &globalMaxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxDegree;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    int globalMaxDegree;\n    MPI_Reduce(&maxDegree, &globalMaxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxDegree;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    int globalMaxDegree;\n    MPI_Reduce(&maxDegree, &globalMaxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxDegree;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int maxDegree = 0;\n    #pragma omp parallel for reduction(max:maxDegree)\n    for (size_t i = 0; i < N; ++i) {\n        int degree = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (A[i * N + j] == 1) {\n                degree++;\n            }\n        }\n        if (degree > maxDegree) {\n            maxDegree = degree;\n        }\n    }\n    int globalMaxDegree;\n    MPI_Reduce(&maxDegree, &globalMaxDegree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxDegree;\n    }\n    return 0;\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "19_graph_shortest_path",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix. A is an undirected graph.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(Kokkos::View<const int**> &A, size_t N, int source, int dest) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::View<int*> distances(\"distances\", N, N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n        distances(i) = (i == source) ? 0 : INT_MAX;\n    });\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n            Kokkos::parallel_for(N, KOKKOS_LAMBDA(int j) {\n                if (A(i, k) && A(k, j)) {\n                    distances(i) = std::min(distances(i), distances(k) + distances(j));\n                }\n            });\n        });\n        Kokkos::fence();\n    }\n\n    return distances(dest);\n}\n",
      "\n    Kokkos::View<int*> distances(\"distances\", N, N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n        distances(i) = (i == source) ? 0 : N;\n    });\n    Kokkos::fence();\n\n    for (int k = 0; k < N; k++) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n            Kokkos::parallel_for(N, KOKKOS_LAMBDA(int j) {\n                if (A(i, k) && A(k, j)) {\n                    distances(i) = std::min(distances(i), distances(j) + 1);\n                }\n            });\n        });\n        Kokkos::fence();\n    }\n\n    return distances(dest);\n}\n",
      "\n    Kokkos::View<int*> distances(\"distances\", N, N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n        distances(source, i) = A(source, i);\n    });\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n            Kokkos::parallel_for(N, KOKKOS_LAMBDA(int j) {\n                if (distances(i, k) + distances(k, j) < distances(i, j)) {\n                    distances(i, j) = distances(i, k) + distances(k, j);\n                }\n            });\n        });\n    }\n    return distances(source, dest);\n}\n",
      "\n    Kokkos::View<int*> distances(\"distances\", N, 0);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n        distances(i) = N; // Initialize distances to N (infinity)\n    });\n    distances(source) = 0; // Set distance from source to itself as 0\n\n    Kokkos::parallel_for(Kokkos::RangePolicy<>(0, N), KOKKOS_LAMBDA(int k) {\n        Kokkos::parallel_for(Kokkos::RangePolicy<>(0, N), KOKKOS_LAMBDA(int i) {\n            Kokkos::parallel_for(Kokkos::RangePolicy<>(0, N), KOKKOS_LAMBDA(int j) {\n                if (A(i, k) && A(k, j)) {\n                    distances(j) = std::min(distances(j), distances(i) + 1);\n                }\n            });\n        });\n    });\n\n    return distances(dest);\n}\n",
      "\n    Kokkos::View<int*> distance(\"distance\", N, N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        distance(i) = (i == source) ? 0 : INT_MAX;\n    });\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n            Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n                if (A(i, k) && A(k, j)) {\n                    distance(i) = std::min(distance(i), distance(k) + distance(j));\n                }\n            });\n        });\n        Kokkos::fence();\n    }\n\n    return distance(dest);\n}\n",
      "\n    Kokkos::View<int*> distances(\"distances\", N, N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n        distances(source, i) = A(source, i);\n    });\n    \n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n            Kokkos::parallel_for(N, KOKKOS_LAMBDA(int j) {\n                if (distances(i, k) + distances(k, j) < distances(i, j)) {\n                    distances(i, j) = distances(i, k) + distances(k, j);\n                }\n            });\n        });\n    }\n    \n    return distances(source, dest);\n}\n",
      "\n    Kokkos::View<int*> distance(\"distance\", N, 0);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        distance(i) = (i == source) ? 0 : INT_MAX;\n    });\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n            Kokkos::parallel_for(Kokkos::TeamThreadRange(N), [=](const int j) {\n                if (A(i, j) && distance(i) != INT_MAX && distance(i) + 1 < distance(j)) {\n                    distance(j) = distance(i) + 1;\n                }\n            });\n        });\n        Kokkos::fence();\n    }\n\n    return distance(dest);\n}\n",
      "\n    Kokkos::View<int*> distances(\"distances\", N, N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n        distances(i) = (i == source) ? 0 : N;\n    });\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n            Kokkos::parallel_for(N, KOKKOS_LAMBDA(int j) {\n                if (A(i, k) && A(k, j)) {\n                    distances(i) = std::min(distances(i), distances(j) + 1);\n                }\n            });\n        });\n        Kokkos::fence();\n    }\n\n    return distances(dest);\n}\n",
      "\n    Kokkos::View<int*> distances(\"distances\", N, N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n        distances(i) = (i == source) ? 0 : N;\n    });\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n            for (int j = 0; j < N; ++j) {\n                if (A(i, k) && A(k, j)) {\n                    distances(i) = std::min(distances(i), distances(k) + distances(j));\n                }\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return distances(dest);\n}\n",
      "\n    Kokkos::View<int*> distances(\"distances\", N, N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        distances(i) = (i == source) ? 0 : INT_MAX;\n    });\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n            Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n                if (A(i, j) == 1) {\n                    distances(j) = std::min(distances(j), distances(i) + 1);\n                }\n            });\n        });\n        Kokkos::fence();\n    }\n\n    return distances(dest);\n}\n",
      "\n    Kokkos::View<int*> distances(\"distances\", N, 0);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        distances(i) = (i == source) ? 0 : N;\n    });\n    \n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n            Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n                if (A(i, k) && A(k, j)) {\n                    distances(i) = std::min(distances(i), distances(j) + 1);\n                    distances(j) = std::min(distances(j), distances(i) + 1);\n                }\n            });\n        });\n    }\n    \n    return distances(dest);\n}\n",
      "\n    Kokkos::View<int*> distances(\"distances\", N, 0);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        distances(i) = (i == source) ? 0 : INT_MAX;\n    });\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n            Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n                if (A(i, j) == 1) {\n                    distances(j) = std::min(distances(j), distances(i) + 1);\n                }\n            });\n        });\n        Kokkos::fence();\n    }\n\n    return distances(dest);\n}\n",
      "\n    Kokkos::View<int*> distances(\"distances\", N, N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n        distances(i) = (i == source) ? 0 : N;\n    });\n    Kokkos::fence();\n\n    int currentDist = 1;\n    while (distances(dest) == N) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n            if (distances(i) == currentDist - 1) {\n                for (int j = 0; j < N; j++) {\n                    if (A(i, j) == 1 && distances(j) == N) {\n                        distances(j) = currentDist;\n                    }\n                }\n            }\n        });\n        Kokkos::fence();\n        currentDist++;\n    }\n\n    return distances(dest);\n}\n",
      "\n    Kokkos::View<int*> distances(\"distances\", N, N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n        distances(i) = (i == source) ? 0 : N;\n    });\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n            Kokkos::parallel_for(N, KOKKOS_LAMBDA(int j) {\n                if (A(i, k) && A(k, j)) {\n                    distances(i) = std::min(distances(i), distances(j) + 1);\n                    distances(j) = std::min(distances(j), distances(i) + 1);\n                }\n            });\n        });\n        Kokkos::fence();\n    }\n\n    return distances(dest);\n}\n",
      "\n    Kokkos::View<int*> distances(\"distances\", N, N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n        distances(source, i) = A(source, i);\n    });\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n            Kokkos::parallel_for(N, KOKKOS_LAMBDA(int j) {\n                if (distances(i, k) + distances(k, j) < distances(i, j)) {\n                    distances(i, j) = distances(i, k) + distances(k, j);\n                }\n            });\n        });\n        Kokkos::fence();\n    }\n\n    return distances(source, dest);\n}\n",
      "\n    Kokkos::View<int*> distances(\"distances\", N, N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n        distances(i) = (i == source) ? 0 : N;\n    });\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n            for (int j = 0; j < N; ++j) {\n                if (A(i, k) && A(k, j)) {\n                    distances(i) = std::min(distances(i), distances(k) + distances(j));\n                }\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return distances(dest);\n}\n",
      "\n    Kokkos::View<int*, Kokkos::HostSpace> distances(\"distances\", N, 0);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        distances(i) = (i == source) ? 0 : N;\n    });\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n            if (A(i, k) == 1) {\n                distances(i) = std::min(distances(i), distances(k) + 1);\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return distances(dest);\n}\n",
      "\n    Kokkos::View<int*> distances(\"distances\", N, 0);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n        distances(i) = (i == source) ? 0 : N;\n    });\n    \n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n            Kokkos::parallel_for(N, KOKKOS_LAMBDA(int j) {\n                if (A(i, k) && A(k, j)) {\n                    distances(i) = std::min(distances(i), distances(j) + 1);\n                    distances(j) = std::min(distances(j), distances(i) + 1);\n                }\n            });\n        });\n    }\n    \n    return distances(dest);\n}\n",
      "\n    Kokkos::View<int*> distances(\"distances\", N, 0);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n        distances(i) = N; // Initialize distances to N (infinity)\n    });\n    distances(source) = 0; // Set distance from source to itself as 0\n\n    Kokkos::parallel_for(Kokkos::RangePolicy<>(0, N), KOKKOS_LAMBDA(int k) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (A(i, j) == 1) {\n                    distances(j) = std::min(distances(j), distances(i) + 1);\n                }\n            }\n        }\n    });\n\n    return distances(dest);\n}\n",
      "\n    Kokkos::View<int*> distances(\"distances\", N, N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n        distances(i) = (i == source) ? 0 : INT_MAX;\n    });\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n            Kokkos::parallel_for(N, KOKKOS_LAMBDA(int j) {\n                if (A(i, j) == 1) {\n                    distances(j) = std::min(distances(j), distances(i) + 1);\n                }\n            });\n        });\n        Kokkos::fence();\n    }\n\n    return distances(dest);\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "19_graph_shortest_path",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    std::vector<int> dist(N, -1);\n    dist[source] = 0;\n    std::queue<int> q;\n    q.push(source);\n\n    while (!q.empty()) {\n        int current = q.front();\n        q.pop();\n        for (int i = 0; i < N; ++i) {\n            if (A[current * N + i] && dist[i] == -1) {\n                dist[i] = dist[current] + 1;\n                q.push(i);\n            }\n        }\n    }\n\n    return dist[dest];\n}\n",
      "\n    std::vector<int> distances(N, -1);\n    distances[source] = 0;\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        if (A[source * N + k] == 1) {\n            distances[k] = 1;\n        }\n    }\n\n    for (int i = 0; i < N; ++i) {\n        if (i != source) {\n            #pragma omp parallel for\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + j] == 1 && distances[j] == -1) {\n                    distances[j] = distances[i] + 1;\n                }\n            }\n        }\n    }\n\n    return distances[dest];\n}\n",
      "\n    std::vector<int> dist(N, -1);\n    dist[source] = 0;\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j]) {\n                    if (dist[i] != -1 && (dist[j] == -1 || dist[i] + dist[j] + 1 < dist[j])) {\n                        dist[j] = dist[i] + dist[j] + 1;\n                    }\n                }\n            }\n        }\n    }\n\n    return dist[dest];\n}\n",
      "\n    std::vector<int> distances(N, -1);\n    distances[source] = 0;\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        if (A[source * N + k] == 1) {\n            distances[k] = 1;\n        }\n    }\n\n    for (int i = 0; i < N; ++i) {\n        if (i != source) {\n            #pragma omp parallel for\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + j] == 1 && distances[j] == -1) {\n                    distances[j] = distances[i] + 1;\n                }\n            }\n        }\n    }\n\n    return distances[dest];\n}\n",
      "\n    std::vector<int> distance(N, -1);\n    distance[source] = 0;\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        if (A[source * N + k] && distance[k] == -1) {\n            distance[k] = distance[source] + 1;\n        }\n    }\n\n    while (distance[dest] == -1) {\n        #pragma omp parallel for\n        for (int i = 0; i < N; ++i) {\n            if (distance[i] == -1) {\n                for (int j = 0; j < N; ++j) {\n                    if (A[i * N + j] && distance[j] != -1) {\n                        distance[i] = distance[j] + 1;\n                        break;\n                    }\n                }\n            }\n        }\n    }\n\n    return distance[dest];\n}\n",
      "\n    std::vector<int> distances(N, -1);\n    distances[source] = 0;\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        if (A[source * N + k] == 1) {\n            distances[k] = 1;\n        }\n    }\n\n    for (int i = 0; i < N; ++i) {\n        if (i != source) {\n            #pragma omp parallel for\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + j] == 1 && distances[j] == -1) {\n                    distances[j] = distances[i] + 1;\n                }\n            }\n        }\n    }\n\n    return distances[dest];\n}\n",
      "\n    std::vector<int> distance(N, -1);\n    distance[source] = 0;\n    std::queue<int> q;\n    q.push(source);\n\n    while (!q.empty()) {\n        int current = q.front();\n        q.pop();\n\n        #pragma omp parallel for\n        for (int i = 0; i < N; ++i) {\n            if (A[current * N + i] && distance[i] == -1) {\n                distance[i] = distance[current] + 1;\n                q.push(i);\n            }\n        }\n    }\n\n    return distance[dest];\n}\n",
      "\n    std::vector<int> distance(N, -1);\n    distance[source] = 0;\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        if (A[source * N + k] && distance[k] == -1) {\n            distance[k] = distance[source] + 1;\n        }\n    }\n\n    while (distance[dest] == -1) {\n        #pragma omp parallel for\n        for (int i = 0; i < N; ++i) {\n            if (distance[i] == -1) {\n                for (int j = 0; j < N; ++j) {\n                    if (A[i * N + j] && distance[j] != -1) {\n                        distance[i] = distance[j] + 1;\n                        break;\n                    }\n                }\n            }\n        }\n    }\n\n    return distance[dest];\n}\n",
      "\n    std::vector<int> dist(N, -1);\n    dist[source] = 0;\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j]) {\n                    if (dist[i] != -1 && (dist[j] == -1 || dist[i] + dist[j] + 1 < dist[j])) {\n                        dist[j] = dist[i] + dist[j] + 1;\n                    }\n                }\n            }\n        }\n    }\n\n    return dist[dest];\n}\n",
      "\n    std::vector<int> dist(N, -1);\n    dist[source] = 0;\n    std::queue<int> q;\n    q.push(source);\n\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        #pragma omp parallel for\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] && dist[v] == -1) {\n                dist[v] = dist[u] + 1;\n                q.push(v);\n            }\n        }\n    }\n\n    return dist[dest];\n}\n",
      "\n    std::vector<int> dist(N, -1);\n    dist[source] = 0;\n    std::queue<int> q;\n    q.push(source);\n\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        #pragma omp parallel for\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] && dist[v] == -1) {\n                dist[v] = dist[u] + 1;\n                q.push(v);\n            }\n        }\n    }\n\n    return dist[dest];\n}\n",
      "\n    std::vector<int> distances(N, -1);\n    distances[source] = 0;\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        if (A[source * N + k] && distances[k] == -1) {\n            distances[k] = distances[source] + 1;\n        }\n    }\n\n    while (distances[dest] == -1) {\n        #pragma omp parallel for\n        for (int i = 0; i < N; ++i) {\n            if (distances[i] == -1) {\n                for (int j = 0; j < N; ++j) {\n                    if (A[i * N + j] && distances[j] != -1) {\n                        distances[i] = distances[j] + 1;\n                        break;\n                    }\n                }\n            }\n        }\n    }\n\n    return distances[dest];\n}\n",
      "\n    std::vector<int> distance(N, -1);\n    distance[source] = 0;\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        if (A[source * N + k] == 1) {\n            distance[k] = 1;\n        }\n    }\n\n    for (int i = 0; i < N; ++i) {\n        if (i != source) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + j] == 1 && distance[i] != -1 && (distance[j] == -1 || distance[j] > distance[i] + 1)) {\n                    distance[j] = distance[i] + 1;\n                }\n            }\n        }\n    }\n\n    return distance[dest];\n}\n",
      "\n    std::vector<int> distance(N, -1);\n    distance[source] = 0;\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        if (A[source * N + k] && distance[k] == -1) {\n            distance[k] = distance[source] + 1;\n        }\n    }\n\n    while (distance[dest] == -1) {\n        #pragma omp parallel for\n        for (int i = 0; i < N; ++i) {\n            if (distance[i] == -1) {\n                for (int j = 0; j < N; ++j) {\n                    if (A[i * N + j] && distance[j] != -1) {\n                        distance[i] = distance[j] + 1;\n                        break;\n                    }\n                }\n            }\n        }\n    }\n\n    return distance[dest];\n}\n",
      "\n    std::vector<int> dist(N, -1);\n    dist[source] = 0;\n    std::queue<int> q;\n    q.push(source);\n\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        #pragma omp parallel for\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] && dist[v] == -1) {\n                dist[v] = dist[u] + 1;\n                q.push(v);\n            }\n        }\n    }\n\n    return dist[dest];\n}\n",
      "\n    std::vector<int> dist(N, -1);\n    dist[source] = 0;\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j]) {\n                    if (dist[i] != -1 && (dist[j] == -1 || dist[i] + dist[j] + 1 < dist[j])) {\n                        dist[j] = dist[i] + dist[j] + 1;\n                    }\n                }\n            }\n        }\n    }\n\n    return dist[dest];\n}\n",
      "\n    std::vector<int> distance(N, -1);\n    distance[source] = 0;\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j]) {\n                    if (distance[i] != -1 && (distance[j] == -1 || distance[i] + distance[j] + 1 < distance[j])) {\n                        distance[j] = distance[i] + distance[j] + 1;\n                    }\n                }\n            }\n        }\n    }\n\n    return distance[dest];\n}\n",
      "\n    std::vector<int> dist(N, -1);\n    dist[source] = 0;\n    std::queue<int> q;\n    q.push(source);\n\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        #pragma omp parallel for\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] && dist[v] == -1) {\n                dist[v] = dist[u] + 1;\n                q.push(v);\n            }\n        }\n    }\n\n    return dist[dest];\n}\n",
      "\n    std::vector<int> distances(N, -1);\n    distances[source] = 0;\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        if (A[source * N + k] && distances[k] == -1) {\n            distances[k] = distances[source] + 1;\n        }\n    }\n\n    while (distances[dest] == -1) {\n        #pragma omp parallel for\n        for (int i = 0; i < N; ++i) {\n            if (distances[i] == -1) {\n                for (int j = 0; j < N; ++j) {\n                    if (A[i * N + j] && distances[j] != -1) {\n                        distances[i] = distances[j] + 1;\n                        break;\n                    }\n                }\n            }\n        }\n    }\n\n    return distances[dest];\n}\n",
      "\n    std::vector<int> distances(N, -1);\n    distances[source] = 0;\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        if (A[source * N + k] == 1) {\n            distances[k] = 1;\n        }\n    }\n\n    for (int i = 0; i < N; ++i) {\n        if (i != source) {\n            #pragma omp parallel for\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + j] == 1 && (distances[j] == -1 || distances[j] > distances[i] + 1)) {\n                    distances[j] = distances[i] + 1;\n                }\n            }\n        }\n    }\n\n    return distances[dest];\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "19_graph_shortest_path",
    "parallelism_model": "serial",
    "prompt": "/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    std::queue<int> q;\n    std::vector<bool> visited(N, false);\n    std::vector<int> distance(N, 0);\n\n    q.push(source);\n    visited[source] = true;\n\n    while (!q.empty()) {\n        int current = q.front();\n        q.pop();\n\n        if (current == dest) {\n            return distance[current];\n        }\n\n        for (int i = 0; i < N; ++i) {\n            if (A[current * N + i] && !visited[i]) {\n                q.push(i);\n                visited[i] = true;\n                distance[i] = distance[current] + 1;\n            }\n        }\n    }\n\n    return -1; // If no path is found\n}\n",
      "\n    std::queue<int> q;\n    std::vector<int> dist(N, -1);\n    dist[source] = 0;\n    q.push(source);\n\n    while (!q.empty()) {\n        int current = q.front();\n        q.pop();\n\n        for (int i = 0; i < N; ++i) {\n            if (A[current * N + i] && dist[i] == -1) {\n                dist[i] = dist[current] + 1;\n                q.push(i);\n            }\n        }\n    }\n\n    return dist[dest];\n}\n",
      "\n    std::queue<int> q;\n    std::vector<int> dist(N, -1);\n    dist[source] = 0;\n    q.push(source);\n\n    while (!q.empty()) {\n        int current = q.front();\n        q.pop();\n\n        for (int i = 0; i < N; ++i) {\n            if (A[current * N + i] && dist[i] == -1) {\n                dist[i] = dist[current] + 1;\n                q.push(i);\n            }\n        }\n    }\n\n    return dist[dest];\n}\n",
      "\n    std::queue<int> q;\n    std::vector<bool> visited(N, false);\n    std::vector<int> distance(N, 0);\n\n    q.push(source);\n    visited[source] = true;\n\n    while (!q.empty()) {\n        int current = q.front();\n        q.pop();\n\n        if (current == dest) {\n            return distance[current];\n        }\n\n        for (int i = 0; i < N; ++i) {\n            if (A[current * N + i] && !visited[i]) {\n                q.push(i);\n                visited[i] = true;\n                distance[i] = distance[current] + 1;\n            }\n        }\n    }\n\n    return -1; // If there is no path from source to dest\n}\n",
      "\n    std::queue<int> q;\n    std::vector<bool> visited(N, false);\n    std::vector<int> distance(N, 0);\n\n    q.push(source);\n    visited[source] = true;\n\n    while (!q.empty()) {\n        int current = q.front();\n        q.pop();\n\n        if (current == dest) {\n            return distance[current];\n        }\n\n        for (int i = 0; i < N; ++i) {\n            if (A[current * N + i] && !visited[i]) {\n                q.push(i);\n                visited[i] = true;\n                distance[i] = distance[current] + 1;\n            }\n        }\n    }\n\n    return -1; // If there is no path from source to dest\n}\n",
      "\n    std::queue<int> q;\n    std::vector<int> dist(N, -1);\n    dist[source] = 0;\n    q.push(source);\n\n    while (!q.empty()) {\n        int current = q.front();\n        q.pop();\n\n        for (int i = 0; i < N; ++i) {\n            if (A[current * N + i] && dist[i] == -1) {\n                dist[i] = dist[current] + 1;\n                q.push(i);\n            }\n        }\n    }\n\n    return dist[dest];\n}\n",
      "\n    std::queue<int> q;\n    std::vector<bool> visited(N, false);\n    std::vector<int> distance(N, 0);\n\n    q.push(source);\n    visited[source] = true;\n\n    while (!q.empty()) {\n        int current = q.front();\n        q.pop();\n\n        if (current == dest) {\n            return distance[current];\n        }\n\n        for (int i = 0; i < N; ++i) {\n            if (A[current * N + i] && !visited[i]) {\n                q.push(i);\n                visited[i] = true;\n                distance[i] = distance[current] + 1;\n            }\n        }\n    }\n\n    return -1; // If no path is found\n}\n",
      "\nstd::queue<int> q;\nstd::vector<int> dist(N, -1);\ndist[source] = 0;\nq.push(source);\n\nwhile (!q.empty()) {\n    int node = q.front();\n    q.pop();\n\n    for (int i = 0; i < N; ++i) {\n        if (A[node * N + i] && dist[i] == -1) {\n            dist[i] = dist[node] + 1;\n            q.push(i);\n        }\n    }\n}\n\nreturn dist[dest];\n}",
      "\n    std::queue<int> q;\n    std::vector<bool> visited(N, false);\n    std::vector<int> distance(N, 0);\n\n    q.push(source);\n    visited[source] = true;\n\n    while (!q.empty()) {\n        int current = q.front();\n        q.pop();\n\n        if (current == dest) {\n            return distance[current];\n        }\n\n        for (int i = 0; i < N; ++i) {\n            if (A[current * N + i] && !visited[i]) {\n                q.push(i);\n                visited[i] = true;\n                distance[i] = distance[current] + 1;\n            }\n        }\n    }\n\n    return -1; // If there is no path from source to dest\n}\n",
      "\n    std::queue<int> q;\n    std::vector<bool> visited(N, false);\n    std::vector<int> distance(N, 0);\n\n    q.push(source);\n    visited[source] = true;\n\n    while (!q.empty()) {\n        int current = q.front();\n        q.pop();\n\n        if (current == dest) {\n            return distance[current];\n        }\n\n        for (int i = 0; i < N; ++i) {\n            if (A[current * N + i] && !visited[i]) {\n                q.push(i);\n                visited[i] = true;\n                distance[i] = distance[current] + 1;\n            }\n        }\n    }\n\n    return -1; // If no path is found\n}\n",
      "\n    std::queue<int> q;\n    std::vector<int> dist(N, -1);\n    dist[source] = 0;\n    q.push(source);\n\n    while (!q.empty()) {\n        int current = q.front();\n        q.pop();\n\n        for (int i = 0; i < N; ++i) {\n            if (A[current * N + i] && dist[i] == -1) {\n                dist[i] = dist[current] + 1;\n                q.push(i);\n            }\n        }\n    }\n\n    return dist[dest];\n}\n",
      "\n    std::queue<int> q;\n    std::vector<int> dist(N, -1);\n    dist[source] = 0;\n    q.push(source);\n\n    while (!q.empty()) {\n        int node = q.front();\n        q.pop();\n        for (int i = 0; i < N; ++i) {\n            if (A[node * N + i] && dist[i] == -1) {\n                dist[i] = dist[node] + 1;\n                q.push(i);\n            }\n        }\n    }\n\n    return dist[dest];\n}\n",
      "\n    std::queue<int> q;\n    std::vector<int> dist(N, -1);\n    dist[source] = 0;\n    q.push(source);\n\n    while (!q.empty()) {\n        int current = q.front();\n        q.pop();\n\n        for (int i = 0; i < N; ++i) {\n            if (A[current * N + i] && dist[i] == -1) {\n                dist[i] = dist[current] + 1;\n                q.push(i);\n            }\n        }\n    }\n\n    return dist[dest];\n}\n",
      "\n    std::queue<int> q;\n    std::vector<int> dist(N, -1);\n    dist[source] = 0;\n    q.push(source);\n\n    while (!q.empty()) {\n        int node = q.front();\n        q.pop();\n        for (int i = 0; i < N; ++i) {\n            if (A[node * N + i] && dist[i] == -1) {\n                dist[i] = dist[node] + 1;\n                q.push(i);\n            }\n        }\n    }\n\n    return dist[dest];\n}\n",
      "\n    std::queue<int> q;\n    std::vector<bool> visited(N, false);\n    std::vector<int> distance(N, 0);\n\n    q.push(source);\n    visited[source] = true;\n\n    while (!q.empty()) {\n        int current = q.front();\n        q.pop();\n\n        if (current == dest) {\n            return distance[current];\n        }\n\n        for (int i = 0; i < N; ++i) {\n            if (A[current * N + i] && !visited[i]) {\n                q.push(i);\n                visited[i] = true;\n                distance[i] = distance[current] + 1;\n            }\n        }\n    }\n\n    return -1; // If there is no path from source to dest\n}\n",
      "\n    std::queue<int> q;\n    std::vector<int> dist(N, -1);\n    dist[source] = 0;\n    q.push(source);\n\n    while (!q.empty()) {\n        int curr = q.front();\n        q.pop();\n\n        for (int i = 0; i < N; ++i) {\n            if (A[curr * N + i] && dist[i] == -1) {\n                dist[i] = dist[curr] + 1;\n                q.push(i);\n            }\n        }\n    }\n\n    return dist[dest];\n}\n",
      "\n    std::queue<int> q;\n    std::vector<bool> visited(N, false);\n    std::vector<int> distance(N, 0);\n\n    q.push(source);\n    visited[source] = true;\n\n    while (!q.empty()) {\n        int current = q.front();\n        q.pop();\n\n        if (current == dest) {\n            return distance[current];\n        }\n\n        for (int i = 0; i < N; ++i) {\n            if (A[current * N + i] && !visited[i]) {\n                q.push(i);\n                visited[i] = true;\n                distance[i] = distance[current] + 1;\n            }\n        }\n    }\n\n    return -1; // If no path is found\n}\n",
      "\n    std::queue<int> q;\n    std::vector<int> dist(N, -1);\n    dist[source] = 0;\n    q.push(source);\n\n    while (!q.empty()) {\n        int current = q.front();\n        q.pop();\n\n        for (int i = 0; i < N; ++i) {\n            if (A[current * N + i] && dist[i] == -1) {\n                dist[i] = dist[current] + 1;\n                q.push(i);\n            }\n        }\n    }\n\n    return dist[dest];\n}\n",
      "\n    std::queue<int> q;\n    std::vector<bool> visited(N, false);\n    std::vector<int> distance(N, 0);\n\n    q.push(source);\n    visited[source] = true;\n\n    while (!q.empty()) {\n        int current = q.front();\n        q.pop();\n\n        if (current == dest) {\n            return distance[current];\n        }\n\n        for (int i = 0; i < N; ++i) {\n            if (A[current * N + i] && !visited[i]) {\n                q.push(i);\n                visited[i] = true;\n                distance[i] = distance[current] + 1;\n            }\n        }\n    }\n\n    return -1; // If there is no path from source to dest\n}\n",
      "\n    std::queue<int> q;\n    std::vector<bool> visited(N, false);\n    std::vector<int> distance(N, 0);\n\n    q.push(source);\n    visited[source] = true;\n\n    while (!q.empty()) {\n        int current = q.front();\n        q.pop();\n\n        if (current == dest) {\n            return distance[current];\n        }\n\n        for (int i = 0; i < N; ++i) {\n            if (A[current * N + i] && !visited[i]) {\n                q.push(i);\n                visited[i] = true;\n                distance[i] = distance[current] + 1;\n            }\n        }\n    }\n\n    return -1; // If no path is found\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "19_graph_shortest_path",
    "parallelism_model": "hip",
    "prompt": "/* Compute the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Store the result in pathLength.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\n__global__ void shortestPathLength(const int *A, size_t N, int source, int dest, int *pathLength) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid == source) {\n        pathLength[tid] = 0;\n    } else {\n        pathLength[tid] = INT_MAX;\n    }\n    __syncthreads();\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j] && pathLength[i] != INT_MAX && pathLength[i] + 1 < pathLength[j]) {\n                    pathLength[j] = pathLength[i] + 1;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid == source) {\n        pathLength[tid] = 0;\n    } else {\n        pathLength[tid] = INT_MAX;\n    }\n    __syncthreads();\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j] && pathLength[i] != INT_MAX && pathLength[i] + 1 < pathLength[j]) {\n                    pathLength[j] = pathLength[i] + 1;\n                }\n            }\n        }\n        __syncthreads();\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid == source) {\n            pathLength[tid] = 0;\n        } else {\n            pathLength[tid] = INT_MAX;\n        }\n    }\n    __syncthreads();\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j] && pathLength[i] != INT_MAX && pathLength[i] + 1 < pathLength[j]) {\n                    pathLength[j] = pathLength[i] + 1;\n                }\n            }\n        }\n        __syncthreads();\n    }\n\n    if (pathLength[dest] == INT_MAX) {\n        pathLength[dest] = -1;\n    }\n}\n",
      "\n    __shared__ int dist[MAX_NODES];\n    dist[threadIdx.x] = (threadIdx.x == source) ? 0 : INT_MAX;\n\n    for (int i = 0; i < N; ++i) {\n        __syncthreads();\n        if (dist[i] < INT_MAX) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + j] && dist[i] + 1 < dist[j]) {\n                    dist[j] = dist[i] + 1;\n                }\n            }\n        }\n    }\n\n    if (threadIdx.x == dest) {\n        *pathLength = dist[dest];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid == source) {\n            pathLength[tid] = 0;\n        } else {\n            pathLength[tid] = INT_MAX;\n        }\n    }\n    __syncthreads();\n\n    for (int k = 0; k < N; k++) {\n        for (int i = 0; i < N; i++) {\n            for (int j = 0; j < N; j++) {\n                if (A[i * N + k] && A[k * N + j] && pathLength[i] != INT_MAX && pathLength[i] + 1 < pathLength[j]) {\n                    pathLength[j] = pathLength[i] + 1;\n                }\n            }\n        }\n        __syncthreads();\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid == 0) {\n        for (int k = 0; k < N; k++) {\n            for (int i = 0; i < N; i++) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + k] && A[k * N + j]) {\n                        A[i * N + j] = 1;\n                    }\n                }\n            }\n        }\n        *pathLength = A[source * N + dest];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid == source) {\n        pathLength[tid] = 0;\n    } else {\n        pathLength[tid] = INT_MAX;\n    }\n    __syncthreads();\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j] && pathLength[i] != INT_MAX && pathLength[i] + 1 < pathLength[j]) {\n                    pathLength[j] = pathLength[i] + 1;\n                }\n            }\n        }\n        __syncthreads();\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid == source) {\n        pathLength[tid] = 0;\n    } else {\n        pathLength[tid] = INT_MAX;\n    }\n    __syncthreads();\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j] && pathLength[i] != INT_MAX && pathLength[i] + 1 < pathLength[j]) {\n                    pathLength[j] = pathLength[i] + 1;\n                }\n            }\n        }\n        __syncthreads();\n    }\n}\n",
      "\n    int INF = 99999;\n    __shared__ int dist[N];\n    for (int i = 0; i < N; i++) {\n        dist[i] = INF;\n    }\n    dist[source] = 0;\n\n    for (int k = 0; k < N; k++) {\n        for (int i = 0; i < N; i++) {\n            for (int j = 0; j < N; j++) {\n                if (A[i * N + j] && dist[i] + 1 < dist[j]) {\n                    dist[j] = dist[i] + 1;\n                }\n            }\n        }\n    }\n\n    *pathLength = dist[dest];\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid == source) {\n            pathLength[tid] = 0;\n        } else {\n            pathLength[tid] = INT_MAX;\n        }\n    }\n    __syncthreads();\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j] && pathLength[i] != INT_MAX && pathLength[i] + 1 < pathLength[j]) {\n                    pathLength[j] = pathLength[i] + 1;\n                }\n            }\n        }\n        __syncthreads();\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid == source) {\n        pathLength[tid] = 0;\n    } else {\n        pathLength[tid] = INT_MAX;\n    }\n    __syncthreads();\n\n    for (int k = 0; k < N; k++) {\n        for (int i = 0; i < N; i++) {\n            for (int j = 0; j < N; j++) {\n                if (A[i * N + k] && A[k * N + j] && pathLength[i] != INT_MAX && pathLength[i] + 1 < pathLength[j]) {\n                    pathLength[j] = pathLength[i] + 1;\n                }\n            }\n        }\n        __syncthreads();\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid == 0) {\n        for (int k = 0; k < N; k++) {\n            for (int i = 0; i < N; i++) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + k] && A[k * N + j]) {\n                        A[i * N + j] = 1;\n                    }\n                }\n            }\n        }\n        *pathLength = A[source * N + dest];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid == source) {\n        pathLength[tid] = 0;\n    } else {\n        pathLength[tid] = INT_MAX;\n    }\n    __syncthreads();\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j] && pathLength[i] != INT_MAX && pathLength[i] + 1 < pathLength[j]) {\n                    pathLength[j] = pathLength[i] + 1;\n                }\n            }\n        }\n        __syncthreads();\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid == 0) {\n        int *dist = new int[N];\n        bool *visited = new bool[N];\n        for (int i = 0; i < N; i++) {\n            dist[i] = INT_MAX;\n            visited[i] = false;\n        }\n        dist[source] = 0;\n        for (int count = 0; count < N - 1; count++) {\n            int u = -1;\n            for (int i = 0; i < N; i++) {\n                if (!visited[i] && (u == -1 || dist[i] < dist[u])) {\n                    u = i;\n                }\n            }\n            visited[u] = true;\n            for (int v = 0; v < N; v++) {\n                if (!visited[v] && A[u * N + v] && dist[u] != INT_MAX && dist[u] + 1 < dist[v]) {\n                    dist[v] = dist[u] + 1;\n                }\n            }\n        }\n        *pathLength = dist[dest];\n        delete[] dist;\n        delete[] visited;\n    }\n}\n",
      "\n    __shared__ int dist[MAX_NODES];\n    dist[threadIdx.x] = INT_MAX;\n    __syncthreads();\n\n    if (threadIdx.x == source) {\n        dist[threadIdx.x] = 0;\n    }\n\n    for (int i = 0; i < N; ++i) {\n        if (A[threadIdx.x * N + i] && dist[i] > dist[threadIdx.x] + 1) {\n            dist[i] = dist[threadIdx.x] + 1;\n        }\n        __syncthreads();\n    }\n\n    if (threadIdx.x == dest) {\n        *pathLength = dist[threadIdx.x];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid == source) {\n            pathLength[tid] = 0;\n        } else {\n            pathLength[tid] = INT_MAX;\n        }\n    }\n    __syncthreads();\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j] && pathLength[i] != INT_MAX && pathLength[i] + 1 < pathLength[j]) {\n                    pathLength[j] = pathLength[i] + 1;\n                }\n            }\n        }\n        __syncthreads();\n    }\n\n    if (pathLength[dest] == INT_MAX) {\n        pathLength[dest] = -1;\n    }\n}\n",
      "\n    int INF = 99999;\n    __shared__ int dist[N][N];\n    \n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            if (A[i * N + j] == 0) {\n                dist[i][j] = INF;\n            } else {\n                dist[i][j] = A[i * N + j];\n            }\n        }\n    }\n    \n    for (int k = 0; k < N; k++) {\n        for (int i = 0; i < N; i++) {\n            for (int j = 0; j < N; j++) {\n                if (dist[i][k] + dist[k][j] < dist[i][j]) {\n                    dist[i][j] = dist[i][k] + dist[k][j];\n                }\n            }\n        }\n    }\n    \n    *pathLength = dist[source][dest];\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        if (tid == source) {\n            pathLength[tid] = 0;\n        } else {\n            pathLength[tid] = INT_MAX;\n        }\n    }\n    __syncthreads();\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j] && pathLength[i] != INT_MAX && pathLength[i] + 1 < pathLength[j]) {\n                    pathLength[j] = pathLength[i] + 1;\n                }\n            }\n        }\n        __syncthreads();\n    }\n\n    if (pathLength[dest] == INT_MAX) {\n        pathLength[dest] = -1;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid == source) {\n            pathLength[tid] = 0;\n        } else {\n            pathLength[tid] = INT_MAX;\n        }\n    }\n    __syncthreads();\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j] && pathLength[i] != INT_MAX && pathLength[i] + 1 < pathLength[j]) {\n                    pathLength[j] = pathLength[i] + 1;\n                }\n            }\n        }\n        __syncthreads();\n    }\n\n    if (pathLength[dest] == INT_MAX) {\n        pathLength[dest] = -1;\n    }\n}\n",
      "\n    int INF = 99999;\n    __shared__ int dist[N][N];\n\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            if (A[i * N + j] == 0) {\n                dist[i][j] = INF;\n            } else {\n                dist[i][j] = A[i * N + j];\n            }\n        }\n    }\n\n    for (int k = 0; k < N; k++) {\n        for (int i = 0; i < N; i++) {\n            for (int j = 0; j < N; j++) {\n                if (dist[i][k] + dist[k][j] < dist[i][j]) {\n                    dist[i][j] = dist[i][k] + dist[k][j];\n                }\n            }\n        }\n    }\n\n    *pathLength = dist[source][dest];\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "19_graph_shortest_path",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The shortest path length is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localMin = N * N; // Initialize local minimum to a large value\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = rank; i < N; i += size) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j]) {\n                    A[i * N + j] = 1; // Update the adjacency matrix to include the new path\n                }\n            }\n        }\n        MPI_Allreduce(&localMin, &localMin, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD); // Reduce local minimum to global minimum\n    }\n\n    if (rank == 0) {\n        return A[source * N + dest]; // Return the shortest path length from source to dest\n    } else {\n        return 0; // Other ranks return 0\n    }\n}\n",
      "\n    // Your code here\n    // Implement Dijkstra's algorithm to find the shortest path length from source to dest\n    // Use MPI to compute in parallel\n    // Return the shortest path length on rank 0\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localMin = N * N; // Initialize local minimum to a large value\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = rank; i < N; i += size) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j]) {\n                    A[i * N + j] = 1; // Update the adjacency matrix to include the new path\n                }\n            }\n        }\n\n        MPI_Allreduce(&localMin, &localMin, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        return A[source * N + dest];\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localMin = N * N; // Initialize local minimum to a large value\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = rank; i < N; i += size) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j]) {\n                    A[i * N + j] = 1; // Update the adjacency matrix to reflect the shortest path\n                }\n            }\n        }\n        MPI_Allreduce(&localMin, &localMin, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD); // Reduce to find the global minimum\n    }\n\n    if (rank == 0) {\n        return A[source * N + dest]; // Return the shortest path length on rank 0\n    } else {\n        return 0; // Other ranks return 0\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_min = N; // Initialize local_min to a large value\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = rank; i < N; i += size) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + j] > A[i * N + k] + A[k * N + j]) {\n                    A[i * N + j] = A[i * N + k] + A[k * N + j];\n                }\n            }\n        }\n        MPI_Allreduce(&local_min, &local_min, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        return A[source * N + dest];\n    } else {\n        return 0; // Other ranks return 0\n    }\n}\n",
      "\n    // Your code here\n    // Implement Dijkstra's algorithm to find the shortest path length from source to dest\n    // Use MPI to compute in parallel\n    // Return the shortest path length on rank 0\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localMin = N * N; // Initialize local minimum to a large value\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = rank; i < N; i += size) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j]) {\n                    A[i * N + j] = 1; // Update the adjacency matrix to include the new path\n                }\n            }\n        }\n\n        MPI_Allreduce(&localMin, &localMin, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        return A[source * N + dest]; // Return the shortest path length from source to dest\n    } else {\n        return 0; // Other ranks return 0\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localMin = N; // Initialize local minimum to maximum possible value\n    int globalMin; // Store the global minimum\n\n    // Calculate local minimum path length\n    for (int k = 0; k < N; ++k) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] + A[k * N + j] < A[i * N + j]) {\n                    A[i * N + j] = A[i * N + k] + A[k * N + j];\n                }\n            }\n        }\n    }\n\n    // Find the shortest path length from source to dest\n    if (rank == 0) {\n        globalMin = A[source * N + dest];\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&localMin, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            if (localMin < globalMin) {\n                globalMin = localMin;\n            }\n        }\n    } else {\n        MPI_Send(&localMin, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n    // Return the global minimum path length on rank 0\n    if (rank == 0) {\n        return globalMin;\n    } else {\n        return 0; // Other ranks return 0\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localMin = N * N; // Initialize localMin to a large value\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = rank; i < N; i += size) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + j] > A[i * N + k] + A[k * N + j]) {\n                    A[i * N + j] = A[i * N + k] + A[k * N + j];\n                }\n            }\n        }\n        MPI_Allreduce(&A[0], &localMin, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    }\n\n    int shortestPath = A[source * N + dest];\n    MPI_Reduce(&shortestPath, &localMin, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return localMin;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localMin = N; // Initialize localMin to a large value\n    for (int k = 0; k < N; ++k) {\n        for (int i = rank; i < N; i += size) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + j] > A[i * N + k] + A[k * N + j]) {\n                    A[i * N + j] = A[i * N + k] + A[k * N + j];\n                }\n            }\n        }\n        MPI_Allreduce(&localMin, &localMin, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        return A[source * N + dest];\n    } else {\n        return 0;\n    }\n}\n",
      "\n    // Your code here\n}",
      "\n    // Your code here\n    // Implement the shortest path length calculation using MPI\n    // Remember to handle the communication between MPI ranks\n    // Return the shortest path length on rank 0\n}\n",
      "\n    // Your code here\n    // Implement the shortest path length calculation using MPI\n    // Return the shortest path length on rank 0\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localMin = N * N; // Initialize local minimum to a large value\n\n    // Calculate local minimum path length\n    for (int k = 0; k < N; ++k) {\n        for (int i = rank; i < N; i += size) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j]) {\n                    A[i * N + j] = 1; // Update the adjacency matrix\n                }\n            }\n        }\n    }\n\n    // Reduce local minimum path lengths to find the global minimum\n    MPI_Allreduce(&localMin, &minPathLength, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n\n    return minPathLength;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localMin = INT_MAX;\n    int globalMin;\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = rank; i < N; i += size) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j]) {\n                    A[i * N + j] = 1;\n                }\n            }\n        }\n        MPI_Allreduce(&localMin, &globalMin, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n        localMin = globalMin;\n    }\n\n    if (rank == 0) {\n        return A[source * N + dest];\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localMin = N; // Initialize local minimum to maximum possible value\n    for (int k = 0; k < N; ++k) {\n        for (int i = rank; i < N; i += size) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j]) {\n                    A[i * N + j] = 1; // Update the adjacency matrix to reflect the shortest path\n                }\n            }\n        }\n        MPI_Allreduce(&localMin, &localMin, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD); // Reduce to find the global minimum\n    }\n\n    if (rank == 0) {\n        return A[source * N + dest]; // Return the shortest path length from source to dest\n    } else {\n        return 0; // Other ranks return 0\n    }\n}\n",
      "\n    // Your code here\n    // Implement Dijkstra's algorithm to find the shortest path length from source to dest\n    // Use MPI to distribute the workload and gather the results\n    // Return the shortest path length on rank 0\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localMin = N * N; // Initialize local minimum path length to a large value\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = rank; i < N; i += size) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + j] > A[i * N + k] + A[k * N + j]) {\n                    A[i * N + j] = A[i * N + k] + A[k * N + j];\n                }\n            }\n        }\n        MPI_Allreduce(&A[0], &A[0], N * N, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        localMin = A[source * N + dest];\n    }\n\n    int globalMin;\n    MPI_Reduce(&localMin, &globalMin, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalMin;\n    } else {\n        return -1; // Return an invalid value for non-root ranks\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localMin = N; // Initialize local minimum to maximum possible value\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = rank; i < N; i += size) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + j] > A[i * N + k] + A[k * N + j]) {\n                    A[i * N + j] = A[i * N + k] + A[k * N + j];\n                }\n            }\n        }\n        MPI_Allreduce(&A[0], &A[0], N * N, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        localMin = A[source * N + dest];\n    }\n\n    MPI_Reduce(&localMin, &localMin, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return localMin;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    // Your code here\n    // Implement the shortest path length calculation using MPI\n    // Remember to return the shortest path length on rank 0\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "19_graph_shortest_path",
    "parallelism_model": "cuda",
    "prompt": "/* Compute the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Store the result in pathLength.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\n__global__ void shortestPathLength(const int *A, size_t N, int source, int dest, int *pathLength) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid == source) {\n            pathLength[tid] = 0;\n        } else {\n            pathLength[tid] = INT_MAX;\n        }\n    }\n    __syncthreads();\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] != 0 && A[k * N + j] != 0) {\n                    if (pathLength[i] != INT_MAX && pathLength[i] + 1 < pathLength[j]) {\n                        pathLength[j] = pathLength[i] + 1;\n                    }\n                }\n            }\n        }\n        __syncthreads();\n    }\n\n    if (pathLength[dest] == INT_MAX) {\n        pathLength[dest] = -1;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid == source) {\n            pathLength[tid] = 0;\n        } else {\n            pathLength[tid] = INT_MAX;\n        }\n    }\n    __syncthreads();\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] != 0 && A[k * N + j] != 0 && pathLength[i] != INT_MAX && pathLength[i] + A[i * N + k] + A[k * N + j] < pathLength[j]) {\n                    pathLength[j] = pathLength[i] + A[i * N + k] + A[k * N + j];\n                }\n            }\n        }\n        __syncthreads();\n    }\n}\n",
      "\n    int INF = N * N; // Set the initial value for infinity\n    int tid = blockIdx.x * blockDim.x + threadIdx.x; // Calculate the thread ID\n    if (tid < N) {\n        for (int k = 0; k < N; ++k) {\n            for (int i = 0; i < N; ++i) {\n                for (int j = 0; j < N; ++j) {\n                    if (A[i * N + k] + A[k * N + j] < A[i * N + j]) {\n                        A[i * N + j] = A[i * N + k] + A[k * N + j];\n                    }\n                }\n            }\n        }\n    }\n    if (tid == 0) {\n        *pathLength = A[source * N + dest]; // Store the result in pathLength\n    }\n}\n",
      "\n    int INF = 99999;\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int k = 0; k < N; k++) {\n            for (int i = 0; i < N; i++) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + k] + A[k * N + j] < A[i * N + j]) {\n                        A[i * N + j] = A[i * N + k] + A[k * N + j];\n                    }\n                }\n            }\n        }\n    }\n    if (tid == 0) {\n        *pathLength = A[source * N + dest];\n    }\n}\n",
      "\n    int INF = 99999;\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int k = 0; k < N; k++) {\n            for (int i = 0; i < N; i++) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + k] + A[k * N + j] < A[i * N + j]) {\n                        A[i * N + j] = A[i * N + k] + A[k * N + j];\n                    }\n                }\n            }\n        }\n    }\n    if (tid == 0) {\n        *pathLength = A[source * N + dest];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid == 0) {\n        // Initialize pathLength to -1\n        *pathLength = -1;\n    }\n    __syncthreads();\n\n    if (A[source * N + dest] == 1) {\n        if (tid == 0) {\n            // If source and dest are directly connected, set pathLength to 1\n            *pathLength = 1;\n        }\n    } else {\n        // Use breadth-first search to find the shortest path length\n        __shared__ bool found;\n        if (tid == 0) {\n            found = false;\n        }\n        __syncthreads();\n\n        if (tid == source) {\n            // Mark source as visited\n            atomicExch(&found, true);\n        }\n        __syncthreads();\n\n        int depth = 0;\n        while (!found) {\n            int numThreads = blockDim.x * gridDim.x;\n            for (int i = tid; i < N; i += numThreads) {\n                if (A[tid * N + i] == 1) {\n                    if (atomicExch(&found, true) == false) {\n                        if (i == dest) {\n                            if (tid == 0) {\n                                *pathLength = depth + 1;\n                            }\n                            return;\n                        }\n                    }\n                }\n            }\n            __syncthreads();\n            depth++;\n        }\n    }\n}\n",
      "\n    int INF = 99999;\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int k = 0; k < N; k++) {\n            for (int i = 0; i < N; i++) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + k] + A[k * N + j] < A[i * N + j]) {\n                        A[i * N + j] = A[i * N + k] + A[k * N + j];\n                    }\n                }\n            }\n        }\n    }\n    if (tid == 0) {\n        *pathLength = A[source * N + dest];\n    }\n}\n",
      "\n    int INF = 9999;\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        for (int k = 0; k < N; k++) {\n            for (int i = 0; i < N; i++) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + k] + A[k * N + j] < A[i * N + j]) {\n                        A[i * N + j] = A[i * N + k] + A[k * N + j];\n                    }\n                }\n            }\n        }\n    }\n    if (tid == 0) {\n        *pathLength = A[source * N + dest];\n    }\n}\n",
      "\n    int INF = 99999;\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int k = 0; k < N; k++) {\n            for (int i = 0; i < N; i++) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + k] + A[k * N + j] < A[i * N + j]) {\n                        A[i * N + j] = A[i * N + k] + A[k * N + j];\n                    }\n                }\n            }\n        }\n    }\n    if (tid == 0) {\n        *pathLength = A[source * N + dest];\n    }\n}\n",
      "\n    // Initialize pathLength for all nodes to infinity\n    for (int i = 0; i < N; i++) {\n        pathLength[i] = INT_MAX;\n    }\n\n    // Set the path length of the source node to 0\n    pathLength[source] = 0;\n\n    // Initialize a queue for BFS traversal\n    queue<int> q;\n    q.push(source);\n\n    // Perform BFS to find the shortest path length\n    while (!q.empty()) {\n        int current = q.front();\n        q.pop();\n\n        for (int neighbor = 0; neighbor < N; neighbor++) {\n            if (A[current * N + neighbor] == 1 && pathLength[neighbor] == INT_MAX) {\n                pathLength[neighbor] = pathLength[current] + 1;\n                q.push(neighbor);\n            }\n        }\n    }\n}\n",
      "\n    int INF = 99999;\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int k = 0; k < N; k++) {\n            for (int i = 0; i < N; i++) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + k] + A[k * N + j] < A[i * N + j]) {\n                        A[i * N + j] = A[i * N + k] + A[k * N + j];\n                    }\n                }\n            }\n        }\n    }\n    if (tid == 0) {\n        *pathLength = A[source * N + dest];\n    }\n}\n",
      "\n    int INF = 99999;\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        for (int k = 0; k < N; k++) {\n            for (int i = 0; i < N; i++) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + k] + A[k * N + j] < A[i * N + j]) {\n                        A[i * N + j] = A[i * N + k] + A[k * N + j];\n                    }\n                }\n            }\n        }\n    }\n    *pathLength = A[source * N + dest];\n}\n",
      "\n    int INF = 99999;\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int k = 0; k < N; k++) {\n            for (int i = 0; i < N; i++) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + k] + A[k * N + j] < A[i * N + j]) {\n                        A[i * N + j] = A[i * N + k] + A[k * N + j];\n                    }\n                }\n            }\n        }\n    }\n    if (tid == 0) {\n        *pathLength = A[source * N + dest];\n    }\n}\n",
      "\n    int INF = 99999;\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int k = 0; k < N; k++) {\n            for (int i = 0; i < N; i++) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + k] + A[k * N + j] < A[i * N + j]) {\n                        A[i * N + j] = A[i * N + k] + A[k * N + j];\n                    }\n                }\n            }\n        }\n    }\n    if (tid == 0) {\n        *pathLength = A[source * N + dest];\n    }\n}\n",
      "\n    int INF = 99999;\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int k = 0; k < N; k++) {\n            for (int i = 0; i < N; i++) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + k] + A[k * N + j] < A[i * N + j]) {\n                        A[i * N + j] = A[i * N + k] + A[k * N + j];\n                    }\n                }\n            }\n        }\n    }\n    if (tid == 0) {\n        *pathLength = A[source * N + dest];\n    }\n}\n",
      "\n    int INF = 99999;\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int k = 0; k < N; k++) {\n            for (int i = 0; i < N; i++) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + k] + A[k * N + j] < A[i * N + j]) {\n                        A[i * N + j] = A[i * N + k] + A[k * N + j];\n                    }\n                }\n            }\n        }\n    }\n    if (tid == 0) {\n        *pathLength = A[source * N + dest];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid == source) {\n            pathLength[tid] = 0;\n        } else {\n            pathLength[tid] = INT_MAX;\n        }\n    }\n    __syncthreads();\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j] && pathLength[i] != INT_MAX && pathLength[i] + 1 < pathLength[j]) {\n                    pathLength[j] = pathLength[i] + 1;\n                }\n            }\n        }\n        __syncthreads();\n    }\n}\n",
      "\n    int INF = 99999;\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int k = 0; k < N; k++) {\n            for (int i = 0; i < N; i++) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + k] + A[k * N + j] < A[i * N + j]) {\n                        A[i * N + j] = A[i * N + k] + A[k * N + j];\n                    }\n                }\n            }\n        }\n    }\n    if (tid == 0) {\n        *pathLength = A[source * N + dest];\n    }\n}\n",
      "\n    int INF = 99999;\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int k = 0; k < N; k++) {\n            for (int i = 0; i < N; i++) {\n                for (int j = 0; j < N; j++) {\n                    if (A[i * N + k] + A[k * N + j] < A[i * N + j]) {\n                        A[i * N + j] = A[i * N + k] + A[k * N + j];\n                    }\n                }\n            }\n        }\n    }\n    if (tid == 0) {\n        *pathLength = A[source * N + dest];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid == source) {\n            pathLength[tid] = 0;\n        } else {\n            pathLength[tid] = INT_MAX;\n        }\n    }\n    __syncthreads();\n\n    for (int k = 0; k < N; ++k) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j] && pathLength[i] != INT_MAX && pathLength[i] + 1 < pathLength[j]) {\n                    pathLength[j] = pathLength[i] + 1;\n                }\n            }\n        }\n        __syncthreads();\n    }\n\n    if (pathLength[dest] == INT_MAX) {\n        pathLength[dest] = -1;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "19_graph_shortest_path",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The shortest path length is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // Your code here\n    // Implement the shortest path length computation using MPI and OpenMP\n    // Return the shortest path length on rank 0\n}\n",
      "\n    // Your code here\n    // Implement the shortest path length computation using MPI and OpenMP\n    // Return the shortest path length on rank 0\n}\n",
      "\n    // Your code here\n    // Implement the shortest path length computation using MPI and OpenMP\n    // Return the shortest path length on rank 0\n}\n",
      "\n    // Your code here\n    // Implement the shortest path length calculation using MPI and OpenMP\n    // Return the shortest path length on rank 0\n}\n",
      "\n    // Your code here\n    // Implement the shortest path length calculation using MPI and OpenMP\n    // Return the shortest path length on rank 0\n}\n",
      "\n    // Your code here\n    // Implement the shortest path length calculation using MPI and OpenMP\n    // Return the shortest path length on rank 0\n}\n",
      "\n    // Your code here\n}",
      "\n    // Your code here\n    // Implement the shortest path length calculation using MPI and OpenMP\n    // Return the shortest path length\n    return 0;\n}\n",
      "\n    // Your code here\n    // Implement the shortest path length calculation using MPI and OpenMP\n    // Return the shortest path length on rank 0\n}\n",
      "\n    // Your code here\n    // Implement the shortest path length calculation using MPI and OpenMP\n    // Return the shortest path length on rank 0\n}\n",
      "\n    // Your code here\n    // Implement the shortest path length computation using MPI and OpenMP\n    // Return the shortest path length on rank 0\n}\n",
      "\n    // Your code here\n    // Implement the shortest path length calculation using MPI and OpenMP\n    // Return the shortest path length on rank 0\n}\n",
      "\n    // Your code here\n    // Implement the shortest path length computation using MPI and OpenMP\n    // Return the shortest path length on rank 0\n}\n",
      "\n    // Your code here\n    // Implement the shortest path length computation using MPI and OpenMP\n    // Return the shortest path length on rank 0\n}\n",
      "\n    // Your code here\n    // Implement the shortest path length computation using MPI and OpenMP\n    // Remember to return the shortest path length on rank 0\n}\n",
      "\n    // Your code here\n    // Implement the shortest path length calculation using MPI and OpenMP\n    // Remember to return the shortest path length on rank 0\n}\n",
      "\n    // Your code here\n    // Implement the shortest path length calculation using MPI and OpenMP\n    // Return the shortest path length on rank 0\n}\n",
      "\n    // Your code here\n    // Implement the shortest path length computation using MPI and OpenMP\n    // Return the shortest path length on rank 0\n}\n",
      "\n    // Your code here\n    // Implement the shortest path length calculation using MPI and OpenMP\n    // Return the shortest path length on rank 0\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_min = INT_MAX;\n    int global_min;\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                if (A[i * N + k] && A[k * N + j]) {\n                    A[i * N + j] = 1;\n                }\n            }\n        }\n    }\n\n    for (int k = 0; k < N; ++k) {\n        if (A[source * N + k] && A[k * N + dest]) {\n            local_min = 2;\n            break;\n        }\n    }\n\n    MPI_Reduce(&local_min, &global_min, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min;\n    } else {\n        return 0;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "28_reduce_smallest_odd_number",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(Kokkos::View<const int*> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& l) {\n        if (x(i) % 2 != 0 && x(i) < l) {\n            l = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& l) {\n        if (x(i) % 2 != 0 && x(i) < l) {\n            l = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& l) {\n        if (x(i) % 2 != 0 && x(i) < l) {\n            l = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}",
      "\n\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& l) {\n        if (x(i) % 2 != 0 && x(i) < l) {\n            l = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& l) {\n        if (x(i) % 2 != 0 && x(i) < l) {\n            l = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& l) {\n        if (x(i) % 2 != 0 && x(i) < l) {\n            l = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& l) {\n        if (x(i) % 2 != 0 && x(i) < l) {\n            l = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& l) {\n        if (x[i] % 2 != 0 && x[i] < l) {\n            l = x[i];\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}",
      "\n\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& l) {\n        if (x(i) % 2 != 0 && x(i) < l) {\n            l = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& l) {\n        if (x(i) % 2 != 0 && x(i) < l) {\n            l = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& l) {\n        if (x(i) % 2 != 0 && x(i) < l) {\n            l = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}",
      "\n\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& l) {\n        if (x(i) % 2 != 0 && x(i) < l) {\n            l = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& l) {\n        if (x(i) % 2 != 0 && x(i) < l) {\n            l = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& l) {\n        if (x(i) % 2 != 0 && x(i) < l) {\n            l = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& l) {\n        if (x(i) % 2 != 0 && x(i) < l) {\n            l = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& l) {\n        if (x(i) % 2 != 0 && x(i) < l) {\n            l = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}",
      "\n\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& l) {\n        if (x(i) % 2 != 0 && x(i) < l) {\n            l = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, int& l) {\n        if (x(i) % 2 != 0 && x(i) < l) {\n            l = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& l) {\n        if (x(i) % 2 != 0 && x(i) < l) {\n            l = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& l) {\n        if (x(i) % 2 != 0 && x(i) < l) {\n            l = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "28_reduce_smallest_odd_number",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    return smallest;\n}",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    return smallest;\n}",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    return smallest;\n}",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    return smallest;\n}",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    return smallest;\n}",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    return smallest;\n}",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    return smallest;\n}",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    return smallest;\n}",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    return smallest;\n}",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    return smallest;\n}",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    return smallest;\n}",
      "\n    int smallest = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 != 0 && (smallest == -1 || x[i] < smallest)) {\n            #pragma omp critical\n            {\n                if (smallest == -1 || x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    return smallest;\n}",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    return smallest;\n}",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    return smallest;\n}",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    return smallest;\n}",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    return smallest;\n}",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    return smallest;\n}",
      "\n    int smallest = INT_MAX;\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n\n    if (smallest == INT_MAX) {\n        return -1; // No odd number found\n    } else {\n        return smallest;\n    }\n}",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    return smallest;\n}",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    return smallest;\n}"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "28_reduce_smallest_odd_number",
    "parallelism_model": "serial",
    "prompt": "/* Return the value of the smallest odd number in the vector x.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int smallest = INT_MAX;\n    for (int num : x) {\n        if (num % 2 != 0 && num < smallest) {\n            smallest = num;\n        }\n    }\n    if (smallest == INT_MAX) {\n        return -1; // If no odd number is found\n    }\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    for (int num : x) {\n        if (num % 2 != 0 && num < smallest) {\n            smallest = num;\n        }\n    }\n    if (smallest == INT_MAX) {\n        return -1; // If no odd number is found\n    }\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    for (int num : x) {\n        if (num % 2 != 0 && num < smallest) {\n            smallest = num;\n        }\n    }\n    if (smallest == INT_MAX) {\n        return -1; // If no odd number is found\n    } else {\n        return smallest;\n    }\n}\n",
      "\n    int smallest = INT_MAX;\n    for (int num : x) {\n        if (num % 2 != 0 && num < smallest) {\n            smallest = num;\n        }\n    }\n    if (smallest == INT_MAX) {\n        return -1; // If no odd number is found\n    } else {\n        return smallest;\n    }\n}\n",
      "\n    int smallest = INT_MAX;\n    for (int num : x) {\n        if (num % 2 != 0 && num < smallest) {\n            smallest = num;\n        }\n    }\n    if (smallest == INT_MAX) {\n        return -1; // If no odd number is found\n    }\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    for (int num : x) {\n        if (num % 2 != 0 && num < smallest) {\n            smallest = num;\n        }\n    }\n    if (smallest == INT_MAX) {\n        return -1; // If no odd number is found\n    } else {\n        return smallest;\n    }\n}\n",
      "\n    int smallest = INT_MAX;\n    for (int num : x) {\n        if (num % 2 != 0 && num < smallest) {\n            smallest = num;\n        }\n    }\n    if (smallest == INT_MAX) {\n        return -1; // If no odd number is found\n    }\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    for (int num : x) {\n        if (num % 2 != 0 && num < smallest) {\n            smallest = num;\n        }\n    }\n    if (smallest == INT_MAX) {\n        return -1; // If no odd number is found\n    }\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    for (int num : x) {\n        if (num % 2 != 0 && num < smallest) {\n            smallest = num;\n        }\n    }\n    if (smallest == INT_MAX) {\n        return -1; // If no odd number is found\n    }\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    for (int num : x) {\n        if (num % 2 != 0 && num < smallest) {\n            smallest = num;\n        }\n    }\n    if (smallest == INT_MAX) {\n        return -1; // If no odd number is found\n    }\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    for (int num : x) {\n        if (num % 2 != 0 && num < smallest) {\n            smallest = num;\n        }\n    }\n    return smallest;\n}",
      "\n    int smallest = INT_MAX;\n    for (int num : x) {\n        if (num % 2 != 0 && num < smallest) {\n            smallest = num;\n        }\n    }\n    if (smallest == INT_MAX) {\n        return -1; // If no odd number is found\n    }\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    for (int num : x) {\n        if (num % 2 != 0 && num < smallest) {\n            smallest = num;\n        }\n    }\n    if (smallest == INT_MAX) {\n        return -1; // If no odd number is found\n    }\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    for (int num : x) {\n        if (num % 2 != 0 && num < smallest) {\n            smallest = num;\n        }\n    }\n    if (smallest == INT_MAX) {\n        return -1; // If no odd number is found\n    }\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    for (int num : x) {\n        if (num % 2 != 0 && num < smallest) {\n            smallest = num;\n        }\n    }\n    if (smallest == INT_MAX) {\n        return -1; // If no odd number is found\n    }\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    for (int num : x) {\n        if (num % 2 != 0 && num < smallest) {\n            smallest = num;\n        }\n    }\n    if (smallest == INT_MAX) {\n        return -1; // If no odd number is found\n    } else {\n        return smallest;\n    }\n}",
      "\n    int smallest = INT_MAX;\n    for (int num : x) {\n        if (num % 2 != 0 && num < smallest) {\n            smallest = num;\n        }\n    }\n    if (smallest == INT_MAX) {\n        return -1; // If no odd number is found\n    }\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    for (int num : x) {\n        if (num % 2 != 0 && num < smallest) {\n            smallest = num;\n        }\n    }\n    if (smallest == INT_MAX) {\n        return -1; // If no odd number is found\n    } else {\n        return smallest;\n    }\n}\n",
      "\n    int smallest = INT_MAX;\n    for (int num : x) {\n        if (num % 2 != 0 && num < smallest) {\n            smallest = num;\n        }\n    }\n    if (smallest == INT_MAX) {\n        return -1; // If no odd number is found\n    }\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    for (int num : x) {\n        if (num % 2 != 0 && num < smallest) {\n            smallest = num;\n        }\n    }\n    if (smallest == INT_MAX) {\n        return -1; // No odd number found\n    } else {\n        return smallest;\n    }\n}"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "28_reduce_smallest_odd_number",
    "parallelism_model": "hip",
    "prompt": "/* Find the value of the smallest odd number in the vector x. Store it in smallest.\n   Use AMD HIP to compute in parallel. The kernel is launched with the same number of threads as elements in x.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\n__global__ void smallestOdd(const int *x, size_t N, int *smallest) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 != 0 && (x[tid] < *smallest || *smallest % 2 == 0)) {\n            atomicMin(smallest, x[tid]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 != 0 && (x[tid] < *smallest || *smallest % 2 == 0)) {\n            atomicMin(smallest, x[tid]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 != 0 && (x[tid] < *smallest || *smallest % 2 == 0)) {\n            atomicMin(smallest, x[tid]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 != 0) {\n            atomicMin(smallest, x[tid]);\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "28_reduce_smallest_odd_number",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on all ranks.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int smallestOdd = INT_MAX;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    int globalSmallestOdd;\n    MPI_Allreduce(&smallestOdd, &globalSmallestOdd, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallestOdd;\n}\n",
      "\n    int smallestOdd = INT_MAX;\n    for (int num : x) {\n        if (num % 2 != 0 && num < smallestOdd) {\n            smallestOdd = num;\n        }\n    }\n    int globalSmallestOdd;\n    MPI_Allreduce(&smallestOdd, &globalSmallestOdd, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallestOdd;\n}",
      "\n    int smallestOdd = INT_MAX;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    int globalSmallestOdd;\n    MPI_Allreduce(&smallestOdd, &globalSmallestOdd, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallestOdd;\n}\n",
      "\n    int smallestOdd = INT_MAX;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    int globalSmallestOdd;\n    MPI_Allreduce(&smallestOdd, &globalSmallestOdd, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallestOdd;\n}",
      "\n    int smallestOdd = INT_MAX;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    int globalSmallestOdd;\n    MPI_Allreduce(&smallestOdd, &globalSmallestOdd, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallestOdd;\n}",
      "\n    int smallestOdd = INT_MAX;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    int globalSmallestOdd;\n    MPI_Allreduce(&smallestOdd, &globalSmallestOdd, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallestOdd;\n}\n",
      "\n    int smallestOdd = INT_MAX;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    int globalSmallestOdd;\n    MPI_Allreduce(&smallestOdd, &globalSmallestOdd, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallestOdd;\n}\n",
      "\n    int smallestOdd = INT_MAX;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    int globalSmallestOdd;\n    MPI_Allreduce(&smallestOdd, &globalSmallestOdd, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallestOdd;\n}\n",
      "\n    int smallestOdd = INT_MAX;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    int globalSmallestOdd;\n    MPI_Allreduce(&smallestOdd, &globalSmallestOdd, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallestOdd;\n}\n",
      "\n    int smallestOdd = INT_MAX;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    int globalSmallestOdd;\n    MPI_Allreduce(&smallestOdd, &globalSmallestOdd, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallestOdd;\n}\n",
      "\n    int smallestOdd = INT_MAX;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    int globalSmallestOdd;\n    MPI_Allreduce(&smallestOdd, &globalSmallestOdd, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallestOdd;\n}\n",
      "\n    int smallestOdd = INT_MAX;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    int globalSmallestOdd;\n    MPI_Allreduce(&smallestOdd, &globalSmallestOdd, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallestOdd;\n}\n",
      "\n    int smallestOdd = INT_MAX;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    int globalSmallestOdd;\n    MPI_Allreduce(&smallestOdd, &globalSmallestOdd, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallestOdd;\n}\n",
      "\n    int smallestOdd = INT_MAX;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    int globalSmallestOdd;\n    MPI_Allreduce(&smallestOdd, &globalSmallestOdd, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallestOdd;\n}",
      "\n    int smallestOdd = INT_MAX;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    int globalSmallestOdd;\n    MPI_Allreduce(&smallestOdd, &globalSmallestOdd, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallestOdd;\n}",
      "\n    int smallestOdd = INT_MAX;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    int globalSmallestOdd;\n    MPI_Allreduce(&smallestOdd, &globalSmallestOdd, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallestOdd;\n}\n",
      "\n    int smallestOdd = INT_MAX;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    int globalSmallestOdd;\n    MPI_Allreduce(&smallestOdd, &globalSmallestOdd, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallestOdd;\n}\n",
      "\n    int smallestOdd = INT_MAX;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    int globalSmallestOdd;\n    MPI_Allreduce(&smallestOdd, &globalSmallestOdd, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallestOdd;\n}",
      "\n    int smallestOdd = INT_MAX;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    int globalSmallestOdd;\n    MPI_Allreduce(&smallestOdd, &globalSmallestOdd, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallestOdd;\n}\n",
      "\n    int smallestOdd = INT_MAX;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallestOdd) {\n            smallestOdd = x[i];\n        }\n    }\n    int globalSmallestOdd;\n    MPI_Allreduce(&smallestOdd, &globalSmallestOdd, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallestOdd;\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "28_reduce_smallest_odd_number",
    "parallelism_model": "cuda",
    "prompt": "/* Find the value of the smallest odd number in the vector x. Store it in smallest.\n   Use CUDA to compute in parallel. The kernel is launched with the same number of threads as elements in x.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\n__global__ void smallestOdd(const int *x, size_t N, int *smallest) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 != 0) {\n            atomicMin(smallest, x[tid]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 != 0) {\n            atomicMin(smallest, x[tid]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 != 0 && (x[tid] < *smallest || *smallest % 2 == 0)) {\n            atomicMin(smallest, x[tid]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 != 0 && (x[tid] < *smallest || *smallest % 2 == 0)) {\n            atomicMin(smallest, x[tid]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 != 0 && (x[tid] < *smallest || *smallest % 2 == 0)) {\n            atomicMin(smallest, x[tid]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 != 0) {\n        atomicMin(smallest, x[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 != 0 && (x[tid] < *smallest || *smallest % 2 == 0)) {\n            atomicMin(smallest, x[tid]);\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "28_reduce_smallest_odd_number",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on all ranks.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    int global_smallest;\n    MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return global_smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    int globalSmallest;\n    MPI_Allreduce(&smallest, &globalSmallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallest;\n}\n",
      "\n    int smallest = INT_MAX;\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n\n    int globalSmallest;\n    MPI_Allreduce(&smallest, &globalSmallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalSmallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    int global_smallest;\n    MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return global_smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n\n    int globalSmallest;\n    MPI_Allreduce(&smallest, &globalSmallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalSmallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    int global_smallest;\n    MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return global_smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    int globalSmallest;\n    MPI_Allreduce(&smallest, &globalSmallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    int globalSmallest;\n    MPI_Allreduce(&smallest, &globalSmallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    int globalSmallest;\n    MPI_Allreduce(&smallest, &globalSmallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallest;\n}\n",
      "\n    int smallest = INT_MAX;\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n\n    int globalSmallest;\n    MPI_Allreduce(&smallest, &globalSmallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n\n    return globalSmallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    int global_smallest;\n    MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return global_smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    int globalSmallest;\n    MPI_Allreduce(&smallest, &globalSmallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    int global_smallest;\n    MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return global_smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    int globalSmallest;\n    MPI_Allreduce(&smallest, &globalSmallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    int globalSmallest;\n    MPI_Allreduce(&smallest, &globalSmallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    int global_smallest;\n    MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return global_smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    int globalSmallest;\n    MPI_Allreduce(&smallest, &globalSmallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    int globalSmallest;\n    MPI_Allreduce(&smallest, &globalSmallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    int globalSmallest;\n    MPI_Allreduce(&smallest, &globalSmallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 != 0 && x[i] < smallest) {\n            #pragma omp critical\n            {\n                if (x[i] < smallest) {\n                    smallest = x[i];\n                }\n            }\n        }\n    }\n    int globalSmallest;\n    MPI_Allreduce(&smallest, &globalSmallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n    return globalSmallest;\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "26_reduce_product_of_inverses",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Use Kokkos to compute product in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(Kokkos::View<const double*> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double product = 1.0;\n    Kokkos::parallel_reduce(x.size() / 2, KOKKOS_LAMBDA(int i, double& update) {\n        update *= x[2 * i];\n        update /= x[2 * i + 1];\n    }, product);\n    return product;\n}",
      "\n    double product = 1.0;\n    Kokkos::parallel_reduce(x.size() / 2, KOKKOS_LAMBDA(int i, double& update) {\n        update *= x[2 * i];\n        update /= x[2 * i + 1];\n    }, product);\n    return product;\n}",
      "\n    double product = 1.0;\n    Kokkos::parallel_reduce(x.size() / 2, KOKKOS_LAMBDA(const int i, double& localProduct) {\n        localProduct *= x[2 * i];\n        localProduct /= x[2 * i + 1];\n    }, Kokkos::Prod<double>(product));\n    return product;\n}",
      "\n    double product = 1.0;\n    Kokkos::parallel_reduce(x.size() / 2, KOKKOS_LAMBDA(const int i, double& update) {\n        update *= x[2 * i];\n        update /= x[2 * i + 1];\n    }, product);\n    return product;\n}",
      "\n    double product = 1.0;\n    Kokkos::parallel_reduce(x.size() / 2, KOKKOS_LAMBDA(const int i, double& update) {\n        update *= x[2 * i];\n        update /= x[2 * i + 1];\n    }, product);\n    return product;\n}",
      "\n    double product = 1.0;\n    Kokkos::parallel_reduce(x.size() / 2, KOKKOS_LAMBDA(const int i, double& localProduct) {\n        localProduct *= x[2 * i];\n        localProduct /= x[2 * i + 1];\n    }, Kokkos::Prod<double>(product));\n    return product;\n}",
      "\n    double product = 1.0;\n    Kokkos::parallel_reduce(x.size() / 2, KOKKOS_LAMBDA(const int i, double& localProduct) {\n        localProduct *= x[2 * i];\n        localProduct /= x[2 * i + 1];\n    }, Kokkos::Prod<double>(product));\n    return product;\n}",
      "\n    double product = 1.0;\n    Kokkos::parallel_reduce(x.size() / 2, KOKKOS_LAMBDA(const int i, double& update) {\n        update *= x[2 * i];\n        update /= x[2 * i + 1];\n    }, product);\n    return product;\n}",
      "\n    double product = 1.0;\n    Kokkos::parallel_reduce(x.size() / 2, KOKKOS_LAMBDA(const int i, double& local_product) {\n        local_product *= x[2 * i];\n        local_product /= x[2 * i + 1];\n    }, Kokkos::Prod<double>(product));\n    return product;\n}",
      "\n    double product = 1.0;\n    Kokkos::parallel_reduce(x.size() / 2, KOKKOS_LAMBDA(const int i, double& lproduct) {\n        lproduct *= x[2 * i];\n        lproduct /= x[2 * i + 1];\n    }, product);\n    return product;\n}",
      "\n    double product = 1.0;\n    Kokkos::parallel_reduce(x.size() / 2, KOKKOS_LAMBDA(const int i, double& update) {\n        update *= x[2 * i];\n        update /= x[2 * i + 1];\n    }, product);\n    return product;\n}",
      "\n    double product = 1.0;\n    Kokkos::parallel_reduce(x.size() / 2, KOKKOS_LAMBDA(const int i, double& localProduct) {\n        localProduct *= x[2 * i];\n        localProduct /= x[2 * i + 1];\n    }, Kokkos::Prod<double>(product));\n    return product;\n}",
      "\n    double product = 1.0;\n    Kokkos::parallel_reduce(x.size() / 2, KOKKOS_LAMBDA(const int i, double& local_product) {\n        local_product *= x[2 * i];\n        local_product /= x[2 * i + 1];\n    }, Kokkos::Prod<double>(product));\n    return product;\n}",
      "\n    double product = 1.0;\n    Kokkos::parallel_reduce(x.size() / 2, KOKKOS_LAMBDA(const int i, double& local_product) {\n        local_product *= x[2 * i];\n        local_product /= x[2 * i + 1];\n    }, Kokkos::Prod<double>(product));\n    return product;\n}",
      "\n    double product = 1.0;\n    Kokkos::parallel_reduce(x.size() / 2, KOKKOS_LAMBDA(int i, double& update) {\n        update *= x[2 * i];\n        update /= x[2 * i + 1];\n    }, product);\n    return product;\n}",
      "\n    double result = 1.0;\n    Kokkos::parallel_reduce(x.size() / 2, KOKKOS_LAMBDA(const int i, double& update) {\n        update *= x[2 * i];\n        update /= x[2 * i + 1];\n    }, result);\n    return result;\n}",
      "\n    double product = 1.0;\n    Kokkos::parallel_reduce(x.size() / 2, KOKKOS_LAMBDA(const int i, double& localProduct) {\n        localProduct *= x[2 * i];\n        localProduct /= x[2 * i + 1];\n    }, Kokkos::Prod<double>(product));\n    return product;\n}",
      "\n    double product = 1.0;\n    Kokkos::parallel_reduce(x.size() / 2, KOKKOS_LAMBDA(const int i, double& lproduct) {\n        lproduct *= x[2 * i];\n        lproduct /= x[2 * i + 1];\n    }, product);\n    return product;\n}",
      "\n    double product = 1.0;\n    Kokkos::parallel_reduce(x.size() / 2, KOKKOS_LAMBDA(const int i, double& local_product) {\n        local_product *= x[2 * i];\n        local_product /= x[2 * i + 1];\n    }, Kokkos::Prod<double>(product));\n    return product;\n}",
      "\n    double product = 1.0;\n    Kokkos::parallel_reduce(x.size() / 2, KOKKOS_LAMBDA(const int i, double& lproduct) {\n        lproduct *= x[2 * i];\n        lproduct /= x[2 * i + 1];\n    }, product);\n    return product;\n}"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "26_reduce_product_of_inverses",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Use OpenMP to compute product in parallel.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1.0 / x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1.0 / x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1.0 / x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1.0 / x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= 1.0 / x[i];\n        } else {\n            product *= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1.0 / x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= 1.0 / x[i];\n        } else {\n            product *= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1.0 / x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1.0 / x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= 1.0 / x[i];\n        } else {\n            product *= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= 1.0 / x[i];\n        } else {\n            product *= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1.0 / x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1.0 / x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= 1.0 / x[i];\n        } else {\n            product *= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1.0 / x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1.0 / x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1.0 / x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1.0 / x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1.0 / x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1.0 / x[i];\n        }\n    }\n    return product;\n}"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "26_reduce_product_of_inverses",
    "parallelism_model": "serial",
    "prompt": "/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= x[i];\n        } else {\n            product /= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= x[i];\n        } else {\n            product /= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= x[i];\n        } else {\n            product /= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= (1.0 / x[i]);\n        } else {\n            product *= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= x[i];\n        } else {\n            product /= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= x[i];\n        } else {\n            product /= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= (1.0 / x[i]);\n        } else {\n            product *= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= (1.0 / x[i]);\n        } else {\n            product *= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= (1.0 / x[i]);\n        } else {\n            product *= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= x[i];\n        } else {\n            product /= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= x[i];\n        } else {\n            product /= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= (1.0 / x[i]);\n        } else {\n            product *= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= x[i];\n        } else {\n            product /= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= (1.0 / x[i]);\n        } else {\n            product *= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= (1.0 / x[i]);\n        } else {\n            product *= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= x[i];\n        } else {\n            product /= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= x[i];\n        } else {\n            product /= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= x[i];\n        } else {\n            product /= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= x[i];\n        } else {\n            product /= x[i];\n        }\n    }\n    return product;\n}",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= (1.0 / x[i]);\n        } else {\n            product *= x[i];\n        }\n    }\n    return product;\n}"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "26_reduce_product_of_inverses",
    "parallelism_model": "hip",
    "prompt": "/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Store the result in product.\n   Use AMD HIP to compute product in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\n__global__ void productWithInverses(const double *x, size_t N, double *product) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i <= tid; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i <= tid; ++i) {\n            if (i % 2 == 1) {\n                result *= (1.0 / x[i]);\n            } else {\n                result *= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i <= tid; ++i) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i <= tid; ++i) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i <= tid; ++i) {\n            if (i % 2 == 1) {\n                result *= (1.0 / x[i]);\n            } else {\n                result *= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (idx % 2 == 1) {\n            atomicMul(product, x[idx]);\n        } else {\n            atomicDiv(product, x[idx]);\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n\n    if (tid < N) {\n        for (size_t i = 0; i <= tid; ++i) {\n            if (i % 2 == 1) {\n                result *= (1.0 / x[i]);\n            } else {\n                result *= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i <= tid; ++i) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i <= tid; ++i) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i <= tid; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n                if (i + 1 < N) {\n                    result *= 1 / x[i + 1];\n                }\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n\n    if (tid < N) {\n        for (int i = 0; i <= tid; ++i) {\n            if (i % 2 == 1) {\n                result *= (1.0 / x[i]);\n            } else {\n                result *= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (size_t i = 0; i <= tid; ++i) {\n            if (i % 2 == 1) {\n                result *= (1.0 / x[i]);\n            } else {\n                result *= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i <= tid; ++i) {\n            if (i % 2 == 1) {\n                result *= (1.0 / x[i]);\n            } else {\n                result *= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double tempProduct = 1.0;\n    if (tid < N) {\n        for (int i = 0; i <= tid; ++i) {\n            if (i % 2 == 1) {\n                tempProduct *= 1.0 / x[i];\n            } else {\n                tempProduct *= x[i];\n            }\n        }\n        atomicMul(product, tempProduct);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i <= tid; ++i) {\n            if (i % 2 == 1) {\n                result *= (1.0 / x[i]);\n            } else {\n                result *= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i <= tid; ++i) {\n            if (i % 2 != 0) {\n                result *= (1.0 / x[i]);\n            } else {\n                result *= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double result = 1.0;\n        for (int i = 0; i <= tid; ++i) {\n            if (i % 2 == 1) {\n                result *= 1.0 / x[i];\n            } else {\n                result *= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double tempProduct = 1.0;\n    if (tid < N) {\n        for (size_t i = 0; i <= tid; ++i) {\n            if (i % 2 == 1) {\n                tempProduct *= 1.0 / x[i];\n            } else {\n                tempProduct *= x[i];\n            }\n        }\n        atomicMul(product, tempProduct);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "26_reduce_product_of_inverses",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Use MPI to compute product in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the product on all ranks.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double product = 1.0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < x.size(); i += size) {\n        if (i % 2 == 1) {\n            product *= x[i];\n            if (i + 1 < x.size()) {\n                product /= x[i + 1];\n            }\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < x.size(); i += size) {\n        if (i % 2 == 1) {\n            product *= (1.0 / x[i]);\n        } else {\n            product *= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < x.size(); i += size) {\n        if (i % 2 == 1) {\n            product *= x[i];\n        } else {\n            product /= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < x.size(); i += size) {\n        if (i % 2 == 1) {\n            product *= x[i];\n        } else {\n            product /= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < x.size(); i += size) {\n        if (i % 2 == 1) {\n            product *= x[i];\n            if (i + 1 < x.size()) {\n                product /= x[i + 1];\n            }\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < x.size(); i += size) {\n        if (i % 2 == 1) {\n            product *= x[i];\n            product /= x[i - 1];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < x.size(); i += size) {\n        if (i % 2 == 1) {\n            product *= (1.0 / x[i]);\n        } else {\n            product *= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double localProduct = 1.0;\n    for (int i = rank; i < x.size(); i += size) {\n        if (i % 2 == 1) {\n            localProduct *= (1.0 / x[i]);\n        } else {\n            localProduct *= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&localProduct, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < x.size(); i += size) {\n        if (i % 2 == 1) {\n            product *= (1.0 / x[i]);\n        } else {\n            product *= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < x.size(); i += size) {\n        if (i % 2 == 1) {\n            product *= x[i];\n        } else {\n            product /= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < x.size(); i += size) {\n        if (i % 2 == 1) {\n            product *= x[i];\n            if (i + 1 < x.size()) {\n                product /= x[i + 1];\n            }\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < x.size(); i += size) {\n        if (i % 2 == 1) {\n            product *= x[i];\n            product /= x[i - 1];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < x.size(); i += size) {\n        if (i % 2 == 1) {\n            product *= (1.0 / x[i]);\n        } else {\n            product *= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < x.size(); i += size) {\n        if (i % 2 == 1) {\n            product *= x[i];\n            if (i + 1 < x.size()) {\n                product /= x[i + 1];\n            }\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product /= x[i];\n        }\n    }\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n    return globalProduct;\n}",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product /= x[i];\n        }\n    }\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n    return globalProduct;\n}",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= x[i];\n        } else {\n            product /= x[i];\n        }\n    }\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < x.size(); i += size) {\n        if (i % 2 == 1) {\n            product *= (1.0 / x[i]);\n        } else {\n            product *= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= x[i];\n        } else {\n            product /= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < x.size(); i += size) {\n        if (i % 2 == 1) {\n            product *= (1.0 / x[i]);\n        } else {\n            product *= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "26_reduce_product_of_inverses",
    "parallelism_model": "cuda",
    "prompt": "/* Compute the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Store the result in product.\n   Use CUDA to compute product in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\n__global__ void productWithInverses(const double *x, size_t N, double *product) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result *= 1 / x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result *= 1 / x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result *= 1.0 / x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result *= 1 / x[i];\n            }\n        }\n        product[tid] = result;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double result = 1.0;\n    if (tid < N) {\n        for (int i = 0; i < N; i++) {\n            if (i % 2 == 1) {\n                result *= x[i];\n            } else {\n                result /= x[i];\n            }\n        }\n        atomicMul(product, result);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "26_reduce_product_of_inverses",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Use MPI and OpenMP to compute product in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the product on all ranks.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= 1.0 / x[i];\n        } else {\n            product *= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= 1.0 / x[i];\n        } else {\n            product *= x[i];\n        }\n    }\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= 1.0 / x[i];\n        } else {\n            product *= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= 1.0 / x[i];\n        } else {\n            product *= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= 1.0 / x[i];\n        } else {\n            product *= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= 1.0 / x[i];\n        } else {\n            product *= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= (1.0 / x[i]);\n        } else {\n            product *= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= 1.0 / x[i];\n        } else {\n            product *= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= 1.0 / x[i];\n        } else {\n            product *= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= (1.0 / x[i]);\n        } else {\n            product *= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= 1.0 / x[i];\n        } else {\n            product *= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= 1.0 / x[i];\n        } else {\n            product *= x[i];\n        }\n    }\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n    return globalProduct;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double product = 1.0;\n\n    #pragma omp parallel for reduction(*:product)\n    for (int i = rank; i < x.size(); i += size) {\n        if (i % 2 == 1) {\n            product *= 1.0 / x[i];\n        } else {\n            product *= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= (1.0 / x[i]);\n        } else {\n            product *= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= 1.0 / x[i];\n        } else {\n            product *= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 1) {\n            product *= 1.0 / x[i];\n        } else {\n            product *= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = rank; i < x.size(); i += size) {\n        if (i % 2 == 1) {\n            product *= 1.0 / x[i];\n        } else {\n            product *= x[i];\n        }\n    }\n\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1.0 / x[i];\n        }\n    }\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1.0 / x[i];\n        }\n    }\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n    return globalProduct;\n}\n",
      "\n    double product = 1.0;\n    #pragma omp parallel for reduction(*:product)\n    for (int i = 0; i < x.size(); i++) {\n        if (i % 2 == 0) {\n            product *= x[i];\n        } else {\n            product *= 1.0 / x[i];\n        }\n    }\n    double globalProduct;\n    MPI_Allreduce(&product, &globalProduct, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n    return globalProduct;\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "29_reduce_sum_of_min_of_pairs",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Use Kokkos to sum in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(Kokkos::View<const double*> const& x, Kokkos::View<const double*> const& y) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double sum = 0;\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += std::min(x(i), y(i));\n    }, sum);\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += (x(i) < y(i)) ? x(i) : y(i);\n    }, sum);\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += std::min(x(i), y(i));\n    }, sum);\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += min(x(i), y(i));\n    }, sum);\n    return sum;\n}\n",
      "\n    double sum = 0;\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += (x(i) < y(i)) ? x(i) : y(i);\n    }, sum);\n    return sum;\n}\n",
      "\n    double sum = 0;\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += std::min(x(i), y(i));\n    }, sum);\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += (x(i) < y(i)) ? x(i) : y(i);\n    }, sum);\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += min(x(i), y(i));\n    }, sum);\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += std::min(x(i), y(i));\n    }, sum);\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += std::min(x(i), y(i));\n    }, sum);\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += std::min(x(i), y(i));\n    }, sum);\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += std::min(x(i), y(i));\n    }, sum);\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += min(x(i), y(i));\n    }, sum);\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += std::min(x(i), y(i));\n    }, sum);\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += (x(i) < y(i)) ? x(i) : y(i);\n    }, sum);\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += std::min(x(i), y(i));\n    }, sum);\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += std::min(x(i), y(i));\n    }, sum);\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += std::min(x(i), y(i));\n    }, sum);\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += std::min(x(i), y(i));\n    }, sum);\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += std::min(x(i), y(i));\n    }, sum);\n    return sum;\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "29_reduce_sum_of_min_of_pairs",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Use OpenMP to sum in parallel.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); ++i) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "29_reduce_sum_of_min_of_pairs",
    "parallelism_model": "serial",
    "prompt": "/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); ++i) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); ++i) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "29_reduce_sum_of_min_of_pairs",
    "parallelism_model": "hip",
    "prompt": "/* Compute the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Store the result in sum.\n   Use AMD HIP to sum in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\n__global__ void sumOfMinimumElements(const double *x, const double *y, size_t N, double *sum) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        sum[i] = min(x[i], y[i]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        sum[i] = min(x[i], y[i]);\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        sum[i] = min(x[i], y[i]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "29_reduce_sum_of_min_of_pairs",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Use MPI to sum in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Return the sum on all ranks.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double sum = 0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += std::min(x[i], y[i]);\n    }\n    double totalSum;\n    MPI_Allreduce(&sum, &totalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    return totalSum;\n}\n",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double totalSum;\n    MPI_Allreduce(&sum, &totalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    return totalSum;\n}\n",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double globalSum;\n    MPI_Allreduce(&sum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    return globalSum;\n}\n",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double globalSum;\n    MPI_Allreduce(&sum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    return globalSum;\n}\n",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double totalSum;\n    MPI_Allreduce(&sum, &totalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    return totalSum;\n}\n",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double totalSum;\n    MPI_Allreduce(&sum, &totalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    return totalSum;\n}\n",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double globalSum;\n    MPI_Allreduce(&sum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    return globalSum;\n}\n",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double totalSum;\n    MPI_Allreduce(&sum, &totalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    return totalSum;\n}\n",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double globalSum;\n    MPI_Allreduce(&sum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    return globalSum;\n}\n",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double totalSum;\n    MPI_Allreduce(&sum, &totalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    return totalSum;\n}\n",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double totalSum;\n    MPI_Allreduce(&sum, &totalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    return totalSum;\n}\n",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double totalSum;\n    MPI_Allreduce(&sum, &totalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    return totalSum;\n}\n",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double totalSum;\n    MPI_Allreduce(&sum, &totalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    return totalSum;\n}\n",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double globalSum;\n    MPI_Allreduce(&sum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    return globalSum;\n}\n",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double totalSum;\n    MPI_Allreduce(&sum, &totalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    return totalSum;\n}\n",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double totalSum;\n    MPI_Allreduce(&sum, &totalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    return totalSum;\n}\n",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double totalSum;\n    MPI_Allreduce(&sum, &totalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    return totalSum;\n}\n",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double totalSum;\n    MPI_Allreduce(&sum, &totalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    return totalSum;\n}\n",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double globalSum;\n    MPI_Allreduce(&sum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    return globalSum;\n}\n",
      "\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    double totalSum;\n    MPI_Allreduce(&sum, &totalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    return totalSum;\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "29_reduce_sum_of_min_of_pairs",
    "parallelism_model": "cuda",
    "prompt": "/* Compute the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Store the result in sum.\n   Use CUDA to sum in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\n__global__ void sumOfMinimumElements(const double *x, const double *y, size_t N, double *sum) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        sum[i] = min(x[i], y[i]);\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        sum[i] = min(x[i], y[i]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        sum[i] = min(x[i], y[i]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        sum[index] = min(x[index], y[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        sum[index] = min(x[index], y[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        sum[index] = min(x[index], y[index]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        sum[i] = min(x[i], y[i]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        sum[index] = min(x[index], y[index]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        sum[tid] = min(x[tid], y[tid]);\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        sum[i] = min(x[i], y[i]);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "29_reduce_sum_of_min_of_pairs",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Use MPI and OpenMP to sum in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Return the sum on all ranks.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int size = x.size();\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < size; i++) {\n        local_sum += std::min(x[i], y[i]);\n    }\n\n    double global_sum = 0.0;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum;\n}\n",
      "\n    int size = x.size();\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < size; i++) {\n        local_sum += std::min(x[i], y[i]);\n    }\n\n    double global_sum = 0.0;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum;\n}\n",
      "\n    int size = x.size();\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < size; i++) {\n        local_sum += std::min(x[i], y[i]);\n    }\n\n    double global_sum = 0.0;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum;\n}\n",
      "\n    int size = x.size();\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < size; i++) {\n        local_sum += std::min(x[i], y[i]);\n    }\n\n    double global_sum = 0.0;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum;\n}\n",
      "\n    int size = x.size();\n    double local_sum = 0.0;\n    \n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < size; i++) {\n        local_sum += std::min(x[i], y[i]);\n    }\n\n    double global_sum = 0.0;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum;\n}\n",
      "\n    int size = x.size();\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < size; i++) {\n        local_sum += std::min(x[i], y[i]);\n    }\n\n    double global_sum = 0.0;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum;\n}\n",
      "\n    int size = x.size();\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < size; i++) {\n        local_sum += std::min(x[i], y[i]);\n    }\n\n    double global_sum = 0.0;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum;\n}\n",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n",
      "\n    int size = x.size();\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < size; i++) {\n        local_sum += std::min(x[i], y[i]);\n    }\n\n    double global_sum = 0.0;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum;\n}\n",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}\n",
      "\n    int size = x.size();\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < size; i++) {\n        local_sum += std::min(x[i], y[i]);\n    }\n\n    double global_sum = 0.0;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum;\n}\n",
      "\n    int size = x.size();\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < size; i++) {\n        local_sum += std::min(x[i], y[i]);\n    }\n\n    double global_sum = 0.0;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum;\n}\n",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += std::min(x[i], y[i]);\n    }\n\n    double totalSum = 0.0;\n    MPI_Allreduce(&sum, &totalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return totalSum;\n}\n",
      "\n    int size = x.size();\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < size; ++i) {\n        local_sum += std::min(x[i], y[i]);\n    }\n\n    double global_sum = 0.0;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum;\n}\n",
      "\n    int size = x.size();\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < size; i++) {\n        local_sum += std::min(x[i], y[i]);\n    }\n\n    double global_sum = 0.0;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum;\n}\n",
      "\n    int size = x.size();\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < size; i++) {\n        local_sum += std::min(x[i], y[i]);\n    }\n\n    double global_sum = 0.0;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum;\n}\n",
      "\n    int size = x.size();\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < size; i++) {\n        local_sum += std::min(x[i], y[i]);\n    }\n\n    double global_sum = 0.0;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum;\n}\n",
      "\n    int size = x.size();\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < size; i++) {\n        local_sum += std::min(x[i], y[i]);\n    }\n\n    double global_sum = 0.0;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum;\n}\n",
      "\n    int size = x.size();\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < size; i++) {\n        local_sum += std::min(x[i], y[i]);\n    }\n\n    double global_sum = 0.0;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum;\n}\n",
      "\n    int size = x.size();\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < size; i++) {\n        local_sum += std::min(x[i], y[i]);\n    }\n\n    double global_sum = 0.0;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum;\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "27_reduce_average",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the average of the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(Kokkos::View<const double*> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n    return sum / x.size();\n}",
      "\n  double sum = 0.0;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n    local_sum += x(i);\n  }, sum);\n  return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n\n    return sum / x.size();\n}",
      "\n  double sum = 0.0;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n    local_sum += x(i);\n  }, sum);\n  return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n\n    return sum / x.size();\n}"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "27_reduce_average",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the average of the vector x.\n   Use OpenMP to compute in parallel.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "27_reduce_average",
    "parallelism_model": "serial",
    "prompt": "/* Return the average of the vector x.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double sum = 0.0;\n    for (double num : x) {\n        sum += num;\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    for (double num : x) {\n        sum += num;\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    for (double num : x) {\n        sum += num;\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    for (double num : x) {\n        sum += num;\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    for (double num : x) {\n        sum += num;\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    for (double num : x) {\n        sum += num;\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    for (double num : x) {\n        sum += num;\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    for (double num : x) {\n        sum += num;\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    for (double num : x) {\n        sum += num;\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0;\n    for (double num : x) {\n        sum += num;\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    for (double num : x) {\n        sum += num;\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    for (double num : x) {\n        sum += num;\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    for (double num : x) {\n        sum += num;\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    for (double num : x) {\n        sum += num;\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    for (double num : x) {\n        sum += num;\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    for (double num : x) {\n        sum += num;\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    for (double num : x) {\n        sum += num;\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    for (double num : x) {\n        sum += num;\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    for (double num : x) {\n        sum += num;\n    }\n    return sum / x.size();\n}",
      "\n    double sum = 0.0;\n    for (double num : x) {\n        sum += num;\n    }\n    return sum / x.size();\n}"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "27_reduce_average",
    "parallelism_model": "hip",
    "prompt": "/* Compute the average of the vector x. Store the result in average.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n   \n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\n__global__ void average(const double *x, size_t N, double *average) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(average, x[tid] / N);\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    sum = atomicAdd(average, sum);\n    __syncthreads();\n    if (tid == 0) {\n        *average /= N;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(average, x[tid] / N);\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(average, x[tid] / N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(average, x[tid] / N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(average, x[tid] / N);\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(average, x[tid] / N);\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(average, x[tid] / N);\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(average, x[tid] / N);\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    sum = atomicAdd(average, sum);\n    __syncthreads();\n    if (tid == 0) {\n        *average /= N;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(average, x[tid] / N);\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(average, x[tid] / N);\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(average, x[tid] / N);\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(average, x[tid] / N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(average, x[tid] / N);\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(average, x[tid] / N);\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(average, x[tid] / N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum += x[tid];\n    }\n    atomicAdd(average, sum / N);\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(average, x[tid] / N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(average, x[tid] / N);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "27_reduce_average",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the average of the vector x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the average on all ranks.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    for (double val : x) {\n        local_sum += val;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    for (double val : x) {\n        local_sum += val;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    for (double val : x) {\n        local_sum += val;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double localSum = 0.0;\n    for (double val : x) {\n        localSum += val;\n    }\n\n    double globalSum;\n    MPI_Allreduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return globalSum / (x.size() * size);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double localSum = 0.0;\n    for (double val : x) {\n        localSum += val;\n    }\n\n    double globalSum;\n    MPI_Allreduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return globalSum / (x.size() * size);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double localSum = 0.0;\n    for (double val : x) {\n        localSum += val;\n    }\n\n    double globalSum;\n    MPI_Allreduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return globalSum / (x.size() * size);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    for (double val : x) {\n        local_sum += val;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    for (double val : x) {\n        local_sum += val;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    for (double val : x) {\n        local_sum += val;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    for (double val : x) {\n        local_sum += val;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    for (double value : x) {\n        local_sum += value;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double localSum = 0.0;\n    for (double val : x) {\n        localSum += val;\n    }\n\n    double globalSum;\n    MPI_Allreduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return globalSum / (size * x.size());\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double localSum = 0.0;\n    for (double val : x) {\n        localSum += val;\n    }\n\n    double globalSum;\n    MPI_Allreduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return globalSum / (size * x.size());\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double localSum = 0.0;\n    for (double val : x) {\n        localSum += val;\n    }\n\n    double globalSum;\n    MPI_Allreduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return globalSum / (size * x.size());\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double localSum = 0.0;\n    for (double val : x) {\n        localSum += val;\n    }\n\n    double globalSum;\n    MPI_Allreduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return globalSum / (x.size() * size);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    for (double val : x) {\n        local_sum += val;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    for (double val : x) {\n        local_sum += val;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    for (double val : x) {\n        local_sum += val;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double localSum = 0.0;\n    for (double val : x) {\n        localSum += val;\n    }\n\n    double globalSum;\n    MPI_Allreduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return globalSum / (size * x.size());\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double localSum = 0.0;\n    for (double val : x) {\n        localSum += val;\n    }\n\n    double globalSum;\n    MPI_Allreduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return globalSum / (size * x.size());\n}"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "27_reduce_average",
    "parallelism_model": "cuda",
    "prompt": "/* Compute the average of the vector x. Store the result in average.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n   \n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\n__global__ void average(const double *x, size_t N, double *average) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    sum = atomicAdd(average, sum);\n    __syncthreads();\n    if (tid == 0) {\n        *average /= N;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    sum = atomicAdd(average, sum);\n    __syncthreads();\n    if (tid == 0) {\n        *average /= N;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    sum = atomicAdd(average, sum);\n    __syncthreads();\n    if (tid == 0) {\n        *average /= N;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    sum = atomicAdd(average, sum);\n    __syncthreads();\n    if (tid == 0) {\n        *average /= N;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    sum = atomicAdd(average, sum);\n    __syncthreads();\n    if (tid == 0) {\n        *average /= N;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    sum = atomicAdd(average, sum);\n    __syncthreads();\n    if (tid == 0) {\n        *average /= N;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    sum = atomicAdd(average, sum);\n    __syncthreads();\n    if (tid == 0) {\n        *average /= N;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    sum = atomicAdd(average, sum);\n    __syncthreads();\n    if (tid == 0) {\n        *average /= N;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    sum = atomicAdd(average, sum);\n    __syncthreads();\n    if (tid == 0) {\n        *average /= N;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    sum = atomicAdd(average, sum);\n    __syncthreads();\n    if (tid == 0) {\n        *average /= N;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    sum = atomicAdd(average, sum);\n    __syncthreads();\n    if (tid == 0) {\n        *average /= N;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    sum = atomicAdd(average, sum);\n    __syncthreads();\n    if (tid == 0) {\n        *average /= N;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    sum = atomicAdd(average, sum);\n    __syncthreads();\n    if (tid == 0) {\n        *average /= N;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(average, x[tid] / N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    sum = atomicAdd(average, sum);\n    __syncthreads();\n    if (tid == 0) {\n        *average /= N;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    sum = atomicAdd(average, sum);\n    __syncthreads();\n    if (tid == 0) {\n        *average /= N;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    atomicAdd(average, sum / N);\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    atomicAdd(average, sum / N);\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    sum = atomicAdd(average, sum);\n    __syncthreads();\n    if (tid == 0) {\n        *average /= N;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    atomicAdd(average, sum / N);\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "27_reduce_average",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the average of the vector x.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the average on all ranks.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); i++) {\n        local_sum += x[i];\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (size * x.size());\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); i++) {\n        local_sum += x[i];\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (size * x.size());\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); ++i) {\n        local_sum += x[i];\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (size * x.size());\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); i++) {\n        local_sum += x[i];\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (size * x.size());\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); ++i) {\n        local_sum += x[i];\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (size * x.size());\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); i++) {\n        local_sum += x[i];\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (size * x.size());\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); i++) {\n        local_sum += x[i];\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (size * x.size());\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); ++i) {\n        local_sum += x[i];\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (size * x.size());\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); i++) {\n        local_sum += x[i];\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (size * x.size());\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); i++) {\n        local_sum += x[i];\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (size * x.size());\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); i++) {\n        local_sum += x[i];\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (size * x.size());\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); i++) {\n        local_sum += x[i];\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (size * x.size());\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); ++i) {\n        local_sum += x[i];\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (size * x.size());\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); ++i) {\n        local_sum += x[i];\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (size * x.size());\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); i++) {\n        local_sum += x[i];\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (size * x.size());\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); i++) {\n        local_sum += x[i];\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (size * x.size());\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); ++i) {\n        local_sum += x[i];\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (size * x.size());\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); ++i) {\n        local_sum += x[i];\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (size * x.size());\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); ++i) {\n        local_sum += x[i];\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (size * x.size());\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0.0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); i++) {\n        local_sum += x[i];\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (size * x.size());\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "25_reduce_xor",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Use Kokkos to reduce in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(Kokkos::View<const bool*> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    bool result = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n        lresult ^= x(i);\n    }, result);\n    return result;\n}",
      "\n    bool result = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n        lresult ^= x(i);\n    }, result);\n    return result;\n}",
      "\n    bool result = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n        lresult ^= x(i);\n    }, result);\n    return result;\n}",
      "\n    bool result = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& local_result) {\n        local_result ^= x(i);\n    }, result);\n    return result;\n}",
      "\n    bool result = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n        lresult ^= x(i);\n    }, result);\n    return result;\n}",
      "\n    bool result = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n        lresult = lresult ^ x(i);\n    }, result);\n    return result;\n}",
      "\n    bool result = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n        lresult ^= x(i);\n    }, result);\n    return result;\n}",
      "\n    bool result = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n        lresult ^= x(i);\n    }, result);\n    return result;\n}",
      "\n    bool result = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n        lresult ^= x(i);\n    }, result);\n    return result;\n}",
      "\n    bool result = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n        lresult ^= x(i);\n    }, result);\n    return result;\n}",
      "\n    bool result = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n        lresult ^= x(i);\n    }, result);\n    return result;\n}",
      "\n    bool result = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n        lresult ^= x(i);\n    }, result);\n    return result;\n}",
      "\n    bool result = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n        lresult ^= x(i);\n    }, result);\n    return result;\n}",
      "\n    bool result = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n        lresult ^= x(i);\n    }, result);\n    return result;\n}",
      "\n    bool result = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n        lresult = lresult ^ x(i);\n    }, result);\n    return result;\n}",
      "\n    bool result = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n        lresult ^= x(i);\n    }, result);\n    return result;\n}",
      "\n    bool result = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n        lresult = lresult ^ x(i);\n    }, result);\n    return result;\n}",
      "\n    bool result = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n        lresult ^= x(i);\n    }, result);\n    return result;\n}",
      "\n    bool result = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n        lresult = lresult ^ x(i);\n    }, result);\n    return result;\n}",
      "\n    bool result = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n        lresult ^= x(i);\n    }, result);\n    return result;\n}"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "25_reduce_xor",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Use OpenMP to reduce in parallel.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); i++) {\n        result ^= x[i];\n    }\n    return result;\n}",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (int i = 0; i < x.size(); ++i) {\n        result ^= x[i];\n    }\n    return result;\n}"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "25_reduce_xor",
    "parallelism_model": "serial",
    "prompt": "/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    bool result = false;\n    for (bool value : x) {\n        result ^= value;\n    }\n    return result;\n}",
      "\n    bool result = false;\n    for (bool value : x) {\n        result ^= value;\n    }\n    return result;\n}",
      "\n    bool result = false;\n    for (bool value : x) {\n        result ^= value;\n    }\n    return result;\n}",
      "\n    bool result = false;\n    for (bool value : x) {\n        result ^= value;\n    }\n    return result;\n}",
      "\n    bool result = false;\n    for (bool value : x) {\n        result ^= value;\n    }\n    return result;\n}",
      "\n    bool result = false;\n    for (bool value : x) {\n        result ^= value;\n    }\n    return result;\n}",
      "\n    bool result = false;\n    for (bool value : x) {\n        result ^= value;\n    }\n    return result;\n}",
      "\n    bool result = false;\n    for (bool value : x) {\n        result ^= value;\n    }\n    return result;\n}",
      "\n    bool result = false;\n    for (bool value : x) {\n        result ^= value;\n    }\n    return result;\n}",
      "\n    bool result = false;\n    for (bool value : x) {\n        result ^= value;\n    }\n    return result;\n}",
      "\n    bool result = false;\n    for (bool value : x) {\n        result ^= value;\n    }\n    return result;\n}",
      "\n    bool result = false;\n    for (bool value : x) {\n        result ^= value;\n    }\n    return result;\n}",
      "\n    bool result = false;\n    for (bool val : x) {\n        result ^= val;\n    }\n    return result;\n}",
      "\n    bool result = false;\n    for (bool value : x) {\n        result ^= value;\n    }\n    return result;\n}",
      "\n    bool result = false;\n    for (bool value : x) {\n        result ^= value;\n    }\n    return result;\n}",
      "\n    bool result = false;\n    for (bool val : x) {\n        result ^= val;\n    }\n    return result;\n}",
      "\n    bool result = false;\n    for (bool value : x) {\n        result ^= value;\n    }\n    return result;\n}",
      "\n    bool result = false;\n    for (bool value : x) {\n        result ^= value;\n    }\n    return result;\n}",
      "\n    bool result = false;\n    for (bool value : x) {\n        result ^= value;\n    }\n    return result;\n}",
      "\n    bool result = false;\n    for (bool value : x) {\n        result ^= value;\n    }\n    return result;\n}"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "25_reduce_xor",
    "parallelism_model": "hip",
    "prompt": "/* Compute the logical XOR reduction of the vector of bools x. Store the result in output.\n   Use AMD HIP to reduce in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\n__global__ void reduceLogicalXOR(const bool *x, size_t N, bool *output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            x[tid] ^= x[tid + s];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (int stride = 1; stride < N; stride *= 2) {\n        if (tid % (2 * stride) == 0) {\n            x[tid] ^= x[tid + stride];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            x[tid] ^= x[tid + s];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    for (int stride = 1; stride < N; stride *= 2) {\n        if (tid % (2 * stride) == 0) {\n            x[tid] ^= x[tid + stride];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    for (int stride = 1; stride < N; stride *= 2) {\n        if (tid % (2 * stride) == 0) {\n            x[tid] ^= x[tid + stride];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (int offset = 1; offset < N; offset *= 2) {\n        if (tid % (2 * offset) == 0) {\n            x[tid] ^= x[tid + offset];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            x[tid] ^= x[tid + s];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (int offset = 1; offset < N; offset *= 2) {\n        if (tid % (2 * offset) == 0) {\n            x[tid] ^= x[tid + offset];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            x[tid] ^= x[tid + s];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    for (int stride = 1; stride < N; stride *= 2) {\n        if (tid % (2 * stride) == 0) {\n            x[tid] ^= x[tid + stride];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    for (int stride = 1; stride < N; stride *= 2) {\n        if (tid % (2 * stride) == 0) {\n            x[tid] ^= x[tid + stride];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (int stride = 1; stride < N; stride *= 2) {\n        if (tid % (2 * stride) == 0) {\n            x[tid] ^= x[tid + stride];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    for (unsigned int stride = 1; stride < N; stride *= 2) {\n        if (tid % (2 * stride) == 0) {\n            x[tid] ^= x[tid + stride];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            x[tid] ^= x[tid + s];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            x[tid] ^= x[tid + s];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            x[tid] ^= x[tid + s];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (int offset = 1; offset < N; offset *= 2) {\n        if (tid % (2 * offset) == 0) {\n            x[tid] ^= x[tid + offset];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (int offset = 1; offset < N; offset *= 2) {\n        if (tid % (2 * offset) == 0) {\n            x[tid] ^= x[tid + offset];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    for (int stride = 1; stride < N; stride *= 2) {\n        if (tid % (2 * stride) == 0) {\n            x[tid] ^= x[tid + stride];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (int offset = 1; offset < N; offset *= 2) {\n        if (tid % (2 * offset) == 0) {\n            x[tid] ^= x[tid + offset];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "25_reduce_xor",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Use MPI to reduce in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on all ranks.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    bool localResult = false;\n    for (bool val : x) {\n        localResult = localResult ^ val;\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    for (bool val : x) {\n        localResult ^= val;\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    for (bool value : x) {\n        localResult ^= value;\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    for (bool value : x) {\n        localResult = localResult ^ value;\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    for (bool value : x) {\n        localResult ^= value;\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    for (bool value : x) {\n        localResult ^= value;\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    for (bool val : x) {\n        localResult = localResult ^ val;\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool result = false;\n    for (bool val : x) {\n        result = result ^ val;\n    }\n    bool globalResult;\n    MPI_Allreduce(&result, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    for (bool value : x) {\n        localResult ^= value;\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    for (bool val : x) {\n        localResult = localResult ^ val;\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    for (bool val : x) {\n        localResult ^= val;\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    for (bool value : x) {\n        localResult = localResult ^ value;\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    for (bool val : x) {\n        localResult = localResult ^ val;\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    for (bool val : x) {\n        localResult = localResult ^ val;\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    for (bool value : x) {\n        localResult ^= value;\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    for (bool value : x) {\n        localResult = localResult ^ value;\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    for (bool value : x) {\n        localResult ^= value;\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    for (bool value : x) {\n        localResult ^= value;\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    for (bool value : x) {\n        localResult ^= value;\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}",
      "\n    bool localResult = false;\n    for (bool val : x) {\n        localResult ^= val;\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "25_reduce_xor",
    "parallelism_model": "cuda",
    "prompt": "/* Compute the logical XOR reduction of the vector of bools x. Store the result in output.\n   Use CUDA to reduce in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\n__global__ void reduceLogicalXOR(const bool *x, size_t N, bool *output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = threadIdx.x;\n    for (int stride = 1; stride < N; stride *= 2) {\n        if (tid % (2 * stride) == 0) {\n            x[tid] ^= x[tid + stride];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (int stride = 1; stride < N; stride *= 2) {\n        if (tid % (2 * stride) == 0) {\n            x[tid] ^= x[tid + stride];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (int stride = 1; stride < N; stride *= 2) {\n        if (tid % (2 * stride) == 0) {\n            x[tid] ^= x[tid + stride];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (int offset = 1; offset < N; offset *= 2) {\n        if (tid % (2 * offset) == 0) {\n            x[tid] ^= x[tid + offset];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (int offset = 1; offset < N; offset *= 2) {\n        if (tid % (2 * offset) == 0) {\n            x[tid] ^= x[tid + offset];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (int stride = 1; stride < N; stride *= 2) {\n        if (tid % (2 * stride) == 0) {\n            x[tid] ^= x[tid + stride];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (int offset = 1; offset < N; offset *= 2) {\n        if (tid % (2 * offset) == 0) {\n            x[tid] ^= x[tid + offset];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            x[tid] ^= x[tid + s];\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (int offset = 1; offset < N; offset *= 2) {\n        if (tid % (2 * offset) == 0) {\n            x[tid] ^= x[tid + offset];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (int stride = 1; stride < N; stride *= 2) {\n        if (tid % (2 * stride) == 0) {\n            x[tid] ^= x[tid + stride];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (unsigned int s = 1; s < N; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            x[tid] ^= x[tid + s];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            x[tid] ^= x[tid + s];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (int stride = 1; stride < N; stride *= 2) {\n        if (tid % (2 * stride) == 0) {\n            x[tid] ^= x[tid + stride];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (int stride = 1; stride < N; stride *= 2) {\n        if (tid % (2 * stride) == 0) {\n            x[tid] ^= x[tid + stride];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (int stride = 1; stride < N; stride *= 2) {\n        if (tid % (2 * stride) == 0) {\n            x[tid] ^= x[tid + stride];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (int stride = 1; stride < N; stride *= 2) {\n        if (tid % (2 * stride) == 0) {\n            x[tid] ^= x[tid + stride];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (int stride = 1; stride < N; stride *= 2) {\n        if (tid % (2 * stride) == 0) {\n            x[tid] ^= x[tid + stride];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            x[tid] ^= x[tid + s];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (int stride = 1; stride < N; stride *= 2) {\n        if (tid % (2 * stride) == 0) {\n            x[tid] ^= x[tid + stride];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    for (int stride = 1; stride < N; stride *= 2) {\n        if (tid % (2 * stride) == 0) {\n            x[tid] ^= x[tid + stride];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        *output = x[0];\n    }\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "25_reduce_xor",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Use MPI and OpenMP to reduce in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on all ranks.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    bool localResult = false;\n    #pragma omp parallel for reduction(^:localResult)\n    for (int i = 0; i < x.size(); ++i) {\n        localResult ^= x[i];\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_BXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    #pragma omp parallel for reduction(^:localResult)\n    for (int i = 0; i < x.size(); ++i) {\n        localResult ^= x[i];\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    #pragma omp parallel for reduction(^:localResult)\n    for (int i = 0; i < x.size(); ++i) {\n        localResult ^= x[i];\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_BXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    #pragma omp parallel for reduction(^:localResult)\n    for (int i = 0; i < x.size(); ++i) {\n        localResult ^= x[i];\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_BXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    #pragma omp parallel for reduction(^:localResult)\n    for (int i = 0; i < x.size(); ++i) {\n        localResult ^= x[i];\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_BXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    #pragma omp parallel for reduction(^:localResult)\n    for (int i = 0; i < x.size(); ++i) {\n        localResult ^= x[i];\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_BXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    #pragma omp parallel for reduction(^:localResult)\n    for (int i = 0; i < x.size(); ++i) {\n        localResult ^= x[i];\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    #pragma omp parallel for reduction(^:localResult)\n    for (int i = 0; i < x.size(); ++i) {\n        localResult ^= x[i];\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_BXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    #pragma omp parallel for reduction(^:localResult)\n    for (int i = 0; i < x.size(); ++i) {\n        localResult ^= x[i];\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    #pragma omp parallel for reduction(^:localResult)\n    for (int i = 0; i < x.size(); ++i) {\n        localResult ^= x[i];\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    #pragma omp parallel for reduction(^:localResult)\n    for (int i = 0; i < x.size(); ++i) {\n        localResult ^= x[i];\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_BXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    #pragma omp parallel for reduction(^:localResult)\n    for (int i = 0; i < x.size(); ++i) {\n        localResult ^= x[i];\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_BXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    #pragma omp parallel for reduction(^:localResult)\n    for (int i = 0; i < x.size(); ++i) {\n        localResult ^= x[i];\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    #pragma omp parallel for reduction(^:localResult)\n    for (int i = 0; i < x.size(); ++i) {\n        localResult ^= x[i];\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_BXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    #pragma omp parallel for reduction(^:localResult)\n    for (int i = 0; i < x.size(); ++i) {\n        localResult ^= x[i];\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_BXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    #pragma omp parallel for reduction(^:localResult)\n    for (int i = 0; i < x.size(); ++i) {\n        localResult ^= x[i];\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_BXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    #pragma omp parallel for reduction(^:localResult)\n    for (int i = 0; i < x.size(); i++) {\n        localResult ^= x[i];\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    #pragma omp parallel for reduction(^:localResult)\n    for (int i = 0; i < x.size(); i++) {\n        localResult ^= x[i];\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    #pragma omp parallel for reduction(^:localResult)\n    for (int i = 0; i < x.size(); ++i) {\n        localResult ^= x[i];\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n",
      "\n    bool localResult = false;\n    #pragma omp parallel for reduction(^:localResult)\n    for (int i = 0; i < x.size(); i++) {\n        localResult ^= x[i];\n    }\n\n    bool globalResult;\n    MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n    return globalResult;\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "33_scan_reverse_prefix_sum",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the reverse prefix sum of the array x into output.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(Kokkos::View<const int*> const& x, Kokkos::View<int*> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n    output(i) = 0;\n    for (int j = 0; j <= i; j++) {\n      output(i) += x(j);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n    int sum = 0;\n    for (int j = 0; j <= i; j++) {\n      sum += x(j);\n    }\n    output(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n    int sum = 0;\n    for (int j = 0; j <= i; j++) {\n      sum += x(j);\n    }\n    output(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n    int sum = 0;\n    for (int j = 0; j <= i; j++) {\n      sum += x(j);\n    }\n    output(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n    output(i) = 0;\n    for (int j = 0; j <= i; j++) {\n      output(i) += x(j);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n    int sum = 0;\n    for (int j = 0; j <= i; j++) {\n      sum += x(j);\n    }\n    output(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    int sum = 0;\n    for (int j = 0; j <= i; j++) {\n      sum += x(j);\n    }\n    output(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    int sum = 0;\n    for (int j = 0; j <= i; j++) {\n      sum += x(j);\n    }\n    output(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n    int sum = 0;\n    for (int j = 0; j <= i; j++) {\n      sum += x(j);\n    }\n    output(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n    output(i) = 0;\n    for (int j = 0; j <= i; j++) {\n      output(i) += x(j);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n    int sum = 0;\n    for (int j = 0; j <= i; j++) {\n      sum += x(j);\n    }\n    output(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n    int sum = 0;\n    for (int j = 0; j <= i; j++) {\n      sum += x(j);\n    }\n    output(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n    int sum = 0;\n    for (int j = 0; j <= i; j++) {\n      sum += x(j);\n    }\n    output(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n    int sum = 0;\n    for (int j = 0; j <= i; j++) {\n      sum += x(j);\n    }\n    output(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  int n = x.extent(0);\n  Kokkos::parallel_for(\"reverse_prefix_sum\", n, KOKKOS_LAMBDA(const int i) {\n    if (i == 0) {\n      output(i) = x(i);\n    } else {\n      output(i) = output(i-1) + x(i);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n    int sum = 0;\n    for (int j = 0; j <= i; j++) {\n      sum += x(j);\n    }\n    output(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n    int sum = 0;\n    for (int j = 0; j <= i; j++) {\n      sum += x(j);\n    }\n    output(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    output(i) = 0;\n    for (int j = 0; j <= i; j++) {\n      output(i) += x(j);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n    int sum = 0;\n    for (int j = 0; j <= i; j++) {\n      sum += x(j);\n    }\n    output(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    output(i) = 0;\n    for (int j = 0; j <= i; j++) {\n      output(i) += x(j);\n    }\n  });\n  Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "33_scan_reverse_prefix_sum",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   Use OpenMP to compute in parallel.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int n = x.size();\n    output.resize(n);\n    output[n-1] = x[n-1];\n    #pragma omp parallel for\n    for (int i = n-2; i >= 0; i--) {\n        output[i] = output[i+1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[n-1] = x[n-1];\n    #pragma omp parallel for\n    for (int i = n-2; i >= 0; i--) {\n        output[i] = output[i+1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n\n    #pragma omp parallel for\n    for (int i = n - 1; i >= 0; i--) {\n        if (i == n - 1) {\n            output[i] = x[i];\n        } else {\n            output[i] = output[i + 1] + x[i];\n        }\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[n-1] = x[n-1];\n    #pragma omp parallel for\n    for (int i = n-2; i >= 0; i--) {\n        output[i] = output[i+1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[n-1] = x[n-1];\n    #pragma omp parallel for\n    for (int i = n-2; i >= 0; i--) {\n        output[i] = output[i+1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[n-1] = x[n-1];\n    #pragma omp parallel for\n    for (int i = n-2; i >= 0; i--) {\n        output[i] = output[i+1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[n-1] = x[n-1];\n    #pragma omp parallel for\n    for (int i = n-2; i >= 0; i--) {\n        output[i] = output[i+1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[n-1] = x[n-1];\n    #pragma omp parallel for\n    for (int i = n-2; i >= 0; i--) {\n        output[i] = output[i+1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        output[i] = x[i] + (i > 0 ? output[i-1] : 0);\n    }\n\n    std::reverse(output.begin(), output.end());\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[n-1] = x[n-1];\n    #pragma omp parallel for\n    for (int i = n-2; i >= 0; i--) {\n        output[i] = output[i+1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n\n    #pragma omp parallel for\n    for (int i = n - 1; i >= 0; i--) {\n        if (i == n - 1) {\n            output[i] = x[i];\n        } else {\n            output[i] = output[i + 1] + x[i];\n        }\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[n-1] = x[n-1];\n    #pragma omp parallel for\n    for (int i = n-2; i >= 0; i--) {\n        output[i] = output[i+1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[n-1] = x[n-1];\n    #pragma omp parallel for\n    for (int i = n-2; i >= 0; i--) {\n        output[i] = output[i+1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        output[i] = x[i];\n    }\n\n    for (int i = n - 2; i >= 0; i--) {\n        output[i] += output[i + 1];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[n-1] = x[n-1];\n    #pragma omp parallel for\n    for (int i = n-2; i >= 0; i--) {\n        output[i] = output[i+1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[n-1] = x[n-1];\n    #pragma omp parallel for\n    for (int i = n-2; i >= 0; i--) {\n        output[i] = output[i+1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[n-1] = x[n-1];\n    #pragma omp parallel for\n    for (int i = n-2; i >= 0; i--) {\n        output[i] = output[i+1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[n-1] = x[n-1];\n    #pragma omp parallel for\n    for (int i = n-2; i >= 0; i--) {\n        output[i] = output[i+1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[n-1] = x[n-1];\n    #pragma omp parallel for\n    for (int i = n-2; i >= 0; i--) {\n        output[i] = output[i+1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[n-1] = x[n-1];\n    #pragma omp parallel for\n    for (int i = n-2; i >= 0; i--) {\n        output[i] = output[i+1] + x[i];\n    }\n}"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "33_scan_reverse_prefix_sum",
    "parallelism_model": "serial",
    "prompt": "/* Compute the reverse prefix sum of the vector x into output.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        output.push_back(sum);\n    }\n}",
      "\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        output.push_back(sum);\n    }\n}",
      "\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        output.push_back(sum);\n    }\n}",
      "\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        output.push_back(sum);\n    }\n}",
      "\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        output.push_back(sum);\n    }\n}",
      "\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        output.push_back(sum);\n    }\n}",
      "\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        output.push_back(sum);\n    }\n}",
      "\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        output.push_back(sum);\n    }\n}",
      "\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        output.push_back(sum);\n    }\n}",
      "\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        output.push_back(sum);\n    }\n}",
      "\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        output.push_back(sum);\n    }\n}",
      "\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        output.push_back(sum);\n    }\n}",
      "\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        output.push_back(sum);\n    }\n}",
      "\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        output.push_back(sum);\n    }\n}",
      "\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        output.push_back(sum);\n    }\n}",
      "\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        output.push_back(sum);\n    }\n}",
      "\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        output[i] = sum;\n    }\n}",
      "\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        output[i] = sum;\n    }\n}",
      "\n    int sum = 0;\n    for (int i = 0; i < x.size(); ++i) {\n        sum += x[i];\n        output.push_back(sum);\n    }\n}",
      "\n    int sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        output[i] = sum;\n    }\n}"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "33_scan_reverse_prefix_sum",
    "parallelism_model": "hip",
    "prompt": "/* Compute the reverse prefix sum of the vector x into output.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\n__global__ void reversePrefixSum(const int *x, int *output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        output[tid] = 0;\n        for (int i = 0; i <= tid; i++) {\n            output[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        output[tid] = 0;\n        for (int i = 0; i <= tid; i++) {\n            output[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        output[tid] = 0;\n        for (int i = 0; i <= tid; i++) {\n            output[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        output[tid] = 0;\n        for (int i = 0; i <= tid; ++i) {\n            output[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        output[tid] = 0;\n        for (int i = 0; i <= tid; i++) {\n            output[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        output[tid] = 0;\n        for (int i = 0; i <= tid; i++) {\n            output[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; ++i) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        output[tid] = 0;\n        for (int i = 0; i <= tid; i++) {\n            output[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        output[tid] = 0;\n        for (int i = 0; i <= tid; i++) {\n            output[tid] += x[i];\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "33_scan_reverse_prefix_sum",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in output on rank 0.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSum = 0;\n    for (int i = 0; i < rank; i++) {\n        localSum += x[i];\n    }\n\n    int globalSum;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output[0] = x[0];\n        for (int i = 1; i < x.size(); i++) {\n            output[i] = output[i-1] + x[i];\n        }\n        int diff = globalSum - output[0];\n        for (int i = 0; i < x.size(); i++) {\n            output[i] += diff;\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<int> prefixSum(n);\n    prefixSum[0] = x[0];\n    for (int i = 1; i < n; i++) {\n        prefixSum[i] = prefixSum[i-1] + x[i];\n    }\n\n    if (rank == 0) {\n        output[n-1] = prefixSum[n-1];\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&output[n-1], 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&prefixSum[n-1], 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<int> prefixSum(n);\n    prefixSum[0] = x[0];\n    for (int i = 1; i < n; i++) {\n        prefixSum[i] = prefixSum[i-1] + x[i];\n    }\n\n    if (rank == 0) {\n        output[n-1] = prefixSum[n-1];\n        for (int i = 0; i < n-1; i++) {\n            output[i] = prefixSum[n-1] - prefixSum[i];\n        }\n        for (int r = 1; r < size; r++) {\n            std::vector<int> recvPrefixSum(n);\n            MPI_Recv(&recvPrefixSum[0], n, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            output[n-1] += recvPrefixSum[n-1];\n            for (int i = 0; i < n-1; i++) {\n                output[i] += recvPrefixSum[n-1] - recvPrefixSum[i];\n            }\n        }\n    } else {\n        MPI_Send(&prefixSum[0], n, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<int> prefixSum(n);\n    prefixSum[0] = x[0];\n    for (int i = 1; i < n; i++) {\n        prefixSum[i] = prefixSum[i-1] + x[i];\n    }\n\n    if (rank == 0) {\n        output[n-1] = prefixSum[n-1];\n        for (int i = 0; i < size-1; i++) {\n            MPI_Recv(&output[n-1], 1, MPI_INT, i+1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            output[n-1] += prefixSum[n-1];\n        }\n    } else {\n        MPI_Send(&prefixSum[n-1], 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n    MPI_Bcast(&output[n-1], 1, MPI_INT, 0, MPI_COMM_WORLD);\n    for (int i = n-2; i >= 0; i--) {\n        output[i] = output[i+1] - x[i];\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<int> prefixSum(n);\n    prefixSum[0] = x[0];\n    for (int i = 1; i < n; i++) {\n        prefixSum[i] = prefixSum[i-1] + x[i];\n    }\n\n    if (rank == 0) {\n        output[n-1] = prefixSum[n-1];\n        for (int i = 0; i < size-1; i++) {\n            MPI_Recv(&output[n-1], 1, MPI_INT, i+1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            output[n-1] += prefixSum[n-1];\n        }\n    } else {\n        MPI_Send(&prefixSum[n-1], 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n    MPI_Bcast(&output[n-1], 1, MPI_INT, 0, MPI_COMM_WORLD);\n    for (int i = n-2; i >= 0; i--) {\n        output[i] = output[i+1] - x[i];\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<int> prefixSum(n);\n    std::vector<int> reversePrefixSum(n);\n\n    // Compute prefix sum\n    prefixSum[0] = x[0];\n    for (int i = 1; i < n; i++) {\n        prefixSum[i] = prefixSum[i-1] + x[i];\n    }\n\n    // Gather prefix sums to rank 0\n    MPI_Gather(&prefixSum[0], n, MPI_INT, &reversePrefixSum[0], n, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // Compute reverse prefix sum on rank 0\n    if (rank == 0) {\n        reversePrefixSum[n-1] = prefixSum[n-1];\n        for (int i = n-2; i >= 0; i--) {\n            reversePrefixSum[i] = reversePrefixSum[i+1] + x[i];\n        }\n    }\n\n    // Scatter reverse prefix sum from rank 0 to all ranks\n    MPI_Scatter(&reversePrefixSum[0], n, MPI_INT, &output[0], n, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<int> prefixSum(n);\n    prefixSum[0] = x[0];\n    for (int i = 1; i < n; i++) {\n        prefixSum[i] = prefixSum[i-1] + x[i];\n    }\n\n    if (rank == 0) {\n        output[n-1] = prefixSum[n-1];\n        for (int i = 1; i < size; i++) {\n            int recvSum;\n            MPI_Recv(&recvSum, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            output[n-1] += recvSum;\n        }\n    } else {\n        MPI_Send(&prefixSum[n-1], 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        for (int i = n-2; i >= 0; i--) {\n            output[i] = output[i+1] - x[i+1];\n        }\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSum = 0;\n    for (int i = 0; i <= rank; i++) {\n        localSum += x[i];\n    }\n\n    int globalSum;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output[0] = globalSum;\n        for (int i = 1; i < x.size(); i++) {\n            output[i] = output[i-1] - x[i-1];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<int> prefixSum(n);\n    std::vector<int> reversePrefixSum(n);\n\n    // Compute prefix sum\n    prefixSum[0] = x[0];\n    for (int i = 1; i < n; ++i) {\n        prefixSum[i] = prefixSum[i - 1] + x[i];\n    }\n\n    // Gather prefix sums to rank 0\n    MPI_Gather(&prefixSum[0], n, MPI_INT, &reversePrefixSum[0], n, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // Compute reverse prefix sum on rank 0\n    if (rank == 0) {\n        reversePrefixSum[n - 1] = prefixSum[n - 1];\n        for (int i = n - 2; i >= 0; --i) {\n            reversePrefixSum[i] = reversePrefixSum[i + 1] + x[i];\n        }\n    }\n\n    // Scatter reverse prefix sum from rank 0 to all ranks\n    MPI_Scatter(&reversePrefixSum[0], n, MPI_INT, &output[0], n, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<int> prefixSum(n);\n    MPI_Allreduce(x.data(), prefixSum.data(), n, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output[n - 1] = prefixSum[n - 1];\n        for (int i = n - 2; i >= 0; --i) {\n            output[i] = output[i + 1] - x[i];\n        }\n    }\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSum = 0;\n    for (int i = 0; i < rank; i++) {\n        localSum += x[i];\n    }\n\n    int globalSum;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output[0] = x[0];\n        for (int i = 1; i < x.size(); i++) {\n            output[i] = globalSum - x[i - 1];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<int> prefixSum(n);\n    MPI_Allreduce(x.data(), prefixSum.data(), n, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output[n - 1] = prefixSum[n - 1];\n        for (int i = n - 2; i >= 0; --i) {\n            output[i] = output[i + 1] - x[i];\n        }\n    }\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    std::vector<int> prefixSum(n);\n    prefixSum[0] = x[0];\n    for (int i = 1; i < n; i++) {\n        prefixSum[i] = prefixSum[i-1] + x[i];\n    }\n\n    if (rank == 0) {\n        output[n-1] = prefixSum[n-1];\n        for (int i = 0; i < n-1; i++) {\n            output[i] = prefixSum[n-1] - prefixSum[i];\n        }\n        for (int r = 1; r < size; r++) {\n            std::vector<int> recvPrefixSum(n);\n            MPI_Recv(&recvPrefixSum[0], n, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int i = 0; i < n; i++) {\n                output[i] += recvPrefixSum[i];\n            }\n        }\n    } else {\n        MPI_Send(&prefixSum[0], n, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_sum = 0;\n    for (int i = 0; i < rank; ++i) {\n        local_sum += x[i];\n    }\n\n    int global_sum;\n    MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output[0] = global_sum;\n        for (int i = 1; i < x.size(); ++i) {\n            output[i] = output[i-1] + x[i];\n        }\n    }\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSum = 0;\n    for (int i = 0; i < rank; i++) {\n        localSum += x[i];\n    }\n\n    int globalSum;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output[0] = globalSum;\n        for (int i = 1; i < x.size(); i++) {\n            output[i] = output[i-1] + x[i-1];\n        }\n    }\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    std::vector<int> prefixSum(n);\n    prefixSum[0] = x[0];\n    for (int i = 1; i < n; i++) {\n        prefixSum[i] = prefixSum[i-1] + x[i];\n    }\n\n    if (rank == 0) {\n        output[n-1] = prefixSum[n-1];\n        for (int i = 0; i < n-1; i++) {\n            output[i] = prefixSum[n-1] - prefixSum[i];\n        }\n        for (int r = 1; r < size; r++) {\n            std::vector<int> recvPrefixSum(n);\n            MPI_Recv(&recvPrefixSum[0], n, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            output[n-1] += recvPrefixSum[n-1];\n            for (int i = 0; i < n-1; i++) {\n                output[i] += recvPrefixSum[n-1] - recvPrefixSum[i];\n            }\n        }\n    } else {\n        MPI_Send(&prefixSum[0], n, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<int> prefixSum(n);\n    prefixSum[0] = x[0];\n    for (int i = 1; i < n; i++) {\n        prefixSum[i] = prefixSum[i - 1] + x[i];\n    }\n\n    if (rank == 0) {\n        output[n - 1] = prefixSum[n - 1];\n        for (int i = 1; i < size; i++) {\n            int recvSum;\n            MPI_Recv(&recvSum, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            output[n - 1] += recvSum;\n        }\n    } else {\n        int sendSum = prefixSum[n - 1];\n        MPI_Send(&sendSum, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n    MPI_Bcast(&output[n - 1], 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = n - 2; i >= 0; i--) {\n        output[i] = output[i + 1] - x[i];\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_sum = 0;\n    for (int i = 0; i < x.size(); ++i) {\n        local_sum += x[i];\n        output[i] = local_sum;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            std::vector<int> recv_data(x.size());\n            MPI_Recv(&recv_data[0], x.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); ++j) {\n                output[j] += recv_data[j];\n            }\n        }\n    } else {\n        MPI_Send(&output[0], x.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    std::vector<int> prefixSum(n);\n    prefixSum[0] = x[0];\n    for (int i = 1; i < n; i++) {\n        prefixSum[i] = prefixSum[i - 1] + x[i];\n    }\n\n    if (rank == 0) {\n        output[n - 1] = prefixSum[n - 1];\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&output[(n / size) * i - 1], 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&prefixSum[(n / size) * rank - 1], 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        local_sum += x[i];\n        output[i] = local_sum;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::vector<int> recv_data(x.size());\n            MPI_Recv(&recv_data[0], x.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                output[j] += recv_data[j];\n            }\n        }\n    } else {\n        MPI_Send(&output[0], x.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "33_scan_reverse_prefix_sum",
    "parallelism_model": "cuda",
    "prompt": "/* Compute the reverse prefix sum of the vector x into output.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\n__global__ void reversePrefixSum(const int *x, int *output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        output[tid] = 0;\n        for (int i = 0; i <= tid; i++) {\n            output[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        output[tid] = 0;\n        for (int i = 0; i <= tid; i++) {\n            output[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        output[tid] = 0;\n        for (int i = 0; i <= tid; i++) {\n            output[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        output[tid] = 0;\n        for (int i = 0; i <= tid; i++) {\n            output[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; ++i) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        output[tid] = 0;\n        for (int i = 0; i <= tid; i++) {\n            output[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        output[tid] = 0;\n        for (int i = 0; i <= tid; i++) {\n            output[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "33_scan_reverse_prefix_sum",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in output on rank 0.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_sum = 0;\n    int local_size = x.size() / size;\n    int remainder = x.size() % size;\n\n    if (rank < remainder) {\n        local_size++;\n    }\n\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = start; i < end; i++) {\n        local_sum += x[i];\n        output[i] = local_sum;\n    }\n\n    if (rank > 0) {\n        MPI_Send(&output[start], local_size, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            int recv_size = (i < remainder) ? local_size + 1 : local_size;\n            MPI_Recv(&output[i * local_size + remainder], recv_size, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            output[i * local_size + remainder] += output[(i - 1) * local_size + remainder];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_sum = 0;\n    std::vector<int> local_output(x.size());\n\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); ++i) {\n        local_sum += x[i];\n        local_output[i] = local_sum;\n    }\n\n    std::vector<int> prefix_sums(size);\n    MPI_Gather(&local_sum, 1, MPI_INT, prefix_sums.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output[x.size() - 1] = prefix_sums[0];\n        for (int i = 1; i < size; ++i) {\n            prefix_sums[i] += prefix_sums[i - 1];\n            output[x.size() - 1] += prefix_sums[i];\n        }\n    }\n\n    MPI_Bcast(&output[x.size() - 1], 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size() - 1; ++i) {\n        output[i] = output[x.size() - 1] - local_output[i];\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_sum = 0;\n    std::vector<int> local_output(x.size());\n\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); i++) {\n        local_sum += x[i];\n        local_output[i] = local_sum;\n    }\n\n    std::vector<int> prefix_sums(size);\n    MPI_Gather(&local_sum, 1, MPI_INT, prefix_sums.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output[x.size() - 1] = prefix_sums[0];\n        for (int i = 1; i < size; i++) {\n            prefix_sums[i] += prefix_sums[i - 1];\n            output[x.size() - 1] += prefix_sums[i];\n        }\n    }\n\n    MPI_Bcast(&output[x.size() - 1], 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Scatter(output.data(), x.size(), MPI_INT, output.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size() - 1; i++) {\n        output[i] = output[x.size() - 1] - output[i];\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_sum = 0;\n    int local_size = x.size() / size;\n    int remainder = x.size() % size;\n\n    if (rank < remainder) {\n        local_size++;\n    }\n\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = start; i < end; i++) {\n        local_sum += x[i];\n        output[i] = local_sum;\n    }\n\n    if (rank > 0) {\n        MPI_Send(&output[start], local_size, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&output[i * local_size], local_size, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            output[i * local_size] += output[(i - 1) * local_size + (i - 1 < remainder ? i - 1 : remainder)];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_sum = 0;\n    int local_size = x.size() / size;\n    int remainder = x.size() % size;\n\n    if (rank < remainder) {\n        local_size++;\n    }\n\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = start; i < end; i++) {\n        local_sum += x[i];\n        output[i] = local_sum;\n    }\n\n    if (rank > 0) {\n        MPI_Send(&output[start], local_size, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            int recv_size = (i < remainder) ? local_size + 1 : local_size;\n            MPI_Recv(&output[i * local_size + remainder], recv_size, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            output[i * local_size + remainder] += output[(i - 1) * local_size + remainder];\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < x.size(); i++) {\n            output[i] = output[i - 1] - output[i];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_sum = 0;\n    std::vector<int> local_output(x.size());\n\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); i++) {\n        local_sum += x[i];\n        local_output[i] = local_sum;\n    }\n\n    std::vector<int> prefix_sums(size);\n    MPI_Gather(&local_sum, 1, MPI_INT, prefix_sums.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output[x.size() - 1] = prefix_sums[0];\n        for (int i = 1; i < size; i++) {\n            prefix_sums[i] += prefix_sums[i - 1];\n            output[x.size() - 1] += prefix_sums[i];\n        }\n    }\n\n    MPI_Bcast(&output[x.size() - 1], 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Scatter(output.data(), x.size(), MPI_INT, output.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n    output[0] = 0;\n    for (int i = 1; i < x.size(); i++) {\n        output[i] = output[x.size() - 1] - output[i - 1];\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_sum = 0;\n    int local_size = x.size() / size;\n    int remainder = x.size() % size;\n\n    if (rank < remainder) {\n        local_size++;\n    }\n\n    int start = rank * (x.size() / size) + std::min(rank, remainder);\n    int end = start + local_size;\n\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = start; i < end; i++) {\n        local_sum += x[i];\n        output[i] = local_sum;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&output[start], local_size, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            int recv_size = (i < remainder) ? (x.size() / size + 1) : (x.size() / size);\n            MPI_Recv(&output[i * (x.size() / size) + std::min(i, remainder)], recv_size, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            if (i < remainder) {\n                for (int j = i * (x.size() / size) + std::min(i, remainder); j < (i + 1) * (x.size() / size) + 1; j++) {\n                    output[j] += output[(i - 1) * (x.size() / size) + std::min(i - 1, remainder) + (x.size() / size)];\n                }\n            } else {\n                for (int j = i * (x.size() / size) + std::min(i, remainder); j < (i + 1) * (x.size() / size); j++) {\n                    output[j] += output[(i - 1) * (x.size() / size) + std::min(i - 1, remainder) + (x.size() / size)];\n                }\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_sum = 0;\n    int local_size = x.size() / size;\n    int remainder = x.size() % size;\n\n    if (rank < remainder) {\n        local_size++;\n    }\n\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = start; i < end; i++) {\n        local_sum += x[i];\n        output[i] = local_sum;\n    }\n\n    if (rank > 0) {\n        MPI_Send(&output[start], local_size, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            int recv_size = (i < remainder) ? local_size + 1 : local_size;\n            MPI_Recv(&output[i * local_size + remainder], recv_size, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            output[i * local_size + remainder] += output[(i - 1) * local_size + remainder];\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < x.size(); i++) {\n            output[i] = output[i - 1] - output[i];\n        }\n        output[0] = 0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_sum = 0;\n    int local_size = x.size() / size;\n    int remainder = x.size() % size;\n\n    if (rank < remainder) {\n        local_size++;\n    }\n\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = start; i < end; i++) {\n        local_sum += x[i];\n        output[i] = local_sum;\n    }\n\n    if (rank > 0) {\n        MPI_Send(&output[start], local_size, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            int recv_size = (i < remainder) ? local_size + 1 : local_size;\n            MPI_Recv(&output[i * local_size + remainder], recv_size, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            output[i * local_size + remainder] += output[(i - 1) * local_size + remainder];\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < x.size(); i++) {\n            output[i] = output[i - 1] - output[i];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_sum = 0;\n    std::vector<int> local_output(x.size());\n\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); ++i) {\n        local_sum += x[i];\n        local_output[i] = local_sum;\n    }\n\n    std::vector<int> prefix_sums(size);\n    MPI_Gather(&local_sum, 1, MPI_INT, prefix_sums.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output[x.size() - 1] = prefix_sums[0];\n        for (int i = 1; i < size; ++i) {\n            prefix_sums[i] += prefix_sums[i - 1];\n            output[x.size() - 1] += prefix_sums[i];\n        }\n    }\n\n    MPI_Bcast(&output[x.size() - 1], 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Scatter(output.data(), x.size(), MPI_INT, output.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_sum = 0;\n    int local_size = x.size() / size;\n    int remainder = x.size() % size;\n\n    if (rank < remainder) {\n        local_size++;\n    }\n\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = start; i < end; i++) {\n        local_sum += x[i];\n        output[i] = local_sum;\n    }\n\n    if (rank > 0) {\n        MPI_Send(&output[start], local_size, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            int recv_size = (i < remainder) ? local_size + 1 : local_size;\n            MPI_Recv(&output[i * local_size + remainder], recv_size, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            output[i * local_size + remainder] += output[(i - 1) * local_size + remainder];\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < x.size(); i++) {\n            output[i] = output[i - 1] - output[i];\n        }\n        output[0] = 0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_sum = 0;\n    std::vector<int> local_output(x.size());\n\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); i++) {\n        local_sum += x[i];\n        local_output[i] = local_sum;\n    }\n\n    std::vector<int> prefix_sums(size);\n    MPI_Gather(&local_sum, 1, MPI_INT, prefix_sums.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output[x.size() - 1] = prefix_sums[0];\n        for (int i = 1; i < size; i++) {\n            prefix_sums[i] += prefix_sums[i - 1];\n            output[x.size() - 1] += prefix_sums[i];\n        }\n    }\n\n    MPI_Bcast(&output[x.size() - 1], 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size() - 1; i++) {\n        output[i] = output[x.size() - 1] - local_output[i];\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_sum = 0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i <= rank; ++i) {\n        local_sum += x[i];\n    }\n\n    int global_sum;\n    MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output[0] = global_sum;\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&output[i], 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&global_sum, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_sum = 0;\n    int local_size = x.size() / size;\n    int remainder = x.size() % size;\n\n    if (rank < remainder) {\n        local_size++;\n    }\n\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = start; i < end; i++) {\n        local_sum += x[i];\n        output[i] = local_sum;\n    }\n\n    if (rank > 0) {\n        MPI_Send(&output[start], local_size, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            int recv_size = (i < remainder) ? local_size + 1 : local_size;\n            MPI_Recv(&output[i * local_size + remainder], recv_size, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            output[i * local_size + remainder] += output[(i - 1) * local_size + remainder];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_sum = 0;\n    std::vector<int> local_output(x.size());\n\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); ++i) {\n        local_sum += x[i];\n        local_output[i] = local_sum;\n    }\n\n    std::vector<int> prefix_sums(size);\n    MPI_Gather(&local_sum, 1, MPI_INT, prefix_sums.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output[x.size() - 1] = prefix_sums[0];\n        for (int i = 1; i < size; ++i) {\n            prefix_sums[i] += prefix_sums[i - 1];\n            output[x.size() - 1] += prefix_sums[i];\n        }\n    }\n\n    MPI_Bcast(&output[x.size() - 1], 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Scatter(output.data(), x.size(), MPI_INT, output.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_sum = 0;\n    std::vector<int> local_output(x.size());\n\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i < x.size(); i++) {\n        local_sum += x[i];\n        local_output[i] = local_sum;\n    }\n\n    std::vector<int> prefix_sums(size);\n    MPI_Gather(&local_sum, 1, MPI_INT, prefix_sums.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output[x.size() - 1] = prefix_sums[0];\n        for (int i = 1; i < size; i++) {\n            prefix_sums[i] += prefix_sums[i - 1];\n            output[x.size() - 1] += prefix_sums[i];\n        }\n    }\n\n    MPI_Bcast(&output[x.size() - 1], 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Scatter(output.data(), x.size(), MPI_INT, output.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_sum = 0;\n    int local_size = x.size() / size;\n    int remainder = x.size() % size;\n\n    if (rank < remainder) {\n        local_size++;\n    }\n\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = start; i < end; i++) {\n        local_sum += x[i];\n        output[i] = local_sum;\n    }\n\n    if (rank > 0) {\n        MPI_Send(&output[start], local_size, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            int recv_size = (i < remainder) ? local_size + 1 : local_size;\n            MPI_Recv(&output[i * local_size + remainder], recv_size, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            output[i * local_size + remainder] += output[(i - 1) * local_size + remainder];\n        }\n    }\n    if (rank == 0) {\n        for (int i = 1; i < x.size(); i++) {\n            output[i] = output[i - 1] + output[i];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_sum = 0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = 0; i <= rank; ++i) {\n        local_sum += x[i];\n    }\n\n    int global_sum;\n    MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output[0] = global_sum;\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&output[i], 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            output[i] += output[i - 1];\n        }\n    } else {\n        MPI_Send(&global_sum, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_sum = 0;\n    int local_size = x.size() / size;\n    int remainder = x.size() % size;\n\n    if (rank < remainder) {\n        local_size++;\n    }\n\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    for (int i = start; i < end; i++) {\n        local_sum += x[i];\n    }\n\n    std::vector<int> prefix_sums(size);\n    MPI_Allgather(&local_sum, 1, MPI_INT, prefix_sums.data(), 1, MPI_INT, MPI_COMM_WORLD);\n\n    int global_sum = 0;\n    for (int i = 0; i < rank; i++) {\n        global_sum += prefix_sums[i];\n    }\n\n    for (int i = start; i < end; i++) {\n        output[i] = global_sum;\n        global_sum += x[i];\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_sum = 0;\n    int local_size = x.size() / size;\n    int remainder = x.size() % size;\n\n    if (rank < remainder) {\n        local_size++;\n    }\n\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    for (int i = start; i < end; i++) {\n        local_sum += x[i];\n    }\n\n    std::vector<int> prefix_sums(size);\n    MPI_Allgather(&local_sum, 1, MPI_INT, prefix_sums.data(), 1, MPI_INT, MPI_COMM_WORLD);\n\n    int global_sum = 0;\n    for (int i = 0; i <= rank; i++) {\n        global_sum += prefix_sums[i];\n    }\n\n    for (int i = start; i < end; i++) {\n        output[i] = global_sum - x[i];\n    }\n}"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "32_scan_sum_of_prefix_sum_array",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(Kokkos::View<const double*> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::View<double*>::HostMirror h_x = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(h_x, x);\n\n    int n = x.extent(0);\n    double sum = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        sum += h_x(i) * (i + 1);\n    }\n\n    return sum;\n}",
      "\n    Kokkos::View<double*>::HostMirror h_x = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(h_x, x);\n\n    int n = x.extent(0);\n    double sum = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        sum += h_x(i) * (i + 1);\n    }\n\n    return sum;\n}",
      "\n    Kokkos::View<double*> prefixSum(\"prefixSum\", x.size());\n    Kokkos::parallel_scan(\"PrefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n        update += x(i);\n        if (final) {\n            prefixSum(i) = update;\n        }\n    });\n    Kokkos::fence();\n\n    double sum = 0;\n    Kokkos::parallel_reduce(\"Sum\", x.size(), KOKKOS_LAMBDA(const int i, double& localSum) {\n        localSum += prefixSum(i);\n    }, sum);\n\n    return sum;\n}",
      "\n    Kokkos::View<double*>::HostMirror h_x = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(h_x, x);\n\n    int n = x.extent(0);\n    double sum = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        sum += h_x(i) * (i + 1);\n    }\n\n    return sum;\n}",
      "\n    Kokkos::View<double*> prefixSum(\"prefixSum\", x.size());\n    Kokkos::parallel_scan(\"PrefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n        update += x(i);\n        if (final) {\n            prefixSum(i) = update;\n        }\n    });\n    Kokkos::fence();\n\n    double sum = 0.0;\n    Kokkos::parallel_reduce(\"SumOfPrefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& localSum) {\n        localSum += prefixSum(i);\n    }, sum);\n\n    return sum;\n}",
      "\n    Kokkos::View<double*> prefixSum(\"prefixSum\", x.size());\n    Kokkos::parallel_scan(x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n        update += x(i);\n        if (final) {\n            prefixSum(i) = update;\n        }\n    });\n    Kokkos::fence();\n\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(const int i, double& localSum) {\n        localSum += prefixSum(i);\n    }, sum);\n\n    return sum;\n}",
      "\n    Kokkos::View<double*> prefixSum(\"prefixSum\", x.size());\n    Kokkos::parallel_scan(x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n        update += x(i);\n        if (final) {\n            prefixSum(i) = update;\n        }\n    });\n    Kokkos::fence();\n\n    double sum = 0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(const int i, double& localSum) {\n        localSum += prefixSum(i);\n    }, sum);\n\n    return sum;\n}",
      "\n    Kokkos::View<double*> prefixSum(\"prefixSum\", x.size());\n    Kokkos::parallel_scan(\"PrefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n        update += x(i);\n        if (final) {\n            prefixSum(i) = update;\n        }\n    });\n    Kokkos::fence();\n\n    double sum = 0;\n    Kokkos::parallel_reduce(\"SumOfPrefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& localSum) {\n        localSum += prefixSum(i);\n    }, sum);\n\n    return sum;\n}",
      "\n    Kokkos::View<double*>::HostMirror h_x = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(h_x, x);\n\n    for (int i = 1; i < h_x.size(); i++) {\n        h_x(i) += h_x(i - 1);\n    }\n\n    double sum = 0;\n    for (int i = 0; i < h_x.size(); i++) {\n        sum += h_x(i);\n    }\n\n    return sum;\n}",
      "\n    Kokkos::View<double*> prefixSum(\"prefixSum\", x.size());\n    Kokkos::parallel_scan(x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n        update += x(i);\n        if (final) {\n            prefixSum(i) = update;\n        }\n    });\n    Kokkos::fence();\n\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(const int i, double& localSum) {\n        localSum += prefixSum(i);\n    }, sum);\n    Kokkos::fence();\n\n    return sum;\n}",
      "\n    Kokkos::View<double*>::HostMirror h_x = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(h_x, x);\n\n    int n = x.extent(0);\n    double sum = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        sum += h_x(i) * (i + 1);\n    }\n\n    return sum;\n}",
      "\n    Kokkos::View<double*>::HostMirror h_x = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(h_x, x);\n\n    int n = x.extent(0);\n    double sum = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        sum += h_x(i) * (i + 1);\n    }\n\n    return sum;\n}",
      "\n    Kokkos::View<double*>::HostMirror h_x = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(h_x, x);\n\n    double sum = 0.0;\n    double prefix_sum = 0.0;\n\n    for (int i = 0; i < x.extent(0); i++) {\n        prefix_sum += h_x(i);\n        sum += prefix_sum;\n    }\n\n    return sum;\n}",
      "\n    Kokkos::View<double*> prefixSum(\"prefixSum\", x.size());\n    Kokkos::parallel_scan(x.size(), KOKKOS_LAMBDA(int i, double& update, const bool final) {\n        update += x(i);\n        if (final) {\n            prefixSum(i) = update;\n        }\n    });\n    Kokkos::fence();\n\n    double sum = 0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& localSum) {\n        localSum += prefixSum(i);\n    }, sum);\n\n    return sum;\n}",
      "\n    Kokkos::View<double*> prefixSum(\"prefixSum\", x.size());\n    Kokkos::parallel_scan(x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n        update += x(i);\n        if (final) {\n            prefixSum(i) = update;\n        }\n    });\n    Kokkos::fence();\n\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(const int i, double& localSum) {\n        localSum += prefixSum(i);\n    }, sum);\n    Kokkos::fence();\n\n    return sum;\n}",
      "\n    Kokkos::View<double*>::HostMirror h_x = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(h_x, x);\n\n    int n = x.extent(0);\n    double sum = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        sum += h_x(i) * (i + 1);\n    }\n\n    return sum;\n}",
      "\n    Kokkos::View<double*> prefixSum(\"prefixSum\", x.size());\n    Kokkos::parallel_scan(x.size(), KOKKOS_LAMBDA(int i, double& update, const bool final) {\n        update += x(i);\n        if (final) {\n            prefixSum(i) = update;\n        }\n    });\n    Kokkos::fence();\n\n    double sum = 0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& localSum) {\n        localSum += prefixSum(i);\n    }, sum);\n\n    return sum;\n}",
      "\n    Kokkos::View<double*> prefixSum(\"prefixSum\", x.size());\n    Kokkos::parallel_scan(\"PrefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n        update += x(i);\n        if (final) {\n            prefixSum(i) = update;\n        }\n    });\n    Kokkos::fence();\n\n    double sum = 0;\n    Kokkos::parallel_reduce(\"SumOfPrefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& localSum) {\n        localSum += prefixSum(i);\n    }, sum);\n\n    return sum;\n}",
      "\n    Kokkos::View<double*> prefix_sum(\"prefix_sum\", x.size());\n    Kokkos::parallel_scan(\"PrefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n        update += x(i);\n        if (final) {\n            prefix_sum(i) = update;\n        }\n    });\n    Kokkos::fence();\n\n    double sum = 0;\n    Kokkos::parallel_reduce(\"SumOfPrefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& local_sum) {\n        local_sum += prefix_sum(i);\n    }, sum);\n\n    return sum;\n}",
      "\n    Kokkos::View<double*> prefixSum(\"prefixSum\", x.size());\n    Kokkos::parallel_scan(x.size(), KOKKOS_LAMBDA(int i, double& update, const bool final) {\n        update += x(i);\n        if (final) {\n            prefixSum(i) = update;\n        }\n    });\n    Kokkos::fence();\n\n    double sum = 0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& localSum) {\n        localSum += prefixSum(i);\n    }, sum);\n\n    return sum;\n}"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "32_scan_sum_of_prefix_sum_array",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double sum = 0;\n    std::vector<double> prefixSum(x.size());\n    prefixSum[0] = x[0];\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); ++i) {\n        prefixSum[i] = prefixSum[i - 1] + x[i];\n    }\n    for (int i = 0; i < x.size(); ++i) {\n        sum += prefixSum[i];\n    }\n    return sum;\n}",
      "\n    double sum = 0.0;\n    std::vector<double> prefixSum(x.size());\n    prefixSum[0] = x[0];\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        prefixSum[i] = prefixSum[i - 1] + x[i];\n    }\n    for (int i = 0; i < x.size(); i++) {\n        sum += prefixSum[i];\n    }\n    return sum;\n}",
      "\n    double sum = 0.0;\n    std::vector<double> prefixSum(x.size());\n    prefixSum[0] = x[0];\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        prefixSum[i] = prefixSum[i-1] + x[i];\n    }\n    for (int i = 0; i < x.size(); i++) {\n        sum += prefixSum[i];\n    }\n    return sum;\n}",
      "\n    double sum = 0.0;\n    std::vector<double> prefixSum(x.size());\n    prefixSum[0] = x[0];\n    \n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        prefixSum[i] = prefixSum[i-1] + x[i];\n    }\n    \n    for (int i = 0; i < x.size(); i++) {\n        sum += prefixSum[i];\n    }\n    \n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    std::vector<double> prefixSum(x.size());\n    prefixSum[0] = x[0];\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        prefixSum[i] = prefixSum[i-1] + x[i];\n    }\n    for (int i = 0; i < x.size(); i++) {\n        sum += prefixSum[i];\n    }\n    return sum;\n}",
      "\n    double sum = 0.0;\n    std::vector<double> prefixSum(x.size());\n    prefixSum[0] = x[0];\n    \n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); ++i) {\n        prefixSum[i] = prefixSum[i-1] + x[i];\n    }\n    \n    for (int i = 0; i < x.size(); ++i) {\n        sum += prefixSum[i];\n    }\n    \n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    std::vector<double> prefixSum(x.size());\n    prefixSum[0] = x[0];\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); ++i) {\n        prefixSum[i] = prefixSum[i - 1] + x[i];\n    }\n    for (int i = 0; i < x.size(); ++i) {\n        sum += prefixSum[i];\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        double prefix_sum = 0;\n        for (int j = 0; j <= i; j++) {\n            prefix_sum += x[j];\n        }\n        sum += prefix_sum;\n    }\n    return sum;\n}",
      "\n    double sum = 0.0;\n    std::vector<double> prefixSum(x.size());\n    prefixSum[0] = x[0];\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 1; i < x.size(); i++) {\n        prefixSum[i] = prefixSum[i-1] + x[i];\n        sum += prefixSum[i];\n    }\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    std::vector<double> prefixSum(x.size());\n    prefixSum[0] = x[0];\n    \n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); ++i) {\n        prefixSum[i] = prefixSum[i-1] + x[i];\n    }\n    \n    for (int i = 0; i < x.size(); ++i) {\n        sum += prefixSum[i];\n    }\n    \n    return sum;\n}\n",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        double prefix_sum = 0;\n        for (int j = 0; j <= i; j++) {\n            prefix_sum += x[j];\n        }\n        sum += prefix_sum;\n    }\n    return sum;\n}",
      "\n    double sum = 0.0;\n    std::vector<double> prefixSum(x.size());\n    prefixSum[0] = x[0];\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        prefixSum[i] = prefixSum[i-1] + x[i];\n    }\n    for (int i = 0; i < x.size(); i++) {\n        sum += prefixSum[i];\n    }\n    return sum;\n}",
      "\n    int n = x.size();\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < n; i++) {\n        double prefix_sum = 0.0;\n        for (int j = 0; j <= i; j++) {\n            prefix_sum += x[j];\n        }\n        sum += prefix_sum;\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        double prefix_sum = 0;\n        for (int j = 0; j <= i; j++) {\n            prefix_sum += x[j];\n        }\n        sum += prefix_sum;\n    }\n    return sum;\n}",
      "\n    double sum = 0.0;\n    std::vector<double> prefixSum(x.size());\n    prefixSum[0] = x[0];\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 1; i < x.size(); i++) {\n        prefixSum[i] = prefixSum[i-1] + x[i];\n        sum += prefixSum[i];\n    }\n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    std::vector<double> prefixSum(x.size());\n    prefixSum[0] = x[0];\n    \n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); ++i) {\n        prefixSum[i] = prefixSum[i-1] + x[i];\n    }\n    \n    for (int i = 0; i < x.size(); ++i) {\n        sum += prefixSum[i];\n    }\n    \n    return sum;\n}\n",
      "\n    double sum = 0.0;\n    std::vector<double> prefixSum(x.size());\n    prefixSum[0] = x[0];\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); ++i) {\n        prefixSum[i] = prefixSum[i - 1] + x[i];\n    }\n    for (int i = 0; i < x.size(); ++i) {\n        sum += prefixSum[i];\n    }\n    return sum;\n}",
      "\n    double sum = 0.0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        double prefixSum = 0.0;\n        for (int j = 0; j <= i; j++) {\n            prefixSum += x[j];\n        }\n        sum += prefixSum;\n    }\n    return sum;\n}",
      "\n    double sum = 0.0;\n    std::vector<double> prefixSum(x.size());\n    \n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (i == 0) {\n            prefixSum[i] = x[i];\n        } else {\n            prefixSum[i] = prefixSum[i - 1] + x[i];\n        }\n    }\n    \n    for (int i = 0; i < prefixSum.size(); i++) {\n        sum += prefixSum[i];\n    }\n    \n    return sum;\n}",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        double prefix_sum = 0;\n        for (int j = 0; j <= i; j++) {\n            prefix_sum += x[j];\n        }\n        sum += prefix_sum;\n    }\n    return sum;\n}"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "32_scan_sum_of_prefix_sum_array",
    "parallelism_model": "serial",
    "prompt": "/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double sum = 0;\n    double prefixSum = 0;\n    for (double num : x) {\n        prefixSum += num;\n        sum += prefixSum;\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    double prefixSum = 0;\n    for (double num : x) {\n        prefixSum += num;\n        sum += prefixSum;\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    double prefixSum = 0;\n    for (double num : x) {\n        prefixSum += num;\n        sum += prefixSum;\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    double prefixSum = 0;\n    for (double num : x) {\n        prefixSum += num;\n        sum += prefixSum;\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    double prefixSum = 0;\n    for (double num : x) {\n        prefixSum += num;\n        sum += prefixSum;\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    double prefixSum = 0;\n    for (double num : x) {\n        prefixSum += num;\n        sum += prefixSum;\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    double prefixSum = 0;\n    for (double num : x) {\n        prefixSum += num;\n        sum += prefixSum;\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    double prefixSum = 0;\n    for (double num : x) {\n        prefixSum += num;\n        sum += prefixSum;\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    double prefixSum = 0;\n    for (double num : x) {\n        prefixSum += num;\n        sum += prefixSum;\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    double prefixSum = 0;\n    for (double num : x) {\n        prefixSum += num;\n        sum += prefixSum;\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    double prefixSum = 0;\n    for (double num : x) {\n        prefixSum += num;\n        sum += prefixSum;\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    double prefixSum = 0;\n    for (double num : x) {\n        prefixSum += num;\n        sum += prefixSum;\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    double prefixSum = 0;\n    for (double num : x) {\n        prefixSum += num;\n        sum += prefixSum;\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    double prefixSum = 0;\n    for (double num : x) {\n        prefixSum += num;\n        sum += prefixSum;\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    double prefixSum = 0;\n    for (double num : x) {\n        prefixSum += num;\n        sum += prefixSum;\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    double prefixSum = 0;\n    for (double num : x) {\n        prefixSum += num;\n        sum += prefixSum;\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    double prefixSum = 0;\n    for (double num : x) {\n        prefixSum += num;\n        sum += prefixSum;\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    double prefixSum = 0;\n    for (double num : x) {\n        prefixSum += num;\n        sum += prefixSum;\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    double prefixSum = 0;\n    for (double num : x) {\n        prefixSum += num;\n        sum += prefixSum;\n    }\n    return sum;\n}",
      "\n    double sum = 0;\n    double prefixSum = 0;\n    for (double num : x) {\n        prefixSum += num;\n        sum += prefixSum;\n    }\n    return sum;\n}"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "32_scan_sum_of_prefix_sum_array",
    "parallelism_model": "hip",
    "prompt": "/* Compute the prefix sum array of the vector x and compute its sum. Store the result in sum.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\n__global__ void sumOfPrefixSum(const double *x, size_t N, double *sum) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 1; i <= tid; i++) {\n            sum[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 1; i <= tid; i++) {\n            sum[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 1; i <= tid; i++) {\n            sum[tid] += x[i];\n        }\n    }\n}\n",
      "\n    size_t tid = threadIdx.x;\n    if (tid < N) {\n        for (size_t i = 1; i <= tid; ++i) {\n            x[tid] += x[tid - i];\n        }\n        sum[tid] = x[tid];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 1; i <= tid; i++) {\n            sum[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 1; i <= tid; i++) {\n            sum[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 1; i <= tid; i++) {\n            sum[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 1; i <= tid; i++) {\n            sum[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 1; i <= tid; i++) {\n            sum[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 1; i <= tid; i++) {\n            sum[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 1; i <= tid; i++) {\n            sum[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 1; i <= tid; i++) {\n            sum[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 1; i <= tid; i++) {\n            sum[tid] += x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 1; i <= tid; i++) {\n            sum[tid] += x[i];\n        }\n    }\n}\n",
      "\n    extern __shared__ double prefixSum[];\n    int tid = threadIdx.x;\n    if (tid < N) {\n        prefixSum[tid] = x[tid];\n    }\n    __syncthreads();\n\n    for (int stride = 1; stride < N; stride *= 2) {\n        int index = 2 * stride * (tid + 1) - 1;\n        if (index < N) {\n            prefixSum[index] += prefixSum[index - stride];\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        *sum = prefixSum[N - 1];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "32_scan_sum_of_prefix_sum_array",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    for (int i = 0; i < n; ++i) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    double globalSum = 0.0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return globalSum;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    double globalSum = 0.0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return globalSum;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    double globalSum = 0.0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return globalSum;\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    double globalSum = 0.0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalSum;\n    } else {\n        return 0.0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    double globalSum = 0.0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return globalSum;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    double globalSum = 0.0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return globalSum;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    for (int i = 0; i < n; ++i) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    double globalSum = 0.0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return globalSum;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    prefixSum[0] = x[0];\n\n    for (int i = 1; i < n; i++) {\n        prefixSum[i] = prefixSum[i - 1] + x[i];\n    }\n\n    double localSum = 0;\n    for (int i = 0; i < n; i++) {\n        localSum += prefixSum[i];\n    }\n\n    double globalSum;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalSum;\n    } else {\n        return 0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    double globalSum = 0.0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return globalSum;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    prefixSum[0] = x[0];\n    for (int i = 1; i < n; i++) {\n        prefixSum[i] = prefixSum[i-1] + x[i];\n    }\n\n    double localSum = 0;\n    for (int i = 0; i < n; i++) {\n        localSum += prefixSum[i];\n    }\n\n    double globalSum;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalSum;\n    } else {\n        return 0.0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    double globalSum = 0.0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return globalSum;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    double globalSum = 0.0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return globalSum;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    for (int i = 0; i < n; ++i) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    double globalSum = 0.0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return globalSum;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    double globalSum = 0.0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return globalSum;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    for (int i = 0; i < n; ++i) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    double globalSum = 0.0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return globalSum;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    double globalSum = 0.0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return globalSum;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n, 0);\n    double sum = 0;\n\n    for (int i = 0; i < n; i++) {\n        if (i > 0) {\n            prefixSum[i] = prefixSum[i - 1] + x[i];\n        } else {\n            prefixSum[i] = x[i];\n        }\n        sum += prefixSum[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            double recvSum;\n            MPI_Recv(&recvSum, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            sum += recvSum;\n        }\n    } else {\n        MPI_Send(&sum, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return sum;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    double globalSum = 0.0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalSum;\n    } else {\n        return 0.0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    double globalSum = 0.0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalSum;\n    } else {\n        return 0.0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    for (int i = 0; i < n; i++) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    double globalSum = 0.0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalSum;\n    } else {\n        return 0.0;\n    }\n}"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "32_scan_sum_of_prefix_sum_array",
    "parallelism_model": "cuda",
    "prompt": "/* Compute the prefix sum array of the vector x and compute its sum. Store the result in sum.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\n__global__ void sumOfPrefixSum(const double *x, size_t N, double *sum) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= idx; i++) {\n            prefixSum += x[i];\n        }\n        sum[idx] = prefixSum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double prefixSum = 0;\n        for (int i = 0; i <= tid; i++) {\n            prefixSum += x[i];\n        }\n        sum[tid] = prefixSum;\n    }\n}"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "32_scan_sum_of_prefix_sum_array",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    prefixSum[0] = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < n; i++) {\n        prefixSum[i] = prefixSum[i-1] + x[i];\n    }\n\n    double localSum = 0.0;\n    #pragma omp parallel for reduction(+:localSum)\n    for (int i = 0; i < n; i++) {\n        localSum += prefixSum[i];\n    }\n\n    double globalSum = 0.0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalSum;\n    } else {\n        return 0.0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    double local_sum = 0;\n    double prefix_sum = 0;\n\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = local_start; i < local_end; i++) {\n        prefix_sum += x[i];\n        local_sum += prefix_sum;\n    }\n\n    double global_sum;\n    MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_sum;\n    } else {\n        return 0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    #pragma omp parallel for reduction(+:localSum)\n    for (int i = 0; i < n; i++) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&prefixSum[0], n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            localSum += prefixSum[n - 1];\n        }\n    } else {\n        MPI_Send(&prefixSum[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n\n    MPI_Bcast(&localSum, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    return localSum;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    #pragma omp parallel for reduction(+:localSum)\n    for (int i = 0; i < n; i++) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&prefixSum[0], n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            localSum += prefixSum[n - 1];\n        }\n    } else {\n        MPI_Send(&prefixSum[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n\n    MPI_Bcast(&localSum, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    return localSum;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    double local_sum = 0;\n    double prefix_sum = 0;\n\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = local_start; i < local_end; i++) {\n        prefix_sum += x[i];\n        local_sum += prefix_sum;\n    }\n\n    double global_sum;\n    MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_sum;\n    } else {\n        return 0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    double local_sum = 0.0;\n    double prefix_sum = 0.0;\n\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = local_start; i < local_end; i++) {\n        prefix_sum += x[i];\n        local_sum += prefix_sum;\n    }\n\n    double global_sum;\n    MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_sum;\n    } else {\n        return 0.0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    double local_sum = 0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = local_start; i < local_end; i++) {\n        double prefix_sum = 0;\n        for (int j = 0; j <= i; j++) {\n            prefix_sum += x[j];\n        }\n        local_sum += prefix_sum;\n    }\n\n    double global_sum;\n    MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_sum;\n    } else {\n        return 0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    double local_sum = 0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = local_start; i < local_end; i++) {\n        double prefix_sum = 0;\n        for (int j = 0; j <= i; j++) {\n            prefix_sum += x[j];\n        }\n        local_sum += prefix_sum;\n    }\n\n    double global_sum;\n    MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_sum;\n    } else {\n        return 0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        prefixSum[i] = x[i];\n        if (i > 0) {\n            prefixSum[i] += prefixSum[i - 1];\n        }\n    }\n\n    double localSum = 0.0;\n    #pragma omp parallel for reduction(+:localSum)\n    for (int i = 0; i < n; i++) {\n        localSum += prefixSum[i];\n    }\n\n    double globalSum;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalSum;\n    } else {\n        return 0.0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    #pragma omp parallel for reduction(+:localSum)\n    for (int i = 0; i < n; ++i) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    if (rank == 0) {\n        double totalSum = 0.0;\n        for (int i = 0; i < n; ++i) {\n            totalSum += prefixSum[i];\n        }\n        return totalSum;\n    } else {\n        return 0.0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n, 0.0);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        if (i > 0) {\n            prefixSum[i] = prefixSum[i - 1] + x[i];\n        } else {\n            prefixSum[i] = x[i];\n        }\n    }\n\n    double localSum = 0.0;\n    #pragma omp parallel for reduction(+:localSum)\n    for (int i = 0; i < n; i++) {\n        localSum += prefixSum[i];\n    }\n\n    double globalSum = 0.0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalSum;\n    } else {\n        return 0.0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    prefixSum[0] = x[0];\n    #pragma omp parallel for\n    for (int i = 1; i < n; ++i) {\n        prefixSum[i] = prefixSum[i-1] + x[i];\n    }\n\n    double localSum = 0.0;\n    #pragma omp parallel for reduction(+:localSum)\n    for (int i = 0; i < n; ++i) {\n        localSum += prefixSum[i];\n    }\n\n    double globalSum = 0.0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalSum;\n    } else {\n        return 0.0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    prefixSum[0] = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < n; i++) {\n        prefixSum[i] = prefixSum[i - 1] + x[i];\n    }\n\n    double localSum = 0.0;\n    #pragma omp parallel for reduction(+:localSum)\n    for (int i = 0; i < n; i++) {\n        localSum += prefixSum[i];\n    }\n\n    double globalSum = 0.0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalSum;\n    } else {\n        return 0.0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    double local_sum = 0;\n    #pragma omp parallel for reduction(+:local_sum)\n    for (int i = local_start; i < local_end; i++) {\n        double prefix_sum = 0;\n        for (int j = 0; j <= i; j++) {\n            prefix_sum += x[j];\n        }\n        local_sum += prefix_sum;\n    }\n\n    double global_sum;\n    MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_sum;\n    } else {\n        return 0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        prefixSum[i] = x[i];\n        if (i > 0) {\n            prefixSum[i] += prefixSum[i - 1];\n        }\n    }\n\n    double localSum = 0.0;\n    #pragma omp parallel for reduction(+:localSum)\n    for (int i = 0; i < n; i++) {\n        localSum += prefixSum[i];\n    }\n\n    double globalSum;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalSum;\n    } else {\n        return 0.0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    #pragma omp parallel for reduction(+:localSum)\n    for (int i = 0; i < n; i++) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&prefixSum[0], n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            localSum += prefixSum[n - 1];\n        }\n    } else {\n        MPI_Send(&prefixSum[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n\n    MPI_Bcast(&localSum, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    return localSum;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    #pragma omp parallel for reduction(+:localSum)\n    for (int i = 0; i < n; i++) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    if (rank == 0) {\n        double globalSum = 0.0;\n        for (int i = 0; i < n; i++) {\n            globalSum += prefixSum[i];\n        }\n        return globalSum;\n    } else {\n        return 0.0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    #pragma omp parallel for reduction(+:localSum)\n    for (int i = 0; i < n; i++) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&prefixSum[0], n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            localSum += prefixSum[n - 1];\n        }\n    } else {\n        MPI_Send(&prefixSum[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n\n    MPI_Bcast(&localSum, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    return localSum;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    #pragma omp parallel for reduction(+:localSum)\n    for (int i = 0; i < n; i++) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    if (rank == 0) {\n        double globalSum = 0.0;\n        for (int i = 0; i < n; i++) {\n            globalSum += prefixSum[i];\n        }\n        for (int i = 1; i < size; i++) {\n            double recvSum;\n            MPI_Recv(&recvSum, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            globalSum += recvSum;\n        }\n        return globalSum;\n    } else {\n        MPI_Send(&localSum, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n        return 0.0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefixSum(n);\n    double localSum = 0.0;\n\n    #pragma omp parallel for reduction(+:localSum)\n    for (int i = 0; i < n; i++) {\n        localSum += x[i];\n        prefixSum[i] = localSum;\n    }\n\n    if (rank == 0) {\n        double globalSum = 0.0;\n        for (int i = 0; i < size; i++) {\n            globalSum += prefixSum[i * (n / size) - 1];\n        }\n        return globalSum;\n    } else {\n        MPI_Send(&prefixSum[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n        return 0.0;\n    }\n}"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "31_scan_scan_with_min_function",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Replace the i-th element of the array x with the minimum value from indices 0 through i.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(Kokkos::View<float*> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n      float minVal = x[0];\n      for (int j = 0; j <= i; j++) {\n          minVal = (x[j] < minVal) ? x[j] : minVal;\n      }\n      x[i] = minVal;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    float min_val = x[0];\n    for (int j = 0; j <= i; j++) {\n      min_val = (x[j] < min_val) ? x[j] : min_val;\n    }\n    x[i] = min_val;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    float min_val = x[0];\n    for (int j = 0; j <= i; j++) {\n      min_val = (x[j] < min_val) ? x[j] : min_val;\n    }\n    x[i] = min_val;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    float min_val = x[0];\n    for (int j = 0; j <= i; j++) {\n      min_val = (x[j] < min_val) ? x[j] : min_val;\n    }\n    x[i] = min_val;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    float min_val = x[0];\n    for (int j = 0; j <= i; j++) {\n      min_val = (x[j] < min_val) ? x[j] : min_val;\n    }\n    x[i] = min_val;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    float min_val = x[0];\n    for (int j = 0; j <= i; j++) {\n      min_val = (x[j] < min_val) ? x[j] : min_val;\n    }\n    x[i] = min_val;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    float min_val = x[0];\n    for (int j = 0; j <= i; j++) {\n      min_val = (x[j] < min_val) ? x[j] : min_val;\n    }\n    x[i] = min_val;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    float min_val = x[0];\n    for (int j = 0; j <= i; j++) {\n      min_val = (x[j] < min_val) ? x[j] : min_val;\n    }\n    x[i] = min_val;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    float min_val = x[0];\n    for (int j = 0; j <= i; j++) {\n      min_val = (x[j] < min_val) ? x[j] : min_val;\n    }\n    x[i] = min_val;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n      float min_val = x[0];\n      for (int j = 0; j <= i; j++) {\n          min_val = (x[j] < min_val) ? x[j] : min_val;\n      }\n      x[i] = min_val;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    float min_val = x[0];\n    for (int j = 0; j <= i; j++) {\n      min_val = (x[j] < min_val) ? x[j] : min_val;\n    }\n    x[i] = min_val;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    float min_val = x[0];\n    for (int j = 0; j <= i; j++) {\n      min_val = (x[j] < min_val) ? x[j] : min_val;\n    }\n    x[i] = min_val;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    float min_val = x[0];\n    for (int j = 0; j <= i; j++) {\n      if (x[j] < min_val) {\n        min_val = x[j];\n      }\n    }\n    x[i] = min_val;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    float min_val = x[0];\n    for (int j = 0; j <= i; j++) {\n      min_val = (x[j] < min_val) ? x[j] : min_val;\n    }\n    x[i] = min_val;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    float min_val = x[0];\n    for (int j = 0; j <= i; j++) {\n      min_val = (x[j] < min_val) ? x[j] : min_val;\n    }\n    x[i] = min_val;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n      float min_val = x[0];\n      for (int j = 0; j <= i; j++) {\n          min_val = (x[j] < min_val) ? x[j] : min_val;\n      }\n      x[i] = min_val;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    float min_val = x[0];\n    for (int j = 0; j <= i; j++) {\n      min_val = (x[j] < min_val) ? x[j] : min_val;\n    }\n    x[i] = min_val;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    float min_val = x[0];\n    for (int j = 0; j <= i; j++) {\n      min_val = (x[j] < min_val) ? x[j] : min_val;\n    }\n    x[i] = min_val;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    float minVal = x[0];\n    for (int j = 0; j <= i; j++) {\n      if (x[j] < minVal) {\n        minVal = x[j];\n      }\n    }\n    x[i] = minVal;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    float min_val = x[0];\n    for (int j = 0; j <= i; j++) {\n      min_val = (x[j] < min_val) ? x[j] : min_val;\n    }\n    x[i] = min_val;\n  });\n  Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "31_scan_scan_with_min_function",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "31_scan_scan_with_min_function",
    "parallelism_model": "serial",
    "prompt": "/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    float min_val = x[0];\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    float min_val = x[0];\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    float min_val = x[0];\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    float min_val = x[0];\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    float min_val = x[0];\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    float min_val = x[0];\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    for (int i = 1; i < x.size(); i++) {\n        float min_val = x[0];\n        for (int j = 1; j <= i; j++) {\n            if (x[j] < min_val) {\n                min_val = x[j];\n            }\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    float min_val = x[0];\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    float min_val = x[0];\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    float min_val = x[0];\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    float min_val = x[0];\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    float min_val = x[0];\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    float min_val = x[0];\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        x[i] = min_val;\n    }\n}",
      "\n    float min_val = x[0];\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        x[i] = min_val;\n    }\n}"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "31_scan_scan_with_min_function",
    "parallelism_model": "hip",
    "prompt": "/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\n__global__ void partialMinimums(float *x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            min_val = min(min_val, x[i]);\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "31_scan_scan_with_min_function",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<float> local_min(n);\n\n    for (int i = 0; i < n; i++) {\n        float min_val;\n        MPI_Allreduce(&x[i], &min_val, 1, MPI_FLOAT, MPI_MIN, MPI_COMM_WORLD);\n        local_min[i] = min_val;\n    }\n\n    if (rank == 0) {\n        x = local_min;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<float> local_min(n);\n    float min_val = x[0];\n\n    for (int i = 0; i < n; i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        local_min[i] = min_val;\n    }\n\n    if (rank == 0) {\n        std::vector<float> global_min(n);\n        MPI_Gather(&local_min[0], n, MPI_FLOAT, &global_min[0], n, MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = global_min;\n    } else {\n        MPI_Gather(&local_min[0], n, MPI_FLOAT, NULL, 0, MPI_FLOAT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<float> local_min(n);\n    float min_val = x[0];\n\n    for (int i = 0; i < n; i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        local_min[i] = min_val;\n    }\n\n    if (rank == 0) {\n        std::vector<float> global_min(n);\n        MPI_Gather(&local_min[0], n, MPI_FLOAT, &global_min[0], n, MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = global_min;\n    } else {\n        MPI_Gather(&local_min[0], n, MPI_FLOAT, NULL, 0, MPI_FLOAT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<float> local_min(n);\n    float min_val = x[0];\n\n    for (int i = 0; i < n; i++) {\n        MPI_Allreduce(&x[i], &min_val, 1, MPI_FLOAT, MPI_MIN, MPI_COMM_WORLD);\n        local_min[i] = min_val;\n    }\n\n    if (rank == 0) {\n        x = local_min;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<float> local_min(n);\n    float min_val = x[0];\n\n    for (int i = 0; i < n; i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        local_min[i] = min_val;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recv_min(n);\n        MPI_Gather(&local_min[0], n, MPI_FLOAT, &recv_min[0], n, MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = recv_min;\n    } else {\n        MPI_Gather(&local_min[0], n, MPI_FLOAT, NULL, 0, MPI_FLOAT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<float> local_min(n);\n    float prefix_min = x[0];\n    local_min[0] = x[0];\n\n    for (int i = 1; i < n; i++) {\n        if (x[i] < prefix_min) {\n            prefix_min = x[i];\n        }\n        local_min[i] = prefix_min;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recv_min(n);\n        MPI_Gather(local_min.data(), n, MPI_FLOAT, recv_min.data(), n, MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = recv_min;\n    } else {\n        MPI_Gather(local_min.data(), n, MPI_FLOAT, nullptr, 0, MPI_FLOAT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<float> local_min(n);\n\n    for (int i = 0; i < n; i++) {\n        float min_val;\n        MPI_Allreduce(&x[i], &min_val, 1, MPI_FLOAT, MPI_MIN, MPI_COMM_WORLD);\n        local_min[i] = min_val;\n    }\n\n    if (rank == 0) {\n        x = local_min;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<float> local_min(n);\n    float min_val = x[0];\n\n    for (int i = 0; i < n; i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        local_min[i] = min_val;\n    }\n\n    if (rank == 0) {\n        std::vector<float> global_min(n);\n        MPI_Gather(&local_min[0], n, MPI_FLOAT, &global_min[0], n, MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = global_min;\n    } else {\n        MPI_Gather(&local_min[0], n, MPI_FLOAT, NULL, 0, MPI_FLOAT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<float> local_min(n);\n    float min_val = x[0];\n\n    for (int i = 0; i < n; i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        local_min[i] = min_val;\n    }\n\n    if (rank == 0) {\n        std::vector<float> global_min(n);\n        MPI_Gather(&local_min[0], n, MPI_FLOAT, &global_min[0], n, MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = global_min;\n    } else {\n        MPI_Gather(&local_min[0], n, MPI_FLOAT, NULL, 0, MPI_FLOAT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<float> local_min(n);\n    float min_val = x[0];\n\n    for (int i = 0; i < n; i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        local_min[i] = min_val;\n    }\n\n    if (rank == 0) {\n        std::vector<float> global_min(n);\n        MPI_Gather(&local_min[0], n, MPI_FLOAT, &global_min[0], n, MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = global_min;\n    } else {\n        MPI_Gather(&local_min[0], n, MPI_FLOAT, NULL, 0, MPI_FLOAT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<float> local_min(n);\n    float min_val = x[0];\n\n    for (int i = 0; i < n; i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        local_min[i] = min_val;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recv_min(n);\n        MPI_Gather(local_min.data(), n, MPI_FLOAT, recv_min.data(), n, MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = recv_min;\n    } else {\n        MPI_Gather(local_min.data(), n, MPI_FLOAT, nullptr, 0, MPI_FLOAT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    float local_min = x[0];\n    float global_min;\n\n    for (int i = 0; i < n; i++) {\n        if (x[i] < local_min) {\n            local_min = x[i];\n        }\n        MPI_Allreduce(&local_min, &global_min, 1, MPI_FLOAT, MPI_MIN, MPI_COMM_WORLD);\n        x[i] = global_min;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<float> local_min(n);\n    float min_val = x[0];\n\n    for (int i = 0; i < n; i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        local_min[i] = min_val;\n    }\n\n    if (rank == 0) {\n        std::vector<float> global_min(n);\n        MPI_Gather(local_min.data(), n, MPI_FLOAT, global_min.data(), n, MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = global_min;\n    } else {\n        MPI_Gather(local_min.data(), n, MPI_FLOAT, NULL, 0, MPI_FLOAT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<float> local_min(n);\n    float global_min;\n\n    for (int i = 0; i < n; i++) {\n        float min_val;\n        MPI_Allreduce(&x[i], &min_val, 1, MPI_FLOAT, MPI_MIN, MPI_COMM_WORLD);\n        local_min[i] = min_val;\n    }\n\n    if (rank == 0) {\n        MPI_Reduce(&local_min[0], &x[0], n, MPI_FLOAT, MPI_MIN, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    float local_min = x[0];\n    float global_min;\n\n    for (int i = 0; i < n; i++) {\n        if (x[i] < local_min) {\n            local_min = x[i];\n        }\n        MPI_Allreduce(&local_min, &global_min, 1, MPI_FLOAT, MPI_MIN, MPI_COMM_WORLD);\n        x[i] = global_min;\n    }\n\n    if (rank == 0) {\n        MPI_Bcast(&x[0], n, MPI_FLOAT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    float local_min = x[0];\n    float global_min;\n\n    for (int i = 0; i < n; i++) {\n        if (x[i] < local_min) {\n            local_min = x[i];\n        }\n        MPI_Allreduce(&local_min, &global_min, 1, MPI_FLOAT, MPI_MIN, MPI_COMM_WORLD);\n        x[i] = global_min;\n    }\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    std::vector<float> local_min(n);\n    float min_val = x[0];\n\n    for (int i = 0; i < n; i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        local_min[i] = min_val;\n    }\n\n    if (rank == 0) {\n        std::vector<float> global_min(n);\n        MPI_Gather(&local_min[0], n, MPI_FLOAT, &global_min[0], n, MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = global_min;\n    } else {\n        MPI_Gather(&local_min[0], n, MPI_FLOAT, NULL, 0, MPI_FLOAT, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<float> local_min(n);\n\n    for (int i = 0; i < n; i++) {\n        float min_val;\n        MPI_Allreduce(&x[i], &min_val, 1, MPI_FLOAT, MPI_MIN, MPI_COMM_WORLD);\n        local_min[i] = min_val;\n    }\n\n    if (rank == 0) {\n        x = local_min;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<float> local_min(n);\n    float min_val = x[0];\n\n    for (int i = 0; i < n; i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        local_min[i] = min_val;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recv_min(n);\n        MPI_Gather(local_min.data(), n, MPI_FLOAT, recv_min.data(), n, MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = recv_min;\n    } else {\n        MPI_Gather(local_min.data(), n, MPI_FLOAT, nullptr, 0, MPI_FLOAT, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<float> local_min(n);\n\n    for (int i = 0; i < n; i++) {\n        float min_val;\n        MPI_Allreduce(&x[i], &min_val, 1, MPI_FLOAT, MPI_MIN, MPI_COMM_WORLD);\n        local_min[i] = min_val;\n    }\n\n    if (rank == 0) {\n        x = local_min;\n    }\n}"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "31_scan_scan_with_min_function",
    "parallelism_model": "cuda",
    "prompt": "/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\n__global__ void partialMinimums(float *x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[0];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[0];\n        for (int i = 0; i <= tid; ++i) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[0];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[0];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[0];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[0];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[0];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[0];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[0];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[0];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[0];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[0];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[0];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[0];\n        for (int i = 0; i <= tid; ++i) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[0];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[0];\n        for (int i = 0; i <= tid; ++i) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[0];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        float min_val = x[tid];\n        for (int i = 0; i <= tid; i++) {\n            if (x[i] < min_val) {\n                min_val = x[i];\n            }\n        }\n        x[tid] = min_val;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "31_scan_scan_with_min_function",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? n : local_start + local_n;\n\n    for (int i = local_start; i < local_end; i++) {\n        float min_val = x[i];\n        for (int j = 0; j <= i; j++) {\n            min_val = std::min(min_val, x[j]);\n        }\n        x[i] = min_val;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[local_start], local_end - local_start, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&x[r * local_n], local_n, MPI_FLOAT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    if (rank == size - 1) {\n        local_end = x.size();\n    }\n\n    float local_min = x[local_start];\n    for (int i = local_start + 1; i < local_end; i++) {\n        if (x[i] < local_min) {\n            local_min = x[i];\n        }\n        x[i] = local_min;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[local_start], local_n, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * local_n], local_n, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_n;\n\n    float min_val = x[local_start];\n    for (int i = local_start + 1; i < local_end; i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        x[i] = min_val;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[local_start], local_end - local_start, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&x[r * local_n], local_n, MPI_FLOAT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? n : local_start + local_n;\n\n    float local_min = x[local_start];\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; i++) {\n        if (x[i] < local_min) {\n            local_min = x[i];\n        }\n        x[i] = local_min;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[local_start], local_n, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * local_n], local_n, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? n : local_start + local_n;\n\n    float local_min = x[local_start];\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; i++) {\n        if (x[i] < local_min) {\n            local_min = x[i];\n        }\n        x[i] = local_min;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[local_start], local_n, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * local_n], local_n, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_n;\n\n    float min_val = x[local_start];\n    for (int i = local_start + 1; i < local_end; i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        x[i] = min_val;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[local_start], local_end - local_start, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * local_n], local_n, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? n : local_start + local_n;\n\n    float local_min = x[local_start];\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; i++) {\n        local_min = std::min(local_min, x[i]);\n        x[i] = local_min;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[local_start], local_n, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * local_n], local_n, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? n : local_start + local_n;\n\n    float local_min = x[local_start];\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; i++) {\n        local_min = std::min(local_min, x[i]);\n        x[i] = local_min;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[local_start], local_n, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * local_n], local_n, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_n;\n\n    float local_min = x[local_start];\n    for (int i = local_start + 1; i < local_end; i++) {\n        if (x[i] < local_min) {\n            local_min = x[i];\n        }\n        x[i] = local_min;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[local_start], local_n, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * local_n], local_n, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_n;\n\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; i++) {\n        float min_val = x[i];\n        for (int j = 0; j <= i; j++) {\n            min_val = std::min(min_val, x[j]);\n        }\n        x[i] = min_val;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[local_start], local_n, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&x[r * local_n], local_n, MPI_FLOAT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? n : local_start + local_n;\n\n    float local_min = x[local_start];\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; i++) {\n        local_min = std::min(local_min, x[i]);\n        x[i] = local_min;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[local_start], local_n, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * local_n], local_n, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? n : local_start + local_n;\n\n    float local_min = x[local_start];\n    #pragma omp parallel for\n    for (int i = local_start + 1; i < local_end; i++) {\n        if (x[i] < local_min) {\n            local_min = x[i];\n        }\n        x[i] = local_min;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[local_start], local_n, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * local_n], local_n, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n```\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_n;\n\n    float min_val = x[local_start];\n    for (int i = local_start + 1; i < local_end; i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        x[i] = min_val;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[local_start], local_n, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * local_n], local_n, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_n;\n\n    for (int i = local_start; i < local_end; i++) {\n        float min_val = x[i];\n        for (int j = 0; j <= i; j++) {\n            min_val = std::min(min_val, x[j]);\n        }\n        x[i] = min_val;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[local_start], local_n, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&x[r * local_n], local_n, MPI_FLOAT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    if (rank == size - 1) {\n        end = x.size();\n        local_n = end - start;\n    }\n\n    float local_min = x[start];\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        if (x[i] < local_min) {\n            local_min = x[i];\n        }\n        x[i] = local_min;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[start], local_n, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * local_n], local_n, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<float> local_min(n);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        float min_val = x[i];\n        for (int j = 0; j <= i; j++) {\n            min_val = std::min(min_val, x[j]);\n        }\n        local_min[i] = min_val;\n    }\n\n    if (rank == 0) {\n        std::vector<float> global_min(n);\n        MPI_Gather(local_min.data(), n, MPI_FLOAT, global_min.data(), n, MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = global_min;\n    } else {\n        MPI_Gather(local_min.data(), n, MPI_FLOAT, NULL, 0, MPI_FLOAT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<float> local_min(n);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        float min_val = x[i];\n        for (int j = 0; j <= i; j++) {\n            min_val = std::min(min_val, x[j]);\n        }\n        local_min[i] = min_val;\n    }\n\n    if (rank == 0) {\n        std::vector<float> global_min(n);\n        MPI_Gather(&local_min[0], n, MPI_FLOAT, &global_min[0], n, MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = global_min;\n    } else {\n        MPI_Gather(&local_min[0], n, MPI_FLOAT, NULL, 0, MPI_FLOAT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    if (rank == size - 1) {\n        local_end = x.size();\n    }\n\n    float local_min = x[local_start];\n    for (int i = local_start + 1; i < local_end; i++) {\n        if (x[i] < local_min) {\n            local_min = x[i];\n        }\n        x[i] = local_min;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[local_start], local_n, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * local_n], local_n, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<float> local_min(n);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        local_min[i] = x[i];\n        for (int j = 0; j <= i; j++) {\n            local_min[i] = std::min(local_min[i], x[j]);\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<float> global_min(n);\n        MPI_Gather(local_min.data(), n, MPI_FLOAT, global_min.data(), n, MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = global_min;\n    } else {\n        MPI_Gather(local_min.data(), n, MPI_FLOAT, NULL, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    if (rank == size - 1) {\n        local_end = x.size();\n    }\n\n    float min_val = x[local_start];\n    for (int i = local_start + 1; i < local_end; i++) {\n        if (x[i] < min_val) {\n            min_val = x[i];\n        }\n        x[i] = min_val;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[local_start], local_n, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * local_n], local_n, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "34_scan_largest_contiguous_subarray_sum",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(Kokkos::View<int*> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int maxSum = x(0);\n    int currentSum = x(0);\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x(i), currentSum + x(i));\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x(0);\n    int currentSum = x(0);\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x(i), currentSum + x(i));\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x(0);\n    int currentSum = x(0);\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x(i), currentSum + x(i));\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x(0);\n    int currentSum = x(0);\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x(i), currentSum + x(i));\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x(0);\n    int currentSum = x(0);\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x(i), currentSum + x(i));\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x(0);\n    int currentSum = x(0);\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x(i), currentSum + x(i));\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x(0);\n    int currentSum = x(0);\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x(i), currentSum + x(i));\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x(0);\n    int currentSum = x(0);\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x(i), currentSum + x(i));\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxEndingHere = x(0);\n    int maxSoFar = x(0);\n\n    for (int i = 1; i < x.size(); i++) {\n        maxEndingHere = std::max(x(i), maxEndingHere + x(i));\n        maxSoFar = std::max(maxSoFar, maxEndingHere);\n    }\n\n    return maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        maxEndingHere = std::max(x[i], maxEndingHere + x[i]);\n        maxSoFar = std::max(maxSoFar, maxEndingHere);\n    }\n\n    return maxSoFar;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "34_scan_largest_contiguous_subarray_sum",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int max_sum = x[0];\n    int current_sum = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        current_sum = std::max(x[i], current_sum + x[i]);\n        max_sum = std::max(max_sum, current_sum);\n    }\n\n    return max_sum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "34_scan_largest_contiguous_subarray_sum",
    "parallelism_model": "serial",
    "prompt": "/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n    \n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n    \n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n    \n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n    \n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n    \n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n    \n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n    \n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n    \n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n    \n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n    \n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n    \n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n    \n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n    \n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n    \n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n    \n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n    \n    return maxSum;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n    \n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n    \n    return maxSum;\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "34_scan_largest_contiguous_subarray_sum",
    "parallelism_model": "hip",
    "prompt": "/* Compute the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Store the result in sum.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\n__global__ void maximumSubarray(const int *x, size_t N, int *sum) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    for (int i = 1; i < N; i++) {\n        currentSum = max(x[i], currentSum + x[i]);\n        maxSum = max(maxSum, currentSum);\n    }\n\n    *sum = maxSum;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (size_t i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "34_scan_largest_contiguous_subarray_sum",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int local_max = x[0];\n    int global_max = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        local_max = std::max(x[i], local_max + x[i]);\n        global_max = std::max(global_max, local_max);\n    }\n\n    int result;\n    MPI_Reduce(&global_max, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return result;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n    int current_max = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        current_max = std::max(x[i], current_max + x[i]);\n        local_max = std::max(local_max, current_max);\n        global_max = std::max(global_max, local_max);\n    }\n\n    return global_max;\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        local_max = std::max(x[i], local_max + x[i]);\n        global_max = std::max(global_max, local_max);\n    }\n\n    int result;\n    MPI_Reduce(&global_max, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return result;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n    \n    int n = x.size();\n    for (int i = 1; i < n; i++) {\n        local_max = std::max(x[i], local_max + x[i]);\n        global_max = std::max(global_max, local_max);\n    }\n    \n    int result;\n    MPI_Reduce(&global_max, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return result;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localMax = x[0];\n    int globalMax = x[0];\n    \n    for (int i = 1; i < x.size(); i++) {\n        localMax = std::max(x[i], localMax + x[i]);\n        globalMax = std::max(globalMax, localMax);\n    }\n    \n    int result;\n    MPI_Reduce(&globalMax, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return result;\n    } else {\n        return 0;  // Return 0 for non-root ranks\n    }\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n\n    int local_sum = x[0];\n    int global_sum = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        local_sum = std::max(x[i], local_sum + x[i]);\n        local_max = std::max(local_max, local_sum);\n        global_max = std::max(global_max, local_max);\n    }\n\n    MPI_Reduce(&global_max, &global_sum, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return global_sum;\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        local_max = std::max(x[i], local_max + x[i]);\n        global_max = std::max(global_max, local_max);\n    }\n\n    int result;\n    MPI_Reduce(&global_max, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    return result;\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        local_max = std::max(x[i], local_max + x[i]);\n        global_max = std::max(global_max, local_max);\n    }\n\n    int result;\n    MPI_Reduce(&global_max, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return result;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n    \n    for (int i = 1; i < x.size(); i++) {\n        local_max = std::max(x[i], local_max + x[i]);\n        global_max = std::max(global_max, local_max);\n    }\n    \n    int result;\n    MPI_Reduce(&global_max, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return result;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        local_max = std::max(x[i], local_max + x[i]);\n        global_max = std::max(global_max, local_max);\n    }\n\n    int result;\n    MPI_Reduce(&global_max, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return result;\n    } else {\n        return 0;  // Return 0 for non-root ranks\n    }\n}\n",
      "\n    int localMax = x[0];\n    int globalMax = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        localMax = std::max(x[i], localMax + x[i]);\n        globalMax = std::max(globalMax, localMax);\n    }\n\n    int result;\n    MPI_Reduce(&globalMax, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return result;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        local_max = std::max(x[i], local_max + x[i]);\n        global_max = std::max(global_max, local_max);\n    }\n\n    int result;\n    MPI_Reduce(&global_max, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    return result;\n}\n",
      "\n    int localMax = x[0];\n    int globalMax = x[0];\n    \n    for (int i = 1; i < x.size(); i++) {\n        localMax = std::max(x[i], localMax + x[i]);\n        globalMax = std::max(globalMax, localMax);\n    }\n    \n    int result;\n    MPI_Reduce(&globalMax, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return result;\n    } else {\n        return 0;  // Return 0 for non-root ranks\n    }\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n    int current_max = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        current_max = std::max(x[i], current_max + x[i]);\n        local_max = std::max(local_max, current_max);\n        global_max = std::max(global_max, local_max);\n    }\n\n    return global_max;\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        local_max = std::max(x[i], local_max + x[i]);\n        global_max = std::max(global_max, local_max);\n    }\n\n    int result;\n    MPI_Reduce(&global_max, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return result;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localMax = x[0];\n    int globalMax = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        localMax = std::max(x[i], localMax + x[i]);\n        globalMax = std::max(globalMax, localMax);\n    }\n\n    int result;\n    MPI_Reduce(&globalMax, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    return result;\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        local_max = std::max(x[i], local_max + x[i]);\n        global_max = std::max(global_max, local_max);\n    }\n\n    int result;\n    MPI_Reduce(&global_max, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return result;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n    \n    for (int i = 1; i < x.size(); i++) {\n        local_max = std::max(x[i], local_max + x[i]);\n        global_max = std::max(global_max, local_max);\n    }\n    \n    int result;\n    MPI_Reduce(&global_max, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return result;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int localMax = x[0];\n    int globalMax = x[0];\n    int currentMax = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        currentMax = std::max(x[i], currentMax + x[i]);\n        localMax = std::max(localMax, currentMax);\n        globalMax = std::max(globalMax, localMax);\n    }\n\n    int result;\n    MPI_Reduce(&localMax, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    return result;\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n    int current_max = x[0];\n\n    for (int i = 1; i < x.size(); i++) {\n        current_max = std::max(x[i], current_max + x[i]);\n        local_max = std::max(local_max, current_max);\n        global_max = std::max(global_max, local_max);\n    }\n\n    int result;\n    MPI_Reduce(&local_max, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return result;\n    } else {\n        return 0;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "34_scan_largest_contiguous_subarray_sum",
    "parallelism_model": "cuda",
    "prompt": "/* Compute the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Store the result in sum.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\n__global__ void maximumSubarray(const int *x, size_t N, int *sum) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (size_t i = 1; i < N; ++i) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (size_t i = 1; i < N; ++i) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (size_t i = 1; i < N; ++i) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (size_t i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (size_t i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (size_t i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (size_t i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (size_t i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (size_t i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (size_t i = 1; i < N; ++i) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n",
      "\n    int maxEndingHere = x[0];\n    int maxSoFar = x[0];\n\n    for (int i = 1; i < N; i++) {\n        maxEndingHere = max(x[i], maxEndingHere + x[i]);\n        maxSoFar = max(maxSoFar, maxEndingHere);\n    }\n\n    *sum = maxSoFar;\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "34_scan_largest_contiguous_subarray_sum",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int local_max = x[0];\n    int global_max = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        local_max = std::max(x[i], local_max + x[i]);\n        #pragma omp critical\n        {\n            global_max = std::max(global_max, local_max);\n        }\n    }\n\n    int result;\n    MPI_Reduce(&global_max, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    return result;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    int globalMaxSum;\n    MPI_Reduce(&maxSum, &globalMaxSum, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxSum;\n    } else {\n        return 0;  // Return 0 for non-root ranks\n    }\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        local_max = std::max(x[i], local_max + x[i]);\n        #pragma omp critical\n        {\n            if (local_max > global_max) {\n                global_max = local_max;\n            }\n        }\n    }\n\n    int result;\n    MPI_Reduce(&global_max, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    return result;\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n    int current_max = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        current_max = std::max(x[i], current_max + x[i]);\n        local_max = std::max(local_max, current_max);\n    }\n\n    MPI_Reduce(&local_max, &global_max, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return global_max;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n\n    int globalMaxSum;\n    MPI_Reduce(&maxSum, &globalMaxSum, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxSum;\n    } else {\n        return 0;  // Return 0 for non-root ranks\n    }\n}\n",
      "\n    int globalMax = x[0];\n    int localMax = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        localMax = std::max(x[i], localMax + x[i]);\n        #pragma omp critical\n        {\n            if (localMax > globalMax) {\n                globalMax = localMax;\n            }\n        }\n    }\n\n    int result;\n    MPI_Reduce(&globalMax, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    return result;\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n    int current_max = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        current_max = std::max(x[i], current_max + x[i]);\n        local_max = std::max(local_max, current_max);\n    }\n\n    MPI_Reduce(&local_max, &global_max, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return global_max;\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        local_max = std::max(x[i], local_max + x[i]);\n        #pragma omp critical\n        {\n            if (local_max > global_max) {\n                global_max = local_max;\n            }\n        }\n    }\n\n    int result;\n    MPI_Reduce(&global_max, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    return result;\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n    int current_max = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        current_max = std::max(x[i], current_max + x[i]);\n        local_max = std::max(local_max, current_max);\n    }\n\n    MPI_Reduce(&local_max, &global_max, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return global_max;\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n    int current_max = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        current_max = std::max(x[i], current_max + x[i]);\n        local_max = std::max(local_max, current_max);\n    }\n\n    MPI_Reduce(&local_max, &global_max, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return global_max;\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        local_max = std::max(x[i], local_max + x[i]);\n        #pragma omp critical\n        {\n            if (local_max > global_max) {\n                global_max = local_max;\n            }\n        }\n    }\n\n    int result;\n    MPI_Reduce(&global_max, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return result;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n    int current_max = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        current_max = std::max(x[i], current_max + x[i]);\n        local_max = std::max(local_max, current_max);\n    }\n\n    MPI_Reduce(&local_max, &global_max, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return global_max;\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n    \n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        local_max = std::max(x[i], local_max + x[i]);\n        #pragma omp critical\n        {\n            global_max = std::max(global_max, local_max);\n        }\n    }\n    \n    int result;\n    MPI_Reduce(&global_max, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return result;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n    int current_max = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        current_max = std::max(x[i], current_max + x[i]);\n        local_max = std::max(local_max, current_max);\n    }\n\n    MPI_Reduce(&local_max, &global_max, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return global_max;\n}\n",
      "\n    int maxSum = x[0];\n    int currentSum = x[0];\n    \n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        currentSum = std::max(x[i], currentSum + x[i]);\n        maxSum = std::max(maxSum, currentSum);\n    }\n    \n    int globalMaxSum;\n    MPI_Reduce(&maxSum, &globalMaxSum, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalMaxSum;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n    \n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        local_max = std::max(x[i], local_max + x[i]);\n        #pragma omp critical\n        {\n            global_max = std::max(global_max, local_max);\n        }\n    }\n    \n    int result;\n    MPI_Reduce(&global_max, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return result;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        local_max = std::max(x[i], local_max + x[i]);\n        #pragma omp critical\n        {\n            global_max = std::max(global_max, local_max);\n        }\n    }\n\n    int result;\n    MPI_Reduce(&global_max, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    return result;\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n    \n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        local_max = std::max(x[i], local_max + x[i]);\n        #pragma omp critical\n        {\n            if (local_max > global_max) {\n                global_max = local_max;\n            }\n        }\n    }\n    \n    int result;\n    MPI_Reduce(&global_max, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return result;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        local_max = std::max(x[i], local_max + x[i]);\n        #pragma omp critical\n        {\n            if (local_max > global_max) {\n                global_max = local_max;\n            }\n        }\n    }\n\n    int result;\n    MPI_Reduce(&global_max, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    return result;\n}\n",
      "\n    int local_max = x[0];\n    int global_max = x[0];\n\n    #pragma omp parallel for\n    for (int i = 1; i < x.size(); i++) {\n        local_max = std::max(x[i], local_max + x[i]);\n        #pragma omp critical\n        {\n            global_max = std::max(global_max, local_max);\n        }\n    }\n\n    int result;\n    MPI_Reduce(&global_max, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    return result;\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "30_scan_prefix_sum",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the prefix sum of the vector x into output.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(Kokkos::View<const double*> const& x, Kokkos::View<double*> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\nKokkos::parallel_scan(\"PrefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n    if (final) {\n        output(i) = update;\n    }\n    update += x(i);\n});\n}",
      "\nKokkos::parallel_scan(\"PrefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n    if (final) {\n        output(i) = update + x(i);\n    }\n    update += x(i);\n});\n}",
      "\nKokkos::parallel_scan(x.size(), KOKKOS_LAMBDA(int i, double& update, const bool final) {\n    if (final) {\n        output(i) = update;\n    }\n    update += x(i);\n});\n}",
      "\nKokkos::parallel_scan(\"PrefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n    if (final) {\n        output(i) = update;\n    }\n    update += x(i);\n});\n}",
      "\nKokkos::parallel_scan(\"prefix_sum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n    if (final) {\n        output(i) = update + x(i);\n    }\n    update += x(i);\n});\n}",
      "\nKokkos::parallel_scan(\"PrefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n    if (final) {\n        output(i) = update;\n    }\n    update += x(i);\n});\n}",
      "\nKokkos::parallel_scan(\"PrefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n    if (final) {\n        output(i) = update;\n    }\n    update += x(i);\n});\n}",
      "\n  Kokkos::parallel_scan(\"PrefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n    if (final) {\n      output(i) = update + x(i);\n    }\n    update += x(i);\n  });\n}",
      "\nKokkos::parallel_scan(\"PrefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n    if (final) {\n        output(i) = update;\n    }\n    update += x(i);\n});\n}",
      "\nKokkos::parallel_scan(\"prefix_sum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n    if (final) {\n        output(i) = update;\n    }\n    update += x(i);\n});\n}",
      "\nKokkos::parallel_scan(x.size(), KOKKOS_LAMBDA(int i, double& update, const bool final) {\n    if (final) {\n        output(i) = update;\n    }\n    update += x(i);\n});\n}",
      "\nKokkos::parallel_scan(\"PrefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n    if (final) {\n        output(i) = update;\n    }\n    update += x(i);\n});\n}",
      "\nKokkos::parallel_scan(\"prefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n    if (final) {\n        output(i) = update;\n    }\n    update += x(i);\n});\n}",
      "\nKokkos::parallel_scan(x.size(), KOKKOS_LAMBDA(int i, double& update, const bool final) {\n    if (final) {\n        output(i) = update;\n    }\n    update += x(i);\n});\n}",
      "\nKokkos::parallel_scan(\"prefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n    if (final) {\n        output(i) = update;\n    }\n    update += x(i);\n});\n}",
      "\nKokkos::parallel_scan(\"PrefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n    if (final) {\n        output(i) = update;\n    }\n    update += x(i);\n});\n}",
      "\n  Kokkos::parallel_scan(\"PrefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n    if (final) {\n      output(i) = update + x(i);\n    }\n    update += x(i);\n  });\n}",
      "\n  Kokkos::parallel_scan(\"PrefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n    if (final) {\n      output(i) = update;\n    }\n    update += x(i);\n  });\n}",
      "\nKokkos::parallel_scan(\"PrefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n    if (final) {\n        output(i) = update;\n    }\n    update += x(i);\n});\n}",
      "\n  Kokkos::parallel_scan(\"PrefixSum\", x.size(), KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n    if (final) {\n      output(i) = update;\n    }\n    update += x(i);\n  });\n}"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "30_scan_prefix_sum",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int n = x.size();\n    output.resize(n);\n    output[0] = x[0];\n    \n    #pragma omp parallel for\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (i > 0) {\n            output[i] = output[i - 1] + x[i];\n        } else {\n            output[i] = x[i];\n        }\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[0] = x[0];\n    #pragma omp parallel for\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[0] = x[0];\n    \n    #pragma omp parallel for\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[0] = x[0];\n    \n    #pragma omp parallel for\n    for (int i = 1; i < n; ++i) {\n        output[i] = output[i-1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[0] = x[0];\n    \n    #pragma omp parallel for\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (i > 0) {\n            output[i] = output[i - 1] + x[i];\n        } else {\n            output[i] = x[i];\n        }\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[0] = x[0];\n    \n    #pragma omp parallel for\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[0] = x[0];\n    #pragma omp parallel for\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[0] = x[0];\n    #pragma omp parallel for\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[0] = x[0];\n    #pragma omp parallel for\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[0] = x[0];\n    \n    #pragma omp parallel for\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[0] = x[0];\n    #pragma omp parallel for\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[0] = x[0];\n    \n    #pragma omp parallel for\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[0] = x[0];\n    #pragma omp parallel for\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[0] = x[0];\n    \n    #pragma omp parallel for\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (i > 0) {\n            output[i] = output[i - 1] + x[i];\n        } else {\n            output[i] = x[i];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (i > 0) {\n            output[i] = output[i - 1] + x[i];\n        } else {\n            output[i] = x[i];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (i > 0) {\n            output[i] = output[i - 1] + x[i];\n        } else {\n            output[i] = x[i];\n        }\n    }\n}",
      "\n    int n = x.size();\n    output.resize(n);\n    output[0] = x[0];\n    \n    #pragma omp parallel for\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "30_scan_prefix_sum",
    "parallelism_model": "serial",
    "prompt": "/* Compute the prefix sum of the vector x into output.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\noutput.resize(x.size());\noutput[0] = x[0];\nfor (int i = 1; i < x.size(); i++) {\n    output[i] = output[i-1] + x[i];\n}\n}",
      "\noutput.resize(x.size());\noutput[0] = x[0];\nfor (int i = 1; i < x.size(); i++) {\n    output[i] = output[i-1] + x[i];\n}\n}",
      "\noutput.resize(x.size());\noutput[0] = x[0];\nfor (int i = 1; i < x.size(); i++) {\n    output[i] = output[i-1] + x[i];\n}\n}",
      "\noutput.resize(x.size());\noutput[0] = x[0];\nfor (int i = 1; i < x.size(); i++) {\n    output[i] = output[i-1] + x[i];\n}\n}",
      "\noutput.resize(x.size());\noutput[0] = x[0];\nfor (int i = 1; i < x.size(); i++) {\n    output[i] = output[i-1] + x[i];\n}\n}",
      "\n    output.resize(x.size());\n    output[0] = x[0];\n    for (int i = 1; i < x.size(); i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}",
      "\noutput.resize(x.size());\noutput[0] = x[0];\nfor (int i = 1; i < x.size(); i++) {\n    output[i] = output[i-1] + x[i];\n}\n}",
      "\noutput.resize(x.size());\noutput[0] = x[0];\nfor (int i = 1; i < x.size(); i++) {\n    output[i] = output[i-1] + x[i];\n}\n}",
      "\n    output.resize(x.size());\n    double sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n        output[i] = sum;\n    }\n}",
      "\noutput.resize(x.size());\noutput[0] = x[0];\nfor (int i = 1; i < x.size(); i++) {\n    output[i] = output[i-1] + x[i];\n}\n}",
      "\n    output.resize(x.size());\n    output[0] = x[0];\n    for (int i = 1; i < x.size(); i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}",
      "\noutput.resize(x.size());\noutput[0] = x[0];\nfor (int i = 1; i < x.size(); i++) {\n    output[i] = output[i-1] + x[i];\n}\n}",
      "\noutput.resize(x.size());\noutput[0] = x[0];\nfor (int i = 1; i < x.size(); i++) {\n    output[i] = output[i-1] + x[i];\n}\n}",
      "\noutput.resize(x.size());\noutput[0] = x[0];\nfor (int i = 1; i < x.size(); i++) {\n    output[i] = output[i-1] + x[i];\n}\n}",
      "\noutput.resize(x.size());\noutput[0] = x[0];\nfor (int i = 1; i < x.size(); i++) {\n    output[i] = output[i-1] + x[i];\n}\n}",
      "\n    output.resize(x.size());\n    output[0] = x[0];\n    for (int i = 1; i < x.size(); i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}",
      "\noutput.resize(x.size());\noutput[0] = x[0];\nfor (int i = 1; i < x.size(); i++) {\n    output[i] = output[i-1] + x[i];\n}\n}",
      "\noutput.resize(x.size());\noutput[0] = x[0];\nfor (int i = 1; i < x.size(); i++) {\n    output[i] = output[i-1] + x[i];\n}\n}",
      "\noutput.resize(x.size());\noutput[0] = x[0];\nfor (int i = 1; i < x.size(); i++) {\n    output[i] = output[i-1] + x[i];\n}\n}",
      "\noutput.resize(x.size());\noutput[0] = x[0];\nfor (int i = 1; i < x.size(); i++) {\n    output[i] = output[i-1] + x[i];\n}\n}"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "30_scan_prefix_sum",
    "parallelism_model": "hip",
    "prompt": "/* Compute the prefix sum of the vector x into output.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\n__global__ void prefixSum(const double* x, double *output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 1; i <= tid; i++) {\n            output[tid] += x[i];\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double sum = 0;\n        for (int i = 0; i <= idx; i++) {\n            sum += x[i];\n        }\n        output[idx] = sum;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 1; i <= tid; i++) {\n            output[tid] += x[i];\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 1; i <= tid; i++) {\n            output[tid] += x[i];\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        output[tid] = x[tid];\n        __syncthreads();\n\n        for (int stride = 1; stride < N; stride *= 2) {\n            int index = tid - stride;\n            if (index >= 0) {\n                output[tid] += output[index];\n            }\n            __syncthreads();\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0.0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 1; i <= tid; i++) {\n            output[tid] += x[i];\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 1; i <= tid; i++) {\n            output[tid] += x[i];\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 1; i <= tid; i++) {\n            output[tid] += x[i];\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        output[tid] = x[tid];\n        __syncthreads();\n        for (int stride = 1; stride < N; stride *= 2) {\n            int index = tid - stride;\n            if (index >= 0) {\n                output[tid] += output[index];\n            }\n            __syncthreads();\n        }\n    }\n}",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        output[index] = x[index];\n        for (int i = 1; i <= index; i++) {\n            output[index] += x[index - i];\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "30_scan_prefix_sum",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in output on rank 0.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n);\n\n    MPI_Allreduce(&x[0], &prefix[0], n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output[0] = prefix[0];\n        for (int i = 1; i < n; ++i) {\n            output[i] = prefix[i] + output[i - 1];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n);\n\n    MPI_Scan(&x[0], &prefix[0], n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output = prefix;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n, 0);\n    prefix[0] = x[0];\n\n    for (int i = 1; i < n; i++) {\n        prefix[i] = prefix[i - 1] + x[i];\n    }\n\n    if (rank == 0) {\n        output = prefix;\n        for (int i = 1; i < size; i++) {\n            std::vector<double> recvPrefix(n);\n            MPI_Recv(&recvPrefix[0], n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                output[j] += recvPrefix[j];\n            }\n        }\n    } else {\n        MPI_Send(&prefix[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    std::vector<double> prefix(n);\n\n    MPI_Allreduce(x.data(), prefix.data(), n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output[0] = prefix[0];\n        for (int i = 1; i < n; ++i) {\n            output[i] = output[i-1] + prefix[i];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n);\n    prefix[0] = x[0];\n\n    for (int i = 1; i < n; i++) {\n        prefix[i] = prefix[i - 1] + x[i];\n    }\n\n    if (rank == 0) {\n        output = prefix;\n        for (int i = 1; i < size; i++) {\n            std::vector<double> recvPrefix(n);\n            MPI_Recv(&recvPrefix[0], n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                output[j] += recvPrefix[j];\n            }\n        }\n    } else {\n        MPI_Send(&prefix[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nint n = x.size();\nstd::vector<double> prefix(n, 0);\nprefix[0] = x[0];\n\nfor (int i = 1; i < n; i++) {\n    prefix[i] = prefix[i - 1] + x[i];\n}\n\nif (rank == 0) {\n    output = prefix;\n    for (int i = 1; i < size; i++) {\n        std::vector<double> recvPrefix(n);\n        MPI_Recv(&recvPrefix[0], n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        for (int j = 0; j < n; j++) {\n            output[j] += recvPrefix[j];\n        }\n    }\n} else {\n    MPI_Send(&prefix[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n}\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n, 0);\n    prefix[0] = x[0];\n\n    for (int i = 1; i < n; i++) {\n        prefix[i] = prefix[i - 1] + x[i];\n    }\n\n    if (rank == 0) {\n        output = prefix;\n        for (int i = 1; i < size; i++) {\n            std::vector<double> recvPrefix(n);\n            MPI_Recv(&recvPrefix[0], n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                output[j] += recvPrefix[j];\n            }\n        }\n    } else {\n        MPI_Send(&prefix[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n);\n    prefix[0] = x[0];\n\n    for (int i = 1; i < n; i++) {\n        prefix[i] = prefix[i - 1] + x[i];\n    }\n\n    if (rank == 0) {\n        output = prefix;\n        for (int i = 1; i < size; i++) {\n            std::vector<double> recvPrefix(n);\n            MPI_Recv(&recvPrefix[0], n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                output[j] += recvPrefix[j];\n            }\n        }\n    } else {\n        MPI_Send(&prefix[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n);\n    prefix[0] = x[0];\n\n    for (int i = 1; i < n; i++) {\n        prefix[i] = prefix[i - 1] + x[i];\n    }\n\n    if (rank == 0) {\n        output = prefix;\n        for (int i = 1; i < size; i++) {\n            std::vector<double> recvPrefix(n);\n            MPI_Recv(&recvPrefix[0], n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                output[j] += recvPrefix[j];\n            }\n        }\n    } else {\n        MPI_Send(&prefix[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n, 0);\n    prefix[0] = x[0];\n\n    for (int i = 1; i < n; i++) {\n        prefix[i] = prefix[i - 1] + x[i];\n    }\n\n    if (rank == 0) {\n        output = prefix;\n        for (int i = 1; i < size; i++) {\n            std::vector<double> recvPrefix(n);\n            MPI_Recv(&recvPrefix[0], n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                output[j] += recvPrefix[j];\n            }\n        }\n    } else {\n        MPI_Send(&prefix[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n);\n    prefix[0] = x[0];\n\n    for (int i = 1; i < n; i++) {\n        prefix[i] = prefix[i - 1] + x[i];\n    }\n\n    if (rank == 0) {\n        output = prefix;\n        for (int i = 1; i < size; i++) {\n            std::vector<double> recvPrefix(n);\n            MPI_Recv(&recvPrefix[0], n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                output[j] += recvPrefix[j];\n            }\n        }\n    } else {\n        MPI_Send(&prefix[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n);\n    prefix[0] = x[0];\n\n    for (int i = 1; i < n; i++) {\n        prefix[i] = prefix[i - 1] + x[i];\n    }\n\n    if (rank == 0) {\n        output = prefix;\n        for (int i = 1; i < size; i++) {\n            std::vector<double> recvPrefix(n);\n            MPI_Recv(&recvPrefix[0], n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                output[j] += recvPrefix[j];\n            }\n        }\n    } else {\n        MPI_Send(&prefix[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n);\n\n    MPI_Allreduce(&x[0], &prefix[0], n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output[0] = prefix[0];\n        for (int i = 1; i < n; ++i) {\n            output[i] = output[i-1] + prefix[i];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n);\n\n    MPI_Scan(x.data(), prefix.data(), n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output = prefix;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n);\n    prefix[0] = x[0];\n    for (int i = 1; i < n; i++) {\n        prefix[i] = prefix[i-1] + x[i];\n    }\n\n    if (rank == 0) {\n        output = prefix;\n        for (int i = 1; i < size; i++) {\n            std::vector<double> recvPrefix(n);\n            MPI_Recv(&recvPrefix[0], n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                output[j] += recvPrefix[j];\n            }\n        }\n    } else {\n        MPI_Send(&prefix[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n);\n    prefix[0] = x[0];\n\n    for (int i = 1; i < n; i++) {\n        prefix[i] = prefix[i - 1] + x[i];\n    }\n\n    if (rank == 0) {\n        output = prefix;\n        for (int i = 1; i < size; i++) {\n            std::vector<double> recvPrefix(n);\n            MPI_Recv(&recvPrefix[0], n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                output[j] += recvPrefix[j];\n            }\n        }\n    } else {\n        MPI_Send(&prefix[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n);\n    prefix[0] = x[0];\n\n    for (int i = 1; i < n; ++i) {\n        prefix[i] = prefix[i - 1] + x[i];\n    }\n\n    if (rank == 0) {\n        output = prefix;\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> recvPrefix(n);\n            MPI_Recv(&recvPrefix[0], n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; ++j) {\n                output[j] += recvPrefix[j];\n            }\n        }\n    } else {\n        MPI_Send(&prefix[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n, 0);\n    prefix[0] = x[0];\n\n    for (int i = 1; i < n; i++) {\n        prefix[i] = prefix[i - 1] + x[i];\n    }\n\n    if (rank == 0) {\n        output = prefix;\n        for (int i = 1; i < size; i++) {\n            std::vector<double> recvPrefix(n);\n            MPI_Recv(&recvPrefix[0], n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                output[j] += recvPrefix[j];\n            }\n        }\n    } else {\n        MPI_Send(&prefix[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n);\n\n    MPI_Scan(&x[0], &prefix[0], n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output = prefix;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n);\n    prefix[0] = x[0];\n\n    for (int i = 1; i < n; i++) {\n        prefix[i] = prefix[i - 1] + x[i];\n    }\n\n    if (rank == 0) {\n        output = prefix;\n        for (int i = 1; i < size; i++) {\n            std::vector<double> recvPrefix(n);\n            MPI_Recv(&recvPrefix[0], n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                output[j] += recvPrefix[j];\n            }\n        }\n    } else {\n        MPI_Send(&prefix[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "30_scan_prefix_sum",
    "parallelism_model": "cuda",
    "prompt": "/* Compute the prefix sum of the vector x into output.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\n__global__ void prefixSum(const double *x, double *output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        output[idx] = x[idx];\n        for (int i = 1; i <= idx; i++) {\n            output[idx] += x[idx - i];\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        output[idx] = x[idx];\n        for (int i = 1; i <= idx; i++) {\n            output[idx] += x[idx - i];\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 1; i <= tid; i++) {\n            output[tid] += x[i];\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        output[idx] = x[idx];\n        for (int i = 1; i <= idx; i++) {\n            output[idx] += x[idx - i];\n        }\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        output[idx] = x[idx];\n        for (int i = 1; i <= idx; i++) {\n            output[idx] += x[idx - i];\n        }\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        output[idx] = x[idx];\n        for (int i = 1; i <= idx; i++) {\n            output[idx] += x[idx - i];\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0.0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double sum = 0;\n        for (int i = 0; i <= idx; i++) {\n            sum += x[i];\n        }\n        output[idx] = sum;\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        output[idx] = x[idx];\n        for (int i = 1; i <= idx; i++) {\n            output[idx] += x[idx - i];\n        }\n    }\n}",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        double sum = 0;\n        for (int i = 0; i <= index; i++) {\n            sum += x[i];\n        }\n        output[index] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0;\n        for (int i = 0; i <= tid; i++) {\n            sum += x[i];\n        }\n        output[tid] = sum;\n    }\n}",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        double sum = 0;\n        for (int i = 0; i <= index; i++) {\n            sum += x[i];\n        }\n        output[index] = sum;\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        output[idx] = x[idx];\n        for (int i = 1; i <= idx; i++) {\n            output[idx] += x[idx - i];\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 1; i <= tid; i++) {\n            output[tid] += x[i];\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "30_scan_prefix_sum",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in output on rank 0.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_output(local_n);\n\n    // Compute local prefix sum using OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < local_n; i++) {\n        if (i > 0) {\n            local_output[i] = local_output[i - 1] + x[rank * local_n + i];\n        } else {\n            local_output[i] = x[rank * local_n + i];\n        }\n    }\n\n    // Gather local results to rank 0\n    MPI_Gather(&local_output[0], local_n, MPI_DOUBLE, &output[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Compute prefix sum on rank 0\n    if (rank == 0) {\n        for (int i = 1; i < x.size(); i++) {\n            output[i] += output[i - 1];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_output(local_n);\n\n    // Compute local prefix sum using OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < local_n; ++i) {\n        local_output[i] = x[i];\n        if (i > 0) {\n            local_output[i] += x[i - 1];\n        }\n    }\n\n    // Gather local results to rank 0\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE, output.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Compute prefix sum on rank 0\n    if (rank == 0) {\n        for (int i = 1; i < x.size(); ++i) {\n            output[i] += output[i - 1];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        prefix[i] = x[i];\n    }\n\n    for (int d = 1; d < n; d *= 2) {\n        for (int i = d; i < n; i++) {\n            prefix[i] += prefix[i - d];\n        }\n    }\n\n    if (rank == 0) {\n        output = prefix;\n        for (int i = 1; i < size; i++) {\n            std::vector<double> recv_prefix(n);\n            MPI_Recv(&recv_prefix[0], n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                output[j] += recv_prefix[j];\n            }\n        }\n    } else {\n        MPI_Send(&prefix[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_output(local_n);\n\n    // Compute local prefix sum using OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < local_n; ++i) {\n        local_output[i] = x[i];\n        if (i > 0) {\n            local_output[i] += x[i - 1];\n        }\n    }\n\n    // Gather local results to rank 0\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE, output.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Compute prefix sum on rank 0\n    if (rank == 0) {\n        for (int i = 1; i < x.size(); ++i) {\n            output[i] += output[i - 1];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_output(local_n);\n\n    // Compute local prefix sum using OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < local_n; ++i) {\n        local_output[i] = x[i];\n        if (i > 0) {\n            local_output[i] += x[i - 1];\n        }\n    }\n\n    // Gather local results to rank 0\n    MPI_Gather(&local_output[0], local_n, MPI_DOUBLE, &output[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Compute prefix sum on rank 0\n    if (rank == 0) {\n        for (int i = 1; i < x.size(); ++i) {\n            output[i] += output[i - 1];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        prefix[i] = x[i];\n    }\n\n    for (int d = 1; d < n; d *= 2) {\n        #pragma omp parallel for\n        for (int i = d; i < n; i++) {\n            prefix[i] += prefix[i - d];\n        }\n    }\n\n    if (rank == 0) {\n        output = prefix;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_prefix(local_size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_size; ++i) {\n        local_prefix[i] = x[i];\n    }\n\n    for (int i = 1; i < size; ++i) {\n        MPI_Send(&x[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n    }\n\n    for (int i = 1; i < size; ++i) {\n        MPI_Recv(&output[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    for (int i = 1; i < size; ++i) {\n        for (int j = i * local_size; j < (i + 1) * local_size; ++j) {\n            local_prefix[j - i * local_size] += local_prefix[j - i * local_size - 1];\n        }\n    }\n\n    for (int i = 1; i < size; ++i) {\n        MPI_Send(&local_prefix[(i + 1) * local_size - 1], 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n    }\n\n    for (int i = 1; i < size; ++i) {\n        MPI_Recv(&output[(i + 1) * local_size - 1], 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        prefix[i] = x[i];\n    }\n\n    for (int d = 1; d < n; d *= 2) {\n        #pragma omp parallel for\n        for (int i = d; i < n; ++i) {\n            prefix[i] += prefix[i - d];\n        }\n    }\n\n    if (rank == 0) {\n        output = prefix;\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> recv_prefix(n);\n            MPI_Recv(&recv_prefix[0], n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; ++j) {\n                output[j] += recv_prefix[j];\n            }\n        }\n    } else {\n        MPI_Send(&prefix[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_output(local_n);\n\n    // Compute local prefix sum using OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < local_n; i++) {\n        local_output[i] = x[i];\n        if (i > 0) {\n            local_output[i] += x[i - 1];\n        }\n    }\n\n    // Gather local results to rank 0\n    MPI_Gather(&local_output[0], local_n, MPI_DOUBLE, &output[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Compute prefix sum on rank 0\n    if (rank == 0) {\n        for (int i = 1; i < x.size(); i++) {\n            output[i] += output[i - 1];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_output(local_size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_size; ++i) {\n        local_output[i] = x[i];\n    }\n\n    for (int i = 1; i < size; ++i) {\n        MPI_Send(&x[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n    }\n\n    for (int i = 1; i < size; ++i) {\n        MPI_Recv(&output[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    for (int i = 1; i < size; ++i) {\n        for (int j = i * local_size; j < x.size(); ++j) {\n            x[j] += x[(i - 1) * local_size + local_size - 1];\n        }\n    }\n\n    if (rank == 0) {\n        output = x;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        prefix[i] = x[i];\n    }\n\n    for (int d = 1; d < n; d *= 2) {\n        if (rank % (2 * d) == 0) {\n            if (rank + d < size) {\n                for (int i = 0; i < n; i++) {\n                    prefix[i] += prefix[i - d];\n                }\n                MPI_Send(&prefix[0], n, MPI_DOUBLE, rank + d, 0, MPI_COMM_WORLD);\n            }\n        } else if (rank % (2 * d) == d) {\n            MPI_Recv(&output[0], n, MPI_DOUBLE, rank - d, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int i = 0; i < n; i++) {\n                prefix[i] = output[i];\n            }\n        }\n    }\n\n    if (rank == 0) {\n        output = prefix;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_output(local_size);\n\n    // Compute local prefix sum using OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < local_size; i++) {\n        local_output[i] = x[i];\n        if (i > 0) {\n            local_output[i] += x[i - 1];\n        }\n    }\n\n    // Gather local results to rank 0\n    MPI_Gather(local_output.data(), local_size, MPI_DOUBLE, output.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Compute prefix sum on rank 0\n    if (rank == 0) {\n        for (int i = 1; i < x.size(); i++) {\n            output[i] += output[i - 1];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_output(local_n);\n\n    // Compute local prefix sum using OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < local_n; ++i) {\n        local_output[i] = x[i];\n        if (i > 0) {\n            local_output[i] += x[i - 1];\n        }\n    }\n\n    // Gather local results to rank 0\n    MPI_Gather(&local_output[0], local_n, MPI_DOUBLE, &output[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Compute prefix sum on rank 0\n    if (rank == 0) {\n        for (int i = 1; i < x.size(); ++i) {\n            output[i] += output[i - 1];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<double> prefix(n, 0.0);\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        for (int j = 0; j < i; j++) {\n            prefix[i] += x[j];\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&prefix[0], n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                output[j] += prefix[j];\n            }\n        }\n    } else {\n        MPI_Send(&prefix[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_output(local_n);\n\n    // Compute local prefix sum using OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < local_n; ++i) {\n        local_output[i] = x[i];\n        if (i > 0) {\n            local_output[i] += x[i - 1];\n        }\n    }\n\n    // Gather local results to rank 0\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE, output.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Compute prefix sum on rank 0\n    if (rank == 0) {\n        for (int i = 1; i < x.size(); ++i) {\n            output[i] += output[i - 1];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_output(local_n);\n\n    // Compute local prefix sum using OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < local_n; i++) {\n        local_output[i] = x[i];\n        if (i > 0) {\n            local_output[i] += x[i - 1];\n        }\n    }\n\n    // Gather local results to rank 0\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE, output.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Compute prefix sum on rank 0\n    if (rank == 0) {\n        for (int i = 1; i < x.size(); i++) {\n            output[i] += output[i - 1];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_output(local_n);\n\n    // Compute local prefix sum using OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < local_n; i++) {\n        if (i > 0) {\n            local_output[i] = local_output[i - 1] + x[rank * local_n + i];\n        } else {\n            local_output[i] = x[rank * local_n + i];\n        }\n    }\n\n    // Gather local results to rank 0\n    MPI_Gather(&local_output[0], local_n, MPI_DOUBLE, &output[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Compute final prefix sum on rank 0\n    if (rank == 0) {\n        for (int i = 1; i < x.size(); i++) {\n            output[i] += output[i - 1];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_sum(local_n, 0.0);\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_n; ++i) {\n        for (int j = 0; j <= i + rank * local_n; ++j) {\n            local_sum[i] += x[j];\n        }\n    }\n\n    std::vector<double> prefix_sums(size, 0.0);\n    MPI_Gather(&local_sum[0], local_n, MPI_DOUBLE, &prefix_sums[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output[0] = prefix_sums[0];\n        for (int i = 1; i < size; ++i) {\n            output[i * local_n] = prefix_sums[i];\n            for (int j = 1; j < local_n; ++j) {\n                output[i * local_n + j] = prefix_sums[i] + local_sum[j - 1];\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_sum(local_n);\n\n    // Compute local prefix sum using OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < local_n; ++i) {\n        local_sum[i] = x[i];\n        if (i > 0) {\n            local_sum[i] += local_sum[i - 1];\n        }\n    }\n\n    // Gather local sums to rank 0\n    MPI_Gather(&local_sum[0], local_n, MPI_DOUBLE, &output[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Compute prefix sum on rank 0\n    if (rank == 0) {\n        for (int i = 1; i < x.size(); ++i) {\n            output[i] += output[i - 1];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_output(local_n);\n\n    // Compute local prefix sum using OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < local_n; ++i) {\n        local_output[i] = x[i];\n        if (i > 0) {\n            local_output[i] += x[i - 1];\n        }\n    }\n\n    // Gather local results to rank 0\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE, output.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Compute prefix sum on rank 0\n    if (rank == 0) {\n        for (int i = 1; i < x.size(); ++i) {\n            output[i] += output[i - 1];\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "56_transform_negate_odds",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(Kokkos::View<int*> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    if (x(i) % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) % 2 == 0) {\n      x(i) /= 2;\n    } else {\n      x(i) = -x(i);\n    }\n  });\n  Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "56_transform_negate_odds",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] % 2 == 0) {\n      x[i] /= 2;\n    } else {\n      x[i] = -x[i];\n    }\n  }\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "56_transform_negate_odds",
    "parallelism_model": "serial",
    "prompt": "/* In the vector x negate the odd values and divide the even values by 2.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (int &num : x) {\n        if (num % 2 == 0) {\n            num /= 2;\n        } else {\n            num = -num;\n        }\n    }\n}",
      "\n    for (int &num : x) {\n        if (num % 2 == 0) {\n            num /= 2;\n        } else {\n            num = -num;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
      "\n    for (int &num : x) {\n        if (num % 2 == 0) {\n            num /= 2;\n        } else {\n            num = -num;\n        }\n    }\n}",
      "\n    for (int &num : x) {\n        if (num % 2 == 0) {\n            num /= 2;\n        } else {\n            num = -num;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
      "\n    for (int &num : x) {\n        if (num % 2 == 0) {\n            num /= 2;\n        } else {\n            num = -num;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
      "\n    for (int &num : x) {\n        if (num % 2 == 0) {\n            num /= 2;\n        } else {\n            num = -num;\n        }\n    }\n}",
      "\n    for (int &num : x) {\n        if (num % 2 == 0) {\n            num /= 2;\n        } else {\n            num = -num;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
      "\n    for (int &num : x) {\n        if (num % 2 == 0) {\n            num /= 2;\n        } else {\n            num = -num;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
      "\n    for (int &num : x) {\n        if (num % 2 == 0) {\n            num /= 2;\n        } else {\n            num = -num;\n        }\n    }\n}",
      "\n    for (int &num : x) {\n        if (num % 2 == 0) {\n            num /= 2;\n        } else {\n            num = -num;\n        }\n    }\n}",
      "\n    for (int &num : x) {\n        if (num % 2 == 0) {\n            num /= 2;\n        } else {\n            num = -num;\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "56_transform_negate_odds",
    "parallelism_model": "hip",
    "prompt": "/* In the vector x negate the odd values and divide the even values by 2.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\n__global__ void negateOddsAndHalveEvens(int *x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "56_transform_negate_odds",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<int> result(n);\n\n    for (int i = rank; i < n; i += size) {\n        if (x[i] % 2 == 0) {\n            result[i] = x[i] / 2;\n        } else {\n            result[i] = -x[i];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<int> temp(n);\n        MPI_Reduce(result.data(), temp.data(), n, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n        x = temp;\n    }\n}",
      "\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(x.data(), x.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int r = 1; r < MPI_Comm_size(MPI_COMM_WORLD, &size); r++) {\n            std::vector<int> temp(x.size());\n            MPI_Recv(temp.data(), x.size(), MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int i = 0; i < x.size(); i++) {\n                x[i] += temp[i];\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    int *localX = new int[localSize];\n    MPI_Scatter(&x[0], localSize, MPI_INT, localX, localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        if (localX[i] % 2 == 0) {\n            localX[i] /= 2;\n        } else {\n            localX[i] = -localX[i];\n        }\n    }\n\n    MPI_Gather(localX, localSize, MPI_INT, &x[0], localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    delete[] localX;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<int> local_x(local_n);\n    std::vector<int> result(n);\n\n    MPI_Scatter(&x[0], local_n, MPI_INT, &local_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        if (local_x[i] % 2 == 0) {\n            local_x[i] /= 2;\n        } else {\n            local_x[i] = -local_x[i];\n        }\n    }\n\n    MPI_Gather(&local_x[0], local_n, MPI_INT, &result[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        x = result;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<int> result(n);\n\n    for (int i = rank; i < n; i += size) {\n        if (x[i] % 2 == 0) {\n            result[i] = x[i] / 2;\n        } else {\n            result[i] = -x[i];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<int> temp(n);\n        MPI_Reduce(result.data(), temp.data(), n, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n        x = temp;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    int *localX = new int[localSize];\n    MPI_Scatter(&x[0], localSize, MPI_INT, localX, localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        if (localX[i] % 2 == 0) {\n            localX[i] /= 2;\n        } else {\n            localX[i] = -localX[i];\n        }\n    }\n\n    MPI_Gather(localX, localSize, MPI_INT, &x[0], localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    delete[] localX;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    int *localX = new int[localSize];\n    MPI_Scatter(&x[0], localSize, MPI_INT, localX, localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        if (localX[i] % 2 == 0) {\n            localX[i] /= 2;\n        } else {\n            localX[i] = -localX[i];\n        }\n    }\n\n    MPI_Gather(localX, localSize, MPI_INT, &x[0], localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    delete[] localX;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<int> result(n);\n\n    for (int i = rank; i < n; i += size) {\n        if (x[i] % 2 == 0) {\n            result[i] = x[i] / 2;\n        } else {\n            result[i] = -x[i];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<int> temp(n);\n        MPI_Reduce(result.data(), temp.data(), n, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n        x = temp;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<int> result(n);\n\n    for (int i = rank; i < n; i += size) {\n        if (x[i] % 2 == 0) {\n            result[i] = x[i] / 2;\n        } else {\n            result[i] = -x[i];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<int> temp(n);\n        MPI_Reduce(result.data(), temp.data(), n, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n        x = temp;\n    } else {\n        MPI_Reduce(result.data(), nullptr, n, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    for (int i = start; i < end; i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<int> recvbuf(x.size());\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&recvbuf[0] + i * local_size, local_size, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        x = recvbuf;\n    } else {\n        MPI_Send(&x[start], local_size, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    int *localX = new int[localSize];\n    MPI_Scatter(&x[0], localSize, MPI_INT, localX, localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        if (localX[i] % 2 == 0) {\n            localX[i] /= 2;\n        } else {\n            localX[i] = -localX[i];\n        }\n    }\n\n    MPI_Gather(localX, localSize, MPI_INT, &x[0], localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    delete[] localX;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<int> result(n);\n\n    for (int i = rank; i < n; i += size) {\n        if (x[i] % 2 == 0) {\n            result[i] = x[i] / 2;\n        } else {\n            result[i] = -x[i];\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(result.data(), n, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(result.data(), n, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                x[j] = result[j];\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<int> result(n);\n\n    for (int i = rank; i < n; i += size) {\n        if (x[i] % 2 == 0) {\n            result[i] = x[i] / 2;\n        } else {\n            result[i] = -x[i];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<int> temp(n);\n        MPI_Reduce(result.data(), temp.data(), n, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n        x = temp;\n    } else {\n        MPI_Reduce(result.data(), nullptr, n, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(x.data(), x.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < MPI_COMM_WORLD; i++) {\n            std::vector<int> temp(x.size());\n            MPI_Recv(temp.data(), x.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                x[j] += temp[j];\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<int> result(n);\n\n    for (int i = rank; i < n; i += size) {\n        if (x[i] % 2 == 0) {\n            result[i] = x[i] / 2;\n        } else {\n            result[i] = -x[i];\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(result.data(), n, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(result.data(), n, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                x[j] = result[j];\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    for (int i = start; i < end; i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * local_n], local_n, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&x[start], local_n, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<int> result(n);\n\n    for (int i = rank; i < n; i += size) {\n        if (x[i] % 2 == 0) {\n            result[i] = x[i] / 2;\n        } else {\n            result[i] = -x[i];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<int> temp(n);\n        MPI_Reduce(result.data(), temp.data(), n, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n        x = temp;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    int start = rank * localSize;\n    int end = start + localSize;\n\n    for (int i = start; i < end; i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&x[r * localSize], localSize, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&x[start], localSize, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(x.data(), x.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int r = 1; r < size; r++) {\n            std::vector<int> temp(x.size());\n            MPI_Recv(temp.data(), x.size(), MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int i = 0; i < x.size(); i++) {\n                x[i] += temp[i];\n            }\n        }\n    }\n}",
      "\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(x.data(), x.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int r = 1; r < MPI_COMM_WORLD.size(); r++) {\n            std::vector<int> recv_data(x.size());\n            MPI_Recv(recv_data.data(), x.size(), MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int i = 0; i < x.size(); i++) {\n                x[i] += recv_data[i];\n            }\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "56_transform_negate_odds",
    "parallelism_model": "cuda",
    "prompt": "/* In the vector x negate the odd values and divide the even values by 2.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\n__global__ void negateOddsAndHalveEvens(int *x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] % 2 == 0) {\n            x[idx] /= 2;\n        } else {\n            x[idx] = -x[idx];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] % 2 == 0) {\n            x[idx] /= 2;\n        } else {\n            x[idx] = -x[idx];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        if (x[index] % 2 == 0) {\n            x[index] /= 2;\n        } else {\n            x[index] = -x[index];\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] % 2 == 0) {\n            x[idx] /= 2;\n        } else {\n            x[idx] = -x[idx];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] % 2 == 0) {\n            x[idx] /= 2;\n        } else {\n            x[idx] = -x[idx];\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] % 2 == 0) {\n            x[idx] /= 2;\n        } else {\n            x[idx] = -x[idx];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            x[tid] /= 2;\n        } else {\n            x[tid] = -x[tid];\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "56_transform_negate_odds",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, NULL, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    std::vector<int> localResult = x;\n    std::vector<int> globalResult(x.size());\n\n    MPI_Gather(&localResult[0], x.size(), MPI_INT, &globalResult[0], x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        x = globalResult;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (rank != 0) {\n        MPI_Send(x.data(), x.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<int> temp(x.size());\n        for (int i = 1; i < MPI_COMM_WORLD; i++) {\n            MPI_Recv(temp.data(), x.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                x[j] += temp[j];\n            }\n        }\n    }\n```\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (rank != 0) {\n        MPI_Send(&x[0], x.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < MPI_COMM_WORLD.size(); i++) {\n            std::vector<int> temp(x.size());\n            MPI_Recv(&temp[0], x.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                x[j] += temp[j];\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n    std::vector<int> localResult = x;\n    std::vector<int> globalResult(x.size());\n    MPI_Gather(&localResult[0], x.size(), MPI_INT, &globalResult[0], x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        x = globalResult;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, NULL, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n    std::vector<int> localResult = x;\n    std::vector<int> globalResult(x.size());\n    MPI_Gather(&localResult[0], x.size(), MPI_INT, &globalResult[0], x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        x = globalResult;\n    }\n```\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n    std::vector<int> localResult = x;\n    std::vector<int> globalResult(x.size());\n    MPI_Gather(&localResult[0], x.size(), MPI_INT, &globalResult[0], x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        x = globalResult;\n    }\n```\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    std::vector<int> localResult = x;\n    std::vector<int> globalResult(x.size());\n\n    MPI_Gather(&localResult[0], x.size(), MPI_INT, &globalResult[0], x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        x = globalResult;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, NULL, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n```\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "59_transform_map_function",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(Kokkos::View<const int*> const& x, Kokkos::View<bool*> &mask) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    mask(i) = (x(i) > 0) && !(x(i) & (x(i) - 1));\n  });\n  Kokkos::fence();\n}",
      "\nKokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    mask(i) = isPowerOfTwo(x(i));\n});\nKokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    mask(i) = (x(i) > 0) && !(x(i) & (x(i) - 1));\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}",
      "\nKokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    mask(i) = isPowerOfTwo(x(i));\n});\nKokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n      mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}",
      "\nKokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    mask(i) = isPowerOfTwo(x(i));\n});\nKokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    mask(i) = (x(i) > 0) && !(x(i) & (x(i) - 1));\n  });\n  Kokkos::fence();\n}",
      "\nKokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    mask(i) = isPowerOfTwo(x(i));\n});\nKokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    mask(i) = (x(i) > 0) && !(x(i) & (x(i) - 1));\n  });\n  Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "59_transform_map_function",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}",
      "\n#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
      "\n#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
      "\n#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
      "\n#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}",
      "\n#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
      "\n#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
      "\n#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); ++i) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
      "\n#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
      "\n#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}",
      "\n#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
      "\n#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "59_transform_map_function",
    "parallelism_model": "serial",
    "prompt": "bool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\nfor (int i = 0; i < x.size(); i++) {\n    mask[i] = isPowerOfTwo(x[i]);\n}\n}",
      "\nfor (int i = 0; i < x.size(); i++) {\n    mask.push_back(isPowerOfTwo(x[i]));\n}\n}",
      "\nfor (int i = 0; i < x.size(); ++i) {\n    mask[i] = isPowerOfTwo(x[i]);\n}\n}",
      "\nfor (int i = 0; i < x.size(); i++) {\n    mask.push_back(isPowerOfTwo(x[i]));\n}\n}",
      "\nfor (int i = 0; i < x.size(); i++) {\n    mask.push_back(isPowerOfTwo(x[i]));\n}\n}",
      "\nfor (int i = 0; i < x.size(); i++) {\n    mask.push_back(isPowerOfTwo(x[i]));\n}\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        mask.push_back(isPowerOfTwo(x[i]));\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}",
      "\nfor (int i = 0; i < x.size(); i++) {\n    mask.push_back(isPowerOfTwo(x[i]));\n}\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        mask.push_back(isPowerOfTwo(x[i]));\n    }\n}",
      "\nfor (int i = 0; i < x.size(); i++) {\n    mask.push_back(isPowerOfTwo(x[i]));\n}\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        mask.push_back(isPowerOfTwo(x[i]));\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        mask.push_back(isPowerOfTwo(x[i]));\n    }\n}",
      "\nfor (int i = 0; i < x.size(); i++) {\n    mask.push_back(isPowerOfTwo(x[i]));\n}\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        mask.push_back(isPowerOfTwo(x[i]));\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "59_transform_map_function",
    "parallelism_model": "hip",
    "prompt": "__device__ bool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\n__global__ void mapPowersOfTwo(const int *x, bool *mask, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "59_transform_map_function",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored in mask on rank 0.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    std::vector<bool> localMask(localSize);\n\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(localX[i]);\n    }\n\n    MPI_Gather(localMask.data(), localSize, MPI_C_BOOL, mask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    std::vector<bool> localMask(localSize);\n\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(localX[i]);\n    }\n\n    MPI_Gather(localMask.data(), localSize, MPI_C_BOOL, mask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    std::vector<bool> localMask(localSize);\n\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(localX[i]);\n    }\n\n    MPI_Gather(localMask.data(), localSize, MPI_C_BOOL, mask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    std::vector<bool> localMask(localSize);\n\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(localX[i]);\n    }\n\n    MPI_Gather(localMask.data(), localSize, MPI_C_BOOL, mask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    std::vector<bool> localMask(localSize);\n\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(localX[i]);\n    }\n\n    MPI_Gather(localMask.data(), localSize, MPI_C_BOOL, mask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    std::vector<bool> localMask(localSize);\n\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(localX[i]);\n    }\n\n    MPI_Gather(localMask.data(), localSize, MPI_C_BOOL, mask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<bool> localMask(localSize);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n    }\n\n    if (rank == 0) {\n        mask.resize(x.size());\n    }\n\n    MPI_Gather(localMask.data(), localSize, MPI_CXX_BOOL, mask.data(), localSize, MPI_CXX_BOOL, 0, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    std::vector<bool> localMask(localSize);\n\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(localX[i]);\n    }\n\n    MPI_Gather(localMask.data(), localSize, MPI_C_BOOL, mask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<bool> localMask(localSize);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n    }\n\n    if (rank == 0) {\n        mask.resize(x.size());\n    }\n\n    MPI_Gather(&localMask[0], localSize, MPI_C_BOOL, &mask[0], localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    std::vector<bool> localMask(localSize);\n\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(localX[i]);\n    }\n\n    MPI_Gather(localMask.data(), localSize, MPI_C_BOOL, mask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<bool> localMask(localSize);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n    }\n\n    MPI_Gather(&localMask[0], localSize, MPI_C_BOOL, &mask[0], localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    std::vector<bool> localMask(localSize);\n\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(localX[i]);\n    }\n\n    MPI_Gather(localMask.data(), localSize, MPI_C_BOOL, mask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    std::vector<bool> localMask(localSize);\n\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(localX[i]);\n    }\n\n    MPI_Gather(localMask.data(), localSize, MPI_C_BOOL, mask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    std::vector<bool> localMask(localSize);\n\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(localX[i]);\n    }\n\n    MPI_Gather(localMask.data(), localSize, MPI_C_BOOL, mask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    std::vector<bool> localMask(localSize);\n\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(localX[i]);\n    }\n\n    MPI_Gather(localMask.data(), localSize, MPI_C_BOOL, mask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    std::vector<bool> localMask(localSize);\n\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(localX[i]);\n    }\n\n    MPI_Gather(localMask.data(), localSize, MPI_C_BOOL, mask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    std::vector<bool> localMask(localSize);\n\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(localX[i]);\n    }\n\n    MPI_Gather(localMask.data(), localSize, MPI_C_BOOL, mask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    std::vector<bool> localMask(localSize);\n\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; ++i) {\n        localMask[i] = isPowerOfTwo(localX[i]);\n    }\n\n    MPI_Gather(localMask.data(), localSize, MPI_C_BOOL, mask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    std::vector<bool> localMask(localSize);\n\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(localX[i]);\n    }\n\n    MPI_Gather(localMask.data(), localSize, MPI_C_BOOL, mask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    std::vector<bool> localMask(localSize);\n\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(localX[i]);\n    }\n\n    MPI_Gather(localMask.data(), localSize, MPI_C_BOOL, mask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "59_transform_map_function",
    "parallelism_model": "cuda",
    "prompt": "__device__ bool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\n__global__ void mapPowersOfTwo(const int *x, bool *mask, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        mask[index] = isPowerOfTwo(x[index]);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "59_transform_map_function",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored in mask on rank 0.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n\n    if (rank == 0) {\n        std::vector<bool> tempMask(x.size());\n        MPI_Gather(mask.data(), x.size(), MPI_CXX_BOOL, tempMask.data(), x.size(), MPI_CXX_BOOL, 0, MPI_COMM_WORLD);\n        mask = tempMask;\n    } else {\n        MPI_Gather(mask.data(), x.size(), MPI_CXX_BOOL, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n\n    if (rank == 0) {\n        std::vector<bool> tempMask(x.size());\n        MPI_Gather(mask.data(), x.size(), MPI_CXX_BOOL, tempMask.data(), x.size(), MPI_CXX_BOOL, 0, MPI_COMM_WORLD);\n        mask = tempMask;\n    } else {\n        MPI_Gather(mask.data(), x.size(), MPI_CXX_BOOL, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n\n    if (rank == 0) {\n        std::vector<bool> globalMask(x.size());\n        MPI_Gather(mask.data(), x.size(), MPI_CXX_BOOL, globalMask.data(), x.size(), MPI_CXX_BOOL, 0, MPI_COMM_WORLD);\n        mask = globalMask;\n    } else {\n        MPI_Gather(mask.data(), x.size(), MPI_CXX_BOOL, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n\n    if (rank == 0) {\n        std::vector<bool> tempMask(x.size());\n        MPI_Gather(mask.data(), x.size(), MPI_CXX_BOOL, tempMask.data(), x.size(), MPI_CXX_BOOL, 0, MPI_COMM_WORLD);\n        mask = tempMask;\n    } else {\n        MPI_Gather(mask.data(), x.size(), MPI_CXX_BOOL, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n\n    if (rank == 0) {\n        std::vector<bool> result(mask.size());\n        MPI_Gather(mask.data(), mask.size(), MPI_CXX_BOOL, result.data(), mask.size(), MPI_CXX_BOOL, 0, MPI_COMM_WORLD);\n        mask = result;\n    } else {\n        MPI_Gather(mask.data(), mask.size(), MPI_CXX_BOOL, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n\n    if (rank == 0) {\n        std::vector<bool> tempMask(x.size());\n        MPI_Gather(mask.data(), x.size(), MPI_CXX_BOOL, tempMask.data(), x.size(), MPI_CXX_BOOL, 0, MPI_COMM_WORLD);\n        mask = tempMask;\n    } else {\n        MPI_Gather(mask.data(), x.size(), MPI_CXX_BOOL, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n\n    if (rank == 0) {\n        std::vector<bool> tempMask(x.size());\n        MPI_Gather(mask.data(), x.size(), MPI_CXX_BOOL, tempMask.data(), x.size(), MPI_CXX_BOOL, 0, MPI_COMM_WORLD);\n        mask = tempMask;\n    } else {\n        MPI_Gather(mask.data(), x.size(), MPI_CXX_BOOL, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n\n    if (rank == 0) {\n        std::vector<bool> tempMask(x.size());\n        MPI_Gather(&mask[0], x.size(), MPI_CXX_BOOL, &tempMask[0], x.size(), MPI_CXX_BOOL, 0, MPI_COMM_WORLD);\n        mask = tempMask;\n    } else {\n        MPI_Gather(&mask[0], x.size(), MPI_CXX_BOOL, NULL, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n\n    if (rank == 0) {\n        std::vector<bool> tempMask(x.size());\n        MPI_Gather(mask.data(), x.size(), MPI_CXX_BOOL, tempMask.data(), x.size(), MPI_CXX_BOOL, 0, MPI_COMM_WORLD);\n        mask = tempMask;\n    } else {\n        MPI_Gather(mask.data(), x.size(), MPI_CXX_BOOL, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n\n    if (rank == 0) {\n        std::vector<bool> tempMask(x.size());\n        MPI_Gather(mask.data(), x.size(), MPI_CXX_BOOL, tempMask.data(), x.size(), MPI_CXX_BOOL, 0, MPI_COMM_WORLD);\n        mask = tempMask;\n    } else {\n        MPI_Gather(mask.data(), x.size(), MPI_CXX_BOOL, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n\n    if (rank == 0) {\n        std::vector<bool> tempMask(x.size());\n        MPI_Gather(mask.data(), x.size(), MPI_CXX_BOOL, tempMask.data(), x.size(), MPI_CXX_BOOL, 0, MPI_COMM_WORLD);\n        mask = tempMask;\n    } else {\n        MPI_Gather(mask.data(), x.size(), MPI_CXX_BOOL, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n\n    if (rank == 0) {\n        std::vector<bool> tempMask(mask.size());\n        MPI_Gather(&mask[0], mask.size(), MPI_CXX_BOOL, &tempMask[0], mask.size(), MPI_CXX_BOOL, 0, MPI_COMM_WORLD);\n        mask = tempMask;\n    } else {\n        MPI_Gather(&mask[0], mask.size(), MPI_CXX_BOOL, NULL, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n\n    if (rank == 0) {\n        std::vector<bool> tempMask(x.size());\n        MPI_Gather(mask.data(), x.size(), MPI_CXX_BOOL, tempMask.data(), x.size(), MPI_CXX_BOOL, 0, MPI_COMM_WORLD);\n        mask = tempMask;\n    } else {\n        MPI_Gather(mask.data(), x.size(), MPI_CXX_BOOL, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n\n    if (rank == 0) {\n        std::vector<bool> tempMask(x.size());\n        MPI_Gather(mask.data(), x.size(), MPI_CXX_BOOL, tempMask.data(), x.size(), MPI_CXX_BOOL, 0, MPI_COMM_WORLD);\n        mask = tempMask;\n    } else {\n        MPI_Gather(mask.data(), x.size(), MPI_CXX_BOOL, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n\n    if (rank == 0) {\n        std::vector<bool> tempMask(mask.size());\n        MPI_Gather(mask.data(), mask.size(), MPI_CXX_BOOL, tempMask.data(), mask.size(), MPI_CXX_BOOL, 0, MPI_COMM_WORLD);\n        mask = tempMask;\n    } else {\n        MPI_Gather(mask.data(), mask.size(), MPI_CXX_BOOL, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n\n    if (rank == 0) {\n        std::vector<bool> tempMask(x.size());\n        MPI_Gather(&mask[0], x.size(), MPI_CXX_BOOL, &tempMask[0], x.size(), MPI_CXX_BOOL, 0, MPI_COMM_WORLD);\n        mask = tempMask;\n    } else {\n        MPI_Gather(&mask[0], x.size(), MPI_CXX_BOOL, NULL, 0, MPI_CXX_BOOL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        mask[i] = isPowerOfTwo(x[i]);\n    }\n\n    if (rank == 0) {\n        std::vector<bool> localMask = mask;\n        for (int i = 1; i < size; i++) {\n            std::vector<bool> recvMask(x.size());\n            MPI_Recv(recvMask.data(), x.size(), MPI_CXX_BOOL, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                localMask[j] = localMask[j] || recvMask[j];\n            }\n        }\n        mask = localMask;\n    } else {\n        MPI_Send(mask.data(), x.size(), MPI_CXX_BOOL, 0, 0, MPI_COMM_WORLD);\n    }\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "57_transform_inverse_offset",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(Kokkos::View<double*> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = 1 - 1 / x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = 1 - 1 / x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = 1 - 1 / x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = 1 - 1 / x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = 1 - 1 / x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = 1 - 1 / x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = 1 - 1 / x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = 1 - 1 / x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = 1 - 1 / x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = 1 - 1 / x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = 1 - 1 / x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    x(i) = 1 - 1 / x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = 1 - 1 / x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = 1 - 1 / x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = 1 - 1 / x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = 1 - 1 / x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    x(i) = 1 - 1 / x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = 1 - 1 / x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = 1 - 1 / x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = 1 - 1 / x(i);\n  });\n  Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "57_transform_inverse_offset",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      x[i] = 1 - 1 / x[i];\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      x[i] = 1 - 1 / x[i];\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      x[i] = 1 - 1 / x[i];\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      x[i] = 1 - 1 / x[i];\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      x[i] = 1 - 1 / x[i];\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      x[i] = 1 - 1 / x[i];\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      x[i] = 1 - 1 / x[i];\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      x[i] = 1 - 1 / x[i];\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      x[i] = 1 - 1 / x[i];\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1 - 1 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      x[i] = 1 - 1 / x[i];\n    }\n  }\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "57_transform_inverse_offset",
    "parallelism_model": "serial",
    "prompt": "/* Replace every element of the vector x with 1-1/x.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "57_transform_inverse_offset",
    "parallelism_model": "hip",
    "prompt": "/* Replace every element of the vector x with 1-1/x.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\n__global__ void oneMinusInverse(double *x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = 1 - 1 / x[tid];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = 1 - 1 / x[tid];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = 1 - 1 / x[tid];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = 1 - 1 / x[tid];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = 1 - 1 / x[tid];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = 1 - 1 / x[tid];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = 1 - 1 / x[tid];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = 1 - 1 / x[tid];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = 1 - 1 / x[tid];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = 1 - 1 / x[tid];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = 1 - 1 / x[tid];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = 1 - 1 / x[tid];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = 1 - 1 / x[tid];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = 1 - 1 / x[tid];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = 1 - 1 / x[tid];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = 1 - 1 / x[tid];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = 1 - 1 / x[tid];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = 1 - 1 / x[tid];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = 1 - 1 / x[tid];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = 1 - 1 / x[tid];\n    }\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "57_transform_inverse_offset",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<double> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_DOUBLE, localX.data(), localSize, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = 1 - 1 / localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_DOUBLE, x.data(), localSize, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_x(local_n);\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE, &local_x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = 1 - 1 / local_x[i];\n    }\n\n    MPI_Gather(&local_x[0], local_n, MPI_DOUBLE, &x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = 1 - 1 / local_x[i];\n    }\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = 1 - 1 / local_x[i];\n    }\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = 1 - 1 / local_x[i];\n    }\n\n    std::vector<double> result(x.size());\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, result.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        x = result;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_x(local_n);\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE, &local_x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = 1 - 1 / local_x[i];\n    }\n\n    MPI_Gather(&local_x[0], local_n, MPI_DOUBLE, &x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    if (rank == 0 && x.size() % size != 0) {\n        for (int i = local_n * size; i < x.size(); i++) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        local_x[i] = 1 - 1 / local_x[i];\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        if (local_x[i] != 0) {\n            local_x[i] = 1 - 1 / local_x[i];\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = 1 - 1 / local_x[i];\n    }\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_x(local_n);\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE, &local_x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = 1 - 1 / local_x[i];\n    }\n\n    MPI_Gather(&local_x[0], local_n, MPI_DOUBLE, &x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    if (rank == 0 && x.size() % size != 0) {\n        for (int i = local_n * size; i < x.size(); i++) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = 1 - 1 / local_x[i];\n    }\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<double> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_DOUBLE, localX.data(), localSize, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = 1 - 1 / localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_DOUBLE, x.data(), localSize, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = 1 - 1 / local_x[i];\n    }\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = 1 - 1 / local_x[i];\n    }\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_x(local_n);\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE, &local_x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        if (local_x[i] != 0) {\n            local_x[i] = 1 - 1 / local_x[i];\n        }\n    }\n\n    MPI_Gather(&local_x[0], local_n, MPI_DOUBLE, &x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_x(local_n);\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE, &local_x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = 1 - 1 / local_x[i];\n    }\n\n    MPI_Gather(&local_x[0], local_n, MPI_DOUBLE, &x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_x(local_n);\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE, &local_x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        if (local_x[i] != 0) {\n            local_x[i] = 1 - 1 / local_x[i];\n        }\n    }\n\n    MPI_Gather(&local_x[0], local_n, MPI_DOUBLE, &x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = 1 - 1 / local_x[i];\n    }\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        if (local_x[i] != 0) {\n            local_x[i] = 1 - 1 / local_x[i];\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_x(local_n);\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE, &local_x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = 1 - 1 / local_x[i];\n    }\n\n    std::vector<double> result(x.size());\n    MPI_Gather(&local_x[0], local_n, MPI_DOUBLE, &result[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        x = result;\n    }\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "57_transform_inverse_offset",
    "parallelism_model": "cuda",
    "prompt": "/* Replace every element of the vector x with 1-1/x.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\n__global__ void oneMinusInverse(double *x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] != 0) {\n            x[idx] = 1 - 1 / x[idx];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] != 0) {\n            x[tid] = 1 - 1 / x[tid];\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] != 0) {\n            x[tid] = 1 - 1 / x[tid];\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] != 0) {\n            x[tid] = 1 - 1 / x[tid];\n        } else {\n            // handle division by zero\n        }\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] != 0) {\n            x[idx] = 1 - 1 / x[idx];\n        } else {\n            // Handle division by zero\n        }\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] != 0) {\n            x[idx] = 1 - 1 / x[idx];\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] != 0) {\n            x[tid] = 1 - 1 / x[tid];\n        } else {\n            // handle division by zero\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] != 0) {\n            x[tid] = 1 - 1 / x[tid];\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] != 0) {\n            x[tid] = 1 - 1 / x[tid];\n        } else {\n            // Handle division by zero\n            // For example, set x[tid] to a specific value or handle it in a way appropriate for the application\n        }\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] != 0) {\n            x[idx] = 1 - 1 / x[idx];\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] != 0) {\n            x[tid] = 1 - 1 / x[tid];\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = 1 - 1 / x[tid];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] != 0) {\n            x[tid] = 1 - 1 / x[tid];\n        } else {\n            // Handle division by zero\n            // For this example, we'll just set the result to 0\n            x[tid] = 0;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] != 0) {\n            x[tid] = 1 - 1 / x[tid];\n        } else {\n            // Handle division by zero\n            // For example, set x[tid] to a specific value or handle it in a different way based on the requirements\n        }\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] != 0) {\n            x[idx] = 1 - 1 / x[idx];\n        } else {\n            // Handle division by zero\n            // For this example, we'll just set the result to 0\n            x[idx] = 0;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] != 0) {\n            x[tid] = 1 - 1 / x[tid];\n        } else {\n            // Handle division by zero\n            // For example, set x[tid] to a specific value or handle it in a way appropriate for the application\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = 1 - 1 / x[tid];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] != 0) {\n            x[tid] = 1 - 1 / x[tid];\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] != 0) {\n            x[tid] = 1 - 1 / x[tid];\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] != 0) {\n            x[tid] = 1 - 1 / x[tid];\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "57_transform_inverse_offset",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, &result[0], x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, NULL, 0, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE, result.data(), x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, &result[0], x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, NULL, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, &result[0], x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, NULL, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, &result[0], x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, NULL, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, &result[0], x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, NULL, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, &result[0], x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, NULL, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, &result[0], x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, NULL, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, &result[0], x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, NULL, 0, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE, result.data(), x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, &result[0], x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, NULL, 0, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, &result[0], x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, NULL, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, &result[0], x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, NULL, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE, result.data(), x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, &result[0], x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, NULL, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, &result[0], x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, NULL, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE, result.data(), x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE, result.data(), x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE, result.data(), x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = 1 - 1 / x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, &result[0], x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, NULL, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "55_transform_relu",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(Kokkos::View<double*> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) < 0) {\n      x(i) = 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n      if (x(i) < 0) {\n          x(i) = 0;\n      }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) < 0) {\n      x(i) = 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n      if (x(i) < 0) {\n          x(i) = 0;\n      }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) < 0) {\n      x(i) = 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) < 0) {\n      x(i) = 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    if (x(i) < 0) {\n      x(i) = 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    if (x(i) < 0) {\n      x(i) = 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    if (x(i) < 0) {\n      x(i) = 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) < 0) {\n      x(i) = 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    if (x(i) < 0) {\n      x(i) = 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) < 0.0) {\n      x(i) = 0.0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    if (x(i) < 0) {\n      x(i) = 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n      if (x(i) < 0) {\n          x(i) = 0;\n      }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n      if (x(i) < 0) {\n          x(i) = 0;\n      }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n      if (x(i) < 0) {\n          x(i) = 0;\n      }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n      if (x(i) < 0) {\n          x(i) = 0;\n      }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n      if (x(i) < 0) {\n          x(i) = 0;\n      }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    if (x(i) < 0) {\n      x(i) = 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    if (x(i) < 0) {\n      x(i) = 0;\n    }\n  });\n  Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "55_transform_relu",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "55_transform_relu",
    "parallelism_model": "serial",
    "prompt": "/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "55_transform_relu",
    "parallelism_model": "hip",
    "prompt": "/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\n__global__ void relu(double *x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}\n",
      "\n    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}\n",
      "\n    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}\n",
      "\n    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        if (x[index] < 0) {\n            x[index] = 0;\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}\n",
      "\n    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}\n",
      "\n    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}\n",
      "\n    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        if (x[index] < 0) {\n            x[index] = 0;\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "55_transform_relu",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? x.size() : start + local_size;\n\n    for (int i = start; i < end; i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<double> temp(x.size());\n        std::copy(x.begin(), x.begin() + local_size, temp.begin());\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&temp[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        x = temp;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? x.size() : start + local_size;\n\n    for (int i = start; i < end; i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<double> temp(x.size());\n        std::copy(x.begin(), x.begin() + local_size, temp.begin());\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&temp[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        x = temp;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    if (rank == size - 1) {\n        end = x.size();\n    }\n\n    for (int i = start; i < end; i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<double> temp(local_size);\n        for (int i = 0; i < local_size; i++) {\n            if (x[i] < 0) {\n                temp[i] = 0;\n            } else {\n                temp[i] = x[i];\n            }\n        }\n\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&x[r * local_size], local_size, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n\n        for (int i = 0; i < local_size; i++) {\n            x[i] = temp[i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? x.size() : start + local_size;\n\n    for (int i = start; i < end; i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<double> temp(x.size());\n        std::copy(x.begin(), x.begin() + local_size, temp.begin());\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&temp[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        x = temp;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? x.size() : start + local_size;\n\n    for (int i = start; i < end; i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<double> temp(local_size);\n        for (int i = 0; i < local_size; i++) {\n            if (x[i] < 0) {\n                temp[i] = 0;\n            } else {\n                temp[i] = x[i];\n            }\n        }\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&x[r * local_size], local_size, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        x = temp;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? x.size() : start + local_size;\n\n    for (int i = start; i < end; i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<double> temp(local_size);\n        for (int i = 0; i < local_size; i++) {\n            if (x[i] < 0) {\n                temp[i] = 0;\n            } else {\n                temp[i] = x[i];\n            }\n        }\n\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    if (rank == size - 1) {\n        end = x.size();\n    }\n\n    for (int i = start; i < end; i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? x.size() : start + local_size;\n\n    for (int i = start; i < end; i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<double> temp(x.size());\n        std::copy(x.begin(), x.begin() + local_size, temp.begin());\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&temp[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        x = temp;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; ++i) {\n        local_x[i] = std::max(0.0, local_x[i]);\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? x.size() : start + local_size;\n\n    for (int i = start; i < end; i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<double> temp(local_size);\n        for (int i = 0; i < local_size; i++) {\n            if (x[i] < 0) {\n                temp[i] = 0;\n            } else {\n                temp[i] = x[i];\n            }\n        }\n\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n\n        x = temp;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    if (rank == size - 1) {\n        end = x.size();\n    }\n\n    for (int i = start; i < end; i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<double> temp(local_size);\n        for (int i = 0; i < local_size; i++) {\n            if (x[i] < 0) {\n                temp[i] = 0;\n            } else {\n                temp[i] = x[i];\n            }\n        }\n\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n\n        for (int i = 0; i < local_size; i++) {\n            x[i] = temp[i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; ++i) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; ++i) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; ++i) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; ++i) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "55_transform_relu",
    "parallelism_model": "cuda",
    "prompt": "/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\n__global__ void relu(double *x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        if (x[index] < 0) {\n            x[index] = 0;\n        }\n    }\n}",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0) {\n            x[idx] = 0;\n        }\n    }\n}",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n```\n}",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        if (x[index] < 0) {\n            x[index] = 0;\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        if (x[index] < 0) {\n            x[index] = 0;\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "55_transform_relu",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    int local_start = rank * local_size;\n    int local_end = (rank == size - 1) ? x.size() : (rank + 1) * local_size;\n\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; ++i) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[local_start], local_end - local_start, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<double> temp(local_end - local_start);\n        for (int i = local_start; i < local_end; ++i) {\n            temp[i - local_start] = x[i];\n        }\n        for (int r = 1; r < size; ++r) {\n            MPI_Recv(&x[0] + r * local_size, local_size, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE, result.data(), x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE, NULL, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, &result[0], x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(&x[0], x.size(), MPI_DOUBLE, NULL, 0, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? x.size() : start + local_size;\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<double> temp(local_size);\n        for (int i = 0; i < local_size; i++) {\n            if (x[i] < 0) {\n                temp[i] = 0;\n            } else {\n                temp[i] = x[i];\n            }\n        }\n\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&x[r * local_size], local_size, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n\n        for (int i = 0; i < local_size; i++) {\n            x[i] = temp[i];\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    if (rank == 0) {\n        int remaining = x.size() % size;\n        for (int i = size * local_size; i < x.size(); i++) {\n            if (x[i] < 0) {\n                x[i] = 0;\n            }\n        }\n        std::vector<double> recv_buffer(x.size());\n        MPI_Gather(x.data(), local_size, MPI_DOUBLE, recv_buffer.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = recv_buffer;\n    } else {\n        MPI_Gather(x.data() + start, local_size, MPI_DOUBLE, nullptr, 0, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    if (rank == 0) {\n        int remaining = x.size() % size;\n        for (int i = size * local_size; i < size * local_size + remaining; i++) {\n            if (x[i] < 0) {\n                x[i] = 0;\n            }\n        }\n        for (int r = 1; r < size; r++) {\n            std::vector<double> recv(local_size);\n            MPI_Recv(&recv[0], local_size, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            x.insert(x.end(), recv.begin(), recv.end());\n        }\n    } else {\n        MPI_Send(&x[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    int local_start = rank * local_size;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_size;\n\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[local_start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<double> temp(local_size);\n        for (int i = 0; i < local_size; i++) {\n            if (x[i] < 0) {\n                temp[i] = 0;\n            } else {\n                temp[i] = x[i];\n            }\n        }\n\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&x[r * local_size], local_size, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n\n        x = temp;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    int local_start = rank * local_size;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_size;\n\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[local_start], local_end - local_start, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<double> temp(local_size);\n        for (int i = 0; i < local_size; i++) {\n            if (x[i] < 0) {\n                temp[i] = 0;\n            } else {\n                temp[i] = x[i];\n            }\n        }\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&x[r * local_size], local_size, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        x = temp;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE, result.data(), x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE, result.data(), x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE, result.data(), x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "58_transform_squaring",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Replace every element of x with the square of its value.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(Kokkos::View<int*> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    x(i) = x(i) * x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = x(i) * x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = x(i) * x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = x(i) * x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = x(i) * x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = x(i) * x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = x(i) * x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = x(i) * x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = x(i) * x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = x(i) * x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = x(i) * x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = x(i) * x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = x(i) * x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = x(i) * x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = x(i) * x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = x(i) * x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = x(i) * x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = x(i) * x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = x(i) * x(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = x(i) * x(i);\n  });\n  Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "58_transform_squaring",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "58_transform_squaring",
    "parallelism_model": "serial",
    "prompt": "/* Replace every element of x with the square of its value.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  for (int i = 0; i < x.size(); i++) {\n      x[i] = x[i] * x[i];\n  }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n  }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n  for (int i = 0; i < x.size(); i++) {\n      x[i] = x[i] * x[i];\n  }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n  for (int i = 0; i < x.size(); i++) {\n      x[i] = x[i] * x[i];\n  }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n  for (int i = 0; i < x.size(); i++) {\n      x[i] = x[i] * x[i];\n  }\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "58_transform_squaring",
    "parallelism_model": "hip",
    "prompt": "/* Replace every element of x with the square of its value.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\n__global__ void squareEach(int *x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        x[tid] = x[tid] * x[tid];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        x[index] = x[index] * x[index];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "58_transform_squaring",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Replace every element of x with the square of its value.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    int *localX = new int[localSize];\n    MPI_Scatter(&x[0], localSize, MPI_INT, localX, localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX, localSize, MPI_INT, &x[0], localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    delete[] localX;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    int *localX = new int[localSize];\n    MPI_Scatter(&x[0], localSize, MPI_INT, localX, localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX, localSize, MPI_INT, &x[0], localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    delete[] localX;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    int *localX = new int[localSize];\n    MPI_Scatter(&x[0], localSize, MPI_INT, localX, localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX, localSize, MPI_INT, &x[0], localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    delete[] localX;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    int *localX = new int[localSize];\n    MPI_Scatter(&x[0], localSize, MPI_INT, localX, localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX, localSize, MPI_INT, &x[0], localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    delete[] localX;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] *= localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] *= localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    int *localX = new int[localSize];\n    MPI_Scatter(&x[0], localSize, MPI_INT, localX, localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX, localSize, MPI_INT, &x[0], localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    delete[] localX;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    int *localX = new int[localSize];\n    MPI_Scatter(&x[0], localSize, MPI_INT, localX, localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] *= localX[i];\n    }\n\n    MPI_Gather(localX, localSize, MPI_INT, &x[0], localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    delete[] localX;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    int *localX = new int[localSize];\n    MPI_Scatter(&x[0], localSize, MPI_INT, localX, localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX, localSize, MPI_INT, &x[0], localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    delete[] localX;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    int *localX = new int[localSize];\n    MPI_Scatter(&x[0], localSize, MPI_INT, localX, localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] *= localX[i];\n    }\n\n    MPI_Gather(localX, localSize, MPI_INT, &x[0], localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    delete[] localX;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] *= localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[0], x.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < MPI_COMM_WORLD; i++) {\n            std::vector<int> temp(x.size());\n            MPI_Recv(&temp[0], x.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                x[j] += temp[j];\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] *= localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] *= localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "58_transform_squaring",
    "parallelism_model": "cuda",
    "prompt": "/* Replace every element of x with the square of its value.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\n__global__ void squareEach(int *x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        x[index] = x[index] * x[index];\n    }\n}",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        x[index] = x[index] * x[index];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        x[index] = x[index] * x[index];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        x[index] = x[index] * x[index];\n    }\n}",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        x[index] = x[index] * x[index];\n    }\n}",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = x[idx] * x[idx];\n    }\n}",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        x[index] = x[index] * x[index];\n    }\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "58_transform_squaring",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, NULL, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[0], x.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<int> temp(x.size());\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&temp[0], x.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                x[j] += temp[j];\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[0], x.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<int> temp(x.size());\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&temp[0], x.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                x[j] += temp[j];\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n\n    if (rank == 0) {\n        std::vector<int> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, result.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "47_sparse_la_spmv",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use Kokkos to parallelize. Assume Kokkos has already been initialized.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, Kokkos::View<const COOElement*> const& A, Kokkos::View<const double*> const& x, double beta, Kokkos::View<double *> &y, size_t M, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.extent(0); j++) {\n            if (A(j).row == i) {\n                sum += A(j).value * x(A(j).column);\n            }\n        }\n        y(i) = alpha * sum + beta * y(i);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.extent(0); j++) {\n            if (A(j).row == i) {\n                sum += A(j).value * x(A(j).column);\n            }\n        }\n        y(i) = alpha * sum + beta * y(i);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.extent(0); j++) {\n            if (A(j).row == i) {\n                sum += A(j).value * x(A(j).column);\n            }\n        }\n        y(i) = alpha * sum + beta * y(i);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.extent(0); j++) {\n            if (A(j).row == i) {\n                sum += A(j).value * x(A(j).column);\n            }\n        }\n        y(i) = alpha * sum + beta * y(i);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.extent(0); j++) {\n            if (A(j).row == i) {\n                sum += A(j).value * x(A(j).column);\n            }\n        }\n        y(i) = alpha * sum + beta * y(i);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.extent(0); j++) {\n            if (A(j).row == i) {\n                sum += A(j).value * x(A(j).column);\n            }\n        }\n        y(i) = alpha * sum + beta * y(i);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.extent(0); j++) {\n            if (A(j).row == i) {\n                sum += A(j).value * x(A(j).column);\n            }\n        }\n        y(i) = alpha * sum + beta * y(i);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.extent(0); j++) {\n            if (A(j).row == i) {\n                sum += A(j).value * x(A(j).column);\n            }\n        }\n        y(i) = alpha * sum + beta * y(i);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.extent(0); j++) {\n            if (A(j).row == i) {\n                sum += alpha * A(j).value * x(A(j).column);\n            }\n        }\n        y(i) = beta * y(i) + sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.extent(0); j++) {\n            if (A(j).row == i) {\n                sum += A(j).value * x(A(j).column);\n            }\n        }\n        y(i) = alpha * sum + beta * y(i);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.extent(0); j++) {\n            if (A(j).row == i) {\n                sum += alpha * A(j).value * x(A(j).column);\n            }\n        }\n        y(i) = beta * y(i) + sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.extent(0); j++) {\n            if (A(j).row == i) {\n                sum += A(j).value * x(A(j).column);\n            }\n        }\n        y(i) = alpha * sum + beta * y(i);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.extent(0); j++) {\n            if (A(j).row == i) {\n                sum += alpha * A(j).value * x(A(j).column);\n            }\n        }\n        y(i) = beta * y(i) + sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.extent(0); ++k) {\n                if (A(k).row == i && A(k).column == j) {\n                    sum += alpha * A(k).value * x(j);\n                }\n            }\n        }\n        y(i) = beta * y(i) + sum;\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.extent(0); j++) {\n            if (A(j).row == i) {\n                sum += A(j).value * x(A(j).column);\n            }\n        }\n        y(i) = alpha * sum + beta * y(i);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.extent(0); j++) {\n            if (A(j).row == i) {\n                sum += alpha * A(j).value * x(A(j).column);\n            }\n        }\n        y(i) = beta * y(i) + sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.extent(0); j++) {\n            if (A(j).row == i) {\n                sum += alpha * A(j).value * x(A(j).column);\n            }\n        }\n        y(i) = beta * y(i) + sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.extent(0); j++) {\n            if (A(j).row == i) {\n                sum += alpha * A(j).value * x(A(j).column);\n            }\n        }\n        y(i) = beta * y(i) + sum;\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.extent(0); j++) {\n            if (A(j).row == i) {\n                sum += alpha * A(j).value * x(A(j).column);\n            }\n        }\n        y(i) = beta * y(i) + sum;\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.extent(0); j++) {\n            if (A(j).row == i) {\n                sum += alpha * A(j).value * x(A(j).column);\n            }\n        }\n        y(i) = beta * y(i) + sum;\n    });\n    Kokkos::fence();\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "47_sparse_la_spmv",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use OpenMP to parallelize.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += alpha * A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = beta * y[i] + sum;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += alpha * A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = beta * y[i] + sum;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += alpha * A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = beta * y[i] + sum;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += alpha * A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = beta * y[i] + sum;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "47_sparse_la_spmv",
    "parallelism_model": "serial",
    "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n    }\n\n    for (size_t i = 0; i < A.size(); ++i) {\n        y[A[i].row] += alpha * A[i].value * x[A[i].column];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n    }\n\n    for (size_t i = 0; i < A.size(); ++i) {\n        y[A[i].row] += alpha * A[i].value * x[A[i].column];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n    }\n\n    for (size_t i = 0; i < A.size(); ++i) {\n        y[A[i].row] += alpha * A[i].value * x[A[i].column];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n    }\n\n    for (size_t i = 0; i < A.size(); ++i) {\n        y[A[i].row] += alpha * A[i].value * x[A[i].column];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n    }\n\n    for (size_t i = 0; i < A.size(); ++i) {\n        y[A[i].row] += alpha * A[i].value * x[A[i].column];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n    }\n\n    for (size_t i = 0; i < A.size(); ++i) {\n        y[A[i].row] += alpha * A[i].value * x[A[i].column];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n    }\n\n    for (size_t i = 0; i < A.size(); ++i) {\n        y[A[i].row] += alpha * A[i].value * x[A[i].column];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n    }\n\n    for (size_t i = 0; i < A.size(); ++i) {\n        y[A[i].row] += alpha * A[i].value * x[A[i].column];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n    }\n\n    for (size_t i = 0; i < A.size(); ++i) {\n        y[A[i].row] += alpha * A[i].value * x[A[i].column];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n    }\n\n    for (size_t i = 0; i < A.size(); ++i) {\n        y[A[i].row] += alpha * A[i].value * x[A[i].column];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n    }\n\n    for (size_t i = 0; i < A.size(); ++i) {\n        y[A[i].row] += alpha * A[i].value * x[A[i].column];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n    }\n\n    for (size_t i = 0; i < A.size(); ++i) {\n        y[A[i].row] += alpha * A[i].value * x[A[i].column];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n    }\n\n    for (size_t i = 0; i < A.size(); ++i) {\n        y[A[i].row] += alpha * A[i].value * x[A[i].column];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n    }\n\n    for (size_t i = 0; i < A.size(); ++i) {\n        y[A[i].row] += alpha * A[i].value * x[A[i].column];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n    }\n\n    for (size_t i = 0; i < A.size(); ++i) {\n        y[A[i].row] += alpha * A[i].value * x[A[i].column];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n    }\n\n    for (size_t i = 0; i < A.size(); ++i) {\n        y[A[i].row] += alpha * A[i].value * x[A[i].column];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n    }\n\n    for (size_t i = 0; i < A.size(); ++i) {\n        y[A[i].row] += alpha * A[i].value * x[A[i].column];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n    }\n\n    for (size_t i = 0; i < A.size(); ++i) {\n        y[A[i].row] += alpha * A[i].value * x[A[i].column];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n    }\n\n    for (size_t i = 0; i < A.size(); ++i) {\n        y[A[i].row] += alpha * A[i].value * x[A[i].column];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n    }\n\n    for (size_t i = 0; i < A.size(); ++i) {\n        y[A[i].row] += alpha * A[i].value * x[A[i].column];\n    }\n}"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "47_sparse_la_spmv",
    "parallelism_model": "hip",
    "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format with sizeA elements.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use AMD HIP to parallelize. The kernel will be launched with at least sizeA threads.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\n__global__ void spmv(double alpha, const COOElement *A, size_t sizeA, const double *x, double beta, double *y, size_t M, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        y[A[tid].row] = beta * y[A[tid].row] + alpha * A[tid].value * x[A[tid].column];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&y[row], alpha * val * x[col] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&y[row], alpha * val * x[col] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column]);\n    }\n    if (threadIdx.x == 0) {\n        y[tid] = alpha * A[tid].value * x[A[tid].column] + beta * y[tid];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column]);\n    }\n    if (threadIdx.x == 0) {\n        y[tid] = alpha * A[tid].value * x[A[tid].column] + beta * y[tid];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&y[row], alpha * val * x[col] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "47_sparse_la_spmv",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use MPI to parallelize. Assume MPI has already been initialized.\n   Every rank has a complete copy of A, x, and y. Store the result in y on rank 0.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += alpha * A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = beta * y[i] + sum;\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += alpha * A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = beta * y[i] + sum;\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += alpha * A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = beta * y[i] + sum;\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "47_sparse_la_spmv",
    "parallelism_model": "cuda",
    "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format with sizeA elements.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use CUDA to parallelize. The kernel will be launched with at least sizeA threads.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\n__global__ void spmv(double alpha, const COOElement *A, size_t sizeA, const double *x, double beta, double *y, size_t M, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&y[row], alpha * val * x[col] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column]);\n    }\n    if (threadIdx.x == 0) {\n        y[tid] = alpha * A[tid].value * x[A[tid].column] + beta * y[tid];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&y[row], alpha * val * x[col] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&y[row], alpha * val * x[col]);\n    }\n    if (threadIdx.x == 0) {\n        y[tid] = alpha * A[tid].value * x[A[tid].column] + beta * y[tid];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&y[row], alpha * val * x[col] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&y[row], alpha * value * x[column] + beta * y[row]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&y[row], alpha * val * x[col] + beta * y[row]);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "47_sparse_la_spmv",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use MPI and OpenMP to parallelize. Assume MPI has already been initialized.\n   Every rank has a complete copy of A, x, and y. Store the result in y on rank 0.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<double> recvbuf(M);\n        MPI::COMM_WORLD.Gather(&y[0], M, MPI::DOUBLE, &recvbuf[0], M, MPI::DOUBLE, 0);\n        y = recvbuf;\n    } else {\n        MPI::COMM_WORLD.Gather(&y[0], M, MPI::DOUBLE, NULL, 0, MPI::DATATYPE_NULL, 0);\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<double> recvbuf(M);\n        MPI::COMM_WORLD.Gather(&y[0], M, MPI::DOUBLE, &recvbuf[0], M, MPI::DOUBLE, 0);\n        y = recvbuf;\n    } else {\n        MPI::COMM_WORLD.Gather(&y[0], M, MPI::DOUBLE, nullptr, 0, MPI::DATATYPE_NULL, 0);\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<double> result(M);\n        MPI::COMM_WORLD.Gather(&y[0], M, MPI::DOUBLE, &result[0], M, MPI::DOUBLE, 0);\n        y = result;\n    } else {\n        MPI::COMM_WORLD.Gather(&y[0], M, MPI::DOUBLE, nullptr, 0, MPI::DOUBLE, 0);\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<double> recvbuf(M);\n        MPI::COMM_WORLD.Gather(&y[0], M, MPI::DOUBLE, &recvbuf[0], M, MPI::DOUBLE, 0);\n        y = recvbuf;\n    } else {\n        MPI::COMM_WORLD.Gather(&y[0], M, MPI::DOUBLE, nullptr, 0, MPI::DATATYPE_NULL, 0);\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<double> recvbuf(M);\n        MPI::COMM_WORLD.Gather(&y[0], M, MPI::DOUBLE, &recvbuf[0], M, MPI::DOUBLE, 0);\n        y = recvbuf;\n    } else {\n        MPI::COMM_WORLD.Gather(&y[0], M, MPI::DOUBLE, NULL, 0, MPI::DATATYPE_NULL, 0);\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<double> result(M);\n        MPI::COMM_WORLD.Gather(&y[0], M, MPI::DOUBLE, &result[0], M, MPI::DOUBLE, 0);\n        y = result;\n    } else {\n        MPI::COMM_WORLD.Gather(&y[0], M, MPI::DOUBLE, nullptr, 0, MPI::DOUBLE, 0);\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<double> recvbuf(M);\n        MPI::COMM_WORLD.Gather(&y[0], M, MPI::DOUBLE, &recvbuf[0], M, MPI::DOUBLE, 0);\n        y = recvbuf;\n    } else {\n        MPI::COMM_WORLD.Gather(&y[0], M, MPI::DOUBLE, nullptr, 0, MPI::DATATYPE_NULL, 0);\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        y[i] = alpha * sum + beta * y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] *= beta;\n        for (size_t j = 0; j < A.size(); ++j) {\n            if (A[j].row == i) {\n                y[i] += alpha * A[j].value * x[A[j].column];\n            }\n        }\n    }\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<double> result(M);\n        MPI::COMM_WORLD.Gather(&y[0], M, MPI::DOUBLE, &result[0], M, MPI::DOUBLE, 0);\n        y = result;\n    } else {\n        MPI::COMM_WORLD.Gather(&y[0], M, MPI::DOUBLE, nullptr, 0, MPI::DOUBLE, 0);\n    }\n}"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "46_sparse_la_spmm",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(Kokkos::View<const COOElement*> &A, Kokkos::View<const COOElement*> &X, Kokkos::View<double**> &Y, size_t M, size_t K, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n    for (size_t j = 0; j < N; j++) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; k++) {\n        for (size_t a = 0; a < A.extent(0); a++) {\n          if (A(a).row == i && A(a).column == k) {\n            for (size_t b = 0; b < X.extent(0); b++) {\n              if (X(b).row == k && X(b).column == j) {\n                sum += A(a).value * X(b).value;\n              }\n            }\n          }\n        }\n      }\n      Y(i, j) = sum;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n    for (size_t j = 0; j < N; j++) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; k++) {\n        for (size_t a = 0; a < A.extent(0); a++) {\n          if (A(a).row == i && X(a).column == j) {\n            sum += A(a).value * X(a).value;\n          }\n        }\n      }\n      Y(i, j) = sum;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n    for (size_t j = 0; j < N; j++) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; k++) {\n        for (size_t a = 0; a < A.extent(0); a++) {\n          if (A(a).row == i && X(a).column == j) {\n            sum += A(a).value * X(a).value;\n          }\n        }\n      }\n      Y(i, j) = sum;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n    for (size_t j = 0; j < N; j++) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; k++) {\n        for (size_t a = 0; a < A.extent(0); a++) {\n          if (A(a).row == i && X(a).column == j) {\n            sum += A(a).value * X(a).value;\n          }\n        }\n      }\n      Y(i, j) = sum;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n    for (size_t j = 0; j < N; j++) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; k++) {\n        for (size_t a = 0; a < A.extent(0); a++) {\n          if (A(a).row == i && X(a).column == j) {\n            sum += A(a).value * X(a).value;\n          }\n        }\n      }\n      Y(i, j) = sum;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"spmm\", M, K, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; ++k) {\n        for (size_t a = 0; a < A.extent(0); ++a) {\n          if (A(a).row == i && A(a).column == k) {\n            for (size_t x = 0; x < X.extent(0); ++x) {\n              if (X(x).row == k && X(x).column == j) {\n                sum += A(a).value * X(x).value;\n              }\n            }\n          }\n        }\n      }\n      Y(i, j) = sum;\n    }\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n    for (size_t j = 0; j < N; j++) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; k++) {\n        for (size_t a = 0; a < A.extent(0); a++) {\n          if (A(a).row == i && X(a).column == j) {\n            sum += A(a).value * X(a).value;\n          }\n        }\n      }\n      Y(i, j) = sum;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n    for (size_t j = 0; j < N; ++j) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; ++k) {\n        for (size_t a = 0; a < A.extent(0); ++a) {\n          if (A(a).row == i && A(a).column == k) {\n            for (size_t x = 0; x < X.extent(0); ++x) {\n              if (X(x).row == k && X(x).column == j) {\n                sum += A(a).value * X(x).value;\n              }\n            }\n          }\n        }\n      }\n      Y(i, j) = sum;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n    for (size_t j = 0; j < N; j++) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; k++) {\n        for (size_t a = 0; a < A.extent(0); a++) {\n          if (A(a).row == i && X(a).column == j) {\n            sum += A(a).value * X(a).value;\n          }\n        }\n      }\n      Y(i, j) = sum;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n    for (size_t j = 0; j < N; ++j) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; ++k) {\n        for (size_t a = 0; a < A.extent(0); ++a) {\n          if (A(a).row == i && A(a).column == k) {\n            for (size_t x = 0; x < X.extent(0); ++x) {\n              if (X(x).row == k && X(x).column == j) {\n                sum += A(a).value * X(x).value;\n              }\n            }\n          }\n        }\n      }\n      Y(i, j) = sum;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n    for (size_t j = 0; j < N; j++) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; k++) {\n        for (size_t a = 0; a < A.extent(0); a++) {\n          if (A(a).row == i && X(a).column == j) {\n            sum += A(a).value * X(a).value;\n          }\n        }\n      }\n      Y(i, j) = sum;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n    for (size_t j = 0; j < N; j++) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; k++) {\n        for (size_t a = 0; a < A.extent(0); a++) {\n          if (A(a).row == i && X(a).column == j) {\n            sum += A(a).value * X(a).value;\n          }\n        }\n      }\n      Y(i, j) = sum;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n      for (size_t j = 0; j < N; ++j) {\n          double sum = 0.0;\n          for (size_t k = 0; k < K; ++k) {\n              for (size_t a = 0; a < A.extent(0); ++a) {\n                  if (A(a).row == i && A(a).column == k) {\n                      for (size_t b = 0; b < X.extent(0); ++b) {\n                          if (X(b).row == k && X(b).column == j) {\n                              sum += A(a).value * X(b).value;\n                          }\n                      }\n                  }\n              }\n          }\n          Y(i, j) = sum;\n      }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n    for (size_t j = 0; j < N; j++) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; k++) {\n        for (size_t a = 0; a < A.extent(0); a++) {\n          if (A(a).row == i && X(a).column == j) {\n            sum += A(a).value * X(a).value;\n          }\n        }\n      }\n      Y(i, j) = sum;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"spmm\", M, K, KOKKOS_LAMBDA (const int i) {\n    for (size_t j = 0; j < N; j++) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; k++) {\n        for (size_t a = 0; a < A.extent(0); a++) {\n          if (A(a).row == i && A(a).column == k) {\n            for (size_t x = 0; x < X.extent(0); x++) {\n              if (X(x).row == k && X(x).column == j) {\n                sum += A(a).value * X(x).value;\n              }\n            }\n          }\n        }\n      }\n      Y(i, j) = sum;\n    }\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t i) {\n    for (size_t j = 0; j < N; ++j) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; ++k) {\n        for (size_t a = 0; a < A.extent(0); ++a) {\n          if (A(a).row == i && A(a).column == k) {\n            for (size_t x = 0; x < X.extent(0); ++x) {\n              if (X(x).row == k && X(x).column == j) {\n                sum += A(a).value * X(x).value;\n              }\n            }\n          }\n        }\n      }\n      Y(i, j) = sum;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"spmm\", M, K, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; k++) {\n        for (size_t a = 0; a < A.extent(0); a++) {\n          if (A(a).row == i && A(a).column == k) {\n            for (size_t b = 0; b < X.extent(0); b++) {\n              if (X(b).row == k && X(b).column == j) {\n                sum += A(a).value * X(b).value;\n              }\n            }\n          }\n        }\n      }\n      Y(i, j) = sum;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"spmm\", M, K, KOKKOS_LAMBDA (const int i) {\n    for (size_t j = 0; j < N; j++) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; k++) {\n        for (size_t a = 0; a < A.extent(0); a++) {\n          if (A(a).row == i && A(a).column == k) {\n            for (size_t x = 0; x < X.extent(0); x++) {\n              if (X(x).row == k && X(x).column == j) {\n                sum += A(a).value * X(x).value;\n              }\n            }\n          }\n        }\n      }\n      Y(i, j) = sum;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"spmm\", M, K, KOKKOS_LAMBDA (const int i) {\n    for (size_t j = 0; j < N; j++) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; k++) {\n        for (size_t a = 0; a < A.extent(0); a++) {\n          if (A(a).row == i && A(a).column == k) {\n            for (size_t x = 0; x < X.extent(0); x++) {\n              if (X(x).row == k && X(x).column == j) {\n                sum += A(a).value * X(x).value;\n              }\n            }\n          }\n        }\n      }\n      Y(i, j) = sum;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"spmm\", M, K, KOKKOS_LAMBDA (const int i) {\n    for (size_t j = 0; j < N; j++) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; k++) {\n        for (size_t a = 0; a < A.extent(0); a++) {\n          if (A(a).row == i && A(a).column == k) {\n            for (size_t x = 0; x < X.extent(0); x++) {\n              if (X(x).row == k && X(x).column == j) {\n                sum += A(a).value * X(x).value;\n              }\n            }\n          }\n        }\n      }\n      Y(i, j) = sum;\n    }\n  });\n  Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "46_sparse_la_spmm",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                sum += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n            Y[i * N + j] = sum;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                sum += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n            Y[i * N + j] = sum;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "46_sparse_la_spmm",
    "parallelism_model": "serial",
    "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "46_sparse_la_spmm",
    "parallelism_model": "hip",
    "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format with sizeA elements.\n   X is a sparse KxN matrix in COO format with sizeX elements. Y is a dense MxN matrix in row-major.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least sizeA threads.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\n__global__ void spmm(const COOElement *A, size_t sizeA, const COOElement *X, size_t sizeX, double *Y, size_t M, size_t K, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t rowA = A[tid].row;\n        size_t colA = A[tid].column;\n        double valA = A[tid].value;\n\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].column == colA) {\n                size_t rowX = X[i].row;\n                double valX = X[i].value;\n                Y[rowA * N + rowX] += valA * valX;\n            }\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t col = A[tid].column;\n        double val = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == col) {\n                Y[row * N + X[i].column] += val * X[i].value;\n            }\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t col = A[tid].column;\n        double val = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == col) {\n                Y[row * N + X[i].column] += val * X[i].value;\n            }\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t col = A[tid].column;\n        double val = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == col) {\n                Y[row * N + X[i].column] += val * X[i].value;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t col = A[tid].column;\n        double val = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == col) {\n                size_t Y_index = row * N + X[i].column;\n                Y[Y_index] += val * X[i].value;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t col = A[tid].column;\n        double val = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == col) {\n                Y[row * N + X[i].column] += val * X[i].value;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == column) {\n                Y[row * N + X[i].column] += value * X[i].value;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t rowA = A[tid].row;\n        size_t colA = A[tid].column;\n        double valA = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == colA) {\n                size_t colX = X[i].column;\n                double valX = X[i].value;\n                Y[rowA * N + colX] += valA * valX;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t rowA = A[tid].row;\n        size_t colA = A[tid].column;\n        double valA = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == colA) {\n                size_t colX = X[i].column;\n                double valX = X[i].value;\n                atomicAdd(&Y[rowA * N + colX], valA * valX);\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t rowA = A[tid].row;\n        size_t colA = A[tid].column;\n        double valA = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == colA) {\n                size_t colX = X[i].column;\n                double valX = X[i].value;\n                atomicAdd(&Y[rowA * N + colX], valA * valX);\n            }\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t rowA = A[tid].row;\n        size_t colA = A[tid].column;\n        double valA = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == colA) {\n                size_t colX = X[i].column;\n                double valX = X[i].value;\n                atomicAdd(&Y[rowA * N + colX], valA * valX);\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t col = A[tid].column;\n        double val = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == col) {\n                size_t Y_index = row * N + X[i].column;\n                Y[Y_index] += val * X[i].value;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t rowA = A[tid].row;\n        size_t colA = A[tid].column;\n        double valA = A[tid].value;\n\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].column == colA) {\n                size_t rowX = X[i].row;\n                double valX = X[i].value;\n                Y[rowA * N + rowX] += valA * valX;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t rowA = A[tid].row;\n        size_t colA = A[tid].column;\n        double valA = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == colA) {\n                size_t colX = X[i].column;\n                double valX = X[i].value;\n                Y[rowA * N + colX] += valA * valX;\n            }\n        }\n    }\n}\n",
      "\n   int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < sizeA) {\n       int row = A[tid].row;\n       int col = A[tid].column;\n       double val = A[tid].value;\n       for (int i = 0; i < sizeX; i++) {\n           if (X[i].row == col) {\n               Y[row * N + X[i].column] += val * X[i].value;\n           }\n       }\n   }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t col = A[tid].column;\n        double val = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == col) {\n                Y[row * N + X[i].column] += val * X[i].value;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t col = A[tid].column;\n        double val = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == col) {\n                Y[row * N + X[i].column] += val * X[i].value;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t col = A[tid].column;\n        double val = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == col) {\n                Y[row * N + X[i].column] += val * X[i].value;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        for (size_t j = 0; j < N; j++) {\n            for (size_t k = 0; k < sizeX; k++) {\n                if (X[k].row == column && X[k].column == j) {\n                    Y[row * N + j] += value * X[k].value;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t rowA = A[tid].row;\n        size_t colA = A[tid].column;\n        double valA = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == colA) {\n                size_t colX = X[i].column;\n                double valX = X[i].value;\n                atomicAdd(&Y[rowA * N + colX], valA * valX);\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "46_sparse_la_spmm",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and X. Store the result in Y on rank 0.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Initialize Y to all zeros\n    Y.resize(M * N, 0.0);\n\n    // Perform matrix multiplication\n    for (size_t i = 0; i < A.size(); ++i) {\n        size_t row = A[i].row;\n        size_t col = A[i].column;\n        double val = A[i].value;\n\n        for (size_t j = 0; j < X.size(); ++j) {\n            if (X[j].row == col) {\n                size_t result_row = row;\n                size_t result_col = X[j].column;\n                double result_val = val * X[j].value;\n                Y[result_row * N + result_col] += result_val;\n            }\n        }\n    }\n}",
      "\n    // Initialize Y to all zeros\n    Y.resize(M * N, 0.0);\n\n    // Perform matrix multiplication\n    for (size_t i = 0; i < A.size(); ++i) {\n        size_t row = A[i].row;\n        size_t col = A[i].column;\n        double val = A[i].value;\n        for (size_t j = 0; j < N; ++j) {\n            Y[row * N + j] += val * X[col * N + j];\n        }\n    }\n\n    // Gather results to rank 0\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<double> localY(M * N);\n        MPI::COMM_WORLD.Gather(&Y[0], M * N, MPI::DOUBLE, &localY[0], M * N, MPI::DOUBLE, 0);\n        Y = localY;\n    }\n```\n}",
      "\n    // Your code here\n}",
      "\n    // Initialize Y to all zeros\n    Y.resize(M * N, 0.0);\n\n    // Perform matrix multiplication\n    for (size_t i = 0; i < A.size(); ++i) {\n        size_t row = A[i].row;\n        size_t col = A[i].column;\n        double val = A[i].value;\n        for (size_t j = 0; j < X.size(); ++j) {\n            if (X[j].row == col) {\n                size_t result_row = row;\n                size_t result_col = X[j].column;\n                Y[result_row * N + result_col] += val * X[j].value;\n            }\n        }\n    }\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Initialize Y to be a dense MxN matrix filled with zeros\n    Y.resize(M * N, 0.0);\n\n    // Perform the matrix multiplication\n    for (size_t i = 0; i < A.size(); ++i) {\n        size_t rowA = A[i].row;\n        size_t colA = A[i].column;\n        double valA = A[i].value;\n\n        for (size_t j = 0; j < X.size(); ++j) {\n            size_t rowX = X[j].row;\n            size_t colX = X[j].column;\n            double valX = X[j].value;\n\n            if (colA == rowX) {\n                Y[rowA * N + colX] += valA * valX;\n            }\n        }\n    }\n}",
      "\n    // Initialize Y to all zeros\n    Y.resize(M * N, 0.0);\n\n    // Perform matrix multiplication\n    for (size_t i = 0; i < A.size(); ++i) {\n        size_t row = A[i].row;\n        size_t col = A[i].column;\n        double val = A[i].value;\n        for (size_t j = 0; j < X.size(); ++j) {\n            if (X[j].row == col) {\n                size_t result_row = row;\n                size_t result_col = X[j].column;\n                Y[result_row * N + result_col] += val * X[j].value;\n            }\n        }\n    }\n}\n",
      "\n    // Initialize Y to zero\n    Y.resize(M * N, 0.0);\n\n    // Perform matrix multiplication\n    for (size_t i = 0; i < A.size(); ++i) {\n        size_t row = A[i].row;\n        size_t col = A[i].column;\n        double val = A[i].value;\n        for (size_t j = 0; j < X.size(); ++j) {\n            if (X[j].row == col) {\n                Y[row * N + X[j].column] += val * X[j].value;\n            }\n        }\n    }\n}\n",
      "\n    // Initialize Y to all zeros\n    Y.resize(M * N, 0.0);\n\n    // Perform matrix multiplication\n    for (size_t i = 0; i < A.size(); ++i) {\n        size_t row = A[i].row;\n        size_t col = A[i].column;\n        double val = A[i].value;\n        for (size_t j = 0; j < X.size(); ++j) {\n            if (X[j].row == col) {\n                Y[row * N + X[j].column] += val * X[j].value;\n            }\n        }\n    }\n}\n",
      "\n    // Initialize Y to zeros\n    Y.resize(M * N, 0.0);\n\n    // Perform matrix multiplication\n    for (size_t i = 0; i < A.size(); ++i) {\n        for (size_t j = 0; j < X.size(); ++j) {\n            if (A[i].column == X[j].row) {\n                Y[A[i].row * N + X[j].column] += A[i].value * X[j].value;\n            }\n        }\n    }\n\n    // Gather results to rank 0\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<double> localY(M * N, 0.0);\n        MPI::COMM_WORLD.Gather(&Y[0], M * N, MPI::DOUBLE, &localY[0], M * N, MPI::DOUBLE, 0);\n        Y = localY;\n    } else {\n        MPI::COMM_WORLD.Gather(&Y[0], M * N, MPI::DOUBLE, nullptr, 0, MPI::DATATYPE_NULL, 0);\n    }\n}\n",
      "\n    // Your code here\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    // Your code here\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "46_sparse_la_spmm",
    "parallelism_model": "cuda",
    "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format with sizeA elements.\n   X is a sparse KxN matrix in COO format with sizeX elements. Y is a dense MxN matrix in row-major.\n   Use CUDA to compute in parallel. The kernel is launched with at least sizeA threads.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\n__global__ void spmm(const COOElement *A, size_t sizeA, const COOElement *X, size_t sizeX, double *Y, size_t M, size_t K, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        for (size_t n = 0; n < N; ++n) {\n            for (size_t k = 0; k < K; ++k) {\n                if (X[k].row == column && X[k].column == n) {\n                    Y[row * N + n] += value * X[k].value;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == column) {\n                size_t resultRow = row;\n                size_t resultColumn = X[i].column;\n                double resultValue = value * X[i].value;\n                atomicAdd(&Y[resultRow * N + resultColumn], resultValue);\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t rowA = A[tid].row;\n        size_t colA = A[tid].column;\n        double valA = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == colA) {\n                size_t colX = X[i].column;\n                double valX = X[i].value;\n                atomicAdd(&Y[rowA * N + colX], valA * valX);\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t rowA = A[tid].row;\n        size_t colA = A[tid].column;\n        double valA = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == colA) {\n                size_t colX = X[i].column;\n                double valX = X[i].value;\n                atomicAdd(&Y[rowA * N + colX], valA * valX);\n            }\n        }\n    }\n}\n",
      "\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M) {\n        for (int i = 0; i < sizeA; i++) {\n            if (A[i].row == row) {\n                for (int j = 0; j < sizeX; j++) {\n                    if (X[j].column == A[i].column) {\n                        atomicAdd(&Y[row * N + X[j].column], A[i].value * X[j].value);\n                    }\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t rowA = A[tid].row;\n        size_t colA = A[tid].column;\n        double valA = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == colA) {\n                size_t colX = X[i].column;\n                double valX = X[i].value;\n                Y[rowA * N + colX] += valA * valX;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t rowA = A[tid].row;\n        size_t colA = A[tid].column;\n        double valA = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == colA) {\n                size_t colX = X[i].column;\n                double valX = X[i].value;\n                Y[rowA * N + colX] += valA * valX;\n            }\n        }\n    }\n}\n",
      "\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M) {\n        for (int i = 0; i < sizeA; i++) {\n            if (A[i].row == row) {\n                int col = A[i].column;\n                double valA = A[i].value;\n                for (int j = 0; j < sizeX; j++) {\n                    if (X[j].row == col) {\n                        int colX = X[j].column;\n                        double valX = X[j].value;\n                        atomicAdd(&Y[row * N + colX], valA * valX);\n                    }\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t rowA = A[tid].row;\n        size_t colA = A[tid].column;\n        double valA = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == colA) {\n                size_t colX = X[i].column;\n                double valX = X[i].value;\n                atomicAdd(&Y[rowA * N + colX], valA * valX);\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t rowA = A[tid].row;\n        size_t colA = A[tid].column;\n        double valA = A[tid].value;\n\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].column == colA) {\n                size_t rowX = X[i].row;\n                double valX = X[i].value;\n                Y[rowA * N + rowX] += valA * valX;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t rowA = A[tid].row;\n        size_t colA = A[tid].column;\n        double valA = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == colA) {\n                size_t colX = X[i].column;\n                double valX = X[i].value;\n                atomicAdd(&Y[rowA * N + colX], valA * valX);\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < sizeX; ++k) {\n                if (X[k].row == column && X[k].column == j) {\n                    Y[row * N + j] += value * X[k].value;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M) {\n        for (int i = 0; i < sizeA; i++) {\n            if (A[i].row == row) {\n                int col = A[i].column;\n                double valA = A[i].value;\n                for (int j = 0; j < sizeX; j++) {\n                    if (X[j].row == col) {\n                        int colX = X[j].column;\n                        double valX = X[j].value;\n                        atomicAdd(&Y[row * N + colX], valA * valX);\n                    }\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < sizeX; ++k) {\n                if (X[k].row == column && X[k].column == j) {\n                    Y[row * N + j] += value * X[k].value;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        for (size_t n = 0; n < N; ++n) {\n            for (size_t k = 0; k < K; ++k) {\n                if (X[k].row == column && X[k].column == n) {\n                    Y[row * N + n] += value * X[k].value;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t rowA = A[tid].row;\n        size_t colA = A[tid].column;\n        double valA = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == colA) {\n                size_t colX = X[i].column;\n                double valX = X[i].value;\n                atomicAdd(&Y[rowA * N + colX], valA * valX);\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t rowA = A[tid].row;\n        size_t colA = A[tid].column;\n        double valA = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == colA) {\n                size_t colX = X[i].column;\n                double valX = X[i].value;\n                atomicAdd(&Y[rowA * N + colX], valA * valX);\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t rowA = A[tid].row;\n        size_t colA = A[tid].column;\n        double valA = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].row == colA) {\n                size_t colX = X[i].column;\n                double valX = X[i].value;\n                atomicAdd(&Y[rowA * N + colX], valA * valX);\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t rowA = A[tid].row;\n        size_t colA = A[tid].column;\n        double valA = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].column == colA) {\n                size_t rowX = X[i].row;\n                double valX = X[i].value;\n                atomicAdd(&Y[rowA * N + rowX], valA * valX);\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t rowA = A[tid].row;\n        size_t colA = A[tid].column;\n        double valA = A[tid].value;\n        for (size_t i = 0; i < sizeX; ++i) {\n            if (X[i].column == colA) {\n                size_t rowX = X[i].row;\n                double valX = X[i].value;\n                Y[rowA * N + rowX] += valA * valX;\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "46_sparse_la_spmm",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and X. Store the result in Y on rank 0.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && X[k].row == j) {\n                        Y[i * N + j] += A[a].value * X[k].value;\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && X[a].column == j) {\n                        Y[i * N + j] += A[a].value * X[a].value;\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            Y[i * N + j] = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                for (size_t a = 0; a < A.size(); ++a) {\n                    if (A[a].row == i && A[a].column == k) {\n                        for (size_t x = 0; x < X.size(); ++x) {\n                            if (X[x].row == k && X[x].column == j) {\n                                Y[i * N + j] += A[a].value * X[x].value;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}",
      "\n    // Your code here\n}"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "45_sparse_la_sparse_solve",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(Kokkos::View<const COOElement*> &A, Kokkos::View<const double*> &b, Kokkos::View<double*> &x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.extent(0); ++k) {\n                if (A(k).row == i && A(k).column == j) {\n                    sum += A(k).value * b(j);\n                }\n            }\n        }\n        x(i) = sum;\n    });\n    Kokkos::fence();\n```\n}",
      "\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.extent(0); ++k) {\n                if (A(k).row == i && A(k).column == j) {\n                    sum += A(k).value * b(j);\n                }\n            }\n        }\n        x(i) = sum;\n    });\n    Kokkos::fence();\n```\n}",
      "\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.extent(0); ++k) {\n                if (A(k).row == i && A(k).column == j) {\n                    sum += A(k).value * b(j);\n                }\n            }\n        }\n        x(i) = sum;\n    });\n    Kokkos::fence();\n```\n}",
      "\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.extent(0); ++k) {\n                if (A(k).row == i && A(k).column == j) {\n                    sum += A(k).value * b(j);\n                }\n            }\n        }\n        x(i) = sum;\n    });\n    Kokkos::fence();\n```\n}",
      "\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.extent(0); ++k) {\n                if (A(k).row == i && A(k).column == j) {\n                    sum += A(k).value * b(j);\n                }\n            }\n        }\n        x(i) = sum;\n    });\n    Kokkos::fence();\n```\n}",
      "\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.extent(0); ++k) {\n                if (A(k).row == i && A(k).column == j) {\n                    sum += A(k).value * b(j);\n                }\n            }\n        }\n        x(i) = sum;\n    });\n    Kokkos::fence();\n```\n}",
      "\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.extent(0); ++k) {\n                if (A(k).row == i && A(k).column == j) {\n                    sum += A(k).value * b(j);\n                }\n            }\n        }\n        x(i) = sum;\n    });\n    Kokkos::fence();\n```\n}",
      "\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.extent(0); ++k) {\n                if (A(k).row == i && A(k).column == j) {\n                    sum += A(k).value * b(j);\n                }\n            }\n        }\n        x(i) = sum;\n    });\n    Kokkos::fence();\n```\n}",
      "\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.extent(0); ++k) {\n                if (A(k).row == i && A(k).column == j) {\n                    sum += A(k).value * b(j);\n                }\n            }\n        }\n        x(i) = sum;\n    });\n    Kokkos::fence();\n```\n}",
      "\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t k = 0; k < N; ++k) {\n            for (size_t j = 0; j < A.extent(0); ++j) {\n                if (A(j).row == i && A(j).column == k) {\n                    sum += A(j).value * b(k);\n                }\n            }\n        }\n        x(i) = sum;\n    });\n    Kokkos::fence();\n```\n}",
      "\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.extent(0); ++k) {\n                if (A(k).row == i && A(k).column == j) {\n                    sum += A(k).value * b(j);\n                }\n            }\n        }\n        x(i) = sum;\n    });\n    Kokkos::fence();\n```\n}",
      "\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.extent(0); ++k) {\n                if (A(k).row == i && A(k).column == j) {\n                    sum += A(k).value * b(j);\n                }\n            }\n        }\n        x(i) = sum;\n    });\n    Kokkos::fence();\n```\n}",
      "\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.extent(0); ++k) {\n                if (A(k).row == i && A(k).column == j) {\n                    sum += A(k).value * b(j);\n                }\n            }\n        }\n        x(i) = sum;\n    });\n    Kokkos::fence();\n```\n}",
      "\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; j++) {\n            for (size_t k = 0; k < A.extent(0); k++) {\n                if (A(k).row == i && A(k).column == j) {\n                    sum += A(k).value * b(j);\n                }\n            }\n        }\n        x(i) = sum;\n    });\n    Kokkos::fence();\n```\n}",
      "\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.extent(0); ++k) {\n                if (A(k).row == i && A(k).column == j) {\n                    sum += A(k).value * b(j);\n                }\n            }\n        }\n        x(i) = sum;\n    });\n    Kokkos::fence();\n```\n}",
      "\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.extent(0); ++k) {\n                if (A(k).row == i && A(k).column == j) {\n                    sum += A(k).value * b(j);\n                }\n            }\n        }\n        x(i) = sum;\n    });\n    Kokkos::fence();\n```\n}",
      "\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.extent(0); ++k) {\n                if (A(k).row == i && A(k).column == j) {\n                    sum += A(k).value * b(j);\n                }\n            }\n        }\n        x(i) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t k = 0; k < N; ++k) {\n            for (size_t j = 0; j < A.extent(0); ++j) {\n                if (A(j).row == i) {\n                    if (A(j).column == k) {\n                        sum += A(j).value * b(k);\n                    }\n                }\n            }\n        }\n        x(i) = sum;\n    });\n    Kokkos::fence();\n```\n}",
      "\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.extent(0); ++k) {\n                if (A(k).row == i && A(k).column == j) {\n                    sum += A(k).value * b(j);\n                }\n            }\n        }\n        x(i) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.extent(0); ++k) {\n                if (A(k).row == i && A(k).column == j) {\n                    sum += A(k).value * b(j);\n                }\n            }\n        }\n        x(i) = sum;\n    });\n    Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "45_sparse_la_sparse_solve",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(std::vector<COOElement> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    sum += A[k].value * x[j];\n                }\n            }\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    sum += A[k].value * x[j];\n                }\n            }\n        }\n        x[i] = (b[i] - sum) / A[i*N + i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    sum += A[k].value * x[j];\n                }\n            }\n        }\n        x[i] = (b[i] - sum) / A[i*N + i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    sum += A[k].value * x[j];\n                }\n            }\n        }\n        x[i] = (b[i] - sum) / A[i*N + i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    sum += A[k].value * x[j];\n                }\n            }\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    sum += A[k].value * x[j];\n                }\n            }\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    sum += A[k].value * x[j];\n                }\n            }\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    sum += A[k].value * x[j];\n                }\n            }\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    sum += A[k].value * x[j];\n                }\n            }\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    sum += A[k].value * x[j];\n                }\n            }\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    sum += A[k].value * x[j];\n                }\n            }\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    sum += A[k].value * x[j];\n                }\n            }\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    sum += A[k].value * x[j];\n                }\n            }\n        }\n        x[i] = (b[i] - sum) / A[i*N + i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    sum += A[k].value * x[j];\n                }\n            }\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    sum += A[k].value * x[j];\n                }\n            }\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    sum += A[k].value * x[j];\n                }\n            }\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    sum += A[k].value * x[j];\n                }\n            }\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    sum += A[k].value * x[j];\n                }\n            }\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    sum += A[k].value * x[j];\n                }\n            }\n        }\n        x[i] = (b[i] - sum) / A[i*N + i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    sum += A[k].value * x[j];\n                }\n            }\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "45_sparse_la_sparse_solve",
    "parallelism_model": "serial",
    "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(std::vector<COOElement> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // Implement the solution for solving the sparse linear system Ax=b\n    // using appropriate algorithm or library functions\n    // and store the result in the vector x.\n}",
      "\n    // Implement the solution for solving the sparse linear system Ax=b\n    // using appropriate algorithm or library (e.g., Eigen, Armadillo, etc.)\n    // and store the result in the vector x.\n}",
      "\n    // Implement the function here\n    // Use a suitable method to solve the linear system, such as Gaussian elimination or LU decomposition\n    // Ensure to handle the sparse matrix representation and efficiently solve the system\n}",
      "\n    // Implement the function body here\n}",
      "\n    // Implement the function here\n    // Use a suitable method to solve the linear system, such as Gaussian elimination or LU decomposition\n    // Ensure to handle the sparse matrix representation efficiently\n}",
      "\n    // Implement the function here\n    // Use a suitable method to solve the linear system, such as Gaussian elimination or LU decomposition\n    // Ensure to handle the sparse matrix A in COO format efficiently\n}",
      "\n    // Implement the solution for solving the linear system Ax=b\n    // ...\n}",
      "\n    // Implement the solution for solving the linear system Ax=b using appropriate method (e.g., Gaussian elimination, LU decomposition, etc.)\n    // Ensure to handle the sparse matrix A in COO format and the dense vectors x and b.\n    // Update the solution in the vector x.\n    // Use the size_t N to determine the size of the vectors and matrix.\n}",
      "\n    // Implement the solution for solving the linear system Ax=b using appropriate method (e.g., Gaussian elimination, LU decomposition, etc.)\n    // The result should be stored in the vector x.\n    // Remember to handle the sparse matrix A in COO format.\n}",
      "\n    // Implement the function body here\n}",
      "\n    // Create a sparse matrix in CSR format from the COO format\n    std::vector<double> values(N, 0.0);\n    std::vector<size_t> colIndex(N + 1, 0);\n    std::vector<size_t> rowIndex;\n    for (size_t i = 0; i < A.size(); ++i) {\n        values[A[i].column] = A[i].value;\n        colIndex[A[i].column + 1]++;\n    }\n    for (size_t i = 1; i <= N; ++i) {\n        colIndex[i] += colIndex[i - 1];\n    }\n    rowIndex.resize(A.size());\n    std::vector<size_t> temp(N, 0);\n    for (size_t i = 0; i < A.size(); ++i) {\n        size_t col = A[i].column;\n        size_t index = colIndex[col] + temp[col];\n        rowIndex[index] = A[i].row;\n        temp[col]++;\n    }\n\n    // Solve the linear system using sparse matrix techniques (e.g., Conjugate Gradient method)\n    // ...\n\n    // Update the x vector with the solution\n    // x = ...\n}\n",
      "\n    // Implement the solution for solving the linear system Ax=b using appropriate method (e.g., Gaussian elimination, LU decomposition, etc.)\n    // The result should be stored in the vector x.\n    // Remember to handle the sparse matrix A in COO format efficiently.\n}",
      "\n    // Implement the function body here\n}",
      "\n    // Convert COO format to CSR format for efficient matrix-vector multiplication\n    std::vector<double> values(N, 0.0);\n    std::vector<size_t> colIndex(N + 1, 0);\n    std::vector<size_t> rowIndex;\n    for (const auto& elem : A) {\n        values[elem.column] = elem.value;\n        colIndex[elem.column]++;\n    }\n    size_t sum = 0;\n    for (size_t i = 0; i < N; i++) {\n        size_t temp = colIndex[i];\n        colIndex[i] = sum;\n        sum += temp;\n    }\n    colIndex[N] = A.size();\n\n    for (const auto& elem : A) {\n        rowIndex.push_back(colIndex[elem.column]);\n    }\n\n    // Perform matrix-vector multiplication\n    for (size_t i = 0; i < N; i++) {\n        double sum = 0.0;\n        for (size_t j = colIndex[i]; j < colIndex[i + 1]; j++) {\n            sum += values[j] * x[rowIndex[j]];\n        }\n        x[i] = sum;\n    }\n\n    // Solve the linear system using the result of matrix-vector multiplication\n    for (size_t i = 0; i < N; i++) {\n        x[i] = (b[i] - x[i]) / values[colIndex[i]];\n    }\n}\n",
      "\n    // Implement the solution to the linear system using appropriate algorithm\n    // ...\n}",
      "\n    // Implement the function body here\n}",
      "\n    // Create a sparse matrix in CSR format from the COO format\n    std::vector<double> val;\n    std::vector<size_t> colIndex;\n    std::vector<size_t> rowPtr;\n    rowPtr.push_back(0);\n    for (size_t i = 0; i < N; ++i) {\n        size_t rowStart = val.size();\n        for (size_t j = 0; j < N; ++j) {\n            bool found = false;\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    val.push_back(A[k].value);\n                    colIndex.push_back(j);\n                    found = true;\n                    break;\n                }\n            }\n            if (!found) {\n                val.push_back(0.0);\n                colIndex.push_back(j);\n            }\n        }\n        rowPtr.push_back(rowStart + N);\n    }\n\n    // Solve the linear system using a sparse solver (e.g., Conjugate Gradient method)\n    // ...\n\n    // Update the x vector with the solution\n    // ...\n}\n",
      "\n    // Create a sparse matrix A in CSR format\n    std::vector<double> values;\n    std::vector<size_t> column_indices;\n    std::vector<size_t> row_pointers(N + 1, 0);\n    \n    for (size_t i = 0; i < A.size(); ++i) {\n        COOElement element = A[i];\n        values.push_back(element.value);\n        column_indices.push_back(element.column);\n        row_pointers[element.row + 1]++;\n    }\n    \n    for (size_t i = 1; i <= N; ++i) {\n        row_pointers[i] += row_pointers[i - 1];\n    }\n    \n    // Solve the linear system using a suitable solver, e.g., direct or iterative solver\n    // For example, using Eigen library:\n    Eigen::SparseMatrix<double> A_sparse(N, N);\n    A_sparse.setFromTriplets(values.begin(), values.end(), column_indices.begin());\n    \n    Eigen::VectorXd b_dense = Eigen::Map<Eigen::VectorXd, Eigen::Unaligned>(b.data(), b.size());\n    Eigen::SparseLU<Eigen::SparseMatrix<double>> solver;\n    solver.analyzePattern(A_sparse);\n    solver.factorize(A_sparse);\n    Eigen::VectorXd x_dense = solver.solve(b_dense);\n    \n    // Convert the solution back to a vector\n    x.assign(x_dense.data(), x_dense.data() + x_dense.size());\n}\n",
      "\n    // Create a sparse matrix A in CSR format\n    std::vector<double> values;\n    std::vector<size_t> column_indices;\n    std::vector<size_t> row_pointers(N + 1, 0);\n\n    for (size_t i = 0; i < A.size(); ++i) {\n        COOElement element = A[i];\n        values.push_back(element.value);\n        column_indices.push_back(element.column);\n        row_pointers[element.row + 1]++;\n    }\n\n    for (size_t i = 1; i <= N; ++i) {\n        row_pointers[i] += row_pointers[i - 1];\n    }\n\n    // Solve the linear system using a suitable method, e.g., Gaussian elimination, LU decomposition, etc.\n    // Update the x vector with the solution\n}\n",
      "\n    // Create a sparse matrix in CSR format from the COO format\n    std::vector<double> val;\n    std::vector<size_t> colIndex;\n    std::vector<size_t> rowPtr;\n    rowPtr.push_back(0);\n    for (size_t i = 0; i < N; ++i) {\n        size_t rowStart = val.size();\n        for (size_t j = 0; j < N; ++j) {\n            bool found = false;\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    val.push_back(A[k].value);\n                    colIndex.push_back(j);\n                    found = true;\n                    break;\n                }\n            }\n            if (!found) {\n                val.push_back(0.0);\n                colIndex.push_back(j);\n            }\n        }\n        rowPtr.push_back(rowStart + N);\n    }\n\n    // Solve the linear system using a sparse solver library or algorithm\n    // For example, using Eigen library:\n    Eigen::SparseMatrix<double> A_sparse(N, N);\n    A_sparse.reserve(val.size());\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = rowPtr[i]; j < rowPtr[i + 1]; ++j) {\n            A_sparse.insert(i, colIndex[j], val[j]);\n        }\n    }\n    Eigen::VectorXd b_dense = Eigen::Map<const Eigen::VectorXd>(b.data(), N);\n    Eigen::SparseLU<Eigen::SparseMatrix<double>> solver;\n    solver.analyzePattern(A_sparse);\n    solver.factorize(A_sparse);\n    Eigen::VectorXd x_dense = solver.solve(b_dense);\n\n    // Convert the solution back to a dense vector\n    x.resize(N);\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = x_dense(i);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "45_sparse_la_sparse_solve",
    "parallelism_model": "hip",
    "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format with sizeA elements. x and b are dense vectors with N elements.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least sizeA threads.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\n__global__ void solveLinearSystem(const COOElement *A, size_t sizeA, const double *b, double *x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], val * b[row]);\n        }\n    }\n}\n",
      "\n    // Solve the linear system using AMD HIP\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&x[row], value * b[column]);\n    }\n}\n",
      "\n    // Implementation of solving linear system using AMD HIP\n    // Your code here\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0.0;\n        for (size_t i = 0; i < sizeA; ++i) {\n            if (A[i].row == tid) {\n                sum += A[i].value * x[A[i].column];\n            }\n        }\n        x[tid] = (b[tid] - sum) / A[tid * N + tid];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], val * b[row]);\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0.0;\n        for (size_t j = 0; j < sizeA; ++j) {\n            if (A[j].row == i) {\n                sum += A[j].value * x[A[j].column];\n            }\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0.0;\n        for (size_t i = 0; i < sizeA; ++i) {\n            if (A[i].row == tid) {\n                sum += A[i].value * x[A[i].column];\n            }\n        }\n        x[tid] = (b[tid] - sum) / A[tid * N + tid];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], val * b[row]);\n        }\n    }\n}\n",
      "\n    // Implementation of solving linear system using AMD HIP\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&x[row], value * b[column]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], val * b[row]);\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "45_sparse_la_sparse_solve",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and b. Store the result in x on rank 0.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(std::vector<COOElement> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // Implement the function body here\n    // Your code here\n}\n",
      "\n    // TODO: Implement the function body here\n}",
      "\n    // Your code here\n}\n",
      "\n    // Get the rank and size of the MPI communicator\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Calculate the number of rows each rank will process\n    size_t rows_per_rank = N / size;\n    size_t start_row = rank * rows_per_rank;\n    size_t end_row = (rank == size - 1) ? N : start_row + rows_per_rank;\n\n    // Perform the computation for the assigned rows\n    for (size_t i = start_row; i < end_row; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    sum += A[k].value * x[j];\n                }\n            }\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n\n    // Gather the results on rank 0\n    if (rank == 0) {\n        std::vector<double> recvbuf(N);\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&x[0] + i * rows_per_rank, rows_per_rank, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&x[start_row], rows_per_rank, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Implement the function body here\n    // Your implementation should use MPI to compute the solution in parallel\n    // Remember to store the result in x on rank 0\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        // ...\n        // Store the result in x\n    }\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Implement the function body here\n}",
      "\n    // Get the rank and size of the MPI communicator\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Calculate the number of rows to be processed by each rank\n    size_t rowsPerRank = N / size;\n    size_t startRow = rank * rowsPerRank;\n    size_t endRow = (rank == size - 1) ? N : startRow + rowsPerRank;\n\n    // Perform the parallel computation\n    for (size_t i = startRow; i < endRow; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < A.size(); ++k) {\n                if (A[k].row == i && A[k].column == j) {\n                    sum += A[k].value * x[j];\n                }\n            }\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n\n    // Gather the results on rank 0\n    if (rank == 0) {\n        std::vector<double> recvBuffer(N);\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&recvBuffer[0] + i * rowsPerRank, rowsPerRank, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        recvBuffer.insert(recvBuffer.begin(), x.begin(), x.end());\n        x = recvBuffer;\n    } else {\n        MPI_Send(&x[startRow], rowsPerRank, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // TODO: Implement the function body here\n}",
      "\n    // Implement the function body here\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        // ...\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        // ...\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        // ...\n        // Store the result in x\n    }\n}\n",
      "\n    // Implement the function body here\n    // Your code here\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "45_sparse_la_sparse_solve",
    "parallelism_model": "cuda",
    "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format with sizeA elements. x and b are dense vectors with N elements.\n   Use CUDA to compute in parallel. The kernel is launched with at least sizeA threads.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\n__global__ void solveLinearSystem(const COOElement *A, size_t sizeA, const double *b, double *x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], -val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], -val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], -val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&x[row], value * b[column]);\n        if (row != column) {\n            atomicAdd(&x[column], -value * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        \n        atomicAdd(&x[row], val * b[col]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], -val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], -val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], -val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], -val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&x[row], value * b[column]);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        int row = A[tid].row;\n        int col = A[tid].column;\n        double val = A[tid].value;\n        atomicAdd(&x[row], val * b[col]);\n        if (row != col) {\n            atomicAdd(&x[col], val * b[row]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t row = A[tid].row;\n        size_t column = A[tid].column;\n        double value = A[tid].value;\n        atomicAdd(&x[row], value * b[column]);\n        if (row != column) {\n            atomicAdd(&x[column], -value * b[row]);\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "45_sparse_la_sparse_solve",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and b. Store the result in x on rank 0.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(std::vector<COOElement> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                for (size_t k = 0; k < A.size(); ++k) {\n                    if (A[k].row == i && A[k].column == j) {\n                        sum += A[k].value * x[j];\n                    }\n                }\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n\n    // Broadcast the result to all ranks\n    MPI_Bcast(&x[0], N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                for (size_t k = 0; k < A.size(); ++k) {\n                    if (A[k].row == i && A[k].column == j) {\n                        sum += A[k].value * x[j];\n                    }\n                }\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n\n    // Broadcast the result to all ranks\n    MPI_Bcast(x.data(), N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                for (size_t k = 0; k < A.size(); ++k) {\n                    if (A[k].row == i && A[k].column == j) {\n                        sum += A[k].value * x[j];\n                    }\n                }\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n\n    // Broadcast the result to all ranks\n    MPI_Bcast(&x[0], N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                for (size_t k = 0; k < A.size(); ++k) {\n                    if (A[k].row == i && A[k].column == j) {\n                        sum += A[k].value * x[j];\n                    }\n                }\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n\n    // Broadcast the result from rank 0 to all other ranks\n    MPI_Bcast(x.data(), N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Assuming A and b are already distributed among ranks\n\n    // Perform parallel computation using OpenMP\n\n    if (rank == 0) {\n        // Gather the results from all ranks to rank 0\n        // Store the result in x\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                for (size_t k = 0; k < A.size(); ++k) {\n                    if (A[k].row == i && A[k].column == j) {\n                        sum += A[k].value * x[j];\n                    }\n                }\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                for (size_t k = 0; k < A.size(); ++k) {\n                    if (A[k].row == i && A[k].column == j) {\n                        sum += A[k].value * x[j];\n                    }\n                }\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n\n    // Broadcast the result from rank 0 to all other ranks\n    MPI_Bcast(&x[0], N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                for (size_t k = 0; k < A.size(); ++k) {\n                    if (A[k].row == i && A[k].column == j) {\n                        sum += A[k].value * x[j];\n                    }\n                }\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n\n    // Broadcast the result from rank 0 to all other ranks\n    MPI_Bcast(&x[0], N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                for (size_t k = 0; k < A.size(); ++k) {\n                    if (A[k].row == i && A[k].column == j) {\n                        sum += A[k].value * x[j];\n                    }\n                }\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n\n    // Broadcast the result to all ranks\n    MPI_Bcast(&x[0], N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Assuming A and b are already distributed among the ranks\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                for (size_t k = 0; k < A.size(); ++k) {\n                    if (A[k].row == i && A[k].column == j) {\n                        sum += A[k].value * x[j];\n                    }\n                }\n            }\n            x[i] = sum;\n        }\n    }\n\n    // Perform necessary communication and synchronization if needed\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                for (size_t k = 0; k < A.size(); ++k) {\n                    if (A[k].row == i && A[k].column == j) {\n                        sum += A[k].value * x[j];\n                    }\n                }\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                for (size_t k = 0; k < A.size(); ++k) {\n                    if (A[k].row == i && A[k].column == j) {\n                        sum += A[k].value * x[j];\n                    }\n                }\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n\n    // Broadcast the result to all ranks\n    MPI_Bcast(x.data(), N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                for (size_t k = 0; k < A.size(); ++k) {\n                    if (A[k].row == i && A[k].column == j) {\n                        sum += A[k].value * x[j];\n                    }\n                }\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n\n    // Broadcast the result to all ranks\n    MPI_Bcast(&x[0], N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                for (size_t k = 0; k < A.size(); ++k) {\n                    if (A[k].row == i && A[k].column == j) {\n                        sum += A[k].value * x[j];\n                    }\n                }\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Assuming A and b are already distributed among the ranks\n\n    // Perform parallel computation using OpenMP\n\n    if (rank == 0) {\n        // Gather the results from all ranks to rank 0\n        // Store the result in x on rank 0\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                for (size_t k = 0; k < A.size(); ++k) {\n                    if (A[k].row == i && A[k].column == j) {\n                        sum += A[k].value * x[j];\n                    }\n                }\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n\n    // Broadcast the result from rank 0 to all other ranks\n    MPI_Bcast(&x[0], N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                for (size_t k = 0; k < A.size(); ++k) {\n                    if (A[k].row == i && A[k].column == j) {\n                        sum += A[k].value * x[j];\n                    }\n                }\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n    // Broadcast the result to all ranks\n    MPI_Bcast(&x[0], N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                for (size_t k = 0; k < A.size(); ++k) {\n                    if (A[k].row == i && A[k].column == j) {\n                        sum += A[k].value * x[j];\n                    }\n                }\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                for (size_t k = 0; k < A.size(); ++k) {\n                    if (A[k].row == i && A[k].column == j) {\n                        sum += A[k].value * x[j];\n                    }\n                }\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n    // Broadcast the result to all ranks\n    MPI_Bcast(&x[0], N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                for (size_t k = 0; k < A.size(); ++k) {\n                    if (A[k].row == i && A[k].column == j) {\n                        sum += A[k].value * x[j];\n                    }\n                }\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n    // Broadcast the result to all ranks\n    MPI_Bcast(&x[0], N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "49_sparse_la_sparse_lu_decomp",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(Kokkos::View<const COOElement*> &A, Kokkos::View<double**> &L, Kokkos::View<double**> &U, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (size_t k = 0; k < N; k++) {\n        U(k, k) = A(k, k);\n        for (size_t i = k + 1; i < N; i++) {\n            L(i, k) = A(i, k) / U(k, k);\n            U(k, i) = A(k, i);\n        }\n        for (size_t i = k + 1; i < N; i++) {\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) -= L(i, k) * U(k, j);\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        U(k, k) = A(k, k);\n        for (size_t i = k + 1; i < N; i++) {\n            L(i, k) = A(i, k) / U(k, k);\n            U(k, i) = A(k, i);\n        }\n        for (size_t i = k + 1; i < N; i++) {\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) -= L(i, k) * U(k, j);\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        U(k, k) = A(k, k);\n        for (size_t i = k + 1; i < N; i++) {\n            L(i, k) = A(i, k) / U(k, k);\n            U(k, i) = A(k, i);\n        }\n        for (size_t i = k + 1; i < N; i++) {\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) = A(i, j) - L(i, k) * U(k, j);\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        U(k, k) = A(k, k);\n        for (size_t i = k + 1; i < N; i++) {\n            L(i, k) = A(i, k) / U(k, k);\n            U(k, i) = A(k, i);\n        }\n        for (size_t i = k + 1; i < N; i++) {\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) -= L(i, k) * U(k, j);\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; ++k) {\n        U(k, k) = A(k, k);\n        for (size_t i = k + 1; i < N; ++i) {\n            L(i, k) = A(i, k) / U(k, k);\n            U(k, i) = A(k, i);\n        }\n        for (size_t i = k + 1; i < N; ++i) {\n            for (size_t j = k + 1; j < N; ++j) {\n                A(i, j) = A(i, j) - L(i, k) * U(k, j);\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        U(k, k) = A(k, k);\n        for (size_t i = k + 1; i < N; i++) {\n            L(i, k) = A(i, k) / U(k, k);\n            U(k, i) = A(k, i);\n        }\n        for (size_t i = k + 1; i < N; i++) {\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) -= L(i, k) * U(k, j);\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        U(k, k) = A(k, k);\n        for (size_t i = k + 1; i < N; i++) {\n            L(i, k) = A(i, k) / U(k, k);\n            U(k, i) = A(k, i);\n        }\n        for (size_t i = k + 1; i < N; i++) {\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) -= L(i, k) * U(k, j);\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        U(k, k) = A(k, k);\n        for (size_t i = k + 1; i < N; i++) {\n            L(i, k) = A(i, k) / U(k, k);\n            U(k, i) = A(k, i);\n        }\n        for (size_t i = k + 1; i < N; i++) {\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) -= L(i, k) * U(k, j);\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        U(k, k) = A(k, k);\n        for (size_t i = k + 1; i < N; i++) {\n            L(i, k) = A(i, k) / U(k, k);\n            U(k, i) = A(k, i);\n        }\n        for (size_t i = k + 1; i < N; i++) {\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) -= L(i, k) * U(k, j);\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        U(k, k) = A(k, k);\n        for (size_t i = k + 1; i < N; i++) {\n            L(i, k) = A(i, k) / U(k, k);\n            U(k, i) = A(k, i);\n        }\n        for (size_t i = k + 1; i < N; i++) {\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) -= L(i, k) * U(k, j);\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        U(k, k) = A(k, k);\n        for (size_t i = k + 1; i < N; i++) {\n            L(i, k) = A(i, k) / U(k, k);\n            U(k, i) = A(k, i);\n        }\n        for (size_t i = k + 1; i < N; i++) {\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) = A(i, j) - L(i, k) * U(k, j);\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        U(k, k) = A(k, k);\n        for (size_t i = k + 1; i < N; i++) {\n            L(i, k) = A(i, k) / U(k, k);\n            U(k, i) = A(k, i);\n        }\n        for (size_t i = k + 1; i < N; i++) {\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) -= L(i, k) * U(k, j);\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        U(k, k) = A(k, k);\n        for (size_t i = k + 1; i < N; i++) {\n            L(i, k) = A(i, k) / U(k, k);\n            U(k, i) = A(k, i);\n        }\n        for (size_t i = k + 1; i < N; i++) {\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) = A(i, j) - L(i, k) * U(k, j);\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        U(k, k) = A(k, k);\n        for (size_t i = k + 1; i < N; i++) {\n            L(i, k) = A(i, k) / U(k, k);\n            U(k, i) = A(k, i);\n        }\n        for (size_t i = k + 1; i < N; i++) {\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) -= L(i, k) * U(k, j);\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        U(k, k) = A(k, k);\n        for (size_t i = k + 1; i < N; i++) {\n            L(i, k) = A(i, k) / U(k, k);\n            U(k, i) = A(k, i);\n        }\n        for (size_t i = k + 1; i < N; i++) {\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) -= L(i, k) * U(k, j);\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        U(k, k) = A(k, k);\n        for (size_t i = k + 1; i < N; i++) {\n            L(i, k) = A(i, k) / U(k, k);\n            U(k, i) = A(k, i);\n        }\n        for (size_t i = k + 1; i < N; i++) {\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) -= L(i, k) * U(k, j);\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        U(k, k) = A(k, k);\n        for (size_t i = k + 1; i < N; i++) {\n            L(i, k) = A(i, k) / U(k, k);\n            U(k, i) = A(k, i);\n        }\n        for (size_t i = k + 1; i < N; i++) {\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) -= L(i, k) * U(k, j);\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        U(k, k) = A(k, k);\n        for (size_t i = k + 1; i < N; i++) {\n            L(i, k) = A(i, k) / U(k, k);\n            U(k, i) = A(k, i);\n        }\n        for (size_t i = k + 1; i < N; i++) {\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) -= L(i, k) * U(k, j);\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        U(k, k) = A(k, k);\n        for (size_t i = k + 1; i < N; i++) {\n            L(i, k) = A(i, k) / U(k, k);\n            U(k, i) = A(k, i);\n        }\n        for (size_t i = k + 1; i < N; i++) {\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) -= L(i, k) * U(k, j);\n            }\n        }\n    }\n}\n",
      "\n  for (size_t k = 0; k < N; k++) {\n      U(k, k) = A(k, k);\n      for (size_t i = k + 1; i < N; i++) {\n          L(i, k) = A(i, k) / U(k, k);\n          U(k, i) = A(k, i);\n      }\n      for (size_t i = k + 1; i < N; i++) {\n          for (size_t j = k + 1; j < N; j++) {\n              A(i, j) = A(i, j) - L(i, k) * U(k, j);\n          }\n      }\n  }\n```\n}"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "49_sparse_la_sparse_lu_decomp",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (i == j) {\n                L[i * N + j] = 1.0;\n                U[i * N + j] = A[i * N + j].value;\n            } else if (i < j) {\n                U[i * N + j] = A[i * N + j].value;\n            } else {\n                L[i * N + j] = A[i * N + j].value / U[j * N + j];\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (i == j) {\n                L[i * N + j] = 1.0;\n                U[i * N + j] = A[i * N + j].value;\n            } else if (i < j) {\n                U[i * N + j] = A[i * N + j].value;\n            } else {\n                L[i * N + j] = A[i * N + j].value / U[j * N + j];\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (i == j) {\n                L[i * N + j] = 1.0;\n                U[i * N + j] = A[i * N + j].value;\n            } else if (i < j) {\n                U[i * N + j] = A[i * N + j].value;\n            } else {\n                L[i * N + j] = A[i * N + j].value / U[j * N + j];\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (i == j) {\n                L[i * N + j] = 1.0;\n                U[i * N + j] = A[i * N + j].value;\n            } else if (i < j) {\n                U[i * N + j] = A[i * N + j].value;\n            } else {\n                L[i * N + j] = A[i * N + j].value / U[j * N + j];\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (i == j) {\n                L[i * N + j] = 1.0;\n                U[i * N + j] = A[i * N + j].value;\n            } else if (i < j) {\n                U[i * N + j] = A[i * N + j].value;\n            } else {\n                L[i * N + j] = A[i * N + j].value / U[j * N + j];\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (i == j) {\n                L[i * N + j] = 1.0;\n                U[i * N + j] = A[i * N + j].value;\n            } else if (i < j) {\n                U[i * N + j] = A[i * N + j].value;\n            } else {\n                L[i * N + j] = A[i * N + j].value / U[j * N + j];\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (i == j) {\n                L[i * N + j] = 1.0;\n                U[i * N + j] = A[i * N + j].value;\n            } else if (i < j) {\n                U[i * N + j] = A[i * N + j].value;\n            } else {\n                L[i * N + j] = A[i * N + j].value / U[j * N + j];\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (i == j) {\n                L[i * N + j] = 1.0;\n                U[i * N + j] = A[i * N + j].value;\n            } else if (i < j) {\n                U[i * N + j] = A[i * N + j].value;\n            } else {\n                L[i * N + j] = A[i * N + j].value / U[j * N + j];\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        L.push_back(1.0);  // Diagonal elements of L are 1\n        U.push_back(A[i * N + i].value);  // Diagonal elements of U are the same as A\n    }\n\n    for (size_t i = 0; i < N; i++) {\n        #pragma omp parallel for\n        for (size_t j = i + 1; j < N; j++) {\n            L[j * N + i] = A[j * N + i].value / U[i * N + i];  // Compute the elements of L\n            U[i * N + j] = A[i * N + j].value;  // Compute the elements of U\n        }\n\n        #pragma omp parallel for\n        for (size_t j = i + 1; j < N; j++) {\n            for (size_t k = i + 1; k < N; k++) {\n                A[j * N + k].value -= L[j * N + i] * U[i * N + k];  // Update the remaining elements of A\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        // Compute U values\n        for (size_t j = i; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < i; ++k) {\n                sum += L[i * N + k] * U[k * N + j];\n            }\n            U[i * N + j] = A[i * N + j].value - sum;\n        }\n\n        // Compute L values\n        for (size_t j = i; j < N; ++j) {\n            if (i == j) {\n                L[i * N + i] = 1.0;\n            } else {\n                double sum = 0.0;\n                for (size_t k = 0; k < i; ++k) {\n                    sum += L[j * N + k] * U[k * N + i];\n                }\n                L[j * N + i] = (A[j * N + i].value - sum) / U[i * N + i];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            if (i > j) {\n                double sum = 0;\n                for (size_t k = 0; k < j; k++) {\n                    sum += L[i * N + k] * U[k * N + j];\n                }\n                L[i * N + j] = (A[i * N + j].value - sum) / U[j * N + j];\n            } else {\n                double sum = 0;\n                for (size_t k = 0; k < i; k++) {\n                    sum += L[i * N + k] * U[k * N + j];\n                }\n                U[i * N + j] = A[i * N + j].value - sum;\n            }\n        }\n    }\n```\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (i == j) {\n                L[i * N + j] = 1.0;\n                U[i * N + j] = A[i * N + j].value;\n            } else if (i < j) {\n                U[i * N + j] = A[i * N + j].value;\n            } else {\n                L[i * N + j] = A[i * N + j].value / U[j * N + j];\n            }\n        }\n        #pragma omp parallel for\n        for (size_t j = i + 1; j < N; ++j) {\n            for (size_t k = i + 1; k < N; ++k) {\n                A[j * N + k].value -= L[j * N + i] * U[i * N + k];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (i == j) {\n                L[i * N + j] = 1.0;\n                U[i * N + j] = A[i * N + j].value;\n            } else if (i < j) {\n                U[i * N + j] = A[i * N + j].value;\n            } else {\n                L[i * N + j] = A[i * N + j].value / U[j * N + j];\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0);  // Diagonal elements of L are 1\n        U.push_back(A[i * N + i].value);  // Diagonal elements of U are the diagonal elements of A\n    }\n\n    for (size_t i = 0; i < N; ++i) {\n        #pragma omp parallel for\n        for (size_t j = i + 1; j < N; ++j) {\n            L[j * N + i] = A[j * N + i].value / U[i * N + i];  // Compute the elements of L\n            U[i * N + j] = A[i * N + j].value;  // Compute the elements of U\n        }\n\n        #pragma omp parallel for\n        for (size_t j = i + 1; j < N; ++j) {\n            for (size_t k = i + 1; k < N; ++k) {\n                A[j * N + k].value -= L[j * N + i] * U[i * N + k];  // Update the remaining elements of A\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0); // Diagonal elements of L are 1\n        for (size_t j = i; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < i; ++k) {\n                sum += L[i * N + k] * U[k * N + j];\n            }\n            U.push_back(j < i ? 0.0 : A[i * N + j].value - sum);\n            sum = 0.0;\n            for (size_t k = 0; k < i; ++k) {\n                sum += L[j * N + k] * U[k * N + i];\n            }\n            L.push_back(j < i ? 0.0 : (A[j * N + i].value - sum) / U[i * N + i]);\n        }\n    }\n```\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0); // Diagonal elements of L are 1\n        U.push_back(A[i * N + i].value); // Diagonal elements of U are the same as A\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = i + 1; j < N; ++j) {\n            size_t index = i * N + j;\n            double multiplier = A[index].value / U[i * N + i];\n            L.push_back(multiplier);\n            U.push_back(A[index].value);\n            for (size_t k = i + 1; k < N; ++k) {\n                A[k * N + j].value -= multiplier * A[k * N + i].value;\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (i == j) {\n                L[i * N + j] = 1.0;\n                U[i * N + j] = A[i * N + j].value;\n            } else if (i > j) {\n                L[i * N + j] = A[i * N + j].value / U[j * N + j];\n                U[i * N + j] = 0.0;\n            } else {\n                L[i * N + j] = 0.0;\n                U[i * N + j] = A[i * N + j].value - L[i * N + j] * U[j * N + j];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (i == j) {\n                L[i * N + j] = 1.0;\n                U[i * N + j] = A[i * N + j].value;\n            } else if (i < j) {\n                U[i * N + j] = A[i * N + j].value;\n            } else {\n                L[i * N + j] = A[i * N + j].value / U[j * N + j];\n            }\n        }\n        #pragma omp parallel for\n        for (size_t j = i + 1; j < N; ++j) {\n            for (size_t k = i + 1; k < N; ++k) {\n                A[j * N + k].value -= L[j * N + i] * U[i * N + k];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (i == j) {\n                L[i * N + j] = 1.0;\n                U[i * N + j] = A[i * N + j].value;\n            } else if (i > j) {\n                double sum = 0.0;\n                for (size_t k = 0; k < j; ++k) {\n                    sum += L[i * N + k] * U[k * N + j];\n                }\n                L[i * N + j] = (A[i * N + j].value - sum) / U[j * N + j];\n                U[i * N + j] = 0.0;\n            } else {\n                double sum = 0.0;\n                for (size_t k = 0; k < i; ++k) {\n                    sum += L[i * N + k] * U[k * N + j];\n                }\n                U[i * N + j] = A[i * N + j].value - sum;\n                L[i * N + j] = 0.0;\n            }\n        }\n    }\n```\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            if (i == j) {\n                L[i * N + j] = 1.0;\n                U[i * N + j] = A[i * N + j].value;\n            } else if (i < j) {\n                U[i * N + j] = A[i * N + j].value;\n            } else {\n                L[i * N + j] = A[i * N + j].value / U[j * N + j];\n            }\n        }\n        #pragma omp parallel for\n        for (size_t j = i + 1; j < N; ++j) {\n            for (size_t k = i + 1; k < N; ++k) {\n                A[j * N + k].value -= L[j * N + i] * U[i * N + k];\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "49_sparse_la_sparse_lu_decomp",
    "parallelism_model": "serial",
    "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0);  // Diagonal elements of L are 1\n        U.push_back(A[i * N + i]);  // Diagonal elements of U are the same as A\n    }\n\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = i + 1; j < N; ++j) {\n            L[j * N + i] = A[j * N + i] / U[i * N + i];  // Compute the elements of L\n            for (size_t k = i + 1; k < N; ++k) {\n                A[j * N + k] -= L[j * N + i] * U[i * N + k];  // Update the elements of A\n            }\n        }\n        for (size_t j = i + 1; j < N; ++j) {\n            U[i * N + j] = A[i * N + j];  // Compute the elements of U\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0);  // Diagonal elements of L are 1\n        U.push_back(A[i * N + i]);  // Diagonal elements of U are the same as A\n        for (size_t j = i + 1; j < N; ++j) {\n            L.push_back(A[j * N + i] / U[i * N + i]);  // Calculate the elements of L\n            U.push_back(A[i * N + j]);  // Calculate the elements of U\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0);  // Diagonal elements of L are 1\n        U.push_back(A[i * N + i]);  // Diagonal elements of U are the same as A\n\n        for (size_t j = i + 1; j < N; ++j) {\n            L.push_back(A[j * N + i] / U[i * N + i]);  // Compute the elements of L\n            U.push_back(A[i * N + j]);  // Compute the elements of U\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0);  // Diagonal of L is 1\n        U.push_back(A[i * N + i].value);  // Diagonal of U is the diagonal of A\n        for (size_t j = i + 1; j < N; ++j) {\n            L.push_back(A[j * N + i].value / U[i * N + i]);  // Compute the values of L below the diagonal\n            U.push_back(A[i * N + j].value);  // Compute the values of U above the diagonal\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0);  // Diagonal elements of L are 1\n        U.push_back(A[i * N + i]);  // Diagonal elements of U are the same as A\n        for (size_t j = i + 1; j < N; ++j) {\n            L.push_back(A[j * N + i] / U[i * N + i]);  // Calculate the elements of L\n            U.push_back(A[i * N + j]);  // Calculate the elements of U\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0); // Diagonal elements of L are 1\n        U.push_back(A[i * N + i].value); // Diagonal elements of U are the same as A\n    }\n\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = i + 1; j < N; ++j) {\n            double factor = A[j * N + i].value / U[i * N + i];\n            L.push_back(factor);\n            U.push_back(A[j * N + j].value - factor * A[j * N + i].value);\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0); // Diagonal elements of L are 1\n        U.push_back(A[i * N + i].value); // Diagonal elements of U are the same as A\n    }\n\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = i + 1; j < N; ++j) {\n            size_t index = i * N + j;\n            double multiplier = A[index].value / U[i * N + i];\n            L[index] = multiplier;\n            U[index] = A[index].value;\n\n            for (size_t k = i + 1; k < N; ++k) {\n                A[k * N + j].value -= multiplier * U[i * N + k];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0);  // Diagonal elements of L are 1\n        U.push_back(A[i * N + i]);  // Diagonal elements of U are the same as A\n        for (size_t j = i + 1; j < N; ++j) {\n            L.push_back(A[j * N + i] / U[i * N + i]);  // Compute the elements of L\n            U.push_back(A[i * N + j]);  // Compute the elements of U\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0);  // Diagonal elements of L are 1\n        U.push_back(A[i * N + i]);  // Diagonal elements of U are the same as A\n\n        for (size_t j = i + 1; j < N; ++j) {\n            L.push_back(A[j * N + i] / U[i * N + i]);  // Compute the elements of L\n            U.push_back(A[i * N + j]);  // Compute the elements of U\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0);  // Diagonal elements of L are 1\n        U.push_back(A[i * N + i]);  // Diagonal elements of U are the same as A\n\n        for (size_t j = i + 1; j < N; ++j) {\n            L.push_back(A[j * N + i] / U[i * N + i]);  // Compute the elements of L\n            U.push_back(A[i * N + j]);  // Compute the elements of U\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0);  // Diagonal elements of L are 1\n        U.push_back(A[i * N + i]);  // Diagonal elements of U are the same as A\n\n        for (size_t j = i + 1; j < N; ++j) {\n            L.push_back(A[j * N + i] / U[i * N + i]);  // Compute the elements of L\n            U.push_back(A[i * N + j]);  // Compute the elements of U\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0);  // Diagonal elements of L are 1\n        U.push_back(A[i * N + i]);  // Diagonal elements of U are the same as A\n\n        for (size_t j = i + 1; j < N; ++j) {\n            L.push_back(A[j * N + i] / U[i * N + i]);  // Calculate the elements of L\n            U.push_back(A[i * N + j]);  // Calculate the elements of U\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0);  // Diagonal elements of L are 1\n        U.push_back(A[i * N + i]);  // Diagonal elements of U are the same as A\n\n        for (size_t j = i + 1; j < N; ++j) {\n            L.push_back(A[j * N + i] / U[i * N + i]);  // Calculate the elements of L\n            U.push_back(A[i * N + j]);  // Calculate the elements of U\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0);  // Diagonal of L is 1\n        U.push_back(A[i * N + i].value);  // Diagonal of U is the diagonal of A\n\n        for (size_t j = i + 1; j < N; ++j) {\n            double factor = A[j * N + i].value / U[i];  // Compute the factor for the current column\n            L.push_back(factor);  // Store the factor in L\n            U.push_back(A[i * N + j].value);  // Store the corresponding value in U\n\n            for (size_t k = i + 1; k < N; ++k) {\n                A[j * N + k].value -= factor * A[i * N + k].value;  // Update the remaining elements of A\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0);  // Diagonal elements of L are 1\n        U.push_back(A[i * N + i]);  // Diagonal elements of U are the same as A\n        for (size_t j = i + 1; j < N; ++j) {\n            L.push_back(A[j * N + i] / U[i * N + i]);  // Compute the elements of L\n            U.push_back(A[i * N + j]);  // Compute the elements of U\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0);  // Diagonal of L is 1\n        U.push_back(A[i * N + i].value);  // Diagonal of U is the diagonal of A\n        for (size_t j = i + 1; j < N; ++j) {\n            double factor = A[j * N + i].value / U[i];  // Compute the factor for the current column\n            L.push_back(factor);  // Store the factor in L\n            U.push_back(A[i * N + j].value);  // Store the corresponding value in U\n            for (size_t k = i + 1; k < N; ++k) {\n                A[j * N + k].value -= factor * A[i * N + k].value;  // Update the remaining elements of A\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0);  // Diagonal elements of L are 1\n        U.push_back(A[i * N + i].value);  // Diagonal elements of U are the corresponding elements of A\n\n        for (size_t j = i + 1; j < N; ++j) {\n            L.push_back(A[j * N + i].value / U[i * N + i]);  // Compute the elements of L\n            U.push_back(A[i * N + j].value);  // Compute the elements of U\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0);  // Diagonal elements of L are 1\n        U.push_back(A[i * N + i].value);  // Diagonal elements of U are the same as A\n        for (size_t j = i + 1; j < N; ++j) {\n            L.push_back(A[j * N + i].value / U[i * N + i]);  // Compute elements of L\n            U.push_back(A[i * N + j].value);  // Compute elements of U\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0);  // Diagonal elements of L are 1\n        U.push_back(A[i * N + i].value);  // Diagonal elements of U are the same as A\n    }\n\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = i + 1; j < N; ++j) {\n            L[j * N + i] = A[j * N + i].value / U[i * N + i];  // Compute the elements of L\n            U[i * N + j] = A[i * N + j].value;  // Compute the elements of U\n        }\n        for (size_t j = i + 1; j < N; ++j) {\n            for (size_t k = i + 1; k < N; ++k) {\n                A[j * N + k].value -= L[j * N + i] * U[i * N + k];  // Update the remaining elements of A\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        L.push_back(1.0); // Diagonal elements of L are 1\n        U.push_back(A[i * N + i].value); // Diagonal elements of U are the same as A\n    }\n\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = i + 1; j < N; ++j) {\n            double factor = A[j * N + i].value / U[i * N + i];\n            L.push_back(factor); // Store the factor in L\n            U.push_back(A[i * N + j].value); // Store the corresponding value in U\n            for (size_t k = i + 1; k < N; ++k) {\n                A[j * N + k].value -= factor * A[i * N + k].value; // Update the remaining elements of A\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "49_sparse_la_sparse_lu_decomp",
    "parallelism_model": "hip",
    "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format with sizeA elements.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least sizeA threads.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\n__global__ void luFactorize(const COOElement *A, size_t sizeA, double *L, double *U, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // Your code here\n}\n",
      "\n    // Write the body of the function here\n}\n",
      "\n    // Write the body of the function here\n}\n",
      "\n    // Write the body of the function here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Implement LU factorization using AMD HIP\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Write the body of the function here\n}\n",
      "\n    // Write the body of the function here\n}\n",
      "\n    // Write the body of the function here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Write the body of the function here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code goes here\n\n}\n",
      "\n    // Write the body of the function here\n}\n",
      "\n    // Your code goes here\n}\n",
      "\n    // Write the body of the function luFactorize here\n    // Your code goes here\n}\n",
      "\n    // Write the body of the function here\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "49_sparse_la_sparse_lu_decomp",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. Store the result in L and U on rank 0.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // Write your code here\n}\n",
      "\n    // Implement LU factorization algorithm here\n    // ...\n}\n",
      "\n    // Write your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Implement LU factorization algorithm here\n    // Your code here\n}\n",
      "\n    // Implement LU factorization algorithm here\n}",
      "\n    // Implement LU factorization algorithm here\n    // This function should compute the LU factorization of the input sparse matrix A\n    // and store the result in the vectors L and U\n}",
      "\n    // Implement LU factorization algorithm here\n    // ...\n}\n",
      "\n    // Write your code here\n}\n",
      "\n    // Implement LU factorization algorithm here\n    // This function should compute the LU factorization of the input sparse matrix A and store the result in L and U\n    // Use MPI to compute in parallel and assume MPI has already been initialized\n    // Every rank should have a complete copy of A and the result should be stored in L and U on rank 0\n}",
      "\n    // Write your code here\n}\n",
      "\n    // Implement LU factorization algorithm here\n    // ...\n}\n",
      "\n    // Implement LU factorization algorithm here\n    // This function should compute the LU factorization of the input sparse matrix A and store the result in L and U\n    // Use MPI to compute in parallel and store the result in L and U on rank 0\n    // Remember to handle the case when A is not factorizable\n    // You can use any suitable algorithm for LU factorization, such as Gaussian elimination with partial pivoting\n    // Make sure to handle the sparse matrix format and parallel computation using MPI\n}",
      "\n    // Implement LU factorization algorithm here\n    // This function should compute the LU factorization of the input sparse matrix A and store the result in L and U\n    // Use MPI to compute in parallel and assume MPI has already been initialized\n    // Every rank should have a complete copy of A and store the result in L and U on rank 0\n}",
      "\n    // Write your code here\n}\n",
      "\n    // Implement LU factorization algorithm here\n}",
      "\n    // Write the body of the function here\n}\n",
      "\n    // Write your code here\n    // Your code should implement LU factorization using MPI to compute in parallel\n}",
      "\n    // Write your code here\n    // Your code should implement LU factorization using MPI to compute in parallel\n}",
      "\n    // Write the body of the function here\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "49_sparse_la_sparse_lu_decomp",
    "parallelism_model": "cuda",
    "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format with sizeA elements.\n   Use CUDA to compute in parallel. The kernel is launched with at least sizeA threads.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\n__global__ void luFactorize(const COOElement *A, size_t sizeA, double *L, double *U, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        for (int j = 0; j < N; j++) {\n            if (j < i) {\n                double sum = 0;\n                for (int k = 0; k < j; k++) {\n                    sum += L[i * N + k] * U[k * N + j];\n                }\n                L[i * N + j] = (A[i * N + j].value - sum) / U[j * N + j];\n            } else {\n                double sum = 0;\n                for (int k = 0; k < i; k++) {\n                    sum += L[i * N + k] * U[k * N + j];\n                }\n                U[i * N + j] = A[i * N + j].value - sum;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t i = A[tid].row;\n        size_t j = A[tid].column;\n        if (i == j) {\n            L[tid] = 1.0;\n            U[tid] = A[tid].value;\n        } else if (i < j) {\n            L[tid] = 0.0;\n            U[tid] = A[tid].value;\n        } else {\n            L[tid] = A[tid].value / U[j * N + j];\n            U[tid] = A[tid].value;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t i = A[tid].row;\n        size_t j = A[tid].column;\n        if (i == j) {\n            L[tid] = 1.0;\n            U[tid] = A[tid].value;\n        } else if (i < j) {\n            L[tid] = 0.0;\n            U[tid] = A[tid].value;\n        } else {\n            L[tid] = A[tid].value / U[j * N + j];\n            U[tid] = 0.0;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t i = A[tid].row;\n        size_t j = A[tid].column;\n        double val = A[tid].value;\n\n        if (i == j) {\n            L[tid] = 1.0;\n            U[tid] = val;\n        } else if (i < j) {\n            L[tid] = 0.0;\n            U[tid] = val;\n        } else {\n            L[tid] = val / U[j * N + j];\n            U[tid] = val;\n        }\n    }\n}\n",
      "\n    int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        for (int j = 0; j < N; j++) {\n            if (j < i) {\n                L[i * N + j] = A[i * N + j].value;\n                U[j * N + i] = 0;\n            } else if (j == i) {\n                L[i * N + j] = 1;\n                U[i * N + j] = A[i * N + j].value;\n            } else {\n                L[i * N + j] = 0;\n                U[i * N + j] = A[i * N + j].value;\n            }\n        }\n    }\n}\n",
      "\n    int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        for (int j = 0; j <= i; j++) {\n            double sum = 0;\n            for (int k = 0; k < j; k++) {\n                sum += L[i * N + k] * U[k * N + j];\n            }\n            if (i == j) {\n                U[i * N + j] = 1;\n                L[i * N + j] = A[i * N + j] - sum;\n            } else {\n                L[i * N + j] = (A[i * N + j] - sum) / U[j * N + j];\n                U[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t i = A[tid].row;\n        size_t j = A[tid].column;\n        double val = A[tid].value;\n\n        if (i == j) {\n            L[tid] = 1.0;\n            U[tid] = val;\n        } else if (i < j) {\n            L[tid] = 0.0;\n            U[tid] = val;\n        } else {\n            L[tid] = val / U[j * N + j];\n            U[tid] = 0.0;\n        }\n    }\n}\n",
      "\n    int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        for (int j = 0; j < N; j++) {\n            if (i == j) {\n                L[i * N + j] = 1.0;\n                U[i * N + j] = A[i * N + j].value;\n            } else if (i < j) {\n                U[i * N + j] = A[i * N + j].value;\n            } else {\n                L[i * N + j] = A[i * N + j].value / U[j * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        if (A[tid].row == A[tid].column) {\n            L[tid] = 1.0;\n            U[tid] = A[tid].value;\n        } else if (A[tid].row < A[tid].column) {\n            L[tid] = 0.0;\n            U[tid] = A[tid].value;\n        } else {\n            L[tid] = A[tid].value / U[A[tid].column * N + A[tid].column];\n            U[tid] = A[tid].value;\n        }\n    }\n}\n",
      "\n    int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        for (int j = 0; j < N; j++) {\n            if (j < i) {\n                double sum = 0;\n                for (int k = 0; k < j; k++) {\n                    sum += L[i * N + k] * U[k * N + j];\n                }\n                L[i * N + j] = (A[i * N + j].value - sum) / U[j * N + j];\n            } else {\n                double sum = 0;\n                for (int k = 0; k < i; k++) {\n                    sum += L[i * N + k] * U[k * N + j];\n                }\n                U[i * N + j] = A[i * N + j].value - sum;\n            }\n        }\n    }\n}\n",
      "\n    int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        for (int j = 0; j <= i; j++) {\n            double sum = 0;\n            for (int k = 0; k < j; k++) {\n                sum += L[i * N + k] * U[k * N + j];\n            }\n            if (i == j) {\n                U[i * N + j] = 1;\n                L[i * N + j] = A[i * N + j] - sum;\n            } else {\n                L[i * N + j] = (A[i * N + j] - sum) / U[j * N + j];\n                U[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        for (int j = 0; j <= i; j++) {\n            double sum = 0;\n            for (int k = 0; k < j; k++) {\n                sum += L[i * N + k] * U[k * N + j];\n            }\n            if (i == j) {\n                U[i * N + j] = 1;\n                L[i * N + j] = A[i * N + j] - sum;\n            } else {\n                L[i * N + j] = (A[i * N + j] - sum) / U[j * N + j];\n                U[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        for (int j = 0; j < N; j++) {\n            if (i == j) {\n                L[i * N + j] = 1.0;\n                U[i * N + j] = A[i * N + j].value;\n            } else if (i < j) {\n                U[i * N + j] = A[i * N + j].value;\n            } else {\n                L[i * N + j] = A[i * N + j].value / U[j * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        for (int j = 0; j < N; j++) {\n            if (j < i) {\n                L[i * N + j] = A[i * N + j].value;\n                U[j * N + i] = 0;\n            } else if (j == i) {\n                L[i * N + j] = 1;\n                U[i * N + j] = A[i * N + j].value;\n            } else {\n                L[i * N + j] = 0;\n                U[i * N + j] = A[i * N + j].value;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        if (A[tid].row == A[tid].column) {\n            L[tid] = 1.0;\n            U[tid] = A[tid].value;\n        } else if (A[tid].row < A[tid].column) {\n            L[tid] = 0.0;\n            U[tid] = A[tid].value;\n        } else {\n            L[tid] = A[tid].value / U[A[tid].column * N + A[tid].column];\n            U[tid] = A[tid].value;\n        }\n    }\n}\n",
      "\n    int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        for (int j = 0; j < N; j++) {\n            if (i == j) {\n                L[i * N + j] = 1.0;\n                U[i * N + j] = A[i * N + j].value;\n            } else if (i < j) {\n                U[i * N + j] = A[i * N + j].value;\n            } else {\n                L[i * N + j] = A[i * N + j].value / U[j * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        for (int j = 0; j < N; j++) {\n            if (j < i) {\n                double sum = 0;\n                for (int k = 0; k < j; k++) {\n                    sum += L[i * N + k] * U[k * N + j];\n                }\n                L[i * N + j] = (A[i * N + j].value - sum) / U[j * N + j];\n            } else {\n                double sum = 0;\n                for (int k = 0; k < i; k++) {\n                    sum += L[i * N + k] * U[k * N + j];\n                }\n                U[i * N + j] = A[i * N + j].value - sum;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t i = A[tid].row;\n        size_t j = A[tid].column;\n        if (i == j) {\n            L[tid] = 1.0;\n            U[tid] = A[tid].value;\n        } else if (i < j) {\n            L[tid] = 0.0;\n            U[tid] = A[tid].value;\n        } else {\n            L[tid] = A[tid].value / U[j * N + j];\n            U[tid] = A[tid].value;\n        }\n    }\n}\n",
      "\n    int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        for (int j = 0; j <= i; j++) {\n            double sum = 0;\n            for (int k = 0; k < j; k++) {\n                sum += L[i * N + k] * U[k * N + j];\n            }\n            L[i * N + j] = (i == j) ? 1 : (A[i * N + j] - sum) / U[j * N + j];\n        }\n        for (int j = i; j < N; j++) {\n            double sum = 0;\n            for (int k = 0; k < i; k++) {\n                sum += L[i * N + k] * U[k * N + j];\n            }\n            U[i * N + j] = A[i * N + j] - sum;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < sizeA) {\n        size_t i = A[tid].row;\n        size_t j = A[tid].column;\n        if (i == j) {\n            L[tid] = 1.0;\n            U[tid] = A[tid].value;\n        } else if (i < j) {\n            L[tid] = 0.0;\n            U[tid] = A[tid].value;\n        } else {\n            L[tid] = A[tid].value / U[j * N + j];\n            U[tid] = A[tid].value;\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "49_sparse_la_sparse_lu_decomp",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. Store the result in L and U on rank 0.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // Implement LU factorization algorithm here using MPI and OpenMP for parallel computation\n    // Remember to store the result in L and U on rank 0\n}\n",
      "\n    // Implement LU factorization algorithm using MPI and OpenMP\n    // This is a placeholder for the implementation\n    // Your code here\n}\n",
      "\n    // Write the body of the function here\n}\n",
      "\n    // Implement LU factorization algorithm using MPI and OpenMP\n    // This is the body of the function luFactorize\n    // Your code goes here\n}\n",
      "\n    // Implement LU factorization algorithm using MPI and OpenMP\n    // This is the body of the function luFactorize\n    // Your implementation here\n}\n",
      "\n    // Implement LU factorization algorithm using MPI and OpenMP\n    // This is the body of the function luFactorize\n    // Your code goes here\n}\n",
      "\n    // Implement LU factorization algorithm using MPI and OpenMP\n    // This is the body of the function luFactorize\n    // Your code goes here\n}\n",
      "\n    // Implement LU factorization algorithm here using MPI and OpenMP\n    // Remember to handle parallelization and communication between MPI ranks\n}\n",
      "\n    // Implement LU factorization algorithm using MPI and OpenMP\n    // This function should be completed with the specific implementation details\n}\n",
      "\n    // Implement LU factorization algorithm using MPI and OpenMP\n    // This is the body of the function luFactorize\n}\n",
      "\n    // Implement LU factorization algorithm here using MPI and OpenMP for parallel computation\n    // Remember to store the result in L and U on rank 0\n}\n",
      "\n    // Implement LU factorization algorithm using MPI and OpenMP\n    // This is the body of the function\n}\n",
      "\n    // Implement LU factorization algorithm using MPI and OpenMP\n    // This is the body of the function luFactorize\n    // Your code goes here\n}\n",
      "\n    // Implement LU factorization algorithm using MPI and OpenMP\n    // This is the body of the function luFactorize\n    // Your code goes here\n}\n",
      "\n    // Implement LU factorization algorithm here\n    // Use MPI and OpenMP to compute in parallel\n    // Store the result in L and U on rank 0\n    // Assume MPI has already been initialized\n    // Every rank has a complete copy of A\n    // N is the size of the matrix\n}\n",
      "\n    // Write the body of the function here\n}\n",
      "\n    // Implement LU factorization algorithm using MPI and OpenMP\n    // This function should compute the LU factorization of the sparse matrix A and store the result in L and U on rank 0\n    // The algorithm should be parallelized using MPI and OpenMP\n    // The implementation should follow the given example and assumptions\n}\n",
      "\n    // Implement LU factorization algorithm here using MPI and OpenMP for parallel computation\n    // Remember to store the result in L and U on rank 0\n}\n",
      "\n    // Write the body of the function here\n}\n",
      "\n    // Implement the LU factorization algorithm using MPI and OpenMP\n    // This function should distribute the work among MPI ranks and use OpenMP for parallel computation\n    // The result should be stored in L and U on rank 0\n    // Your implementation here\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "48_sparse_la_sparse_axpy",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors. Store the result in z.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\nvoid sparseAxpy(double alpha, Kokkos::View<const Element*> &x, Kokkos::View<const Element*> &y, Kokkos::View<double*> &z) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(x(i).index) = alpha * x(i).value + y(x(i).index).value;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(x(i).index) = alpha * x(i).value + y(x(i).index).value;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(x(i).index) = alpha * x(i).value + y(x(i).index).value;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(x(i).index) = alpha * x(i).value + y(x(i).index).value;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(x(i).index) = alpha * x(i).value + y(x(i).index).value;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(x(i).index) = alpha * x(i).value + y(x(i).index).value;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(x(i).index) = alpha * x(i).value + y(x(i).index).value;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(x(i).index) = alpha * x(i).value + y(x(i).index).value;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(x(i).index) = alpha * x(i).value + y(x(i).index).value;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(x(i).index) = alpha * x(i).value + y(x(i).index).value;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(x(i).index) = alpha * x(i).value + y(x(i).index).value;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(x(i).index) = alpha * x(i).value + y(x(i).index).value;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(x(i).index) = alpha * x(i).value + y(x(i).index).value;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(x(i).index) = alpha * x(i).value + y(x(i).index).value;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(x(i).index) = alpha * x(i).value + y(x(i).index).value;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(x(i).index) = alpha * x(i).value + y(x(i).index).value;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(x(i).index) = alpha * x(i).value + y(x(i).index).value;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(x(i).index) = alpha * x(i).value + y(x(i).index).value;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(x(i).index) = alpha * x(i).value + y(x(i).index).value;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(x(i).index) = alpha * x(i).value + y(x(i).index).value;\n  });\n  Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "48_sparse_la_sparse_axpy",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors. Store the result in z.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\nvoid sparseAxpy(double alpha, std::vector<Element> const& x, std::vector<Element> const& y, std::vector<double> &z) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z.push_back({x[i].index, alpha * x[i].value});\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < y.size(); ++i) {\n        bool found = false;\n        for (size_t j = 0; j < z.size(); ++j) {\n            if (z[j].index == y[i].index) {\n                z[j].value += y[i].value;\n                found = true;\n                break;\n            }\n        }\n        if (!found) {\n            z.push_back({y[i].index, y[i].value});\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tz[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n\t}\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tz[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n\t}\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "48_sparse_la_sparse_axpy",
    "parallelism_model": "serial",
    "prompt": "struct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors. Store the result in z.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\nvoid sparseAxpy(double alpha, std::vector<Element> const& x, std::vector<Element> const& y, std::vector<double> &z) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    size_t i = 0, j = 0;\n    while (i < x.size() && j < y.size()) {\n        if (x[i].index < y[j].index) {\n            z.push_back(x[i].value * alpha);\n            i++;\n        } else if (x[i].index > y[j].index) {\n            z.push_back(y[j].value);\n            j++;\n        } else {\n            z.push_back(x[i].value * alpha + y[j].value);\n            i++;\n            j++;\n        }\n    }\n    while (i < x.size()) {\n        z.push_back(x[i].value * alpha);\n        i++;\n    }\n    while (j < y.size()) {\n        z.push_back(y[j].value);\n        j++;\n    }\n}",
      "\n    z = y;\n    for (const Element& elem : x) {\n        z[elem.index].value += alpha * elem.value;\n    }\n}",
      "\n    size_t i = 0, j = 0;\n    while (i < x.size() && j < y.size()) {\n        if (x[i].index < y[j].index) {\n            z.push_back(x[i].value * alpha);\n            i++;\n        } else if (x[i].index > y[j].index) {\n            z.push_back(y[j].value);\n            j++;\n        } else {\n            z.push_back(x[i].value * alpha + y[j].value);\n            i++;\n            j++;\n        }\n    }\n    while (i < x.size()) {\n        z.push_back(x[i].value * alpha);\n        i++;\n    }\n    while (j < y.size()) {\n        z.push_back(y[j].value);\n        j++;\n    }\n}",
      "\n    size_t i = 0, j = 0;\n    while (i < x.size() && j < y.size()) {\n        if (x[i].index < y[j].index) {\n            z.push_back(x[i].value * alpha);\n            i++;\n        } else if (x[i].index > y[j].index) {\n            z.push_back(y[j].value);\n            j++;\n        } else {\n            z.push_back(x[i].value * alpha + y[j].value);\n            i++;\n            j++;\n        }\n    }\n    while (i < x.size()) {\n        z.push_back(x[i].value * alpha);\n        i++;\n    }\n    while (j < y.size()) {\n        z.push_back(y[j].value);\n        j++;\n    }\n}",
      "\n    size_t i = 0, j = 0;\n    while (i < x.size() && j < y.size()) {\n        if (x[i].index < y[j].index) {\n            z.push_back(x[i].value * alpha);\n            i++;\n        } else if (x[i].index > y[j].index) {\n            z.push_back(y[j].value);\n            j++;\n        } else {\n            z.push_back(x[i].value * alpha + y[j].value);\n            i++;\n            j++;\n        }\n    }\n    while (i < x.size()) {\n        z.push_back(x[i].value * alpha);\n        i++;\n    }\n    while (j < y.size()) {\n        z.push_back(y[j].value);\n        j++;\n    }\n}",
      "\n    size_t i = 0, j = 0;\n    while (i < x.size() && j < y.size()) {\n        if (x[i].index < y[j].index) {\n            z.push_back(x[i].value * alpha);\n            i++;\n        } else if (x[i].index > y[j].index) {\n            z.push_back(y[j].value);\n            j++;\n        } else {\n            z.push_back(x[i].value * alpha + y[j].value);\n            i++;\n            j++;\n        }\n    }\n    while (i < x.size()) {\n        z.push_back(x[i].value * alpha);\n        i++;\n    }\n    while (j < y.size()) {\n        z.push_back(y[j].value);\n        j++;\n    }\n}",
      "\n    size_t i = 0, j = 0;\n    while (i < x.size() && j < y.size()) {\n        if (x[i].index < y[j].index) {\n            z.push_back(x[i].value * alpha);\n            i++;\n        } else if (x[i].index > y[j].index) {\n            z.push_back(y[j].value);\n            j++;\n        } else {\n            z.push_back(x[i].value * alpha + y[j].value);\n            i++;\n            j++;\n        }\n    }\n    while (i < x.size()) {\n        z.push_back(x[i].value * alpha);\n        i++;\n    }\n    while (j < y.size()) {\n        z.push_back(y[j].value);\n        j++;\n    }\n}",
      "\n    size_t i = 0, j = 0;\n    while (i < x.size() && j < y.size()) {\n        if (x[i].index < y[j].index) {\n            z.push_back(x[i].value * alpha);\n            i++;\n        } else if (x[i].index > y[j].index) {\n            z.push_back(y[j].value);\n            j++;\n        } else {\n            z.push_back(x[i].value * alpha + y[j].value);\n            i++;\n            j++;\n        }\n    }\n    while (i < x.size()) {\n        z.push_back(x[i].value * alpha);\n        i++;\n    }\n    while (j < y.size()) {\n        z.push_back(y[j].value);\n        j++;\n    }\n}",
      "\n    size_t i = 0, j = 0;\n    while (i < x.size() && j < y.size()) {\n        if (x[i].index < y[j].index) {\n            z.push_back(x[i].value * alpha);\n            i++;\n        } else if (x[i].index > y[j].index) {\n            z.push_back(y[j].value);\n            j++;\n        } else {\n            z.push_back(x[i].value * alpha + y[j].value);\n            i++;\n            j++;\n        }\n    }\n    while (i < x.size()) {\n        z.push_back(x[i].value * alpha);\n        i++;\n    }\n    while (j < y.size()) {\n        z.push_back(y[j].value);\n        j++;\n    }\n}",
      "\n    size_t i = 0, j = 0;\n    while (i < x.size() && j < y.size()) {\n        if (x[i].index < y[j].index) {\n            z.push_back(x[i].value * alpha);\n            i++;\n        } else if (x[i].index > y[j].index) {\n            z.push_back(y[j].value);\n            j++;\n        } else {\n            z.push_back(x[i].value * alpha + y[j].value);\n            i++;\n            j++;\n        }\n    }\n    while (i < x.size()) {\n        z.push_back(x[i].value * alpha);\n        i++;\n    }\n    while (j < y.size()) {\n        z.push_back(y[j].value);\n        j++;\n    }\n}",
      "\n    size_t i = 0, j = 0;\n    while (i < x.size() && j < y.size()) {\n        if (x[i].index < y[j].index) {\n            z.push_back(x[i].value * alpha);\n            i++;\n        } else if (x[i].index > y[j].index) {\n            z.push_back(y[j].value);\n            j++;\n        } else {\n            z.push_back(x[i].value * alpha + y[j].value);\n            i++;\n            j++;\n        }\n    }\n    while (i < x.size()) {\n        z.push_back(x[i].value * alpha);\n        i++;\n    }\n    while (j < y.size()) {\n        z.push_back(y[j].value);\n        j++;\n    }\n}",
      "\n    size_t i = 0, j = 0;\n    while (i < x.size() && j < y.size()) {\n        if (x[i].index == y[j].index) {\n            z.push_back(alpha * x[i].value + y[j].value);\n            i++;\n            j++;\n        } else if (x[i].index < y[j].index) {\n            z.push_back(alpha * x[i].value);\n            i++;\n        } else {\n            z.push_back(y[j].value);\n            j++;\n        }\n    }\n    while (i < x.size()) {\n        z.push_back(alpha * x[i].value);\n        i++;\n    }\n    while (j < y.size()) {\n        z.push_back(y[j].value);\n        j++;\n    }\n}",
      "\n    size_t i = 0, j = 0;\n    while (i < x.size() && j < y.size()) {\n        if (x[i].index == y[j].index) {\n            z.push_back(alpha * x[i].value + y[j].value);\n            i++;\n            j++;\n        } else if (x[i].index < y[j].index) {\n            z.push_back(alpha * x[i].value);\n            i++;\n        } else {\n            z.push_back(y[j].value);\n            j++;\n        }\n    }\n    while (i < x.size()) {\n        z.push_back(alpha * x[i].value);\n        i++;\n    }\n    while (j < y.size()) {\n        z.push_back(y[j].value);\n        j++;\n    }\n}",
      "\n    size_t i = 0, j = 0;\n    while (i < x.size() && j < y.size()) {\n        if (x[i].index < y[j].index) {\n            z.push_back(x[i].value * alpha);\n            i++;\n        } else if (x[i].index > y[j].index) {\n            z.push_back(y[j].value);\n            j++;\n        } else {\n            z.push_back(x[i].value * alpha + y[j].value);\n            i++;\n            j++;\n        }\n    }\n    while (i < x.size()) {\n        z.push_back(x[i].value * alpha);\n        i++;\n    }\n    while (j < y.size()) {\n        z.push_back(y[j].value);\n        j++;\n    }\n}",
      "\n    size_t i = 0, j = 0;\n    while (i < x.size() && j < y.size()) {\n        if (x[i].index == y[j].index) {\n            z.push_back(alpha * x[i].value + y[j].value);\n            i++;\n            j++;\n        } else if (x[i].index < y[j].index) {\n            z.push_back(alpha * x[i].value);\n            i++;\n        } else {\n            z.push_back(y[j].value);\n            j++;\n        }\n    }\n    while (i < x.size()) {\n        z.push_back(alpha * x[i].value);\n        i++;\n    }\n    while (j < y.size()) {\n        z.push_back(y[j].value);\n        j++;\n    }\n}",
      "\n    size_t i = 0, j = 0;\n    while (i < x.size() && j < y.size()) {\n        if (x[i].index == y[j].index) {\n            z.push_back(alpha * x[i].value + y[j].value);\n            i++;\n            j++;\n        } else if (x[i].index < y[j].index) {\n            z.push_back(alpha * x[i].value);\n            i++;\n        } else {\n            z.push_back(y[j].value);\n            j++;\n        }\n    }\n    while (i < x.size()) {\n        z.push_back(alpha * x[i].value);\n        i++;\n    }\n    while (j < y.size()) {\n        z.push_back(y[j].value);\n        j++;\n    }\n}",
      "\n    size_t i = 0, j = 0;\n    while (i < x.size() && j < y.size()) {\n        if (x[i].index < y[j].index) {\n            z.push_back(x[i].value * alpha);\n            i++;\n        } else if (x[i].index > y[j].index) {\n            z.push_back(y[j].value);\n            j++;\n        } else {\n            z.push_back(x[i].value * alpha + y[j].value);\n            i++;\n            j++;\n        }\n    }\n    while (i < x.size()) {\n        z.push_back(x[i].value * alpha);\n        i++;\n    }\n    while (j < y.size()) {\n        z.push_back(y[j].value);\n        j++;\n    }\n}",
      "\n    size_t i = 0, j = 0;\n    while (i < x.size() && j < y.size()) {\n        if (x[i].index < y[j].index) {\n            z.push_back(alpha * x[i].value);\n            i++;\n        } else if (x[i].index > y[j].index) {\n            z.push_back(y[j].value);\n            j++;\n        } else {\n            z.push_back(alpha * x[i].value + y[j].value);\n            i++;\n            j++;\n        }\n    }\n    while (i < x.size()) {\n        z.push_back(alpha * x[i].value);\n        i++;\n    }\n    while (j < y.size()) {\n        z.push_back(y[j].value);\n        j++;\n    }\n}",
      "\n    size_t i = 0, j = 0;\n    while (i < x.size() && j < y.size()) {\n        if (x[i].index < y[j].index) {\n            z.push_back(alpha * x[i].value);\n            i++;\n        } else if (x[i].index > y[j].index) {\n            z.push_back(y[j].value);\n            j++;\n        } else {\n            z.push_back(alpha * x[i].value + y[j].value);\n            i++;\n            j++;\n        }\n    }\n    while (i < x.size()) {\n        z.push_back(alpha * x[i].value);\n        i++;\n    }\n    while (j < y.size()) {\n        z.push_back(y[j].value);\n        j++;\n    }\n}",
      "\n    size_t i = 0, j = 0;\n    while (i < x.size() && j < y.size()) {\n        if (x[i].index == y[j].index) {\n            z.push_back(alpha * x[i].value + y[j].value);\n            i++;\n            j++;\n        } else if (x[i].index < y[j].index) {\n            z.push_back(alpha * x[i].value);\n            i++;\n        } else {\n            z.push_back(y[j].value);\n            j++;\n        }\n    }\n    while (i < x.size()) {\n        z.push_back(alpha * x[i].value);\n        i++;\n    }\n    while (j < y.size()) {\n        z.push_back(y[j].value);\n        j++;\n    }\n}"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "48_sparse_la_sparse_axpy",
    "parallelism_model": "hip",
    "prompt": "struct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors of size Nx and Ny. Store the result in z.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x or y.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\n__global__ void sparseAxpy(double alpha, const Element *x, const Element *y, double *z, size_t Nx, size_t Ny, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n\tsize_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < Nx) {\n\t\tz[tid] = alpha * x[tid].value;\n\t}\n\tif (tid < Ny) {\n\t\tatomicAdd(&z[y[tid].index], y[tid].value);\n\t}\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        }\n        if (tid < Ny) {\n            z[tid] += y[tid].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        }\n        if (tid < Ny) {\n            z[tid] += y[tid].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        }\n        if (tid < Ny) {\n            z[tid] += y[tid].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        }\n        if (tid < Ny) {\n            z[tid] += y[tid].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        }\n        if (tid < Ny) {\n            z[tid] += y[tid].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        }\n        if (tid < Ny) {\n            z[tid] += y[tid].value;\n        }\n    }\n}\n",
      "\n\tsize_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < Nx) {\n\t\tz[tid] = alpha * x[tid].value;\n\t} else if (tid < Ny) {\n\t\tsize_t index = y[tid - Nx].index;\n\t\tz[index] += y[tid - Nx].value;\n\t}\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        }\n        if (tid < Ny) {\n            z[tid] += y[tid].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        }\n        if (tid < Ny) {\n            z[tid] += y[tid].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        }\n        if (tid < Ny) {\n            z[tid] += y[tid].value;\n        }\n    }\n}\n",
      "\n\tsize_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < Nx) {\n\t\tz[tid] = alpha * x[tid].value;\n\t} else if (tid < Ny) {\n\t\tsize_t index = y[tid - Nx].index;\n\t\tz[index] += y[tid - Nx].value;\n\t}\n}\n",
      "\n\tsize_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < Nx) {\n\t\tz[tid] = alpha * x[tid].value;\n\t}\n\tif (tid < Ny) {\n\t\tatomicAdd(&z[y[tid].index], y[tid].value);\n\t}\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        }\n        if (tid < Ny) {\n            z[tid] += y[tid].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        }\n        if (tid < Ny) {\n            z[tid] += y[tid].value;\n        }\n    }\n}\n",
      "\n\tsize_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < Nx) {\n\t\tz[tid] = alpha * x[tid].value;\n\t} else if (tid < Ny) {\n\t\tsize_t index = y[tid].index;\n\t\tz[index] += y[tid].value;\n\t}\n}\n",
      "\n\tsize_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < Nx) {\n\t\tz[tid] = alpha * x[tid].value;\n\t}\n\tif (tid < Ny) {\n\t\tatomicAdd(&z[y[tid].index], y[tid].value);\n\t}\n}\n",
      "\n\tsize_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < N) {\n\t\tif (tid < Nx) {\n\t\t\tz[tid] = alpha * x[tid].value;\n\t\t}\n\t\tif (tid < Ny) {\n\t\t\tatomicAdd(&z[y[tid].index], y[tid].value);\n\t\t}\n\t}\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        }\n        if (tid < Ny) {\n            z[tid] += y[tid].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        }\n        if (tid < Ny) {\n            z[tid] += y[tid].value;\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "48_sparse_la_sparse_axpy",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors. Store the result in z.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Store the result in z on rank 0.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\nvoid sparseAxpy(double alpha, std::vector<Element> const& x, std::vector<Element> const& y, std::vector<double> &z) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n    for (size_t i = 0; i < y.size(); ++i) {\n        if (z[i] == 0) {\n            z[i] = y[i].value;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n    for (size_t i = 0; i < y.size(); ++i) {\n        if (z[y[i].index] == 0) {\n            z[y[i].index] = y[i].value;\n        }\n    }\n}",
      "\n    for (const Element& elem : x) {\n        z[elem.index] = alpha * elem.value;\n    }\n    for (const Element& elem : y) {\n        z[elem.index] += elem.value;\n    }\n}",
      "\n    for (const Element& elem : x) {\n        z[elem.index] = alpha * elem.value;\n    }\n\n    for (const Element& elem : y) {\n        z[elem.index] += elem.value;\n    }\n}",
      "\n    for (const Element& elem : x) {\n        z[elem.index] = alpha * elem.value;\n    }\n\n    for (const Element& elem : y) {\n        z[elem.index] += elem.value;\n    }\n}",
      "\n    for (const Element& elem : x) {\n        z[elem.index] = alpha * elem.value;\n    }\n\n    for (const Element& elem : y) {\n        z[elem.index] += elem.value;\n    }\n}",
      "\n    for (const Element& elem : x) {\n        z[elem.index] = alpha * elem.value;\n    }\n    for (const Element& elem : y) {\n        z[elem.index] += elem.value;\n    }\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n    for (size_t i = 0; i < y.size(); ++i) {\n        if (z[y[i].index] == 0) {\n            z[y[i].index] = y[i].value;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    for (const Element& elem : x) {\n        z[elem.index] = alpha * elem.value;\n    }\n    for (const Element& elem : y) {\n        z[elem.index] += elem.value;\n    }\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n    for (size_t i = 0; i < y.size(); ++i) {\n        if (z[i] == 0) {\n            z[i] = y[i].value;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n    for (size_t i = 0; i < y.size(); ++i) {\n        if (z[y[i].index] == 0) {\n            z[y[i].index] = y[i].value;\n        }\n    }\n}",
      "\n    for (const Element& elem : x) {\n        z[elem.index] = alpha * elem.value;\n    }\n    for (const Element& elem : y) {\n        z[elem.index] += elem.value;\n    }\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n    for (size_t i = 0; i < y.size(); ++i) {\n        if (z[y[i].index] == 0) {\n            z[y[i].index] = y[i].value;\n        }\n    }\n}",
      "\n    for (const Element& elem : x) {\n        z[elem.index] = alpha * elem.value;\n    }\n    for (const Element& elem : y) {\n        z[elem.index] += elem.value;\n    }\n}",
      "\n    for (const Element& elem : x) {\n        z[elem.index] = alpha * elem.value;\n    }\n    for (const Element& elem : y) {\n        z[elem.index] += elem.value;\n    }\n}",
      "\n    for (const Element& elem : x) {\n        z[elem.index] = alpha * elem.value;\n    }\n\n    for (const Element& elem : y) {\n        z[elem.index] += elem.value;\n    }\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    for (const Element& elem : x) {\n        z[elem.index] = alpha * elem.value;\n    }\n\n    for (const Element& elem : y) {\n        z[elem.index] += elem.value;\n    }\n}",
      "\n    for (const Element& elem : x) {\n        z[elem.index] = alpha * elem.value;\n    }\n    for (const Element& elem : y) {\n        z[elem.index] += elem.value;\n    }\n}"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "48_sparse_la_sparse_axpy",
    "parallelism_model": "cuda",
    "prompt": "struct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors of size Nx and Ny. Store the result in z.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x or y.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\n__global__ void sparseAxpy(double alpha, const Element *x, const Element *y, double *z, size_t Nx, size_t Ny, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        } else {\n            size_t y_index = tid - Nx;\n            z[tid] = y[y_index].value;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[x[tid].index] = alpha * x[tid].value + y[x[tid].index];\n        } else {\n            z[y[tid - Nx].index] = y[tid - Nx].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        } else {\n            size_t y_index = tid - Nx;\n            z[tid] = y[y_index].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        }\n        if (tid < Ny) {\n            z[tid] += y[tid].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        } else {\n            size_t y_index = tid - Nx;\n            z[tid] = y[y_index].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        } else {\n            size_t y_index = tid - Nx;\n            z[tid] = y[y_index].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        }\n        if (tid < Ny) {\n            z[tid] += y[tid].value;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[x[tid].index] = alpha * x[tid].value + y[x[tid].index];\n        } else {\n            z[y[tid - Nx].index] = y[tid - Nx].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        }\n        if (tid < Ny) {\n            z[tid] += y[tid].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        } else {\n            size_t y_index = tid - Nx;\n            z[tid] = y[y_index].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        } else {\n            size_t y_index = tid - Nx;\n            z[tid] = y[y_index].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        } else {\n            size_t y_index = tid - Nx;\n            z[tid] = y[y_index].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        } else {\n            size_t y_index = tid - Nx;\n            z[tid] = y[y_index].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        }\n        if (tid < Ny) {\n            z[tid] += y[tid].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        } else {\n            size_t y_index = tid - Nx;\n            z[tid] = y[y_index].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        } else {\n            size_t y_index = tid - Nx;\n            z[tid] = y[y_index].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        } else {\n            size_t y_index = tid - Nx;\n            z[tid] = y[y_index].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        }\n        if (tid < Ny) {\n            z[tid] += y[tid].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        } else {\n            size_t y_index = tid - Nx;\n            z[tid] = y[y_index].value;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (tid < Nx) {\n            z[tid] = alpha * x[tid].value;\n        } else {\n            size_t y_index = tid - Nx;\n            z[tid] = y[y_index].value;\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "48_sparse_la_sparse_axpy",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors. Store the result in z.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Store the result in z on rank 0.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\nvoid sparseAxpy(double alpha, std::vector<Element> const& x, std::vector<Element> const& y, std::vector<double> &z) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tz[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n\t}\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tz[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n\t}\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        z[x[i].index] = alpha * x[i].value + y[x[i].index].value;\n    }\n}"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "23_histogram_first_letter_counts",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(Kokkos::View<const char**> const& s, Kokkos::View<size_t[26]> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(int i) {\n    char firstLetter = s(i)[0];\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n      Kokkos::atomic_fetch_add(&bins(firstLetter - 'a'), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(s.size(), KOKKOS_LAMBDA(int i) {\n    char firstLetter = s(i)[0];\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n      Kokkos::atomic_increment(&bins(firstLetter - 'a'));\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(int i) {\n    int index = s(i)[0] - 'a';\n    if (index >= 0 && index < 26) {\n      Kokkos::atomic_fetch_add(&bins(index), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n    int index = s(i)[0] - 'a';\n    if (index >= 0 && index < 26) {\n      Kokkos::atomic_fetch_add(&bins(index), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(int i) {\n    int index = s(i)[0] - 'a';\n    if (index >= 0 && index < 26) {\n      Kokkos::atomic_increment(&bins(index));\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n    char firstLetter = s(i)[0];\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n      Kokkos::atomic_fetch_add(&bins(firstLetter - 'a'), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(int i) {\n    char firstLetter = s(i)[0];\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n      Kokkos::atomic_fetch_add(&bins(firstLetter - 'a'), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n    char firstLetter = s(i)[0];\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n      int index = firstLetter - 'a';\n      Kokkos::atomic_fetch_add(&bins(index), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n    int index = s(i)[0] - 'a';\n    if (index >= 0 && index < 26) {\n      Kokkos::atomic_fetch_add(&bins(index), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(int i) {\n    int index = s(i)[0] - 'a';\n    if (index >= 0 && index < 26) {\n      Kokkos::atomic_fetch_add(&bins(index), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(int i) {\n    int index = s(i)[0] - 'a';\n    if (index >= 0 && index < 26) {\n      Kokkos::atomic_fetch_add(&bins(index), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n    char firstLetter = s(i)[0];\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n      Kokkos::atomic_fetch_add(&bins(firstLetter - 'a'), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n    int index = s(i)[0] - 'a';\n    if (index >= 0 && index < 26) {\n      Kokkos::atomic_fetch_add(&bins(index), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n    char firstLetter = s(i)[0];\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n      Kokkos::atomic_fetch_add(&bins(firstLetter - 'a'), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(int i) {\n    char firstLetter = s(i)[0];\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n      Kokkos::atomic_fetch_add(&bins(firstLetter - 'a'), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n    char firstLetter = s(i)[0];\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n      Kokkos::atomic_fetch_add(&bins(firstLetter - 'a'), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(int i) {\n      int index = s(i)[0] - 'a';\n      if (index >= 0 && index < 26) {\n          Kokkos::atomic_fetch_add(&bins(index), 1);\n      }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(int i) {\n    char firstLetter = s(i)[0];\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n      Kokkos::atomic_fetch_add(&bins(firstLetter - 'a'), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(int i) {\n    char firstLetter = s(i)[0];\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n      Kokkos::atomic_fetch_add(&bins(firstLetter - 'a'), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(int i) {\n    char firstLetter = s(i)[0];\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n      Kokkos::atomic_fetch_add(&bins(firstLetter - 'a'), 1);\n    }\n  });\n  Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "23_histogram_first_letter_counts",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            #pragma omp atomic\n            ++bins[firstLetter - 'a'];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n}",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < s.size(); ++i) {\n    char firstLetter = s[i][0];\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n}\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            #pragma omp atomic\n            ++bins[firstLetter - 'a'];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            #pragma omp atomic\n            ++bins[firstLetter - 'a'];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            #pragma omp atomic\n            ++bins[firstLetter - 'a'];\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "23_histogram_first_letter_counts",
    "parallelism_model": "serial",
    "prompt": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}",
      "\n    for (const std::string& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}",
      "\n    for (const std::string& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}",
      "\n    for (const std::string& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}",
      "\n    for (const std::string& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}",
      "\n    for (const std::string& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}",
      "\n    for (const auto &str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}",
      "\n    for (const std::string& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}",
      "\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}",
      "\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}",
      "\n    for (const std::string& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}",
      "\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}",
      "\n    for (const std::string& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}",
      "\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}",
      "\n    for (const std::string& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}",
      "\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}",
      "\n    for (const std::string& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}",
      "\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}",
      "\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}",
      "\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "23_histogram_first_letter_counts",
    "parallelism_model": "hip",
    "prompt": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\n__global__ void firstLetterCounts(const char **s, size_t N, size_t bins[26]) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        char firstChar = s[tid][0];\n        if (firstChar >= 'a' && firstChar <= 'z') {\n            atomicAdd(&bins[firstChar - 'a'], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        char firstLetter = s[tid][0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            atomicAdd(&bins[firstLetter - 'a'], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        char firstLetter = s[tid][0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            atomicAdd(&bins[firstLetter - 'a'], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        char firstLetter = s[tid][0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "23_histogram_first_letter_counts",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of s. The result is stored in bins on rank 0.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            localBins[firstLetter - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        std::array<size_t, 26> tempBins = {0};\n        for (int i = 0; i < size; i++) {\n            MPI_Recv(&tempBins, 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += tempBins[j];\n            }\n        }\n    } else {\n        MPI_Send(&localBins, 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            localBins[firstLetter - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        std::array<size_t, 26> globalBins = {0};\n        MPI_Reduce(localBins.data(), globalBins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        bins = globalBins;\n    } else {\n        MPI_Reduce(localBins.data(), nullptr, 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            localBins[firstLetter - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        std::array<size_t, 26> globalBins = {0};\n        MPI_Reduce(localBins.data(), globalBins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        bins = globalBins;\n    } else {\n        MPI_Reduce(localBins.data(), nullptr, 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            localBins[firstLetter - 'a']++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            localBins[firstLetter - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        std::array<size_t, 26> globalBins = {0};\n        MPI_Reduce(localBins.data(), globalBins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        bins = globalBins;\n    } else {\n        MPI_Reduce(localBins.data(), nullptr, 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            localBins[firstLetter - 'a']++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            localBins[firstLetter - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        std::array<size_t, 26> globalBins = {0};\n        MPI_Reduce(localBins.data(), globalBins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        bins = globalBins;\n    } else {\n        MPI_Reduce(localBins.data(), nullptr, 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            localBins[firstLetter - 'a']++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            localBins[firstLetter - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        std::array<size_t, 26> globalBins = {0};\n        MPI_Reduce(localBins.data(), globalBins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        bins = globalBins;\n    } else {\n        MPI_Reduce(localBins.data(), nullptr, 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            localBins[firstLetter - 'a']++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            localBins[firstLetter - 'a']++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            localBins[firstLetter - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        std::array<size_t, 26> globalBins = {0};\n        MPI_Reduce(localBins.data(), globalBins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        bins = globalBins;\n    } else {\n        MPI_Reduce(localBins.data(), nullptr, 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            localBins[firstLetter - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        std::array<size_t, 26> globalBins = {0};\n        MPI_Reduce(localBins.data(), globalBins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        bins = globalBins;\n    } else {\n        MPI_Reduce(localBins.data(), nullptr, 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            localBins[firstLetter - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        std::array<size_t, 26> globalBins = {0};\n        MPI_Reduce(localBins.data(), globalBins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        bins = globalBins;\n    } else {\n        MPI_Reduce(localBins.data(), nullptr, 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            localBins[firstLetter - 'a']++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            localBins[firstLetter - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        std::array<size_t, 26> globalBins = {0};\n        MPI_Reduce(localBins.data(), globalBins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        bins = globalBins;\n    } else {\n        MPI_Reduce(localBins.data(), nullptr, 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            localBins[firstLetter - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        std::array<size_t, 26> tempBins = {0};\n        MPI_Reduce(localBins.data(), tempBins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        bins = tempBins;\n    } else {\n        MPI_Reduce(localBins.data(), nullptr, 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            localBins[firstLetter - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        std::array<size_t, 26> globalBins = {0};\n        MPI_Reduce(localBins.data(), globalBins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        bins = globalBins;\n    } else {\n        MPI_Reduce(localBins.data(), nullptr, 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            localBins[firstLetter - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 26> receivedBins;\n            MPI_Recv(receivedBins.data(), 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += receivedBins[j];\n            }\n        }\n    } else {\n        MPI_Send(localBins.data(), 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        bins = localBins;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n\n    for (const auto& str : s) {\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            localBins[firstLetter - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 26> recvBins;\n            MPI_Recv(&recvBins, 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += recvBins[j];\n            }\n        }\n    } else {\n        MPI_Send(&localBins, 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        bins = localBins;\n    }\n}"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "23_histogram_first_letter_counts",
    "parallelism_model": "cuda",
    "prompt": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\n__global__ void firstLetterCounts(const char **s, size_t N, size_t bins[26]) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "23_histogram_first_letter_counts",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of s. The result is stored in bins on rank 0.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::array<size_t, 26> localBins;\n        MPI::COMM_WORLD.Reduce(&bins[0], &localBins[0], 26, MPI::UNSIGNED_LONG, MPI::SUM, 0);\n        bins = localBins;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> local_bins = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char first_letter = s[i][0];\n        int bin_index = first_letter - 'a';\n        local_bins[bin_index]++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::array<size_t, 26> localBins;\n        MPI::COMM_WORLD.Reduce(&bins[0], &localBins[0], 26, MPI::UNSIGNED_LONG, MPI::SUM, 0);\n        bins = localBins;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n\n    std::array<size_t, 26> localBins;\n    MPI_Reduce(bins.data(), localBins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        bins = localBins;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            localBins[firstLetter - 'a']++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::array<size_t, 26> localBins;\n        MPI::COMM_WORLD.Reduce(&bins[0], &localBins[0], 26, MPI::UNSIGNED_LONG, MPI::SUM, 0);\n        bins = localBins;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::array<size_t, 26> localBins;\n        MPI::COMM_WORLD.Reduce(&bins[0], &localBins[0], 26, MPI::UNSIGNED_LONG, MPI::SUM, 0);\n        bins = localBins;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n\n    std::array<size_t, 26> localBins;\n    MPI_Reduce(bins.data(), localBins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        bins = localBins;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::array<size_t, 26> localBins;\n        MPI::COMM_WORLD.Reduce(&bins[0], &localBins[0], 26, MPI::UNSIGNED_LONG, MPI::SUM, 0);\n        bins = localBins;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::array<size_t, 26> localBins;\n        MPI::COMM_WORLD.Reduce(&bins[0], &localBins[0], 26, MPI::UNSIGNED_LONG, MPI::SUM, 0);\n        bins = localBins;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n\n    std::array<size_t, 26> localBins;\n    MPI_Reduce(bins.data(), localBins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        bins = localBins;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::array<size_t, 26> localBins;\n        MPI::COMM_WORLD.Reduce(&bins[0], &localBins[0], 26, MPI::UNSIGNED_LONG, MPI::SUM, 0);\n        bins = localBins;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> local_bins = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char first_letter = s[i][0];\n        if (first_letter >= 'a' && first_letter <= 'z') {\n            local_bins[first_letter - 'a']++;\n        }\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        #pragma omp atomic\n        ++bins[firstLetter - 'a'];\n    }\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::array<size_t, 26> localBins;\n        MPI::COMM_WORLD.Reduce(&bins[0], &localBins[0], 26, MPI::UNSIGNED_LONG, MPI::SUM, 0);\n        bins = localBins;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); ++i) {\n        int index = s[i][0] - 'a';\n        #pragma omp atomic\n        bins[index]++;\n    }\n\n    std::array<size_t, 26> localBins;\n    MPI_Reduce(bins.data(), localBins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        bins = localBins;\n    }\n}"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "21_histogram_bin_0-100",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(Kokkos::View<const double*> const& x, Kokkos::View<size_t[10]> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    int bin_index = x(i) / 10;\n    Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    int bin_index = x(i) / 10;\n    Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n        int bin_index = x(i) / 10;\n        Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        int bin_index = x(i) / 10;\n        Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        int bin_index = x(i) / 10;\n        Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n    });\n    Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    int bin_index = x(i) / 10;\n    Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    int bin_index = x(i) / 10;\n    Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    int bin_index = x(i) / 10;\n    Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        int bin_index = x(i) / 10;\n        Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n    });\n    Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    int bin_index = x(i) / 10;\n    Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    int bin_index = x(i) / 10;\n    if (bin_index < 10) {\n      Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        int bin_index = x(i) / 10;\n        if (bin_index < 10) {\n            Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n        }\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        int bin_index = x(i) / 10;\n        Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        int bin_index = x(i) / 10;\n        Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n    });\n    Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    int bin_index = x(i) / 10;\n    Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    int bin_index = x(i) / 10;\n    Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        int bin_index = x(i) / 10;\n        Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n        int bin_index = x(i) / 10;\n        Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n    });\n    Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    int bin_index = x(i) / 10;\n    Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n        int bin_index = x(i) / 10;\n        Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n    });\n    Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "21_histogram_bin_0-100",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin_index = x[i] / 10;\n        #pragma omp atomic\n        ++bins[bin_index];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin_index = x[i] / 10;\n        #pragma omp atomic\n        ++bins[bin_index];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin_index = x[i] / 10;\n        #pragma omp atomic\n        ++bins[bin_index];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin_index = x[i] / 10;\n        #pragma omp atomic\n        ++bins[bin_index];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin_index = x[i] / 10;\n        #pragma omp atomic\n        ++bins[bin_index];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin_index = x[i] / 10;\n        #pragma omp atomic\n        ++bins[bin_index];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin_index = x[i] / 10;\n        #pragma omp atomic\n        ++bins[bin_index];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin_index = x[i] / 10;\n        #pragma omp atomic\n        ++bins[bin_index];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin_index = x[i] / 10;\n        #pragma omp atomic\n        ++bins[bin_index];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin_index = x[i] / 10;\n        #pragma omp atomic\n        ++bins[bin_index];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t binIndex = x[i] / 10;\n        #pragma omp atomic\n        ++bins[binIndex];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin_index = x[i] / 10;\n        #pragma omp atomic\n        ++bins[bin_index];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin_index = x[i] / 10;\n        #pragma omp atomic\n        ++bins[bin_index];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin_index = x[i] / 10;\n        #pragma omp atomic\n        ++bins[bin_index];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin_index = x[i] / 10;\n        #pragma omp atomic\n        ++bins[bin_index];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin_index = x[i] / 10;\n        #pragma omp atomic\n        ++bins[bin_index];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin_index = x[i] / 10;\n        #pragma omp atomic\n        ++bins[bin_index];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin_index = x[i] / 10;\n        #pragma omp atomic\n        ++bins[bin_index];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin_index = x[i] / 10;\n        #pragma omp atomic\n        ++bins[bin_index];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin = x[i] / 10;\n        #pragma omp atomic\n        ++bins[bin];\n    }\n}"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "21_histogram_bin_0-100",
    "parallelism_model": "serial",
    "prompt": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (double val : x) {\n        size_t binIndex = static_cast<size_t>(val) / 10;\n        if (binIndex < 10) {\n            bins[binIndex]++;\n        }\n    }\n}",
      "\n    for (double value : x) {\n        int binIndex = value / 10;\n        if (binIndex < 10) {\n            bins[binIndex]++;\n        }\n    }\n}",
      "\n    for (double value : x) {\n        int binIndex = value / 10;\n        if (binIndex < 10) {\n            bins[binIndex]++;\n        }\n    }\n}",
      "\n    for (double value : x) {\n        int binIndex = value / 10;\n        if (binIndex < 10) {\n            bins[binIndex]++;\n        }\n    }\n}",
      "\n    for (double value : x) {\n        int binIndex = value / 10;\n        if (binIndex < 10) {\n            bins[binIndex]++;\n        }\n    }\n}",
      "\n    for (double value : x) {\n        int binIndex = value / 10;\n        if (binIndex < 10) {\n            bins[binIndex]++;\n        }\n    }\n}",
      "\n    for (double value : x) {\n        int binIndex = value / 10;\n        if (binIndex < 10) {\n            bins[binIndex]++;\n        }\n    }\n}",
      "\n    for (double value : x) {\n        size_t binIndex = static_cast<size_t>(value) / 10;\n        if (binIndex < 10) {\n            bins[binIndex]++;\n        }\n    }\n}",
      "\n    for (double val : x) {\n        size_t binIndex = static_cast<size_t>(val) / 10;\n        if (binIndex < 10) {\n            bins[binIndex]++;\n        }\n    }\n}",
      "\n    for (double value : x) {\n        size_t binIndex = static_cast<size_t>(value) / 10;\n        if (binIndex < 10) {\n            bins[binIndex]++;\n        }\n    }\n}",
      "\n    for (double value : x) {\n        int binIndex = value / 10;\n        if (binIndex < 10) {\n            bins[binIndex]++;\n        }\n    }\n}",
      "\n    for (double value : x) {\n        int binIndex = value / 10;\n        if (binIndex < 10) {\n            bins[binIndex]++;\n        }\n    }\n}",
      "\n    for (double value : x) {\n        int binIndex = value / 10;\n        if (binIndex < 10) {\n            bins[binIndex]++;\n        }\n    }\n}",
      "\n    for (double value : x) {\n        int binIndex = value / 10;\n        if (binIndex < 10) {\n            bins[binIndex]++;\n        }\n    }\n}",
      "\n    for (double value : x) {\n        size_t binIndex = static_cast<size_t>(value) / 10;\n        if (binIndex < 10) {\n            bins[binIndex]++;\n        }\n    }\n}",
      "\n    for (double value : x) {\n        size_t binIndex = static_cast<size_t>(value) / 10;\n        if (binIndex < 10) {\n            bins[binIndex]++;\n        }\n    }\n}",
      "\n    for (double val : x) {\n        size_t binIndex = static_cast<size_t>(val) / 10;\n        if (binIndex < 10) {\n            bins[binIndex]++;\n        }\n    }\n}",
      "\n    for (double value : x) {\n        int binIndex = value / 10;\n        if (binIndex < 10) {\n            bins[binIndex]++;\n        }\n    }\n}",
      "\n    for (double val : x) {\n        size_t binIndex = static_cast<size_t>(val) / 10;\n        if (binIndex < 10) {\n            bins[binIndex]++;\n        }\n    }\n}",
      "\n    for (double value : x) {\n        int binIndex = value / 10;\n        if (binIndex < 10) {\n            bins[binIndex]++;\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "21_histogram_bin_0-100",
    "parallelism_model": "hip",
    "prompt": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use AMD HIP to compute in parallel. The kernel is initialized with at least as many threads as values in x.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\n__global__ void binsBy10Count(const double *x, size_t N, size_t bins[10]) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int bin = x[tid] / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int bin = x[tid] / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int bin = x[tid] / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int bin = x[tid] / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "21_histogram_bin_0-100",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 10> localBins = {0};\n\n    size_t localCount = x.size() / size;\n    size_t start = rank * localCount;\n    size_t end = (rank == size - 1) ? x.size() : start + localCount;\n\n    for (size_t i = start; i < end; ++i) {\n        int binIndex = x[i] / 10;\n        if (binIndex < 10) {\n            localBins[binIndex]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 10> localBins = {0};\n\n    for (size_t i = rank; i < x.size(); i += size) {\n        int binIndex = x[i] / 10;\n        if (binIndex < 10) {\n            localBins[binIndex]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 10> localBins = {0};\n\n    size_t localCount = x.size() / size;\n    size_t start = rank * localCount;\n    size_t end = (rank == size - 1) ? x.size() : start + localCount;\n\n    for (size_t i = start; i < end; ++i) {\n        int binIndex = x[i] / 10;\n        if (binIndex < 10) {\n            localBins[binIndex]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 10> localBins = {0};\n\n    size_t localCount = x.size() / size;\n    size_t remainder = x.size() % size;\n\n    size_t start = rank * localCount + std::min(rank, remainder);\n    size_t end = start + localCount + (rank < remainder ? 1 : 0);\n\n    for (size_t i = start; i < end; ++i) {\n        int binIndex = x[i] / 10;\n        if (binIndex < 10) {\n            localBins[binIndex]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 10> localBins = {0};\n\n    size_t localCount = x.size() / size;\n    size_t start = rank * localCount;\n    size_t end = (rank == size - 1) ? x.size() : start + localCount;\n\n    for (size_t i = start; i < end; ++i) {\n        int binIndex = x[i] / 10;\n        if (binIndex < 10) {\n            localBins[binIndex]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 10> localBins = {0};\n\n    size_t localCount = x.size() / size;\n    size_t start = rank * localCount;\n    size_t end = (rank == size - 1) ? x.size() : start + localCount;\n\n    for (size_t i = start; i < end; ++i) {\n        int binIndex = x[i] / 10;\n        if (binIndex < 10) {\n            localBins[binIndex]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 10> localBins = {0};\n\n    size_t localCount = x.size() / size;\n    size_t start = rank * localCount;\n    size_t end = (rank == size - 1) ? x.size() : start + localCount;\n\n    for (size_t i = start; i < end; ++i) {\n        int binIndex = x[i] / 10;\n        if (binIndex < 10) {\n            localBins[binIndex]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 10> localBins = {0};\n\n    size_t localCount = x.size() / size;\n    size_t start = rank * localCount;\n    size_t end = (rank == size - 1) ? x.size() : start + localCount;\n\n    for (size_t i = start; i < end; ++i) {\n        int binIndex = x[i] / 10;\n        if (binIndex < 10) {\n            localBins[binIndex]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 10> localBins = {0};\n\n    size_t localCount = x.size() / size;\n    size_t start = rank * localCount;\n    size_t end = (rank == size - 1) ? x.size() : start + localCount;\n\n    for (size_t i = start; i < end; ++i) {\n        int binIndex = x[i] / 10;\n        if (binIndex < 10) {\n            localBins[binIndex]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 10> localBins = {0};\n\n    size_t localCount = x.size() / size;\n    size_t start = rank * localCount;\n    size_t end = (rank == size - 1) ? x.size() : start + localCount;\n\n    for (size_t i = start; i < end; ++i) {\n        int binIndex = x[i] / 10;\n        localBins[binIndex]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 10> localBins = {0};\n\n    size_t localCount = x.size() / size;\n    size_t start = rank * localCount;\n    size_t end = (rank == size - 1) ? x.size() : start + localCount;\n\n    for (size_t i = start; i < end; ++i) {\n        int binIndex = x[i] / 10;\n        if (binIndex < 10) {\n            localBins[binIndex]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 10> localBins = {0};\n\n    size_t localCount = x.size() / size;\n    size_t start = rank * localCount;\n    size_t end = (rank == size - 1) ? x.size() : start + localCount;\n\n    for (size_t i = start; i < end; ++i) {\n        int binIndex = x[i] / 10;\n        if (binIndex < 10) {\n            localBins[binIndex]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[10] = {0};\n\n    int local_size = x.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? x.size() : start + local_size;\n\n    for (int i = start; i < end; ++i) {\n        int bin_index = x[i] / 10;\n        local_counts[bin_index]++;\n    }\n\n    MPI_Reduce(local_counts, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 10> localBins = {0};\n\n    size_t localCount = x.size() / size;\n    size_t start = rank * localCount;\n    size_t end = (rank == size - 1) ? x.size() : start + localCount;\n\n    for (size_t i = start; i < end; ++i) {\n        int binIndex = x[i] / 10;\n        if (binIndex < 10) {\n            localBins[binIndex]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[10] = {0};\n\n    for (size_t i = rank; i < x.size(); i += size) {\n        int binIndex = x[i] / 10;\n        localBins[binIndex]++;\n    }\n\n    MPI_Reduce(localBins, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 10> localBins = {0};\n\n    size_t localCount = x.size() / size;\n    size_t start = rank * localCount;\n    size_t end = (rank == size - 1) ? x.size() : start + localCount;\n\n    for (size_t i = start; i < end; ++i) {\n        int binIndex = x[i] / 10;\n        if (binIndex < 10) {\n            localBins[binIndex]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_bins[10] = {0};\n\n    int local_count = x.size() / size;\n    int start = rank * local_count;\n    int end = (rank == size - 1) ? x.size() : start + local_count;\n\n    for (int i = start; i < end; ++i) {\n        int bin_index = x[i] / 10;\n        local_bins[bin_index]++;\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[10] = {0};\n\n    size_t localSize = x.size() / size;\n    size_t start = rank * localSize;\n    size_t end = (rank == size - 1) ? x.size() : start + localSize;\n\n    for (size_t i = start; i < end; ++i) {\n        int binIndex = x[i] / 10;\n        localBins[binIndex]++;\n    }\n\n    MPI_Reduce(localBins, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[10] = {0};\n\n    for (size_t i = rank; i < x.size(); i += size) {\n        int binIndex = x[i] / 10;\n        localBins[binIndex]++;\n    }\n\n    MPI_Reduce(localBins, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[10] = {0};\n\n    for (size_t i = rank; i < x.size(); i += size) {\n        int binIndex = x[i] / 10;\n        localBins[binIndex]++;\n    }\n\n    MPI_Reduce(localBins, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "21_histogram_bin_0-100",
    "parallelism_model": "cuda",
    "prompt": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use CUDA to compute in parallel. The kernel is initialized with at least as many threads as values in x.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\n__global__ void binsBy10Count(const double *x, size_t N, size_t bins[10]) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int binIndex = x[tid] / 10;\n        atomicAdd(&bins[binIndex], 1);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "21_histogram_bin_0-100",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[10] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int bin_index = x[i] / 10;\n        #pragma omp atomic\n        local_counts[bin_index]++;\n    }\n\n    MPI_Reduce(local_counts, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int bin_index = x[i] / 10;\n        #pragma omp atomic\n        bins[bin_index]++;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            std::array<size_t, 10> recv_bins;\n            MPI_Recv(recv_bins.data(), 10, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 10; ++j) {\n                bins[j] += recv_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(bins.data(), 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_bins[10] = {0};\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int bin_index = x[i] / 10;\n        if (bin_index < 10) {\n            #pragma omp atomic\n            local_bins[bin_index]++;\n        }\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[10] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int bin_index = x[i] / 10;\n        #pragma omp atomic\n        local_counts[bin_index]++;\n    }\n\n    MPI_Reduce(local_counts, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[10] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int bin_index = x[i] / 10;\n        #pragma omp atomic\n        local_counts[bin_index]++;\n    }\n\n    MPI_Reduce(local_counts, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[10] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int bin_index = x[i] / 10;\n        #pragma omp atomic\n        local_counts[bin_index]++;\n    }\n\n    MPI_Reduce(local_counts, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[10] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int bin_index = x[i] / 10;\n        #pragma omp atomic\n        local_counts[bin_index]++;\n    }\n\n    MPI_Reduce(local_counts, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int bin_index = x[i] / 10;\n        #pragma omp atomic\n        bins[bin_index]++;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            std::array<size_t, 10> recv_bins;\n            MPI_Recv(recv_bins.data(), 10, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 10; ++j) {\n                bins[j] += recv_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(bins.data(), 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[10] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int bin_index = x[i] / 10;\n        #pragma omp atomic\n        local_counts[bin_index]++;\n    }\n\n    MPI_Reduce(local_counts, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[10] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int bin_index = x[i] / 10;\n        #pragma omp atomic\n        local_counts[bin_index]++;\n    }\n\n    MPI_Reduce(local_counts, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[10] = {0};\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int bin_index = x[i] / 10;\n        if (bin_index < 10) {\n            #pragma omp atomic\n            local_counts[bin_index]++;\n        }\n    }\n\n    int global_counts[10] = {0};\n    MPI_Reduce(local_counts, global_counts, 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 10; ++i) {\n            bins[i] = global_counts[i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[10] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        int bin_index = x[i] / 10;\n        #pragma omp atomic\n        local_counts[bin_index]++;\n    }\n\n    MPI_Reduce(local_counts, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[10] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int bin_index = x[i] / 10;\n        #pragma omp atomic\n        local_counts[bin_index]++;\n    }\n\n    MPI_Reduce(local_counts, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[10] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int bin_index = x[i] / 10;\n        #pragma omp atomic\n        local_counts[bin_index]++;\n    }\n\n    MPI_Reduce(local_counts, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[10] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int bin_index = x[i] / 10;\n        #pragma omp atomic\n        local_counts[bin_index]++;\n    }\n\n    MPI_Reduce(local_counts, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[10] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int bin_index = x[i] / 10;\n        #pragma omp atomic\n        local_counts[bin_index]++;\n    }\n\n    MPI_Reduce(local_counts, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[10] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int bin_index = x[i] / 10;\n        #pragma omp atomic\n        local_counts[bin_index]++;\n    }\n\n    MPI_Reduce(local_counts, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 10> localBins = {0};\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int binIndex = x[i] / 10;\n        if (binIndex < 10) {\n            #pragma omp atomic\n            localBins[binIndex]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[10] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int bin_index = x[i] / 10;\n        if (bin_index < 10) {\n            #pragma omp atomic\n            local_counts[bin_index]++;\n        }\n    }\n\n    MPI_Reduce(local_counts, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[10] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int bin_index = x[i] / 10;\n        #pragma omp atomic\n        local_counts[bin_index]++;\n    }\n\n    MPI_Reduce(local_counts, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "22_histogram_count_quadrants",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use Kokkos to count in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(Kokkos::View<const Point*> const& points, Kokkos::View<size_t[4]> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(const int i) {\n        if (points(i).x > 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (points(i).x < 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (points(i).x < 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (points(i).x > 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n}",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        if (points(i).x > 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (points(i).x < 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (points(i).x < 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (points(i).x > 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n}",
      "\n  Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n      if (points(i).x > 0 && points(i).y > 0) {\n          Kokkos::atomic_fetch_add(&bins(0), 1);\n      } else if (points(i).x < 0 && points(i).y > 0) {\n          Kokkos::atomic_fetch_add(&bins(1), 1);\n      } else if (points(i).x < 0 && points(i).y < 0) {\n          Kokkos::atomic_fetch_add(&bins(2), 1);\n      } else if (points(i).x > 0 && points(i).y < 0) {\n          Kokkos::atomic_fetch_add(&bins(3), 1);\n      }\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        if (points(i).x > 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (points(i).x < 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (points(i).x < 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (points(i).x > 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n}",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(const int i) {\n        if (points(i).x > 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (points(i).x < 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (points(i).x < 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (points(i).x > 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n}",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        if (points(i).x > 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (points(i).x < 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (points(i).x < 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (points(i).x > 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n}",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        if (points(i).x > 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (points(i).x < 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (points(i).x < 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (points(i).x > 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n}",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        if (points(i).x > 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (points(i).x < 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (points(i).x < 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (points(i).x > 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n}",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(const int i) {\n        if (points(i).x > 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (points(i).x < 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (points(i).x < 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (points(i).x > 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n}",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        if (points(i).x > 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (points(i).x < 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (points(i).x < 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (points(i).x > 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n}",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(const int i) {\n        if (points(i).x > 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (points(i).x < 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (points(i).x < 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (points(i).x > 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n}",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(const int i) {\n        const Point& p = points(i);\n        if (p.x > 0 && p.y > 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (p.x < 0 && p.y > 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (p.x < 0 && p.y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (p.x > 0 && p.y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n}",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(const int i) {\n        const Point& p = points(i);\n        if (p.x > 0 && p.y > 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (p.x < 0 && p.y > 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (p.x < 0 && p.y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (p.x > 0 && p.y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n}",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        if (points(i).x > 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (points(i).x < 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (points(i).x < 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (points(i).x > 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n}",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        if (points(i).x > 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (points(i).x < 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (points(i).x < 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (points(i).x > 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n}",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(const int i) {\n        if (points(i).x > 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (points(i).x < 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (points(i).x < 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (points(i).x > 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n}",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        if (points(i).x > 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (points(i).x < 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (points(i).x < 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (points(i).x > 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        if (points(i).x > 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (points(i).x < 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (points(i).x < 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (points(i).x > 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n    Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n      if (points(i).x > 0 && points(i).y > 0) {\n          Kokkos::atomic_fetch_add(&bins(0), 1);\n      } else if (points(i).x < 0 && points(i).y > 0) {\n          Kokkos::atomic_fetch_add(&bins(1), 1);\n      } else if (points(i).x < 0 && points(i).y < 0) {\n          Kokkos::atomic_fetch_add(&bins(2), 1);\n      } else if (points(i).x > 0 && points(i).y < 0) {\n          Kokkos::atomic_fetch_add(&bins(3), 1);\n      }\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        if (points(i).x > 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (points(i).x < 0 && points(i).y > 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (points(i).x < 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (points(i).x > 0 && points(i).y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n    Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "22_histogram_count_quadrants",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use OpenMP to count in parallel.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[2];\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[2];\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[2];\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[2];\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[2];\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[2];\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[2];\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[2];\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[2];\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[2];\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[2];\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[2];\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[2];\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[2];\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[2];\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[2];\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[2];\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[2];\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "22_histogram_count_quadrants",
    "parallelism_model": "serial",
    "prompt": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (const Point& p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x > 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (const Point& p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x > 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (const Point& p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x > 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (const Point& p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x > 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (const Point& p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x > 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (const Point& p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x > 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (const Point& p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x > 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (const Point& p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x > 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (const Point& p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x > 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (const Point& p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x > 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (const Point& p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x > 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (const Point& p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x > 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (const Point& p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x > 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (const Point& p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x > 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            bins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            bins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            bins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (const Point& p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x > 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (const Point& p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x > 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            bins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            bins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            bins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (const Point& p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x > 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (const Point& p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x > 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "22_histogram_count_quadrants",
    "parallelism_model": "hip",
    "prompt": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use AMD HIP to count in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\n__global__ void countQuadrants(const Point *points, size_t N, size_t bins[4]) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "22_histogram_count_quadrants",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use MPI to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The result is stored in bins on rank 0.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[4] = {0, 0, 0, 0};\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            localBins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            localBins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            localBins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localBins[4] = {0, 0, 0, 0};\n\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            localBins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            localBins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            localBins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[4] = {0, 0, 0, 0};\n\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            localBins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            localBins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            localBins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[4] = {0, 0, 0, 0};\n\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            localBins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            localBins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            localBins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[4] = {0, 0, 0, 0};\n\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            localBins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            localBins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            localBins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[4] = {0, 0, 0, 0};\n\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            localBins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            localBins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            localBins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[4] = {0, 0, 0, 0};\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            localBins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            localBins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            localBins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localBins[4] = {0, 0, 0, 0};\n\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            localBins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            localBins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            localBins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[4] = {0, 0, 0, 0};\n\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            localBins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            localBins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            localBins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localBins[4] = {0, 0, 0, 0};\n\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            localBins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            localBins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            localBins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localBins[4] = {0, 0, 0, 0};\n\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            localBins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            localBins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            localBins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[4] = {0, 0, 0, 0};\n\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            localBins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            localBins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            localBins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> localBins = {0, 0, 0, 0};\n\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            localBins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            localBins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            localBins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[4] = {0, 0, 0, 0};\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            localBins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            localBins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            localBins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[4] = {0, 0, 0, 0};\n\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            localBins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            localBins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            localBins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> localBins = {0, 0, 0, 0};\n\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            localBins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            localBins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            localBins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[4] = {0, 0, 0, 0};\n\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            localBins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            localBins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            localBins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[4] = {0, 0, 0, 0};\n\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            localBins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            localBins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            localBins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[4] = {0, 0, 0, 0};\n\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            localBins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            localBins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            localBins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[4] = {0, 0, 0, 0};\n    for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            localBins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            localBins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            localBins[2]++;\n        } else if (point.x > 0 && point.y < 0) {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "22_histogram_count_quadrants",
    "parallelism_model": "cuda",
    "prompt": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use CUDA to count in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\n__global__ void countQuadrants(const Point *points, size_t N, size_t bins[4]) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "22_histogram_count_quadrants",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use MPI and OpenMP to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localBins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> localBins = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = rank; i < points.size(); i += size) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[3]++;\n        }\n    }\n\n    for (int i = 0; i < 4; i++) {\n        MPI_Reduce(&localBins[i], &bins[i], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bins.fill(0);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n\n    if (rank == 0) {\n        std::array<size_t, 4> localBins;\n        MPI_Reduce(bins.data(), localBins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        bins = localBins;\n    } else {\n        MPI_Reduce(bins.data(), nullptr, 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localBins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bins.fill(0);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n\n    if (rank == 0) {\n        std::array<size_t, 4> recv_bins;\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(recv_bins.data(), 4, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 4; ++j) {\n                bins[j] += recv_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(bins.data(), 4, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> localBins = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            localBins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            localBins[3]++;\n        }\n    }\n\n    for (int i = 0; i < 4; ++i) {\n        MPI_Reduce(&localBins[i], &bins[i], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        int quadrant = 0;\n        if (points[i].x > 0 && points[i].y > 0) {\n            quadrant = 0;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            quadrant = 1;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            quadrant = 2;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            quadrant = 3;\n        }\n        #pragma omp critical\n        {\n            bins[quadrant]++;\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "24_histogram_count_quartile",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use Kokkos to compute in parallel. Assume that Kokkos has already been initialized.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(Kokkos::View<const double*> const& x, Kokkos::View<size_t[4]> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    double val = x(i);\n    if (val - floor(val) < 0.25) {\n      Kokkos::atomic_fetch_add(&bins(0), 1);\n    } else if (val - floor(val) < 0.5) {\n      Kokkos::atomic_fetch_add(&bins(1), 1);\n    } else if (val - floor(val) < 0.75) {\n      Kokkos::atomic_fetch_add(&bins(2), 1);\n    } else {\n      Kokkos::atomic_fetch_add(&bins(3), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double val = x(i);\n        int binIndex = static_cast<int>(val * 4) % 4;\n        Kokkos::atomic_fetch_add(&bins(binIndex), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double value = x(i);\n        int binIndex = value * 4;\n        if (binIndex == 4) {\n            binIndex = 3;\n        }\n        Kokkos::atomic_fetch_add(&bins(binIndex), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double val = x(i);\n        if (val - floor(val) < 0.25) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (val - floor(val) < 0.5) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (val - floor(val) < 0.75) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double val = x(i);\n        int binIndex = static_cast<int>(val * 4) % 4;\n        Kokkos::atomic_fetch_add(&bins(binIndex), 1);\n    });\n    Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    double val = x(i);\n    if (val - floor(val) < 0.25) {\n      Kokkos::atomic_fetch_add(&bins(0), 1);\n    } else if (val - floor(val) < 0.5) {\n      Kokkos::atomic_fetch_add(&bins(1), 1);\n    } else if (val - floor(val) < 0.75) {\n      Kokkos::atomic_fetch_add(&bins(2), 1);\n    } else {\n      Kokkos::atomic_fetch_add(&bins(3), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double val = x(i);\n        int binIndex = static_cast<int>(val * 4) % 4;\n        Kokkos::atomic_fetch_add(&bins(binIndex), 1);\n    });\n    Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    double val = x(i);\n    if (val - floor(val) < 0.25) {\n      Kokkos::atomic_fetch_add(&bins(0), 1);\n    } else if (val - floor(val) < 0.5) {\n      Kokkos::atomic_fetch_add(&bins(1), 1);\n    } else if (val - floor(val) < 0.75) {\n      Kokkos::atomic_fetch_add(&bins(2), 1);\n    } else {\n      Kokkos::atomic_fetch_add(&bins(3), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double val = x(i);\n        int bin = static_cast<int>(val * 4) % 4;\n        Kokkos::atomic_fetch_add(&bins(bin), 1);\n    });\n    Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    double value = x(i);\n    int binIndex = static_cast<int>(value * 4) % 4;\n    Kokkos::atomic_fetch_add(&bins(binIndex), 1);\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        double val = x(i);\n        if (val - floor(val) < 0.25) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (val - floor(val) < 0.5) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (val - floor(val) < 0.75) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n    Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    double val = x(i);\n    if (val - floor(val) < 0.25) {\n      Kokkos::atomic_fetch_add(&bins(0), 1);\n    } else if (val - floor(val) < 0.5) {\n      Kokkos::atomic_fetch_add(&bins(1), 1);\n    } else if (val - floor(val) < 0.75) {\n      Kokkos::atomic_fetch_add(&bins(2), 1);\n    } else {\n      Kokkos::atomic_fetch_add(&bins(3), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double val = x(i);\n        int bin = static_cast<int>(val * 4) % 4;\n        Kokkos::atomic_fetch_add(&bins(bin), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    double val = x(i);\n    if (val - floor(val) < 0.25) {\n      Kokkos::atomic_fetch_add(&bins(0), 1);\n    } else if (val - floor(val) < 0.5) {\n      Kokkos::atomic_fetch_add(&bins(1), 1);\n    } else if (val - floor(val) < 0.75) {\n      Kokkos::atomic_fetch_add(&bins(2), 1);\n    } else {\n      Kokkos::atomic_fetch_add(&bins(3), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    double val = x(i);\n    if (val - floor(val) < 0.25) {\n      Kokkos::atomic_fetch_add(&bins(0), 1);\n    } else if (val - floor(val) < 0.5) {\n      Kokkos::atomic_fetch_add(&bins(1), 1);\n    } else if (val - floor(val) < 0.75) {\n      Kokkos::atomic_fetch_add(&bins(2), 1);\n    } else {\n      Kokkos::atomic_fetch_add(&bins(3), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double val = x(i);\n        int bin = static_cast<int>(val * 4) % 4;\n        Kokkos::atomic_fetch_add(&bins(bin), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double value = x(i);\n        int binIndex = static_cast<int>(value * 4) % 4;\n        Kokkos::atomic_fetch_add(&bins(binIndex), 1);\n    });\n    Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    double fractional_part = x(i) - floor(x(i));\n    if (fractional_part < 0.25) {\n      Kokkos::atomic_fetch_add(&bins(0), 1);\n    } else if (fractional_part < 0.5) {\n      Kokkos::atomic_fetch_add(&bins(1), 1);\n    } else if (fractional_part < 0.75) {\n      Kokkos::atomic_fetch_add(&bins(2), 1);\n    } else {\n      Kokkos::atomic_fetch_add(&bins(3), 1);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double val = x(i);\n        int bin = static_cast<int>(val * 4) % 4;\n        Kokkos::atomic_fetch_add(&bins(bin), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double val = x(i);\n        int bin = static_cast<int>(val * 4) % 4;\n        Kokkos::atomic_fetch_add(&bins(bin), 1);\n    });\n    Kokkos::fence();\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "24_histogram_count_quartile",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (fractionalPart < 0.5) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (fractionalPart < 0.75) {\n            #pragma omp atomic\n            ++bins[2];\n        } else {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (fractionalPart < 0.5) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (fractionalPart < 0.75) {\n            #pragma omp atomic\n            ++bins[2];\n        } else {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (fractionalPart < 0.5) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (fractionalPart < 0.75) {\n            #pragma omp atomic\n            ++bins[2];\n        } else {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (fractionalPart < 0.5) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (fractionalPart < 0.75) {\n            #pragma omp atomic\n            ++bins[2];\n        } else {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (fractionalPart < 0.5) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (fractionalPart < 0.75) {\n            #pragma omp atomic\n            ++bins[2];\n        } else {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractional_part = x[i] - floor(x[i]);\n        if (fractional_part < 0.25) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (fractional_part < 0.5) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (fractional_part < 0.75) {\n            #pragma omp atomic\n            ++bins[2];\n        } else {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (fractionalPart < 0.5) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (fractionalPart < 0.75) {\n            #pragma omp atomic\n            ++bins[2];\n        } else {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (fractionalPart < 0.5) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (fractionalPart < 0.75) {\n            #pragma omp atomic\n            ++bins[2];\n        } else {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (fractionalPart < 0.5) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (fractionalPart < 0.75) {\n            #pragma omp atomic\n            ++bins[2];\n        } else {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (fractionalPart < 0.5) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (fractionalPart < 0.75) {\n            #pragma omp atomic\n            ++bins[2];\n        } else {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (fractionalPart < 0.5) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (fractionalPart < 0.75) {\n            #pragma omp atomic\n            ++bins[2];\n        } else {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (fractionalPart < 0.5) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (fractionalPart < 0.75) {\n            #pragma omp atomic\n            ++bins[2];\n        } else {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractional_part = x[i] - floor(x[i]);\n        if (fractional_part < 0.25) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (fractional_part < 0.5) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (fractional_part < 0.75) {\n            #pragma omp atomic\n            ++bins[2];\n        } else {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (fractionalPart < 0.5) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (fractionalPart < 0.75) {\n            #pragma omp atomic\n            ++bins[2];\n        } else {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (fractionalPart < 0.5) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (fractionalPart < 0.75) {\n            #pragma omp atomic\n            ++bins[2];\n        } else {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (fractionalPart < 0.5) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (fractionalPart < 0.75) {\n            #pragma omp atomic\n            ++bins[2];\n        } else {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (fractionalPart < 0.5) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (fractionalPart < 0.75) {\n            #pragma omp atomic\n            ++bins[2];\n        } else {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (fractionalPart < 0.5) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (fractionalPart < 0.75) {\n            #pragma omp atomic\n            ++bins[2];\n        } else {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (fractionalPart < 0.5) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (fractionalPart < 0.75) {\n            #pragma omp atomic\n            ++bins[2];\n        } else {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (fractionalPart < 0.5) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (fractionalPart < 0.75) {\n            #pragma omp atomic\n            ++bins[2];\n        } else {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "24_histogram_count_quartile",
    "parallelism_model": "serial",
    "prompt": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (double num : x) {\n        double fractionalPart = num - static_cast<int>(num);\n        if (fractionalPart < 0.25) {\n            bins[0]++;\n        } else if (fractionalPart < 0.5) {\n            bins[1]++;\n        } else if (fractionalPart < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (double num : x) {\n        double fractionalPart = num - std::floor(num);\n        if (fractionalPart < 0.25) {\n            bins[0]++;\n        } else if (fractionalPart < 0.5) {\n            bins[1]++;\n        } else if (fractionfractionalPart < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (double num : x) {\n        double fractionalPart = num - std::floor(num);\n        if (fractionalPart < 0.25) {\n            bins[0]++;\n        } else if (fractionalPart < 0.5) {\n            bins[1]++;\n        } else if (fractionfractionalPart < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (double num : x) {\n        double fractionalPart = num - std::floor(num);\n        if (fractionalPart < 0.25) {\n            bins[0]++;\n        } else if (fractionalPart < 0.5) {\n            bins[1]++;\n        } else if (fractionfractionalPart < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (double num : x) {\n        double fractionalPart = num - std::floor(num);\n        if (fractionalPart < 0.25) {\n            bins[0]++;\n        } else if (fractionalPart < 0.5) {\n            bins[1]++;\n        } else if (fractionfractionalPart < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (double num : x) {\n        double fractionalPart = num - std::floor(num);\n        if (fractionalPart < 0.25) {\n            bins[0]++;\n        } else if (fractionalPart < 0.5) {\n            bins[1]++;\n        } else if (fractionfractionalPart < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (double num : x) {\n        double fractionalPart = num - std::floor(num);\n        if (fractionalPart < 0.25) {\n            bins[0]++;\n        } else if (fractionalPart < 0.5) {\n            bins[1]++;\n        } else if (fractionfractionalPart < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (double num : x) {\n        double fractionalPart = num - std::floor(num);\n        if (fractionalPart < 0.25) {\n            bins[0]++;\n        } else if (fractionalPart < 0.5) {\n            bins[1]++;\n        } else if (fractionfractionalPart < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (double num : x) {\n        double fractionalPart = num - std::floor(num);\n        if (fractionalPart < 0.25) {\n            bins[0]++;\n        } else if (fractionalPart < 0.5) {\n            bins[1]++;\n        } else if (fractionfractionalPart < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (double num : x) {\n        double fractionalPart = num - std::floor(num);\n        if (fractionalPart < 0.25) {\n            bins[0]++;\n        } else if (fractionalPart < 0.5) {\n            bins[1]++;\n        } else if (fractionfractionalPart < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (double num : x) {\n        double fractionalPart = num - std::floor(num);\n        if (fractionalPart < 0.25) {\n            bins[0]++;\n        } else if (fractionalPart < 0.5) {\n            bins[1]++;\n        } else if (fractionfractionalPart < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (double num : x) {\n        double fractionalPart = num - std::floor(num);\n        if (fractionalPart < 0.25) {\n            bins[0]++;\n        } else if (fractionalPart < 0.5) {\n            bins[1]++;\n        } else if (fractionfractionalPart < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (double num : x) {\n        double fractionalPart = num - static_cast<int>(num);\n        if (fractionalPart < 0.25) {\n            bins[0]++;\n        } else if (fractionalPart < 0.5) {\n            bins[1]++;\n        } else if (fractionalPart < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (double num : x) {\n        double fractionalPart = num - std::floor(num);\n        if (fractionalPart < 0.25) {\n            bins[0]++;\n        } else if (fractionalPart < 0.5) {\n            bins[1]++;\n        } else if (fractionfractionalPart < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (double num : x) {\n        double fractionalPart = num - std::floor(num);\n        if (fractionalPart < 0.25) {\n            bins[0]++;\n        } else if (fractionalPart < 0.5) {\n            bins[1]++;\n        } else if (fractionfractionalPart < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (double num : x) {\n        double fractionalPart = num - std::floor(num);\n        if (fractionalPart < 0.25) {\n            bins[0]++;\n        } else if (fractionalPart < 0.5) {\n            bins[1]++;\n        } else if (fractionfractionalPart < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (double num : x) {\n        double fractionalPart = num - std::floor(num);\n        if (fractionalPart < 0.25) {\n            bins[0]++;\n        } else if (fractionalPart < 0.5) {\n            bins[1]++;\n        } else if (fractionalPart < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (double num : x) {\n        double fractionalPart = num - std::floor(num);\n        if (fractionalPart < 0.25) {\n            bins[0]++;\n        } else if (fractionalPart < 0.5) {\n            bins[1]++;\n        } else if (fractionalPart < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (double num : x) {\n        double fractionalPart = num - std::floor(num);\n        if (fractionalPart < 0.25) {\n            bins[0]++;\n        } else if (fractionalPart < 0.5) {\n            bins[1]++;\n        } else if (fractionalPart < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
      "\n    for (double num : x) {\n        double fractionalPart = num - std::floor(num);\n        if (fractionalPart < 0.25) {\n            bins[0]++;\n        } else if (fractionalPart < 0.5) {\n            bins[1]++;\n        } else if (fractionalPart < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "24_histogram_count_quartile",
    "parallelism_model": "hip",
    "prompt": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\n__global__ void countQuartiles(const double *x, size_t N, size_t bins[4]) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid];\n        double fractionalPart = val - floor(val);\n        if (fractionalPart < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (fractionalPart < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (fractionalPart < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "24_histogram_count_quartile",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use MPI to compute in parallel. Assume that MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[4] = {0, 0, 0, 0};\n    for (double value : x) {\n        double fractional_part = value - floor(value);\n        if (fractional_part < 0.25) {\n            local_counts[0]++;\n        } else if (fractional_part < 0.5) {\n            local_counts[1]++;\n        } else if (fractional_part < 0.75) {\n            local_counts[2]++;\n        } else {\n            local_counts[3]++;\n        }\n    }\n\n    int global_counts[4];\n    MPI_Reduce(local_counts, global_counts, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; i++) {\n            bins[i] = global_counts[i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localCounts[4] = {0, 0, 0, 0};\n    for (size_t i = rank; i < x.size(); i += size) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localCounts[0]++;\n        } else if (fractionalPart < 0.5) {\n            localCounts[1]++;\n        } else if (fractionalPart < 0.75) {\n            localCounts[2]++;\n        } else {\n            localCounts[3]++;\n        }\n    }\n\n    int globalCounts[4];\n    MPI_Reduce(localCounts, globalCounts, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; i++) {\n            bins[i] = globalCounts[i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_counts[4] = {0, 0, 0, 0};\n    for (size_t i = rank; i < x.size(); i += size) {\n        double fractional_part = x[i] - floor(x[i]);\n        if (fractional_part < 0.25) {\n            local_counts[0]++;\n        } else if (fractional_part < 0.5) {\n            local_counts[1]++;\n        } else if (fractional_part < 0.75) {\n            local_counts[2]++;\n        } else {\n            local_counts[3]++;\n        }\n    }\n\n    MPI_Reduce(local_counts, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> localBins = {0, 0, 0, 0};\n\n    for (size_t i = rank; i < x.size(); i += size) {\n        double fractionalPart = x[i] - std::floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localBins[0]++;\n        } else if (fractionalPart < 0.5) {\n            localBins[1]++;\n        } else if (fractionjsonPart < 0.75) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> localBins = {0, 0, 0, 0};\n    for (size_t i = rank; i < x.size(); i += size) {\n        double fractionalPart = x[i] - std::floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localBins[0]++;\n        } else if (fractionalPart < 0.5) {\n            localBins[1]++;\n        } else if (fractionjsonPart < 0.75) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localCounts[4] = {0, 0, 0, 0};\n    for (size_t i = rank; i < x.size(); i += size) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localCounts[0]++;\n        } else if (fractionalPart < 0.5) {\n            localCounts[1]++;\n        } else if (fractionalPart < 0.75) {\n            localCounts[2]++;\n        } else {\n            localCounts[3]++;\n        }\n    }\n\n    int globalCounts[4] = {0, 0, 0, 0};\n    MPI_Reduce(localCounts, globalCounts, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; i++) {\n            bins[i] = globalCounts[i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localCounts[4] = {0, 0, 0, 0};\n    for (size_t i = rank; i < x.size(); i += size) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localCounts[0]++;\n        } else if (fractionalPart < 0.5) {\n            localCounts[1]++;\n        } else if (fractionalPart < 0.75) {\n            localCounts[2]++;\n        } else {\n            localCounts[3]++;\n        }\n    }\n\n    int globalCounts[4] = {0, 0, 0, 0};\n    MPI_Reduce(localCounts, globalCounts, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; i++) {\n            bins[i] = globalCounts[i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localCounts[4] = {0, 0, 0, 0};\n    for (size_t i = rank; i < x.size(); i += size) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localCounts[0]++;\n        } else if (fractionalPart < 0.5) {\n            localCounts[1]++;\n        } else if (fractionalPart < 0.75) {\n            localCounts[2]++;\n        } else {\n            localCounts[3]++;\n        }\n    }\n\n    size_t globalCounts[4] = {0, 0, 0, 0};\n    MPI_Reduce(localCounts, globalCounts, 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; i++) {\n            bins[i] = globalCounts[i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localBins[4] = {0, 0, 0, 0};\n    for (size_t i = rank; i < x.size(); i += size) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localBins[0]++;\n        } else if (fractionalPart < 0.5) {\n            localBins[1]++;\n        } else if (fractionalPart < 0.75) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localCounts[4] = {0, 0, 0, 0};\n    for (size_t i = rank; i < x.size(); i += size) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localCounts[0]++;\n        } else if (fractionalPart < 0.5) {\n            localCounts[1]++;\n        } else if (fractionalPart < 0.75) {\n            localCounts[2]++;\n        } else {\n            localCounts[3]++;\n        }\n    }\n\n    int globalCounts[4] = {0, 0, 0, 0};\n    MPI_Reduce(localCounts, globalCounts, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; i++) {\n            bins[i] = globalCounts[i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> localBins = {0, 0, 0, 0};\n    for (size_t i = rank; i < x.size(); i += size) {\n        double fractionalPart = x[i] - std::floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localBins[0]++;\n        } else if (fractionalPart < 0.5) {\n            localBins[1]++;\n        } else if (fractionjsonPart < 0.75) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localCounts[4] = {0, 0, 0, 0};\n    for (size_t i = rank; i < x.size(); i += size) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localCounts[0]++;\n        } else if (fractionalPart < 0.5) {\n            localCounts[1]++;\n        } else if (fractionalPart < 0.75) {\n            localCounts[2]++;\n        } else {\n            localCounts[3]++;\n        }\n    }\n\n    int globalCounts[4] = {0, 0, 0, 0};\n    MPI_Reduce(localCounts, globalCounts, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; i++) {\n            bins[i] = globalCounts[i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localCounts[4] = {0, 0, 0, 0};\n    for (size_t i = rank; i < x.size(); i += size) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localCounts[0]++;\n        } else if (fractionalPart < 0.5) {\n            localCounts[1]++;\n        } else if (fractionalPart < 0.75) {\n            localCounts[2]++;\n        } else {\n            localCounts[3]++;\n        }\n    }\n\n    int globalCounts[4] = {0, 0, 0, 0};\n    MPI_Reduce(localCounts, globalCounts, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; i++) {\n            bins[i] = globalCounts[i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localBins[4] = {0, 0, 0, 0};\n    for (size_t i = rank; i < x.size(); i += size) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localBins[0]++;\n        } else if (fractionalPart < 0.5) {\n            localBins[1]++;\n        } else if (fractionalPart < 0.75) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> localBins = {0, 0, 0, 0};\n\n    for (size_t i = rank; i < x.size(); i += size) {\n        double fractionalPart = x[i] - std::floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localBins[0]++;\n        } else if (fractionalPart < 0.5) {\n            localBins[1]++;\n        } else if (fractionjsonPart < 0.75) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localCounts[4] = {0, 0, 0, 0};\n    for (size_t i = rank; i < x.size(); i += size) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localCounts[0]++;\n        } else if (fractionalPart < 0.5) {\n            localCounts[1]++;\n        } else if (fractionalPart < 0.75) {\n            localCounts[2]++;\n        } else {\n            localCounts[3]++;\n        }\n    }\n\n    int globalCounts[4] = {0, 0, 0, 0};\n    MPI_Reduce(localCounts, globalCounts, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; i++) {\n            bins[i] = globalCounts[i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> localBins = {0, 0, 0, 0};\n    for (size_t i = rank; i < x.size(); i += size) {\n        double fractionalPart = x[i] - std::floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localBins[0]++;\n        } else if (fractionalPart < 0.5) {\n            localBins[1]++;\n        } else if (fractionjsonPart < 0.75) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localCounts[4] = {0, 0, 0, 0};\n    for (size_t i = rank; i < x.size(); i += size) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localCounts[0]++;\n        } else if (fractionalPart < 0.5) {\n            localCounts[1]++;\n        } else if (fractionalPart < 0.75) {\n            localCounts[2]++;\n        } else {\n            localCounts[3]++;\n        }\n    }\n\n    int globalCounts[4] = {0, 0, 0, 0};\n    MPI_Reduce(localCounts, globalCounts, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; i++) {\n            bins[i] = globalCounts[i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localCounts[4] = {0, 0, 0, 0};\n    for (size_t i = rank; i < x.size(); i += size) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localCounts[0]++;\n        } else if (fractionalPart < 0.5) {\n            localCounts[1]++;\n        } else if (fractionalPart < 0.75) {\n            localCounts[2]++;\n        } else {\n            localCounts[3]++;\n        }\n    }\n\n    int globalCounts[4] = {0, 0, 0, 0};\n    MPI_Reduce(localCounts, globalCounts, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; i++) {\n            bins[i] = globalCounts[i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localCounts[4] = {0, 0, 0, 0};\n    for (size_t i = rank; i < x.size(); i += size) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localCounts[0]++;\n        } else if (fractionalPart < 0.5) {\n            localCounts[1]++;\n        } else if (fractionalPart < 0.75) {\n            localCounts[2]++;\n        } else {\n            localCounts[3]++;\n        }\n    }\n\n    int globalCounts[4];\n    MPI_Reduce(localCounts, globalCounts, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; i++) {\n            bins[i] = globalCounts[i];\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "24_histogram_count_quartile",
    "parallelism_model": "cuda",
    "prompt": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\n__global__ void countQuartiles(const double *x, size_t N, size_t bins[4]) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid];\n        double fractionalPart = val - floor(val);\n        if (fractionalPart < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (fractionalPart < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (fractionalPart < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double value = x[tid];\n        double fractionalPart = value - floor(value);\n        if (fractionalPart < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (fractionalPart < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (fractionalPart < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double value = x[tid] - floor(x[tid]);\n        if (value < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (value < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (value < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid] - floor(x[tid]);\n        if (val < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (val < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (val < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "24_histogram_count_quartile",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use MPI and OpenMP to compute in parallel. Assume that MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractional_part = x[i] - floor(x[i]);\n        if (fractional_part < 0.25) {\n            local_counts[0]++;\n        } else if (fractional_part < 0.5) {\n            local_counts[1]++;\n        } else if (fractional_part < 0.75) {\n            local_counts[2]++;\n        } else {\n            local_counts[3]++;\n        }\n    }\n\n    int global_counts[4] = {0, 0, 0, 0};\n    MPI_Reduce(local_counts, global_counts, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; ++i) {\n            bins[i] = global_counts[i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[4] = {0, 0, 0, 0};\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractional_part = x[i] - floor(x[i]);\n        if (fractional_part < 0.25) {\n            local_counts[0]++;\n        } else if (fractional_part < 0.5) {\n            local_counts[1]++;\n        } else if (fractional_part < 0.75) {\n            local_counts[2]++;\n        } else {\n            local_counts[3]++;\n        }\n    }\n\n    int global_counts[4] = {0, 0, 0, 0};\n    MPI_Reduce(local_counts, global_counts, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; ++i) {\n            bins[i] = global_counts[i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localBins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localBins[0]++;\n        } else if (fractionalPart < 0.5) {\n            localBins[1]++;\n        } else if (fractionalPart < 0.75) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localBins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localBins[0]++;\n        } else if (fractionalPart < 0.5) {\n            localBins[1]++;\n        } else if (fractionalPart < 0.75) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localBins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localBins[0]++;\n        } else if (fractionalPart < 0.5) {\n            localBins[1]++;\n        } else if (fractionalPart < 0.75) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localBins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localBins[0]++;\n        } else if (fractionalPart < 0.5) {\n            localBins[1]++;\n        } else if (fractionalPart < 0.75) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localBins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localBins[0]++;\n        } else if (fractionalPart < 0.5) {\n            localBins[1]++;\n        } else if (fractionalPart < 0.75) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localBins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localBins[0]++;\n        } else if (fractionalPart < 0.5) {\n            localBins[1]++;\n        } else if (fractionalPart < 0.75) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localBins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localBins[0]++;\n        } else if (fractionalPart < 0.5) {\n            localBins[1]++;\n        } else if (fractionalPart < 0.75) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localBins[4] = {0, 0, 0, 0};\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localBins[0]++;\n        } else if (fractionalPart < 0.5) {\n            localBins[1]++;\n        } else if (fractionalPart < 0.75) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n\n    for (int i = 0; i < 4; ++i) {\n        size_t globalBin;\n        MPI_Reduce(&localBins[i], &globalBin, 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        if (rank == 0) {\n            bins[i] = globalBin;\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localBins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localBins[0]++;\n        } else if (fractionalPart < 0.5) {\n            localBins[1]++;\n        } else if (fractionalPart < 0.75) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localBins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localBins[0]++;\n        } else if (fractionalPart < 0.5) {\n            localBins[1]++;\n        } else if (fractionalPart < 0.75) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localBins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localBins[0]++;\n        } else if (fractionalPart < 0.5) {\n            localBins[1]++;\n        } else if (fractionalPart < 0.75) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[4] = {0, 0, 0, 0};\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractional_part = x[i] - floor(x[i]);\n        if (fractional_part >= 0 && fractional_part < 0.25) {\n            local_counts[0]++;\n        } else if (fractional_part >= 0.25 && fractional_part < 0.5) {\n            local_counts[1]++;\n        } else if (fractional_part >= 0.5 && fractional_part < 0.75) {\n            local_counts[2]++;\n        } else if (fractional_part >= 0.75 && fractional_part < 1) {\n            local_counts[3]++;\n        }\n    }\n\n    int global_counts[4] = {0, 0, 0, 0};\n    MPI_Reduce(local_counts, global_counts, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; ++i) {\n            bins[i] = global_counts[i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localBins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart < 0.25) {\n            localBins[0]++;\n        } else if (fractionalPart < 0.5) {\n            localBins[1]++;\n        } else if (fractionalPart < 0.75) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractional_part = x[i] - floor(x[i]);\n        if (fractional_part >= 0 && fractional_part < 0.25) {\n            local_counts[0]++;\n        } else if (fractional_part >= 0.25 && fractional_part < 0.5) {\n            local_counts[1]++;\n        } else if (fractional_part >= 0.5 && fractional_part < 0.75) {\n            local_counts[2]++;\n        } else if (fractional_part >= 0.75 && fractional_part < 1) {\n            local_counts[3]++;\n        }\n    }\n\n    int global_counts[4] = {0, 0, 0, 0};\n    MPI_Reduce(local_counts, global_counts, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; ++i) {\n            bins[i] = global_counts[i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_bins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractional_part = x[i] - floor(x[i]);\n        if (fractional_part < 0.25) {\n            local_bins[0]++;\n        } else if (fractional_part < 0.5) {\n            local_bins[1]++;\n        } else if (fractional_part < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localBins[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart >= 0 && fractionalPart < 0.25) {\n            localBins[0]++;\n        } else if (fractionalPart >= 0.25 && fractionalPart < 0.5) {\n            localBins[1]++;\n        } else if (fractionalPart >= 0.5 && fractionalPart < 0.75) {\n            localBins[2]++;\n        } else if (fractionalPart >= 0.75 && fractionalPart < 1) {\n            localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractional_part = x[i] - floor(x[i]);\n        if (fractional_part >= 0 && fractional_part < 0.25) {\n            local_counts[0]++;\n        } else if (fractional_part >= 0.25 && fractional_part < 0.5) {\n            local_counts[1]++;\n        } else if (fractional_part >= 0.5 && fractional_part < 0.75) {\n            local_counts[2]++;\n        } else if (fractional_part >= 0.75 && fractional_part < 1) {\n            local_counts[3]++;\n        }\n    }\n\n    int global_counts[4] = {0, 0, 0, 0};\n    MPI_Reduce(local_counts, global_counts, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; ++i) {\n            bins[i] = global_counts[i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_counts[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fractional_part = x[i] - floor(x[i]);\n        if (fractional_part < 0.25) {\n            local_counts[0]++;\n        } else if (fractional_part < 0.5) {\n            local_counts[1]++;\n        } else if (fractional_part < 0.75) {\n            local_counts[2]++;\n        } else {\n            local_counts[3]++;\n        }\n    }\n\n    int global_counts[4] = {0, 0, 0, 0};\n    MPI_Reduce(local_counts, global_counts, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; ++i) {\n            bins[i] = global_counts[i];\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "20_histogram_pixel_histogram",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use Kokkos to count in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(Kokkos::View<const int*> const& image, Kokkos::View<size_t[256]> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        Kokkos::atomic_fetch_add(&bins(image(i)), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        Kokkos::atomic_fetch_add(&bins(image(i)), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        Kokkos::atomic_fetch_add(&bins(image(i)), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(int i) {\n        Kokkos::atomic_fetch_add(&bins(image(i)), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        Kokkos::atomic_fetch_add(&bins(image(i)), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(int i) {\n        Kokkos::atomic_fetch_add(&bins(image(i)), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        Kokkos::atomic_fetch_add(&bins(image(i)), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        Kokkos::atomic_fetch_add(&bins(image(i)), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        Kokkos::atomic_fetch_add(&bins(image(i)), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        Kokkos::atomic_fetch_add(&bins(image(i)), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(int i) {\n        Kokkos::atomic_fetch_add(&bins(image(i)), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        Kokkos::atomic_fetch_add(&bins(image(i)), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(int i) {\n        Kokkos::atomic_fetch_add(&bins(image(i)), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(int i) {\n        Kokkos::atomic_fetch_add(&bins(image(i)), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(int i) {\n        Kokkos::atomic_fetch_add(&bins(image(i)), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        Kokkos::atomic_fetch_add(&bins(image(i)), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        Kokkos::atomic_fetch_add(&bins(image(i)), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        Kokkos::atomic_fetch_add(&bins(image(i)), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        Kokkos::atomic_fetch_add(&bins(image(i)), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        Kokkos::atomic_fetch_add(&bins(image(i)), 1);\n    });\n    Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "20_histogram_pixel_histogram",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use OpenMP to count in parallel.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        ++bins[image[i]];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        ++bins[image[i]];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        ++bins[image[i]];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        ++bins[image[i]];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        ++bins[image[i]];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        ++bins[image[i]];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        ++bins[image[i]];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        ++bins[image[i]];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        ++bins[image[i]];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        ++bins[image[i]];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        ++bins[image[i]];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        ++bins[image[i]];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        ++bins[image[i]];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        ++bins[image[i]];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        ++bins[image[i]];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        ++bins[image[i]];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        ++bins[image[i]];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        ++bins[image[i]];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        ++bins[image[i]];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        ++bins[image[i]];\n    }\n}"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "20_histogram_pixel_histogram",
    "parallelism_model": "serial",
    "prompt": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
      "\n    for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
      "\n    for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
      "\n    for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
      "\n    for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
      "\n    for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
      "\n    for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
      "\n    for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
      "\n    for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
      "\n    for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
      "\n    for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
      "\n    for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
      "\n    for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
      "\n    for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
      "\n    for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
      "\n    for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
      "\n    for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
      "\n    for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
      "\n    for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
      "\n    for (int pixel : image) {\n        bins[pixel]++;\n    }\n}"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "20_histogram_pixel_histogram",
    "parallelism_model": "hip",
    "prompt": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use AMD HIP to count in parallel. The kernel is launched with at least N threads.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n__global__ void pixelCounts(const int *image, size_t N, size_t bins[256]) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "20_histogram_pixel_histogram",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use MPI to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    int localStart = rank * (image.size() / size);\n    int localEnd = (rank + 1) * (image.size() / size);\n    if (rank == size - 1) {\n        localEnd = image.size();\n    }\n\n    for (int i = localStart; i < localEnd; ++i) {\n        localBins[image[i]]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    int localStart = rank * (image.size() / size);\n    int localEnd = (rank + 1) * (image.size() / size);\n    if (rank == size - 1) {\n        localEnd = image.size();\n    }\n\n    for (int i = localStart; i < localEnd; ++i) {\n        localBins[image[i]]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> local_bins = {0};\n\n    size_t local_size = image.size() / size;\n    size_t start = rank * local_size;\n    size_t end = (rank == size - 1) ? image.size() : start + local_size;\n\n    for (size_t i = start; i < end; ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    int localStart = rank * (image.size() / size);\n    int localEnd = (rank + 1) * (image.size() / size);\n    if (rank == size - 1) {\n        localEnd = image.size();\n    }\n\n    for (int i = localStart; i < localEnd; ++i) {\n        localBins[image[i]]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    int localStart = rank * (image.size() / size);\n    int localEnd = (rank + 1) * (image.size() / size);\n    if (rank == size - 1) {\n        localEnd = image.size();\n    }\n\n    for (int i = localStart; i < localEnd; ++i) {\n        localBins[image[i]]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    int localStart = rank * (image.size() / size);\n    int localEnd = (rank + 1) * (image.size() / size);\n\n    for (int i = localStart; i < localEnd; ++i) {\n        localBins[image[i]]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    int localStart = rank * (image.size() / size);\n    int localEnd = (rank + 1) * (image.size() / size);\n\n    if (rank == size - 1) {\n        localEnd = image.size();\n    }\n\n    for (int i = localStart; i < localEnd; ++i) {\n        localBins[image[i]]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> local_bins = {0};\n\n    int local_start = rank * (image.size() / size);\n    int local_end = (rank + 1) * (image.size() / size);\n\n    for (int i = local_start; i < local_end; ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    int localStart = rank * (image.size() / size);\n    int localEnd = (rank == size - 1) ? image.size() : (rank + 1) * (image.size() / size);\n\n    for (int i = localStart; i < localEnd; ++i) {\n        localBins[image[i]]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    int localStart = rank * (image.size() / size);\n    int localEnd = (rank == size - 1) ? image.size() : (rank + 1) * (image.size() / size);\n\n    for (int i = localStart; i < localEnd; ++i) {\n        localBins[image[i]]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    int localStart = rank * (image.size() / size);\n    int localEnd = (rank + 1) * (image.size() / size);\n    if (rank == size - 1) {\n        localEnd = image.size();\n    }\n\n    for (int i = localStart; i < localEnd; ++i) {\n        localBins[image[i]]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> local_bins = {0};\n\n    int local_start = rank * (image.size() / size);\n    int local_end = (rank == size - 1) ? image.size() : (rank + 1) * (image.size() / size);\n\n    for (int i = local_start; i < local_end; ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> local_bins = {0};\n\n    int local_start = rank * (image.size() / size);\n    int local_end = (rank == size - 1) ? image.size() : (rank + 1) * (image.size() / size);\n\n    for (int i = local_start; i < local_end; ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    int localStart = rank * (image.size() / size);\n    int localEnd = (rank + 1) * (image.size() / size);\n\n    for (int i = localStart; i < localEnd; i++) {\n        localBins[image[i]]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localBins[256] = {0};\n\n    int localStart = rank * (image.size() / size);\n    int localEnd = (rank + 1) * (image.size() / size);\n    if (rank == size - 1) {\n        localEnd = image.size();\n    }\n\n    for (int i = localStart; i < localEnd; ++i) {\n        localBins[image[i]]++;\n    }\n\n    MPI_Reduce(localBins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    int localStart = rank * (image.size() / size);\n    int localEnd = (rank + 1) * (image.size() / size);\n\n    for (int i = localStart; i < localEnd; ++i) {\n        localBins[image[i]]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> local_bins = {0};\n\n    int local_start = rank * (image.size() / size);\n    int local_end = (rank == size - 1) ? image.size() : (rank + 1) * (image.size() / size);\n\n    for (int i = local_start; i < local_end; ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> local_bins = {0};\n    for (int i = rank; i < image.size(); i += size) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    int localStart = rank * (image.size() / size);\n    int localEnd = (rank + 1) * (image.size() / size);\n\n    for (int i = localStart; i < localEnd; ++i) {\n        localBins[image[i]]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> local_bins = {0};\n\n    int local_start = rank * (image.size() / size);\n    int local_end = (rank + 1) * (image.size() / size);\n\n    if (rank == size - 1) {\n        local_end = image.size();\n    }\n\n    for (int i = local_start; i < local_end; ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "20_histogram_pixel_histogram",
    "parallelism_model": "cuda",
    "prompt": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use CUDA to count in parallel. The kernel is launched with at least N threads.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n__global__ void pixelCounts(const int *image, size_t N, size_t bins[256]) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "20_histogram_pixel_histogram",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use MPI and OpenMP to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_bins[256] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_bins[256] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_bins[256] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> local_bins = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_bins[256] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_bins[256] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_bins[256] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_bins[256] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_bins[256] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_bins[256] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_bins[256] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> local_bins = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_bins[256] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_bins[256] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_bins[256] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_bins[256] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_bins[256] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> local_bins = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_bins[256] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_bins[256] = {0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "38_search_find_the_first_even_number",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the index of the first even number in the vector x.\n   Use Kokkos to parallelize the search. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(Kokkos::View<const int*> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    size_t result = x.extent(0);\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, size_t& lsum) {\n        if (x(i) % 2 == 0 && i < lsum) {\n            lsum = i;\n        }\n    }, Kokkos::Min<size_t>(result));\n    return result;\n}",
      "\n    size_t result = x.extent(0);\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, size_t& lsum) {\n        if (x(i) % 2 == 0 && i < lsum) {\n            lsum = i;\n        }\n    }, Kokkos::Min<size_t>(result));\n    return result;\n}",
      "\n    size_t result = x.extent(0);\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, size_t& lresult) {\n        if (x(i) % 2 == 0 && i < lresult) {\n            lresult = i;\n        }\n    }, Kokkos::Min<size_t>(result));\n    return result;\n}",
      "\n    size_t result = x.extent(0);\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, size_t& lresult) {\n        if (x(i) % 2 == 0) {\n            lresult = i;\n        }\n    }, result);\n    return result;\n}",
      "\n    size_t result = x.extent(0);\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, size_t& lresult) {\n        if (x(i) % 2 == 0 && i < lresult) {\n            lresult = i;\n        }\n    }, Kokkos::Min<size_t>(result));\n    return result;\n}",
      "\n    size_t result = x.extent(0);\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, size_t& lsum) {\n        if (x(i) % 2 == 0 && i < lsum) {\n            lsum = i;\n        }\n    }, Kokkos::Min<size_t>(result));\n    return result;\n}",
      "\n    size_t result = x.extent(0);\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, size_t& lresult) {\n        if (x(i) % 2 == 0 && i < lresult) {\n            lresult = i;\n        }\n    }, Kokkos::Min<size_t>(result));\n    return result;\n}",
      "\n    size_t result = x.extent(0);\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, size_t& lresult) {\n        if (x(i) % 2 == 0 && i < lresult) {\n            lresult = i;\n        }\n    }, Kokkos::Min<size_t>(result));\n    return result;\n}",
      "\n    size_t result = x.extent(0);\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, size_t& lresult) {\n        if (x(i) % 2 == 0 && i < lresult) {\n            lresult = i;\n        }\n    }, Kokkos::Min<size_t>(result));\n    return result;\n}",
      "\n    size_t result = x.extent(0);\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, size_t& lresult) {\n        if (x(i) % 2 == 0 && i < lresult) {\n            lresult = i;\n        }\n    }, Kokkos::Min<size_t>(result));\n    return result;\n}",
      "\n    size_t result = x.extent(0);\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, size_t& lresult) {\n        if (x(i) % 2 == 0 && i < lresult) {\n            lresult = i;\n        }\n    }, Kokkos::Min<size_t>(result));\n    return result;\n}",
      "\n    size_t result = x.extent(0);\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, size_t& lresult) {\n        if (x(i) % 2 == 0 && i < lresult) {\n            lresult = i;\n        }\n    }, Kokkos::Min<size_t>(result));\n    return result;\n}",
      "\n    size_t result = x.extent(0);\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, size_t& lsum) {\n        if (x(i) % 2 == 0 && i < lsum) {\n            lsum = i;\n        }\n    }, Kokkos::Min<size_t>(result));\n    return result;\n}",
      "\n    size_t result = x.extent(0);\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, size_t& lresult) {\n        if (x(i) % 2 == 0 && i < lresult) {\n            lresult = i;\n        }\n    }, Kokkos::Min<size_t>(result));\n    return result;\n}",
      "\n    size_t result = x.size();\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, size_t& lresult) {\n        if (x(i) % 2 == 0 && i < lresult) {\n            lresult = i;\n        }\n    }, Kokkos::Min<size_t>(result));\n    return result;\n}\n",
      "\n    size_t result = x.extent(0);\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, size_t& lsum) {\n        if (x(i) % 2 == 0 && i < lsum) {\n            lsum = i;\n        }\n    }, Kokkos::Min<size_t>(result));\n    return result;\n}",
      "\n    size_t result = x.extent(0);\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, size_t& lval) {\n        if (x(i) % 2 == 0 && i < lval) {\n            lval = i;\n        }\n    }, Kokkos::Min<size_t>(result));\n    return result;\n}\n",
      "\n    size_t result = Kokkos::Impl::kk_reduce( \"findFirstEven\", x.extent(0), KOKKOS_LAMBDA(int i, int& lresult) {\n        if (x(i) % 2 == 0 && lresult == x.extent(0)) {\n            lresult = i;\n        }\n    });\n    return result;\n}",
      "\n    size_t result = Kokkos::Impl::INVALID_INDEX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, size_t& lresult) {\n        if (x(i) % 2 == 0 && lresult == Kokkos::Impl::INVALID_INDEX) {\n            lresult = i;\n        }\n    }, result);\n    return result;\n}",
      "\n    size_t result = x.extent(0);\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, size_t& lresult) {\n        if (x(i) % 2 == 0 && i < lresult) {\n            lresult = i;\n        }\n    }, Kokkos::Min<size_t>(result));\n    return result;\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "38_search_find_the_first_even_number",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the index of the first even number in the vector x.\n   Use OpenMP to parallelize the search.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();  // If no even number is found\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();  // If no even number is found\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size(); // Return x.size() if no even number is found\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();  // Return x.size() if no even number is found\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size(); // Return x.size() if no even number is found\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();  // Return x.size() if no even number is found\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();  // Return x.size() if no even number is found\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();  // Return x.size() if no even number is found\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size(); // Return x.size() if no even number is found\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size(); // If no even number is found\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();  // If no even number is found\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size(); // Return x.size() if no even number is found\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size(); // Return x.size() if no even number is found\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size(); // Return x.size() if no even number is found\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();  // Return x.size() if no even number is found\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size();  // If no even number is found\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size(); // Return x.size() if no even number is found\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size(); // Return x.size() if no even number is found\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size(); // Return size if no even number is found\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return x.size(); // Return x.size() if no even number is found\n}"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "38_search_find_the_first_even_number",
    "parallelism_model": "serial",
    "prompt": "/* Return the index of the first even number in the vector x.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return -1; // If no even number is found\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return -1; // If no even number is found\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return -1; // If no even number is found\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return -1;\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return -1; // If no even number is found\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return -1; // If no even number is found\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return -1;\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return -1; // If no even number is found\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return -1; // If no even number is found\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return -1; // If no even number is found\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return -1; // If no even number is found\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return -1; // If no even number is found\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return -1; // If no even number is found\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return -1; // If no even number is found\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return -1; // If no even number is found\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return -1; // If no even number is found\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return -1;\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return -1; // If no even number is found\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return -1;\n}",
      "\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] % 2 == 0) {\n            return i;\n        }\n    }\n    return -1;\n}"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "38_search_find_the_first_even_number",
    "parallelism_model": "hip",
    "prompt": "/* Find the index of the first even number in the vector x. Store it in firstEvenIndex.\n   Use AMD HIP to parallelize the search. The kernel is launched with at least as many threads as values in x.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\n__global__ void findFirstEven(const int *x, size_t N, size_t *firstEvenIndex) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "38_search_find_the_first_even_number",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the index of the first even number in the vector x.\n   Use MPI to parallelize the search. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_result = x.size(); // Initialize with a value greater than the vector size\n\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n            break;\n        }\n    }\n\n    size_t global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return x.size(); // Return a value greater than the vector size for non-root ranks\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_result = x.size(); // Initialize local_result to the size of the vector as a placeholder\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i; // Update local_result if an even number is found\n            break;\n        }\n    }\n\n    size_t global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result; // Return the global result on rank 0\n    } else {\n        return x.size(); // Return a placeholder value on other ranks\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_result = x.size(); // Initialize local_result to the size of the vector\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i; // Update local_result if an even number is found\n            break;\n        }\n    }\n\n    size_t global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result; // Return the global result on rank 0\n    } else {\n        return x.size(); // Return the size of the vector on other ranks\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_result = x.size(); // Initialize with a value greater than the size of the vector\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n            break;\n        }\n    }\n\n    size_t global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return x.size(); // Return a value greater than the size of the vector for non-root ranks\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_result = x.size(); // Initialize local_result to the size of the vector as a signal that no even number is found\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n            break;\n        }\n    }\n\n    size_t global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return x.size(); // Return the size of the vector as a signal that no even number is found\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_result = x.size(); // Initialize with a value greater than the vector size\n\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n            break;\n        }\n    }\n\n    size_t global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return x.size(); // Return a value greater than the vector size for non-root ranks\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_result = x.size(); // Initialize local_result to the size of the vector, indicating no even number found\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n            break;\n        }\n    }\n\n    size_t global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return x.size(); // Return the size of the vector on non-root ranks\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_result = x.size(); // Initialize local_result to the size of the vector as a flag for not finding any even number\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n            break;\n        }\n    }\n\n    size_t global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return x.size(); // Return the size of the vector as a flag for not finding any even number\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_index = -1;\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_index = i;\n            break;\n        }\n    }\n\n    int global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_result = x.size(); // Initialize local_result to the size of the vector as a flag for not finding any even number\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n            break;\n        }\n    }\n\n    size_t global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return x.size(); // Return the size of the vector as a flag for not finding any even number\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_result = x.size(); // Initialize local_result to the size of the vector as a flag for not finding any even number\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n            break;\n        }\n    }\n\n    size_t global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return x.size(); // Return the size of the vector as a flag for not finding any even number\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_result = x.size();\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n            break;\n        }\n    }\n\n    size_t global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return x.size();\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_result = x.size(); // Initialize local_result to the size of the vector\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n            break;\n        }\n    }\n\n    size_t global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return x.size(); // Return the size of the vector on non-root ranks\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_result = x.size(); // Initialize with a value greater than the size of the vector\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n            break;\n        }\n    }\n\n    size_t global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return x.size(); // Return a value greater than the size of the vector for non-root ranks\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_result = x.size(); // Initialize local_result to the size of the vector as a flag if no even number is found\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n            break;\n        }\n    }\n\n    size_t global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return x.size(); // Return the size of the vector as a flag if no even number is found on rank 0\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = x.size(); // Initialize local_index to the size of the vector as a flag\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_index = i;\n            break;\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0; // Return 0 for non-root ranks\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_result = x.size(); // Initialize local_result to the size of the vector as a flag for not finding any even number\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n            break;\n        }\n    }\n\n    size_t global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return x.size(); // Return the size of the vector as a flag for not finding any even number\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_result = x.size(); // Initialize local_result to a value greater than the size of the vector\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n            break;\n        }\n    }\n\n    size_t global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return x.size(); // Return a value greater than the size of the vector for non-root ranks\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_result = x.size(); // Initialize local_result to the size of the vector as a flag for not finding any even number\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n            break;\n        }\n    }\n\n    size_t global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return x.size(); // Return the size of the vector as a flag for not finding any even number\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_result = x.size(); // Initialize local_result to the size of the vector as a flag for not finding any even number\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n            break;\n        }\n    }\n\n    size_t global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return x.size(); // Return the size of the vector as a flag for not finding any even number\n    }\n}"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "38_search_find_the_first_even_number",
    "parallelism_model": "cuda",
    "prompt": "/* Find the index of the first even number in the vector x. Store it in firstEvenIndex.\n   Use CUDA to parallelize the search. The kernel is launched with at least as many threads as values in x.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\n__global__ void findFirstEven(const int *x, size_t N, size_t *firstEvenIndex) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 == 0) {\n        atomicMin(firstEvenIndex, tid);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 == 0) {\n        atomicMin(firstEvenIndex, tid);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 == 0) {\n        atomicMin(firstEvenIndex, tid);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 == 0) {\n        atomicMin(firstEvenIndex, tid);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 == 0) {\n        atomicMin(firstEvenIndex, tid);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 == 0) {\n        atomicMin(firstEvenIndex, tid);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 == 0) {\n        atomicMin(firstEvenIndex, tid);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 == 0) {\n        atomicMin(firstEvenIndex, tid);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N && x[tid] % 2 == 0) {\n        atomicMin(firstEvenIndex, tid);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] % 2 == 0) {\n            atomicMin(firstEvenIndex, tid);\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "38_search_find_the_first_even_number",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the index of the first even number in the vector x.\n   Use MPI and OpenMP to parallelize the search. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = -1;\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return -1;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = -1;\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return -1;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = -1;\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return -1;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = -1;\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return 0;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = -1;\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return -1;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = -1;\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return -1;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = -1;\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return -1;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = -1;\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return -1;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = -1;\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return -1;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = -1;\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return -1;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = -1;\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return -1;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = -1;\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return -1;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = -1;\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return -1;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = -1;\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return -1;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = -1;\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return -1;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_result = x.size(); // Initialize local_result to the size of the vector\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            #pragma omp critical\n            {\n                if (i < local_result) {\n                    local_result = i;\n                }\n            }\n        }\n    }\n\n    size_t global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return x.size();\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = -1;\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = -1;\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return -1;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = -1;\n\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return -1;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = -1;\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            local_result = i;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return -1;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "36_search_check_if_array_contains_value",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use Kokkos to search in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(Kokkos::View<const int*> const& x, int target) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    bool found = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n        if (x(i) == target) {\n            lfound = true;\n        }\n    }, found);\n    return found;\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    if (x(i) == target) {\n      Kokkos::atomic_set(&result, true);\n    }\n  });\n  Kokkos::fence();\n  return result;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}",
      "\n  bool result = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n    if (x(i) == target) {\n      lresult = true;\n    }\n  }, result);\n  return result;\n}",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}",
      "\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& result) {\n    if (x(i) == target) {\n      result = true;\n    }\n  }, Kokkos::Min<bool>(false));\n  return Kokkos::Min<bool>::value;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}",
      "\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& result) {\n    if (x(i) == target) {\n      result = true;\n    }\n  }, Kokkos::Min<bool>(false));\n  return Kokkos::Min<bool>::value;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}",
      "\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& result) {\n    if (x(i) == target) {\n      result = true;\n    }\n  }, Kokkos::Min<bool>(false));\n  return Kokkos::Min<bool>::value;\n}\n",
      "\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& result) {\n    if (x(i) == target) {\n      result = true;\n    }\n  }, Kokkos::Min<bool>(false));\n  return Kokkos::Min<bool>::value;\n}\n",
      "\n    bool result = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n        if (x(i) == target) {\n            lresult = true;\n        }\n    }, result);\n    return result;\n}",
      "\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& result) {\n    if (x(i) == target) {\n      result = true;\n    }\n  }, Kokkos::Min<bool>(false));\n  return Kokkos::Min<bool>::value;\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    if (x(i) == target) {\n      Kokkos::atomic_set(&result, true);\n    }\n  });\n  Kokkos::fence();\n  return result;\n}\n",
      "\n  bool result = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n    if (x(i) == target) {\n      lresult = true;\n    }\n  }, result);\n  return result;\n}",
      "\n  bool result = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lresult) {\n    if (x(i) == target) {\n      lresult = true;\n    }\n  }, result);\n  return result;\n}",
      "\n    bool found = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n        if (x(i) == target) {\n            lfound = true;\n        }\n    }, found);\n    return found;\n}",
      "\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& result) {\n    if (x(i) == target) {\n      result = true;\n    }\n  }, Kokkos::Min<bool>());\n  return result;\n}"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "36_search_check_if_array_contains_value",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use OpenMP to search in parallel.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    bool found = false;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            found = true;\n        }\n    }\n    return found;\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    bool found = false;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            found = true;\n        }\n    }\n    return found;\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    bool found = false;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            found = true;\n        }\n    }\n    return found;\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    bool found = false;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            found = true;\n        }\n    }\n    return found;\n}"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "36_search_check_if_array_contains_value",
    "parallelism_model": "serial",
    "prompt": "/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (int i : x) {\n        if (i == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    for (int i : x) {\n        if (i == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    for (int i : x) {\n        if (i == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    for (int i : x) {\n        if (i == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    for (int i : x) {\n        if (i == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    for (int i : x) {\n        if (i == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    for (int i : x) {\n        if (i == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    for (int i : x) {\n        if (i == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    for (int i : x) {\n        if (i == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    for (int i : x) {\n        if (i == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    for (int i : x) {\n        if (i == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    for (int i : x) {\n        if (i == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    for (int i : x) {\n        if (i == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            return true;\n        }\n    }\n    return false;\n}",
      "\n    for (int i : x) {\n        if (i == target) {\n            return true;\n        }\n    }\n    return false;\n}"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "36_search_check_if_array_contains_value",
    "parallelism_model": "hip",
    "prompt": "/* Set `found` to true if the vector x contains the value `target`. Set it to false otherwise.\n   Use AMD HIP to search in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\n__global__ void contains(const int *x, size_t N, int target, bool *found) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "36_search_check_if_array_contains_value",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_result = 0;\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] == target) {\n            local_result = 1;\n            break;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (global_result == 1);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_result = false;\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] == target) {\n            local_result = true;\n            break;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_LOR, 0, MPI_COMM_WORLD);\n\n    return (rank == 0) ? global_result : false;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result = false;\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] == target) {\n            result = true;\n            break;\n        }\n    }\n\n    bool global_result;\n    MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LOR, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_result = 0;\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] == target) {\n            local_result = 1;\n            break;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (global_result == 1);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_result = 0;\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] == target) {\n            local_result = 1;\n            break;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (global_result == 1);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_result = false;\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] == target) {\n            local_result = true;\n            break;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_LOR, 0, MPI_COMM_WORLD);\n\n    return (rank == 0) ? global_result : false;\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_result = 0;\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] == target) {\n            local_result = 1;\n            break;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (global_result == 1);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localResult = 0;\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] == target) {\n            localResult = 1;\n            break;\n        }\n    }\n\n    int globalResult;\n    MPI_Reduce(&localResult, &globalResult, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (globalResult == 1);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_result = false;\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] == target) {\n            local_result = true;\n            break;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_LOR, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_result = 0;\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] == target) {\n            local_result = 1;\n            break;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (global_result == 1);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result = false;\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] == target) {\n            result = true;\n            break;\n        }\n    }\n\n    bool global_result;\n    MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LOR, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_result = 0;\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] == target) {\n            local_result = 1;\n            break;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (global_result == 1);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_result = 0;\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] == target) {\n            local_result = 1;\n            break;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (global_result == 1);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_result = 0;\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] == target) {\n            local_result = 1;\n            break;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (global_result == 1);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localResult = 0;\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] == target) {\n            localResult = 1;\n            break;\n        }\n    }\n\n    int globalResult;\n    MPI_Reduce(&localResult, &globalResult, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (globalResult == 1);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_result = 0;\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] == target) {\n            local_result = 1;\n            break;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (global_result == 1);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int localResult = 0;\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] == target) {\n            localResult = 1;\n            break;\n        }\n    }\n\n    int globalResult;\n    MPI_Reduce(&localResult, &globalResult, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (globalResult == 1);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localResult = 0;\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] == target) {\n            localResult = 1;\n            break;\n        }\n    }\n\n    int globalResult;\n    MPI_Reduce(&localResult, &globalResult, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (globalResult == 1);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result = false;\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] == target) {\n            result = true;\n            break;\n        }\n    }\n\n    bool global_result;\n    MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LOR, 0, MPI_COMM_WORLD);\n\n    return (rank == 0) ? global_result : false;\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_result = false;\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] == target) {\n            local_result = true;\n            break;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_LOR, 0, MPI_COMM_WORLD);\n\n    return (rank == 0) ? global_result : false;\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "36_search_check_if_array_contains_value",
    "parallelism_model": "cuda",
    "prompt": "/* Set `found` to true if the vector x contains the value `target`. Set it to false otherwise.\n   Use CUDA to search in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\n__global__ void contains(const int *x, size_t N, int target, bool *found) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == target) {\n            *found = true;\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "36_search_check_if_array_contains_value",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use MPI and OpenMP to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            local_result = 1;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (global_result == 1);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_result = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            local_result = 1;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (global_result == 1);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] == target) {\n            local_result = 1;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (global_result == 1);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] == target) {\n            local_result = 1;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (global_result == 1);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_result = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            local_result = 1;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result == 1;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_result = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            local_result = 1;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result == 1;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            local_result = 1;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (global_result == 1);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            bool result = true;\n            MPI_Reduce(&result, &result, 1, MPI_C_BOOL, MPI_LOR, 0, MPI_COMM_WORLD);\n            if (rank == 0) {\n                return result;\n            }\n        }\n    }\n\n    bool result = false;\n    MPI_Reduce(&result, &result, 1, MPI_C_BOOL, MPI_LOR, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        return result;\n    }\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_result = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            local_result = 1;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result == 1;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            local_result = 1;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (global_result == 1);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            local_result = 1;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (global_result == 1);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            bool result = true;\n            MPI_Reduce(&result, &result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n            if (rank == 0) {\n                return result;\n            }\n        }\n    }\n\n    bool result = false;\n    MPI_Reduce(&result, &result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        return result;\n    }\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_result = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            local_result = 1;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result == 1;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_result = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            local_result = 1;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result == 1;\n    } else {\n        return false;\n    }\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_result = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            local_result = 1;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result == 1;\n    } else {\n        return false;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            local_result = 1;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (global_result == 1);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = 0;\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            local_result = 1;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (global_result == 1);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            local_result = 1;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (global_result == 1);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            local_result = 1;\n        }\n    }\n\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return (global_result == 1);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result = false;\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            result = true;\n        }\n    }\n\n    bool global_result;\n    MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LOR, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "39_search_xor_contains",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use Kokkos to search in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(Kokkos::View<const int*> const& x, Kokkos::View<const int*> const& y, int val) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& result) {\n        if (x(i) == val) {\n            result = !result;\n        }\n    }, Kokkos::ReductionLogicalOr<bool>(result));\n\n    Kokkos::parallel_reduce(y.size(), KOKKOS_LAMBDA(int i, bool& result) {\n        if (y(i) == val) {\n            result = !result;\n        }\n    }, Kokkos::ReductionLogicalOr<bool>(result));\n\n    return result;\n}\n",
      "\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& result) {\n        if (x(i) == val) {\n            result = !result;\n        }\n    }, Kokkos::Min<bool>());\n    \n    Kokkos::parallel_reduce(y.size(), KOKKOS_LAMBDA(int i, bool& result) {\n        if (y(i) == val) {\n            result = !result;\n        }\n    }, Kokkos::Min<bool>());\n\n    return result;\n}\n",
      "\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& result) {\n        if (x(i) == val) {\n            result = !result;\n        }\n    }, Kokkos::ReductionLogicalOr<bool>(result));\n\n    Kokkos::parallel_reduce(y.size(), KOKKOS_LAMBDA(int i, bool& result) {\n        if (y(i) == val) {\n            result = !result;\n        }\n    }, Kokkos::ReductionLogicalOr<bool>(result));\n\n    return result;\n}\n",
      "\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& result) {\n        if (x(i) == val) {\n            result = !result;\n        }\n    }, Kokkos::ReductionLogicalOr<bool>(result));\n\n    Kokkos::parallel_reduce(y.size(), KOKKOS_LAMBDA(int i, bool& result) {\n        if (y(i) == val) {\n            result = !result;\n        }\n    }, Kokkos::ReductionLogicalOr<bool>(result));\n\n    return result;\n}\n",
      "\n  int count_x = 0;\n  int count_y = 0;\n\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& local_count) {\n    if (x(i) == val) {\n      local_count++;\n    }\n  }, count_x);\n\n  Kokkos::parallel_reduce(y.size(), KOKKOS_LAMBDA(int i, int& local_count) {\n    if (y(i) == val) {\n      local_count++;\n    }\n  }, count_y);\n\n  return (count_x == 1) != (count_y == 1);\n}\n",
      "\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& result) {\n        if (x(i) == val) {\n            result = !result;\n        }\n    }, Kokkos::ReductionLogicalOr<bool>(result));\n\n    Kokkos::parallel_reduce(y.size(), KOKKOS_LAMBDA(int i, bool& result) {\n        if (y(i) == val) {\n            result = !result;\n        }\n    }, Kokkos::ReductionLogicalOr<bool>(result));\n\n    return result;\n}\n",
      "\n  int count_x = 0;\n  int count_y = 0;\n\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& local_count) {\n    if (x(i) == val) {\n      local_count++;\n    }\n  }, count_x);\n\n  Kokkos::parallel_reduce(y.size(), KOKKOS_LAMBDA(int i, int& local_count) {\n    if (y(i) == val) {\n      local_count++;\n    }\n  }, count_y);\n\n  return (count_x == 1) != (count_y == 1);\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n        if (x(i) == val) {\n            for (int j = 0; j < y.extent(0); j++) {\n                if (y(j) == val) {\n                    return false;\n                }\n            }\n            return true;\n        } else if (y(i) == val) {\n            for (int j = 0; j < x.extent(0); j++) {\n                if (x(j) == val) {\n                    return false;\n                }\n            }\n            return true;\n        }\n    });\n    return false;\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n        if (x(i) == val) {\n            y(i) = 0;\n        }\n        if (y(i) == val) {\n            x(i) = 0;\n        }\n    });\n\n    int count = 0;\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, int& local_count) {\n        if (x(i) == 0 || y(i) == 0) {\n            local_count++;\n        }\n    }, count);\n\n    return count == 1;\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    if (x(i) == val) {\n      for (int j = 0; j < y.extent(0); j++) {\n        if (y(j) == val) {\n          return false;\n        }\n      }\n      return true;\n    } else if (y(i) == val) {\n      for (int j = 0; j < x.extent(0); j++) {\n        if (x(j) == val) {\n          return false;\n        }\n      }\n      return true;\n    }\n  });\n  return false;\n}\n",
      "\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& result) {\n        if (x(i) == val) {\n            result = !result;\n        }\n    }, Kokkos::LogicalTrue{});\n\n    Kokkos::parallel_reduce(y.size(), KOKKOS_LAMBDA(int i, bool& result) {\n        if (y(i) == val) {\n            result = !result;\n        }\n    }, Kokkos::LogicalTrue{});\n}\n",
      "\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& result) {\n        if (x(i) == val) {\n            result = !result;\n        }\n    }, Kokkos::ReductionLogicalOr<bool>(result));\n\n    Kokkos::parallel_reduce(y.size(), KOKKOS_LAMBDA(int i, bool& result) {\n        if (y(i) == val) {\n            result = !result;\n        }\n    }, Kokkos::ReductionLogicalOr<bool>(result));\n\n    return result;\n}\n",
      "\n  Kokkos::parallel_reduce(x.dimension_0(), KOKKOS_LAMBDA(int i, bool& result) {\n    if (x(i) == val) {\n      result = !result;\n    }\n  }, Kokkos::Sum<bool>());\n\n  Kokkos::parallel_reduce(y.dimension_0(), KOKKOS_LAMBDA(int i, bool& result) {\n    if (y(i) == val) {\n      result = !result;\n    }\n  }, Kokkos::Sum<bool>());\n}\n",
      "\n  int count_x = 0;\n  int count_y = 0;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& local_count) {\n    if (x(i) == val) {\n      local_count++;\n    }\n  }, count_x);\n\n  Kokkos::parallel_reduce(y.size(), KOKKOS_LAMBDA(int i, int& local_count) {\n    if (y(i) == val) {\n      local_count++;\n    }\n  }, count_y);\n\n  return (count_x == 1) != (count_y == 1);\n}\n",
      "\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& result) {\n        if (x(i) == val) {\n            result = !result;\n        }\n    }, Kokkos::ReductionLogicalOr<bool>(result));\n\n    Kokkos::parallel_reduce(y.size(), KOKKOS_LAMBDA(int i, bool& result) {\n        if (y(i) == val) {\n            result = !result;\n        }\n    }, Kokkos::ReductionLogicalOr<bool>(result));\n\n    return result;\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    if (x(i) == val) {\n      for (int j = 0; j < y.extent(0); ++j) {\n        if (y(j) == val) {\n          return false;\n        }\n      }\n      return true;\n    } else if (y(i) == val) {\n      for (int j = 0; j < x.extent(0); ++j) {\n        if (x(j) == val) {\n          return false;\n        }\n      }\n      return true;\n    }\n  });\n  return false;\n}\n",
      "\n    Kokkos::parallel_for(\"xor_contains\", x.extent(0), KOKKOS_LAMBDA(const int i) {\n        if (x(i) == val) {\n            for (int j = 0; j < y.extent(0); j++) {\n                if (y(j) == val) {\n                    return false;\n                }\n            }\n            return true;\n        } else if (y(i) == val) {\n            for (int j = 0; j < x.extent(0); j++) {\n                if (x(j) == val) {\n                    return false;\n                }\n            }\n            return true;\n        }\n    });\n    return false;\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        if (x(i) == val) {\n            for (int j = 0; j < y.extent(0); ++j) {\n                if (y(j) == val) {\n                    return false;\n                }\n            }\n            return true;\n        } else if (y(i) == val) {\n            for (int j = 0; j < x.extent(0); ++j) {\n                if (x(j) == val) {\n                    return false;\n                }\n            }\n            return true;\n        }\n    });\n    return false;\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        if (x(i) == val) {\n            for (int j = 0; j < y.extent(0); ++j) {\n                if (y(j) == val) {\n                    return false;\n                }\n            }\n            return true;\n        } else if (y(i) == val) {\n            for (int j = 0; j < x.extent(0); ++j) {\n                if (x(j) == val) {\n                    return false;\n                }\n            }\n            return true;\n        }\n    });\n    return false;\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        if (x(i) == val) {\n            for (int j = 0; j < y.extent(0); j++) {\n                if (y(j) == val) {\n                    return false;\n                }\n            }\n            return true;\n        } else if (y(i) == val) {\n            for (int j = 0; j < x.extent(0); j++) {\n                if (x(j) == val) {\n                    return false;\n                }\n            }\n            return true;\n        }\n    });\n    return false;\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "39_search_xor_contains",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use OpenMP to search in parallel.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            for (int i = 0; i < x.size(); ++i) {\n                if (x[i] == val) {\n                    foundInX = true;\n                    break;\n                }\n            }\n        }\n\n        #pragma omp section\n        {\n            for (int i = 0; i < y.size(); ++i) {\n                if (y[i] == val) {\n                    foundInY = true;\n                    break;\n                }\n            }\n        }\n    }\n\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            foundInX = true;\n        }\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            foundInY = true;\n        }\n    }\n\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            for (int i = 0; i < x.size(); ++i) {\n                if (x[i] == val) {\n                    foundInX = true;\n                    break;\n                }\n            }\n        }\n\n        #pragma omp section\n        {\n            for (int i = 0; i < y.size(); ++i) {\n                if (y[i] == val) {\n                    foundInY = true;\n                    break;\n                }\n            }\n        }\n    }\n\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            for (int i = 0; i < x.size(); ++i) {\n                if (x[i] == val) {\n                    foundInX = true;\n                    break;\n                }\n            }\n        }\n\n        #pragma omp section\n        {\n            for (int i = 0; i < y.size(); ++i) {\n                if (y[i] == val) {\n                    foundInY = true;\n                    break;\n                }\n            }\n        }\n    }\n\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            for (int i = 0; i < x.size(); ++i) {\n                if (x[i] == val) {\n                    foundInX = true;\n                    break;\n                }\n            }\n        }\n\n        #pragma omp section\n        {\n            for (int i = 0; i < y.size(); ++i) {\n                if (y[i] == val) {\n                    foundInY = true;\n                    break;\n                }\n            }\n        }\n    }\n\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            for (int i = 0; i < x.size(); ++i) {\n                if (x[i] == val) {\n                    foundInX = true;\n                    break;\n                }\n            }\n        }\n\n        #pragma omp section\n        {\n            for (int i = 0; i < y.size(); ++i) {\n                if (y[i] == val) {\n                    foundInY = true;\n                    break;\n                }\n            }\n        }\n    }\n\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            for (int i = 0; i < x.size(); ++i) {\n                if (x[i] == val) {\n                    foundInX = true;\n                    break;\n                }\n            }\n        }\n\n        #pragma omp section\n        {\n            for (int i = 0; i < y.size(); ++i) {\n                if (y[i] == val) {\n                    foundInY = true;\n                    break;\n                }\n            }\n        }\n    }\n\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            for (int i = 0; i < x.size(); i++) {\n                if (x[i] == val) {\n                    foundInX = true;\n                    break;\n                }\n            }\n        }\n\n        #pragma omp section\n        {\n            for (int i = 0; i < y.size(); i++) {\n                if (y[i] == val) {\n                    foundInY = true;\n                    break;\n                }\n            }\n        }\n    }\n\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            for (int i = 0; i < x.size(); ++i) {\n                if (x[i] == val) {\n                    foundInX = true;\n                    break;\n                }\n            }\n        }\n\n        #pragma omp section\n        {\n            for (int i = 0; i < y.size(); ++i) {\n                if (y[i] == val) {\n                    foundInY = true;\n                    break;\n                }\n            }\n        }\n    }\n\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            for (int i = 0; i < x.size(); ++i) {\n                if (x[i] == val) {\n                    foundInX = true;\n                    break;\n                }\n            }\n        }\n\n        #pragma omp section\n        {\n            for (int i = 0; i < y.size(); ++i) {\n                if (y[i] == val) {\n                    foundInY = true;\n                    break;\n                }\n            }\n        }\n    }\n\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] == val) {\n            foundInX = true;\n        }\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < y.size(); ++i) {\n        if (y[i] == val) {\n            foundInY = true;\n        }\n    }\n\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            for (int i = 0; i < x.size(); ++i) {\n                if (x[i] == val) {\n                    foundInX = true;\n                    break;\n                }\n            }\n        }\n\n        #pragma omp section\n        {\n            for (int i = 0; i < y.size(); ++i) {\n                if (y[i] == val) {\n                    foundInY = true;\n                    break;\n                }\n            }\n        }\n    }\n\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            for (int i = 0; i < x.size(); ++i) {\n                if (x[i] == val) {\n                    foundInX = true;\n                    break;\n                }\n            }\n        }\n\n        #pragma omp section\n        {\n            for (int i = 0; i < y.size(); ++i) {\n                if (y[i] == val) {\n                    foundInY = true;\n                    break;\n                }\n            }\n        }\n    }\n\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            for (int i = 0; i < x.size(); ++i) {\n                if (x[i] == val) {\n                    foundInX = true;\n                    break;\n                }\n            }\n        }\n\n        #pragma omp section\n        {\n            for (int i = 0; i < y.size(); ++i) {\n                if (y[i] == val) {\n                    foundInY = true;\n                    break;\n                }\n            }\n        }\n    }\n\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            for (int i = 0; i < x.size(); ++i) {\n                if (x[i] == val) {\n                    foundInX = true;\n                    break;\n                }\n            }\n        }\n\n        #pragma omp section\n        {\n            for (int i = 0; i < y.size(); ++i) {\n                if (y[i] == val) {\n                    foundInY = true;\n                    break;\n                }\n            }\n        }\n    }\n\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            for (int i = 0; i < x.size(); ++i) {\n                if (x[i] == val) {\n                    foundInX = true;\n                    break;\n                }\n            }\n        }\n\n        #pragma omp section\n        {\n            for (int i = 0; i < y.size(); ++i) {\n                if (y[i] == val) {\n                    foundInY = true;\n                    break;\n                }\n            }\n        }\n    }\n\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            for (int i = 0; i < x.size(); ++i) {\n                if (x[i] == val) {\n                    foundInX = true;\n                    break;\n                }\n            }\n        }\n\n        #pragma omp section\n        {\n            for (int i = 0; i < y.size(); ++i) {\n                if (y[i] == val) {\n                    foundInY = true;\n                    break;\n                }\n            }\n        }\n    }\n\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            for (int i = 0; i < x.size(); ++i) {\n                if (x[i] == val) {\n                    foundInX = true;\n                    break;\n                }\n            }\n        }\n\n        #pragma omp section\n        {\n            for (int i = 0; i < y.size(); ++i) {\n                if (y[i] == val) {\n                    foundInY = true;\n                    break;\n                }\n            }\n        }\n    }\n\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            for (int i = 0; i < x.size(); ++i) {\n                if (x[i] == val) {\n                    foundInX = true;\n                    break;\n                }\n            }\n        }\n\n        #pragma omp section\n        {\n            for (int i = 0; i < y.size(); ++i) {\n                if (y[i] == val) {\n                    foundInY = true;\n                    break;\n                }\n            }\n        }\n    }\n\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            for (int i = 0; i < x.size(); ++i) {\n                if (x[i] == val) {\n                    foundInX = true;\n                    break;\n                }\n            }\n        }\n\n        #pragma omp section\n        {\n            for (int i = 0; i < y.size(); ++i) {\n                if (y[i] == val) {\n                    foundInY = true;\n                    break;\n                }\n            }\n        }\n    }\n\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "39_search_xor_contains",
    "parallelism_model": "serial",
    "prompt": "/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    bool valInX = std::find(x.begin(), x.end(), val) != x.end();\n    bool valInY = std::find(y.begin(), y.end(), val) != y.end();\n    return (valInX && !valInY) || (!valInX && valInY);\n}",
      "\n    bool valInX = std::find(x.begin(), x.end(), val) != x.end();\n    bool valInY = std::find(y.begin(), y.end(), val) != y.end();\n    return (valInX && !valInY) || (!valInX && valInY);\n}",
      "\n    bool valInX = std::find(x.begin(), x.end(), val) != x.end();\n    bool valInY = std::find(y.begin(), y.end(), val) != y.end();\n    return (valInX && !valInY) || (!valInX && valInY);\n}",
      "\n    bool valInX = std::find(x.begin(), x.end(), val) != x.end();\n    bool valInY = std::find(y.begin(), y.end(), val) != y.end();\n    return (valInX && !valInY) || (!valInX && valInY);\n}",
      "\n    bool valInX = std::find(x.begin(), x.end(), val) != x.end();\n    bool valInY = std::find(y.begin(), y.end(), val) != y.end();\n    return (valInX && !valInY) || (!valInX && valInY);\n}",
      "\n    bool valInX = std::find(x.begin(), x.end(), val) != x.end();\n    bool valInY = std::find(y.begin(), y.end(), val) != y.end();\n    return (valInX && !valInY) || (!valInX && valInY);\n}",
      "\n    bool valInX = std::find(x.begin(), x.end(), val) != x.end();\n    bool valInY = std::find(y.begin(), y.end(), val) != y.end();\n    return (valInX && !valInY) || (!valInX && valInY);\n}",
      "\n    bool valInX = std::find(x.begin(), x.end(), val) != x.end();\n    bool valInY = std::find(y.begin(), y.end(), val) != y.end();\n    return (valInX && !valInY) || (!valInX && valInY);\n}",
      "\n    bool foundInX = std::find(x.begin(), x.end(), val) != x.end();\n    bool foundInY = std::find(y.begin(), y.end(), val) != y.end();\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}",
      "\n    bool valInX = std::find(x.begin(), x.end(), val) != x.end();\n    bool valInY = std::find(y.begin(), y.end(), val) != y.end();\n    return (valInX || valInY) && !(valInX && valInY);\n}",
      "\n    bool valInX = std::find(x.begin(), x.end(), val) != x.end();\n    bool valInY = std::find(y.begin(), y.end(), val) != y.end();\n    return (valInX && !valInY) || (!valInX && valInY);\n}",
      "\n    bool valInX = std::find(x.begin(), x.end(), val) != x.end();\n    bool valInY = std::find(y.begin(), y.end(), val) != y.end();\n    return (valInX && !valInY) || (!valInX && valInY);\n}",
      "\n    bool valInX = std::find(x.begin(), x.end(), val) != x.end();\n    bool valInY = std::find(y.begin(), y.end(), val) != y.end();\n    return (valInX && !valInY) || (!valInX && valInY);\n}",
      "\n    bool foundInX = std::find(x.begin(), x.end(), val) != x.end();\n    bool foundInY = std::find(y.begin(), y.end(), val) != y.end();\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}",
      "\n    bool valInX = std::find(x.begin(), x.end(), val) != x.end();\n    bool valInY = std::find(y.begin(), y.end(), val) != y.end();\n    return (valInX && !valInY) || (!valInX && valInY);\n}",
      "\n    bool valInX = std::find(x.begin(), x.end(), val) != x.end();\n    bool valInY = std::find(y.begin(), y.end(), val) != y.end();\n    return (valInX && !valInY) || (!valInX && valInY);\n}",
      "\n    bool inX = std::find(x.begin(), x.end(), val) != x.end();\n    bool inY = std::find(y.begin(), y.end(), val) != y.end();\n    return (inX && !inY) || (!inX && inY);\n}",
      "\n    bool inX = std::find(x.begin(), x.end(), val) != x.end();\n    bool inY = std::find(y.begin(), y.end(), val) != y.end();\n    return (inX && !inY) || (!inX && inY);\n}",
      "\n    bool valInX = std::find(x.begin(), x.end(), val) != x.end();\n    bool valInY = std::find(y.begin(), y.end(), val) != y.end();\n    return (valInX && !valInY) || (!valInX && valInY);\n}",
      "\n    bool valInX = std::find(x.begin(), x.end(), val) != x.end();\n    bool valInY = std::find(y.begin(), y.end(), val) != y.end();\n    return (valInX && !valInY) || (!valInX && valInY);\n}"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "39_search_xor_contains",
    "parallelism_model": "hip",
    "prompt": "/* Set `found` to true if `val` is only in one of vectors x or y.\n   Set it to false if it is in both or neither.\n   Use AMD HIP to search in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\n__global__ void xorContains(const int *x, const int *y, size_t N, int val, bool *found) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        found[tid] = (x[tid] == val) ^ (y[tid] == val);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        found[tid] = (x[tid] == val) ^ (y[tid] == val);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        found[tid] = (x[tid] == val) ^ (y[tid] == val);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "39_search_xor_contains",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Return the result on rank 0.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int count_x = 0, count_y = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n    int total_count = count_x + count_y;\n    int global_total_count;\n    MPI_Reduce(&total_count, &global_total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    return global_total_count == 1;\n}\n",
      "\n    int count_x = 0;\n    int count_y = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n    int total_count = count_x + count_y;\n    int global_total_count;\n    MPI_Reduce(&total_count, &global_total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (global_total_count == 1) {\n        return true;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int count_x = 0;\n    int count_y = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n    int total_count = count_x + count_y;\n    int global_total_count;\n    MPI_Reduce(&total_count, &global_total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    return global_total_count == 1;\n}\n",
      "\n    int count_x = 0;\n    int count_y = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n    int total_count = count_x + count_y;\n    int global_total_count;\n    MPI_Reduce(&total_count, &global_total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    return global_total_count == 1;\n}\n",
      "\n    int count_x = 0;\n    int count_y = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n    int total_count = count_x + count_y;\n    int global_total_count;\n    MPI_Reduce(&total_count, &global_total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    return global_total_count == 1;\n}\n",
      "\n    int count_x = 0, count_y = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n    int total_count = count_x + count_y;\n    int global_count;\n    MPI_Reduce(&total_count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    return global_count == 1;\n}\n",
      "\n    int count_x = 0;\n    int count_y = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n    int total_count = count_x + count_y;\n    int global_total_count;\n    MPI_Reduce(&total_count, &global_total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (global_total_count == 1) {\n        return true;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int count_x = 0, count_y = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n    int total_count;\n    MPI_Reduce(&count_x, &total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&count_y, &total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    return total_count == 1;\n}\n",
      "\n    int count_x = 0, count_y = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n    int total_count = count_x + count_y;\n    int global_total_count;\n    MPI_Reduce(&total_count, &global_total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    return global_total_count == 1;\n}\n",
      "\n    int count_x = 0, count_y = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n    int total_count = count_x + count_y;\n    int global_total_count;\n    MPI_Reduce(&total_count, &global_total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    return global_total_count == 1;\n}\n",
      "\n    int count_x = 0, count_y = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n    int total_count = count_x + count_y;\n    int global_total_count;\n    MPI_Reduce(&total_count, &global_total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    return global_total_count == 1;\n}\n",
      "\n    int count_x = std::count(x.begin(), x.end(), val);\n    int count_y = std::count(y.begin(), y.end(), val);\n    int total_count = count_x + count_y;\n\n    int global_total_count;\n    MPI_Reduce(&total_count, &global_total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return global_total_count == 1;\n}\n",
      "\n    int count_x = 0, count_y = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n    int total_count = count_x + count_y;\n    int global_total_count;\n    MPI_Reduce(&total_count, &global_total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    return global_total_count == 1;\n}\n",
      "\n    int count_x = std::count(x.begin(), x.end(), val);\n    int count_y = std::count(y.begin(), y.end(), val);\n    int total_count = count_x + count_y;\n\n    int global_total_count;\n    MPI_Reduce(&total_count, &global_total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return global_total_count == 1;\n}\n",
      "\n    int count_x = 0, count_y = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n\n    int total_count = count_x + count_y;\n    int global_total_count;\n    MPI_Reduce(&total_count, &global_total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (global_total_count == 1) {\n        return true;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int count_x = 0, count_y = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n\n    int total_count = count_x + count_y;\n    int global_total_count;\n    MPI_Reduce(&total_count, &global_total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (global_total_count == 1) {\n        return true;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int count_x = 0, count_y = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n    int total_count = count_x + count_y;\n    int global_count;\n    MPI_Reduce(&total_count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    return global_count == 1;\n}\n",
      "\n    int count_x = 0, count_y = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n    int total_count = count_x + count_y;\n    int global_count;\n    MPI_Reduce(&total_count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    return global_count == 1;\n}\n",
      "\n    int count_x = 0, count_y = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n    int total_count = count_x + count_y;\n    int global_count;\n    MPI_Reduce(&total_count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    return global_count == 1;\n}\n",
      "\n    int count_x = 0, count_y = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n    int total_count = count_x + count_y;\n    int global_count;\n    MPI_Reduce(&total_count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    return global_count == 1;\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "39_search_xor_contains",
    "parallelism_model": "cuda",
    "prompt": "/* Set `found` to true if `val` is only in one of vectors x or y.\n   Set it to false if it is in both or neither.\n   Use CUDA to search in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\n__global__ void xorContains(const int *x, const int *y, size_t N, int val, bool *found) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        found[tid] = (x[tid] == val) ^ (y[tid] == val);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x ^ in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x ^ in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        \n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        \n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x ^ in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        found[tid] = (x[tid] == val) ^ (y[tid] == val);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += gridDim.x * blockDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        found[tid] = (x[tid] == val) ^ (y[tid] == val);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x ^ in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x ^ in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        bool in_x = false;\n        bool in_y = false;\n        for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n            if (x[i] == val) {\n                in_x = true;\n            }\n            if (y[i] == val) {\n                in_y = true;\n            }\n        }\n        found[tid] = (in_x && !in_y) || (!in_x && in_y);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "39_search_xor_contains",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use MPI and OpenMP to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Return the result on rank 0.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int count_x = 0;\n    int count_y = 0;\n\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n\n    int total_count = count_x + count_y;\n\n    int global_total_count;\n    MPI_Reduce(&total_count, &global_total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (global_total_count == 1) {\n        return true;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int count_x = 0, count_y = 0;\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n    \n    int total_count_x, total_count_y;\n    MPI_Reduce(&count_x, &total_count_x, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&count_y, &total_count_y, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    \n    if ((total_count_x == 1 && total_count_y == 0) || (total_count_x == 0 && total_count_y == 1)) {\n        return true;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int count_x = 0;\n    int count_y = 0;\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n    int total_count = count_x + count_y;\n    int global_total_count;\n    MPI_Reduce(&total_count, &global_total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    return global_total_count == 1;\n}\n",
      "\n    int count_x = 0, count_y = 0;\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n\n    int total_count_x, total_count_y;\n    MPI_Reduce(&count_x, &total_count_x, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&count_y, &total_count_y, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if ((total_count_x == 1 && total_count_y == 0) || (total_count_x == 0 && total_count_y == 1)) {\n        return true;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int count_x = 0, count_y = 0;\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n    \n    int total_count_x, total_count_y;\n    MPI_Reduce(&count_x, &total_count_x, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&count_y, &total_count_y, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    \n    if ((total_count_x == 1 && total_count_y == 0) || (total_count_x == 0 && total_count_y == 1)) {\n        return true;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int count_x = 0, count_y = 0;\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n\n    int total_count_x, total_count_y;\n    MPI_Reduce(&count_x, &total_count_x, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&count_y, &total_count_y, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if ((total_count_x == 1 && total_count_y == 0) || (total_count_x == 0 && total_count_y == 1)) {\n        return true;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int count_x = 0;\n    int count_y = 0;\n\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n\n    int total_count = count_x + count_y;\n    int global_total_count;\n    MPI_Reduce(&total_count, &global_total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (global_total_count == 1) {\n        return true;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int count_x = 0, count_y = 0;\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n\n    int total_count_x, total_count_y;\n    MPI_Reduce(&count_x, &total_count_x, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&count_y, &total_count_y, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if ((total_count_x == 1 && total_count_y == 0) || (total_count_x == 0 && total_count_y == 1)) {\n        return true;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int count_x = 0;\n    int count_y = 0;\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n\n    int total_count = 0;\n    MPI_Reduce(&count_x, &total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&count_y, &total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (total_count == 1) {\n        return true;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int count_x = 0, count_y = 0;\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n\n    int total_count_x, total_count_y;\n    MPI_Reduce(&count_x, &total_count_x, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&count_y, &total_count_y, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if ((total_count_x == 1 && total_count_y == 0) || (total_count_x == 0 && total_count_y == 1)) {\n        return true;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int count_x = 0;\n    int count_y = 0;\n\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n\n    int total_count = count_x + count_y;\n    int global_total_count;\n\n    MPI_Reduce(&total_count, &global_total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (global_total_count == 1) {\n        return true;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int countX = 0, countY = 0;\n    #pragma omp parallel for reduction(+:countX, countY)\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            countX++;\n        }\n    }\n    #pragma omp parallel for reduction(+:countX, countY)\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            countY++;\n        }\n    }\n\n    int totalX, totalY;\n    MPI_Reduce(&countX, &totalX, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&countY, &totalY, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if ((totalX == 1 && totalY == 0) || (totalX == 0 && totalY == 1)) {\n        return true;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int count_x = 0, count_y = 0;\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n\n    int total_count_x, total_count_y;\n    MPI_Reduce(&count_x, &total_count_x, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&count_y, &total_count_y, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if ((total_count_x == 1 && total_count_y == 0) || (total_count_x == 0 && total_count_y == 1)) {\n        return true;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int count_x = 0;\n    int count_y = 0;\n\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n\n    int total_count = count_x + count_y;\n    int global_total_count;\n\n    MPI_Reduce(&total_count, &global_total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (global_total_count == 1) {\n        return true;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int count_x = 0;\n    int count_y = 0;\n\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n\n    int total_count = count_x + count_y;\n\n    int global_total_count;\n    MPI_Reduce(&total_count, &global_total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (global_total_count == 1) {\n        return true;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int countX = 0, countY = 0;\n    #pragma omp parallel for reduction(+:countX, countY)\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            countX++;\n        }\n    }\n    #pragma omp parallel for reduction(+:countX, countY)\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            countY++;\n        }\n    }\n\n    int globalCountX, globalCountY;\n    MPI_Reduce(&countX, &globalCountX, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&countY, &globalCountY, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if ((globalCountX == 1 && globalCountY == 0) || (globalCountX == 0 && globalCountY == 1)) {\n        return true;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int count_x = 0;\n    int count_y = 0;\n\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n\n    int total_count = count_x + count_y;\n    int global_total_count;\n\n    MPI_Reduce(&total_count, &global_total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (global_total_count == 1) {\n        return true;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int count_x = 0, count_y = 0;\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n\n    int total_count_x, total_count_y;\n    MPI_Reduce(&count_x, &total_count_x, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&count_y, &total_count_y, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if ((total_count_x == 1 && total_count_y == 0) || (total_count_x == 0 && total_count_y == 1)) {\n        return true;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int count_x = 0, count_y = 0;\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < y.size(); ++i) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n\n    int total_count_x, total_count_y;\n    MPI_Reduce(&count_x, &total_count_x, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&count_y, &total_count_y, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if ((total_count_x == 1 && total_count_y == 0) || (total_count_x == 0 && total_count_y == 1)) {\n        return true;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int count_x = 0;\n    int count_y = 0;\n\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            count_x++;\n        }\n    }\n\n    #pragma omp parallel for reduction(+:count_x, count_y)\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            count_y++;\n        }\n    }\n\n    int total_count = count_x + count_y;\n    int global_total_count;\n\n    MPI_Reduce(&total_count, &global_total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (global_total_count == 1) {\n        return true;\n    } else {\n        return false;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "35_search_search_for_last_struct_by_key",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Book {\n   const char *title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use Kokkos to search in parallel. Assume Kokkos is already initialized.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(Kokkos::View<const Book*> const& books) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    size_t lastIndex = 0;\n    Kokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& l) {\n        if (books(i).pages < 100) {\n            l = i;\n        }\n    }, lastIndex);\n    return lastIndex;\n}\n",
      "\n    size_t lastIndex = 0;\n    Kokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& l) {\n        if (books(i).pages < 100) {\n            l = i;\n        }\n    }, lastIndex);\n    return lastIndex;\n}\n",
      "\n    size_t lastIndex = 0;\n    Kokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& l) {\n        if (books(i).pages < 100) {\n            l = i;\n        }\n    }, lastIndex);\n    return lastIndex;\n}\n",
      "\n    size_t lastIndex = 0;\n    Kokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& l) {\n        if (books(i).pages < 100) {\n            l = i;\n        }\n    }, lastIndex);\n    return lastIndex;\n}\n",
      "\n    size_t lastIndex = 0;\n    Kokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& l) {\n        if (books(i).pages < 100) {\n            l = i;\n        }\n    }, lastIndex);\n    return lastIndex;\n}\n",
      "\n    size_t lastIndex = 0;\n    Kokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& l) {\n        if (books(i).pages < 100) {\n            l = i;\n        }\n    }, lastIndex);\n    return lastIndex;\n}\n",
      "\n    size_t lastIndex = 0;\n    Kokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& l) {\n        if (books(i).pages < 100) {\n            l = i;\n        }\n    }, lastIndex);\n    return lastIndex;\n}\n",
      "\n    size_t lastIndex = 0;\n    Kokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& l) {\n        if (books(i).pages < 100) {\n            l = i;\n        }\n    }, lastIndex);\n    return lastIndex;\n}\n",
      "\n    size_t lastIndex = 0;\n    Kokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& l) {\n        if (books(i).pages < 100) {\n            l = i;\n        }\n    }, lastIndex);\n    return lastIndex;\n}\n",
      "\n    size_t lastIndex = 0;\n    Kokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& l) {\n        if (books(i).pages < 100) {\n            l = i;\n        }\n    }, lastIndex);\n    return lastIndex;\n}\n",
      "\n    size_t lastIndex = 0;\n    Kokkos::parallel_reduce(books.extent(0), KOKKOS_LAMBDA(int i, size_t& l) {\n        if (books(i).pages < 100) {\n            l = i;\n        }\n    }, lastIndex);\n    return lastIndex;\n}\n",
      "\n    size_t lastIndex = 0;\n    Kokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& l) {\n        if (books(i).pages < 100) {\n            l = i;\n        }\n    }, lastIndex);\n    return lastIndex;\n}\n",
      "\n    size_t lastIndex = 0;\n    Kokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& l) {\n        if (books(i).pages < 100) {\n            l = i;\n        }\n    }, lastIndex);\n    return lastIndex;\n}\n",
      "\n    size_t lastIndex = 0;\n    Kokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& l) {\n        if (books(i).pages < 100) {\n            l = i;\n        }\n    }, lastIndex);\n    return lastIndex;\n}\n",
      "\n    int lastShortBookIndex = -1;\n    Kokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, int& lsb) {\n        if (books(i).pages < 100) {\n            lsb = i;\n        }\n    }, lastShortBookIndex);\n    return lastShortBookIndex;\n}\n",
      "\n    size_t lastIndex = 0;\n    Kokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& l) {\n        if (books(i).pages < 100) {\n            l = i;\n        }\n    }, lastIndex);\n    return lastIndex;\n}\n",
      "\nsize_t lastIndex = 0;\nKokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& l) {\n    if (books(i).pages < 100) {\n        l = i;\n    }\n}, lastIndex);\nreturn lastIndex;\n}",
      "\nsize_t lastIndex = 0;\nKokkos::parallel_reduce(books.extent(0), KOKKOS_LAMBDA(int i, size_t& l) {\n    if (books(i).pages < 100) {\n        l = i;\n    }\n}, lastIndex);\nreturn lastIndex;\n}",
      "\nsize_t lastIndex = 0;\nKokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& l) {\n    if (books(i).pages < 100) {\n        l = i;\n    }\n}, lastIndex);\nreturn lastIndex;\n}",
      "\nsize_t lastIndex = 0;\nKokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& l) {\n    if (books(i).pages < 100) {\n        l = i;\n    }\n}, lastIndex);\nreturn lastIndex;\n}"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "35_search_search_for_last_struct_by_key",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use OpenMP to search in parallel.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    size_t lastIndex = 0;\n    #pragma omp parallel for\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastShortBookIndex = i;\n        }\n    }\n    return lastShortBookIndex;\n}",
      "\n    size_t lastIndex = 0;\n    #pragma omp parallel for\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    #pragma omp parallel for\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    #pragma omp parallel for\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    #pragma omp parallel for\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    #pragma omp parallel for\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    #pragma omp parallel for\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            {\n                lastShortBookIndex = i;\n            }\n        }\n    }\n    return lastShortBookIndex;\n}",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastShortBookIndex = i;\n        }\n    }\n    return lastShortBookIndex;\n}",
      "\n    size_t lastIndex = 0;\n    #pragma omp parallel for\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastShortBookIndex = i;\n        }\n    }\n    return lastShortBookIndex;\n}",
      "\n    size_t lastIndex = 0;\n    #pragma omp parallel for\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastShortBookIndex = i;\n        }\n    }\n    return lastShortBookIndex;\n}",
      "\n    size_t lastIndex = 0;\n    #pragma omp parallel for\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    #pragma omp parallel for\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastShortBookIndex = i;\n        }\n    }\n    return lastShortBookIndex;\n}",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            {\n                lastShortBookIndex = i;\n            }\n        }\n    }\n    return lastShortBookIndex;\n}",
      "\n    size_t lastIndex = 0;\n    #pragma omp parallel for\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    #pragma omp parallel for\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "35_search_search_for_last_struct_by_key",
    "parallelism_model": "serial",
    "prompt": "struct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    size_t lastIndex = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    for (int i = books.size() - 1; i >= 0; --i) {\n        if (books[i].pages < 100) {\n            return i;\n        }\n    }\n    return -1;\n}",
      "\n    size_t lastIndex = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}",
      "\n    size_t lastIndex = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            lastIndex = i;\n        }\n    }\n    return lastIndex;\n}"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "35_search_search_for_last_struct_by_key",
    "parallelism_model": "hip",
    "prompt": "struct Book {\n   const char* title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Store the result in lastShortBookIndex.\n   Use AMD HIP to search in parallel. The kernel is launched with one thread for every book element.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\n__global__ void findLastShortBook(const Book *books, size_t N, size_t *lastShortBookIndex) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        if (books[index].pages < 100) {\n            atomicExch(lastShortBookIndex, index);\n        }\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = threadIdx.x;\n    if (index < N) {\n        if (books[index].pages < 100) {\n            atomicExch(lastShortBookIndex, index);\n        }\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = threadIdx.x;\n    if (index < N) {\n        if (books[index].pages < 100) {\n            atomicExch(lastShortBookIndex, index);\n        }\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "35_search_search_for_last_struct_by_key",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use MPI to search in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of books. Return the result on rank 0.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t result = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            result = i;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            size_t temp;\n            MPI_Recv(&temp, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            if (temp > result) {\n                result = temp;\n            }\n        }\n    } else {\n        MPI_Send(&result, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return result;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t result = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            result = i;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            size_t temp;\n            MPI_Recv(&temp, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            if (temp > result) {\n                result = temp;\n            }\n        }\n    } else {\n        MPI_Send(&result, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return result;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localIndex = -1;\n    for (int i = rank; i < books.size(); i += size) {\n        if (books[i].pages < 100) {\n            localIndex = i;\n        }\n    }\n\n    int globalIndex;\n    MPI_Reduce(&localIndex, &globalIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalIndex;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localIndex = -1;\n    for (int i = rank; i < books.size(); i += size) {\n        if (books[i].pages < 100) {\n            localIndex = i;\n        }\n    }\n\n    int globalIndex;\n    MPI_Reduce(&localIndex, &globalIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalIndex;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localIndex = -1;\n    for (int i = rank; i < books.size(); i += size) {\n        if (books[i].pages < 100) {\n            localIndex = i;\n        }\n    }\n\n    int globalIndex;\n    MPI_Reduce(&localIndex, &globalIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalIndex;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localIndex = -1;\n    for (int i = rank; i < books.size(); i += size) {\n        if (books[i].pages < 100) {\n            localIndex = i;\n        }\n    }\n\n    int globalIndex;\n    MPI_Reduce(&localIndex, &globalIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalIndex;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t result = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            result = i;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            size_t temp;\n            MPI_Recv(&temp, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            if (temp > result) {\n                result = temp;\n            }\n        }\n    } else {\n        MPI_Send(&result, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return result;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localIndex = -1;\n    for (int i = rank; i < books.size(); i += size) {\n        if (books[i].pages < 100) {\n            localIndex = i;\n        }\n    }\n\n    int globalIndex;\n    MPI_Reduce(&localIndex, &globalIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalIndex;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t result = 0;\n    size_t local_result = 0;\n\n    for (size_t i = rank; i < books.size(); i += size) {\n        if (books[i].pages < 100) {\n            local_result = i;\n        }\n    }\n\n    MPI_Reduce(&local_result, &result, 1, MPI_UNSIGNED_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return result;\n    } else {\n        return 0;  // or some other value to indicate no result on non-root ranks\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localIndex = -1;\n    for (int i = rank; i < books.size(); i += size) {\n        if (books[i].pages < 100) {\n            localIndex = i;\n        }\n    }\n\n    int globalIndex;\n    MPI_Reduce(&localIndex, &globalIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalIndex;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t result = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            result = i;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            size_t temp;\n            MPI_Recv(&temp, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            if (temp > result) {\n                result = temp;\n            }\n        }\n    } else {\n        MPI_Send(&result, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return result;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t result = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            result = i;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            size_t temp;\n            MPI_Recv(&temp, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            if (temp > result) {\n                result = temp;\n            }\n        }\n    } else {\n        MPI_Send(&result, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return result;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localResult = -1;\n    for (int i = rank; i < books.size(); i += size) {\n        if (books[i].pages < 100) {\n            localResult = i;\n        }\n    }\n\n    int globalResult;\n    MPI_Reduce(&localResult, &globalResult, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalResult;\n    } else {\n        return -1;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localIndex = -1;\n    for (int i = rank; i < books.size(); i += size) {\n        if (books[i].pages < 100) {\n            localIndex = i;\n        }\n    }\n\n    int globalIndex;\n    MPI_Reduce(&localIndex, &globalIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalIndex;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t result = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            result = i;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            size_t temp;\n            MPI_Recv(&temp, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            if (temp > result) {\n                result = temp;\n            }\n        }\n    } else {\n        MPI_Send(&result, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return result;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localIndex = -1;\n    for (int i = rank; i < books.size(); i += size) {\n        if (books[i].pages < 100) {\n            localIndex = i;\n        }\n    }\n\n    int globalIndex;\n    MPI_Reduce(&localIndex, &globalIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalIndex;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t result = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            result = i;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            size_t temp;\n            MPI_Recv(&temp, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            if (temp > result) {\n                result = temp;\n            }\n        }\n    } else {\n        MPI_Send(&result, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return result;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localIndex = -1;\n    for (int i = rank; i < books.size(); i += size) {\n        if (books[i].pages < 100) {\n            localIndex = i;\n        }\n    }\n\n    int globalIndex;\n    MPI_Reduce(&localIndex, &globalIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalIndex;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t result = 0;\n    for (size_t i = 0; i < books.size(); ++i) {\n        if (books[i].pages < 100) {\n            result = i;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            size_t temp;\n            MPI_Recv(&temp, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            if (temp > result) {\n                result = temp;\n            }\n        }\n    } else {\n        MPI_Send(&result, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return result;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localIndex = -1;\n    for (int i = rank; i < books.size(); i += size) {\n        if (books[i].pages < 100) {\n            localIndex = i;\n        }\n    }\n\n    int globalIndex;\n    MPI_Reduce(&localIndex, &globalIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalIndex;\n    } else {\n        return 0;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "35_search_search_for_last_struct_by_key",
    "parallelism_model": "cuda",
    "prompt": "struct Book {\n   const char* title;\n   int pages;\n};\n\n/* Find the index of the last Book item in the vector books where Book.pages is less than 100.\n   Store the result in lastShortBookIndex.\n   Use CUDA to search in parallel. The kernel is launched with one thread for every book element.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\n__global__ void findLastShortBook(const Book *books, size_t N, size_t *lastShortBookIndex) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N && books[index].pages < 100) {\n        atomicExch(lastShortBookIndex, index);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "35_search_search_for_last_struct_by_key",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use MPI and OpenMP to search in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of books. Return the result on rank 0.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            {\n                lastShortBookIndex = i;\n            }\n        }\n    }\n\n    int globalLastShortBookIndex = -1;\n    MPI_Reduce(&lastShortBookIndex, &globalLastShortBookIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalLastShortBookIndex;\n    } else {\n        return -1;\n    }\n}\n",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastShortBookIndex = i;\n        }\n    }\n\n    int globalLastShortBookIndex = -1;\n    MPI_Reduce(&lastShortBookIndex, &globalLastShortBookIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (globalLastShortBookIndex == -1) {\n        return -1;\n    } else {\n        return static_cast<size_t>(globalLastShortBookIndex);\n    }\n}\n",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastShortBookIndex = i;\n        }\n    }\n    int globalLastShortBookIndex = -1;\n    MPI_Reduce(&lastShortBookIndex, &globalLastShortBookIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    return globalLastShortBookIndex;\n}\n",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastShortBookIndex = i;\n        }\n    }\n    \n    int globalLastShortBookIndex = -1;\n    MPI_Reduce(&lastShortBookIndex, &globalLastShortBookIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalLastShortBookIndex;\n    } else {\n        return -1;\n    }\n}\n",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastShortBookIndex = i;\n        }\n    }\n\n    int globalLastShortBookIndex = -1;\n    MPI_Reduce(&lastShortBookIndex, &globalLastShortBookIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    return globalLastShortBookIndex;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localIndex = -1;\n    #pragma omp parallel for\n    for (size_t i = rank; i < books.size(); i += size) {\n        if (books[i].pages < 100) {\n            localIndex = i;\n        }\n    }\n\n    int globalIndex = -1;\n    MPI_Reduce(&localIndex, &globalIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalIndex;\n    } else {\n        return -1;\n    }\n}\n",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            {\n                lastShortBookIndex = i;\n            }\n        }\n    }\n\n    int globalLastShortBookIndex = -1;\n    MPI_Reduce(&lastShortBookIndex, &globalLastShortBookIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalLastShortBookIndex;\n    } else {\n        return -1;\n    }\n}\n",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastShortBookIndex = i;\n        }\n    }\n\n    int globalLastShortBookIndex = -1;\n    MPI_Reduce(&lastShortBookIndex, &globalLastShortBookIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalLastShortBookIndex;\n    } else {\n        return -1;\n    }\n}\n",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastShortBookIndex = i;\n        }\n    }\n\n    int globalLastShortBookIndex = -1;\n    MPI_Reduce(&lastShortBookIndex, &globalLastShortBookIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalLastShortBookIndex;\n    } else {\n        return -1; // Return a default value for non-root ranks\n    }\n}",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastShortBookIndex = i;\n        }\n    }\n\n    int globalLastShortBookIndex = -1;\n    MPI_Reduce(&lastShortBookIndex, &globalLastShortBookIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalLastShortBookIndex;\n    } else {\n        return -1;\n    }\n}\n",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            {\n                lastShortBookIndex = i;\n            }\n        }\n    }\n\n    int globalLastShortBookIndex = -1;\n    MPI_Reduce(&lastShortBookIndex, &globalLastShortBookIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalLastShortBookIndex;\n    } else {\n        return -1;\n    }\n}\n",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastShortBookIndex = i;\n        }\n    }\n\n    int globalLastShortBookIndex = -1;\n    MPI_Reduce(&lastShortBookIndex, &globalLastShortBookIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (globalLastShortBookIndex == -1) {\n        return -1;\n    } else {\n        return static_cast<size_t>(globalLastShortBookIndex);\n    }\n}\n",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastShortBookIndex = i;\n        }\n    }\n\n    int globalLastShortBookIndex = -1;\n    MPI_Reduce(&lastShortBookIndex, &globalLastShortBookIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalLastShortBookIndex;\n    } else {\n        return -1;\n    }\n}\n",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastShortBookIndex = i;\n        }\n    }\n\n    int globalLastShortBookIndex = -1;\n    MPI_Reduce(&lastShortBookIndex, &globalLastShortBookIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (globalLastShortBookIndex == -1) {\n        return -1;\n    } else {\n        return static_cast<size_t>(globalLastShortBookIndex);\n    }\n}\n",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            {\n                lastShortBookIndex = i;\n            }\n        }\n    }\n\n    int globalLastShortBookIndex = -1;\n    MPI_Reduce(&lastShortBookIndex, &globalLastShortBookIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalLastShortBookIndex;\n    } else {\n        return -1;\n    }\n}\n",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastShortBookIndex = i;\n        }\n    }\n\n    int globalLastShortBookIndex = -1;\n    MPI_Reduce(&lastShortBookIndex, &globalLastShortBookIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalLastShortBookIndex;\n    } else {\n        return -1;\n    }\n}\n",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastShortBookIndex = i;\n        }\n    }\n\n    int globalLastShortBookIndex = -1;\n    MPI_Reduce(&lastShortBookIndex, &globalLastShortBookIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalLastShortBookIndex;\n    } else {\n        return -1;\n    }\n}\n",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            lastShortBookIndex = i;\n        }\n    }\n\n    int globalLastShortBookIndex = -1;\n    MPI_Reduce(&lastShortBookIndex, &globalLastShortBookIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalLastShortBookIndex;\n    } else {\n        return -1;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localIndex = -1;\n\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            localIndex = i;\n        }\n    }\n\n    int globalIndex;\n    MPI_Reduce(&localIndex, &globalIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalIndex;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int lastShortBookIndex = -1;\n    #pragma omp parallel for\n    for (int i = 0; i < books.size(); i++) {\n        if (books[i].pages < 100) {\n            #pragma omp critical\n            {\n                lastShortBookIndex = i;\n            }\n        }\n    }\n\n    int globalLastShortBookIndex = -1;\n    MPI_Reduce(&lastShortBookIndex, &globalLastShortBookIndex, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return globalLastShortBookIndex;\n    } else {\n        return -1;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "37_search_find_the_closest_number_to_pi",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Use Kokkos to search in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(Kokkos::View<const double*> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x(0) - M_PI);\n\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& lminDiff, size_t& lclosestIndex) {\n        double diff = std::abs(x(i) - M_PI);\n        if (diff < lminDiff) {\n            lminDiff = diff;\n            lclosestIndex = i;\n        }\n    }, Kokkos::Min<double>(minDiff), Kokkos::Min<size_t>(closestIndex));\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x(0) - M_PI);\n\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& lminDiff, size_t& lclosestIndex) {\n        double diff = std::abs(x(i) - M_PI);\n        if (diff < lminDiff) {\n            lminDiff = diff;\n            lclosestIndex = i;\n        }\n    }, Kokkos::Min<double>(minDiff), Kokkos::Min<size_t>(closestIndex));\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x(0) - M_PI);\n\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& lminDiff, size_t& lclosestIndex) {\n        double diff = std::abs(x(i) - M_PI);\n        if (diff < lminDiff) {\n            lminDiff = diff;\n            lclosestIndex = i;\n        }\n    }, Kokkos::Min<double>(minDiff), Kokkos::Min<size_t>(closestIndex));\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x(0) - M_PI);\n\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& lminDiff, size_t& lclosestIndex) {\n        double diff = std::abs(x(i) - M_PI);\n        if (diff < lminDiff) {\n            lminDiff = diff;\n            lclosestIndex = i;\n        }\n    }, Kokkos::Min<double>(minDiff), Kokkos::Min<size_t>(closestIndex));\n\n    return closestIndex;\n}\n",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x(0) - M_PI);\n\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& lminDiff, size_t& lclosestIndex) {\n        double diff = std::abs(x(i) - M_PI);\n        if (diff < lminDiff) {\n            lminDiff = diff;\n            lclosestIndex = i;\n        }\n    }, Kokkos::Min<double>(minDiff), Kokkos::Min<size_t>(closestIndex));\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x(0) - M_PI);\n\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& lminDiff, size_t& lclosestIndex) {\n        double diff = std::abs(x(i) - M_PI);\n        if (diff < lminDiff) {\n            lminDiff = diff;\n            lclosestIndex = i;\n        }\n    }, Kokkos::Min<double>(minDiff), Kokkos::Min<size_t>(closestIndex));\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x(0) - M_PI);\n\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& lminDiff, size_t& lclosestIndex) {\n        double diff = std::abs(x(i) - M_PI);\n        if (diff < lminDiff) {\n            lminDiff = diff;\n            lclosestIndex = i;\n        }\n    }, Kokkos::Min<double>(minDiff), Kokkos::Min<size_t>(closestIndex));\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x(0) - M_PI);\n\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& lminDiff, size_t& lclosestIndex) {\n        double diff = std::abs(x(i) - M_PI);\n        if (diff < lminDiff) {\n            lminDiff = diff;\n            lclosestIndex = i;\n        }\n    }, Kokkos::Min<double>(minDiff), Kokkos::Min<size_t>(closestIndex));\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x(0) - M_PI);\n\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& lminDiff, size_t& lclosestIndex) {\n        double diff = std::abs(x(i) - M_PI);\n        if (diff < lminDiff) {\n            lminDiff = diff;\n            lclosestIndex = i;\n        }\n    }, Kokkos::Min<double>(minDiff), Kokkos::Min<size_t>(closestIndex));\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x(0) - M_PI);\n\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& lminDiff, size_t& lclosestIndex) {\n        double diff = std::abs(x(i) - M_PI);\n        if (diff < lminDiff) {\n            lminDiff = diff;\n            lclosestIndex = i;\n        }\n    }, Kokkos::Min<double>(minDiff), Kokkos::Min<size_t>(closestIndex));\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x(0) - M_PI);\n\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& lminDiff, size_t& lclosestIndex) {\n        double diff = std::abs(x(i) - M_PI);\n        if (diff < lminDiff) {\n            lminDiff = diff;\n            lclosestIndex = i;\n        }\n    }, Kokkos::Min<double>(minDiff), Kokkos::Min<size_t>(closestIndex));\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x(0) - M_PI);\n\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& lminDiff, size_t& lclosestIndex) {\n        double diff = std::abs(x(i) - M_PI);\n        if (diff < lminDiff) {\n            lminDiff = diff;\n            lclosestIndex = i;\n        }\n    }, Kokkos::Min<double>(minDiff), Kokkos::Min<size_t>(closestIndex));\n\n    return closestIndex;\n}\n",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x(0) - M_PI);\n\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& lminDiff, size_t& lclosestIndex) {\n        double diff = std::abs(x(i) - M_PI);\n        if (diff < lminDiff) {\n            lminDiff = diff;\n            lclosestIndex = i;\n        }\n    }, Kokkos::Min<double>(minDiff), Kokkos::Min<size_t>(closestIndex));\n\n    return closestIndex;\n}\n",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x(0) - M_PI);\n\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& lminDiff, size_t& lclosestIndex) {\n        double diff = std::abs(x(i) - M_PI);\n        if (diff < lminDiff) {\n            lminDiff = diff;\n            lclosestIndex = i;\n        }\n    }, Kokkos::Min<double>(minDiff), Kokkos::Min<size_t>(closestIndex));\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x(0) - M_PI);\n\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& lminDiff, size_t& lclosestIndex) {\n        double diff = std::abs(x(i) - M_PI);\n        if (diff < lminDiff) {\n            lminDiff = diff;\n            lclosestIndex = i;\n        }\n    }, Kokkos::Min<double>(minDiff), Kokkos::Min<size_t>(closestIndex));\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x(0) - M_PI);\n\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& lminDiff, size_t& lclosestIndex) {\n        double diff = std::abs(x(i) - M_PI);\n        if (diff < lminDiff) {\n            lminDiff = diff;\n            lclosestIndex = i;\n        }\n    }, Kokkos::Min<double>(minDiff), Kokkos::Min<size_t>(closestIndex));\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x(0) - M_PI);\n\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& lminDiff, size_t& lclosestIndex) {\n        double diff = std::abs(x(i) - M_PI);\n        if (diff < lminDiff) {\n            lminDiff = diff;\n            lclosestIndex = i;\n        }\n    }, Kokkos::Min<double>(minDiff), Kokkos::Min<size_t>(closestIndex));\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x(0) - M_PI);\n\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& lminDiff, size_t& lclosestIndex) {\n        double diff = std::abs(x(i) - M_PI);\n        if (diff < lminDiff) {\n            lminDiff = diff;\n            lclosestIndex = i;\n        }\n    }, Kokkos::Min<double>(minDiff), Kokkos::Min<size_t>(closestIndex));\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x(0) - M_PI);\n\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& lminDiff, size_t& lclosestIndex) {\n        double diff = std::abs(x(i) - M_PI);\n        if (diff < lminDiff) {\n            lminDiff = diff;\n            lclosestIndex = i;\n        }\n    }, Kokkos::Min<double>(minDiff), Kokkos::Min<size_t>(closestIndex));\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x(0) - M_PI);\n\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& lminDiff, size_t& lclosestIndex) {\n        double diff = std::abs(x(i) - M_PI);\n        if (diff < lminDiff) {\n            lminDiff = diff;\n            lclosestIndex = i;\n        }\n    }, Kokkos::Min<double>(minDiff), Kokkos::Min<size_t>(closestIndex));\n\n    return closestIndex;\n}"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "37_search_find_the_closest_number_to_pi",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Use OpenMP to search in parallel.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    size_t closestIndex = 0;\n    double minDifference = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < x.size(); ++i) {\n        double difference = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (difference < minDifference) {\n                minDifference = difference;\n                closestIndex = i;\n            }\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < minDiff) {\n                minDiff = diff;\n                closestIndex = i;\n            }\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < minDiff) {\n                minDiff = diff;\n                closestIndex = i;\n            }\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < minDiff) {\n                minDiff = diff;\n                closestIndex = i;\n            }\n        }\n    }\n\n    return closestIndex;\n}\n",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < minDiff) {\n                minDiff = diff;\n                closestIndex = i;\n            }\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < minDiff) {\n                minDiff = diff;\n                closestIndex = i;\n            }\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < minDiff) {\n                minDiff = diff;\n                closestIndex = i;\n            }\n        }\n    }\n\n    return closestIndex;\n}\n",
      "\n    size_t closestIndex = 0;\n    double minDifference = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < x.size(); ++i) {\n        double difference = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (difference < minDifference) {\n                minDifference = difference;\n                closestIndex = i;\n            }\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < minDiff) {\n                minDiff = diff;\n                closestIndex = i;\n            }\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < minDiff) {\n                minDiff = diff;\n                closestIndex = i;\n            }\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDifference = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < x.size(); ++i) {\n        double difference = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (difference < minDifference) {\n                minDifference = difference;\n                closestIndex = i;\n            }\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < minDiff) {\n                minDiff = diff;\n                closestIndex = i;\n            }\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < minDiff) {\n                minDiff = diff;\n                closestIndex = i;\n            }\n        }\n    }\n\n    return closestIndex;\n}\n",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < minDiff) {\n                minDiff = diff;\n                closestIndex = i;\n            }\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < minDiff) {\n                minDiff = diff;\n                closestIndex = i;\n            }\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < minDiff) {\n                minDiff = diff;\n                closestIndex = i;\n            }\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < minDiff) {\n                minDiff = diff;\n                closestIndex = i;\n            }\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < minDiff) {\n                minDiff = diff;\n                closestIndex = i;\n            }\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDifference = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < x.size(); ++i) {\n        double difference = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (difference < minDifference) {\n                minDifference = difference;\n                closestIndex = i;\n            }\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDiff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < minDiff) {\n                minDiff = diff;\n                closestIndex = i;\n            }\n        }\n    }\n\n    return closestIndex;\n}"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "37_search_find_the_closest_number_to_pi",
    "parallelism_model": "serial",
    "prompt": "/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double minDiff = std::abs(x[0] - M_PI);\n    size_t index = 0;\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < minDiff) {\n            minDiff = diff;\n            index = i;\n        }\n    }\n    return index;\n}",
      "\n    double closestDiff = std::abs(x[0] - M_PI);\n    size_t closestIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < closestDiff) {\n            closestDiff = diff;\n            closestIndex = i;\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    double minDiff = std::abs(x[0] - M_PI);\n    size_t index = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < minDiff) {\n            minDiff = diff;\n            index = i;\n        }\n    }\n\n    return index;\n}",
      "\n    double closestDiff = std::abs(x[0] - M_PI);\n    size_t closestIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < closestDiff) {\n            closestDiff = diff;\n            closestIndex = i;\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    double closestDiff = std::abs(x[0] - M_PI);\n    size_t closestIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < closestDiff) {\n            closestDiff = diff;\n            closestIndex = i;\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    double minDiff = std::abs(x[0] - M_PI);\n    size_t index = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < minDiff) {\n            minDiff = diff;\n            index = i;\n        }\n    }\n\n    return index;\n}",
      "\n    size_t closestIndex = 0;\n    double minDifference = std::abs(x[0] - M_PI);\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double difference = std::abs(x[i] - M_PI);\n        if (difference < minDifference) {\n            minDifference = difference;\n            closestIndex = i;\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    double closestDiff = std::abs(x[0] - M_PI);\n    size_t closestIndex = 0;\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < closestDiff) {\n            closestDiff = diff;\n            closestIndex = i;\n        }\n    }\n    return closestIndex;\n}",
      "\n    double closestDiff = std::abs(x[0] - M_PI);\n    size_t closestIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < closestDiff) {\n            closestDiff = diff;\n            closestIndex = i;\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    double closestDiff = std::abs(x[0] - M_PI);\n    size_t closestIndex = 0;\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < closestDiff) {\n            closestDiff = diff;\n            closestIndex = i;\n        }\n    }\n    return closestIndex;\n}",
      "\n    double minDiff = std::abs(x[0] - M_PI);\n    size_t minIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < minDiff) {\n            minDiff = diff;\n            minIndex = i;\n        }\n    }\n\n    return minIndex;\n}",
      "\n    size_t closestIndex = 0;\n    double minDifference = std::abs(x[0] - M_PI);\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double difference = std::abs(x[i] - M_PI);\n        if (difference < minDifference) {\n            minDifference = difference;\n            closestIndex = i;\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    double closestDiff = std::abs(x[0] - M_PI);\n    size_t closestIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < closestDiff) {\n            closestDiff = diff;\n            closestIndex = i;\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    double minDiff = std::abs(x[0] - M_PI);\n    size_t index = 0;\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < minDiff) {\n            minDiff = diff;\n            index = i;\n        }\n    }\n    return index;\n}",
      "\n    double minDiff = std::abs(x[0] - M_PI);\n    size_t closestIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < minDiff) {\n            minDiff = diff;\n            closestIndex = i;\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    double closestDiff = std::abs(x[0] - M_PI);\n    size_t closestIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < closestDiff) {\n            closestDiff = diff;\n            closestIndex = i;\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    double closestDiff = std::abs(x[0] - M_PI);\n    size_t closestIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < closestDiff) {\n            closestDiff = diff;\n            closestIndex = i;\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    double closestDiff = std::abs(x[0] - M_PI);\n    size_t closestIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < closestDiff) {\n            closestDiff = diff;\n            closestIndex = i;\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    double closestDiff = std::abs(x[0] - M_PI);\n    size_t closestIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < closestDiff) {\n            closestDiff = diff;\n            closestIndex = i;\n        }\n    }\n\n    return closestIndex;\n}",
      "\n    double minDiff = std::abs(x[0] - M_PI);\n    size_t index = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < minDiff) {\n            minDiff = diff;\n            index = i;\n        }\n    }\n\n    return index;\n}"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "37_search_find_the_closest_number_to_pi",
    "parallelism_model": "hip",
    "prompt": "/* Find the index of the value in the vector x that is closest to the math constant PI. Store the index in closestToPiIndex.\n   Use M_PI for the value of PI.\n   Use AMD HIP to search in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\n__global__ void findClosestToPi(const double *x, size_t N, size_t *closestToPiIndex) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = fabs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = fabs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = fabs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = fabs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = fabs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = fabs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = fabs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = fabs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        tid += blockDim.x * gridDim.x;\n        while (tid < N) {\n            double diff = abs(x[tid] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = tid;\n            }\n            tid += blockDim.x * gridDim.x;\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double closestDiff = abs(x[tid] - M_PI);\n        size_t closestIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < closestDiff) {\n                closestDiff = diff;\n                closestIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, closestIndex);\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t index = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                index = i;\n            }\n        }\n        atomicMin(closestToPiIndex, index);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = fabs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = fabs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double closestDiff = abs(x[tid] - M_PI);\n        size_t closestIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < closestDiff) {\n                closestDiff = diff;\n                closestIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, closestIndex);\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = fabs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = fabs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = fabs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = fabs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "37_search_find_the_closest_number_to_pi",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double closestDiff = std::abs(x[0] - M_PI);\n    size_t closestIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < closestDiff) {\n            closestDiff = diff;\n            closestIndex = i;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            size_t index;\n            MPI_Recv(&index, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            double diff = std::abs(x[index] - M_PI);\n            if (diff < closestDiff) {\n                closestDiff = diff;\n                closestIndex = index;\n            }\n        }\n    } else {\n        MPI_Send(&closestIndex, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return closestIndex;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double minDiff = std::abs(x[0] - M_PI);\n    size_t minIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < minDiff) {\n            minDiff = diff;\n            minIndex = i;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            size_t recvIndex;\n            double recvDiff;\n            MPI_Recv(&recvIndex, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Recv(&recvDiff, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            if (recvDiff < minDiff) {\n                minDiff = recvDiff;\n                minIndex = recvIndex;\n            }\n        }\n    } else {\n        MPI_Send(&minIndex, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(&minDiff, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return minIndex;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            local_index = i;\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            local_index = i;\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double closestDiff = std::abs(x[0] - M_PI);\n    size_t closestIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < closestDiff) {\n            closestDiff = diff;\n            closestIndex = i;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            size_t index;\n            double diff;\n            MPI_Recv(&index, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Recv(&diff, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            if (diff < closestDiff) {\n                closestDiff = diff;\n                closestIndex = index;\n            }\n        }\n    } else {\n        MPI_Send(&closestIndex, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(&closestDiff, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return closestIndex;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double minDiff = std::abs(x[0] - M_PI);\n    size_t minIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < minDiff) {\n            minDiff = diff;\n            minIndex = i;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            size_t recvIndex;\n            double recvDiff;\n            MPI_Recv(&recvIndex, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Recv(&recvDiff, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            if (recvDiff < minDiff) {\n                minDiff = recvDiff;\n                minIndex = recvIndex;\n            }\n        }\n    } else {\n        MPI_Send(&minIndex, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(&minDiff, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return minIndex;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double closestDiff = std::abs(x[0] - M_PI);\n    size_t closestIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < closestDiff) {\n            closestDiff = diff;\n            closestIndex = i;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            size_t recvIndex;\n            double recvDiff;\n            MPI_Recv(&recvIndex, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Recv(&recvDiff, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            if (recvDiff < closestDiff) {\n                closestDiff = recvDiff;\n                closestIndex = recvIndex;\n            }\n        }\n    } else {\n        MPI_Send(&closestIndex, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(&closestDiff, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return closestIndex;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            local_index = i;\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            local_index = i;\n        }\n    }\n\n    size_t global_index;\n    double global_min_diff;\n\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&min_diff, &global_min_diff, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double minDiff = std::abs(x[0] - M_PI);\n    size_t closestIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < minDiff) {\n            minDiff = diff;\n            closestIndex = i;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            size_t recvIndex;\n            MPI_Recv(&recvIndex, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            double recvValue = x[recvIndex];\n            double diff = std::abs(recvValue - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                closestIndex = recvIndex;\n            }\n        }\n    } else {\n        MPI_Send(&closestIndex, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return closestIndex;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            local_index = i;\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            local_index = i;\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            local_index = i;\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;  // Return a dummy value on other ranks\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double closestDiff = std::abs(x[0] - M_PI);\n    size_t closestIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < closestDiff) {\n            closestDiff = diff;\n            closestIndex = i;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            size_t recvIndex;\n            double recvDiff;\n            MPI_Recv(&recvIndex, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Recv(&recvDiff, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            if (recvDiff < closestDiff) {\n                closestDiff = recvDiff;\n                closestIndex = recvIndex;\n            }\n        }\n    } else {\n        MPI_Send(&closestIndex, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(&closestDiff, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return closestIndex;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            local_index = i;\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double closestDiff = std::abs(x[0] - M_PI);\n    size_t closestIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < closestDiff) {\n            closestDiff = diff;\n            closestIndex = i;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            size_t recvIndex;\n            double recvDiff;\n            MPI_Recv(&recvIndex, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Recv(&recvDiff, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            if (recvDiff < closestDiff) {\n                closestDiff = recvDiff;\n                closestIndex = recvIndex;\n            }\n        }\n    } else {\n        MPI_Send(&closestIndex, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(&closestDiff, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return closestIndex;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double closestDiff = std::abs(x[0] - M_PI);\n    size_t closestIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < closestDiff) {\n            closestDiff = diff;\n            closestIndex = i;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            size_t recvIndex;\n            double recvDiff;\n            MPI_Recv(&recvIndex, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Recv(&recvDiff, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            if (recvDiff < closestDiff) {\n                closestDiff = recvDiff;\n                closestIndex = recvIndex;\n            }\n        }\n    } else {\n        MPI_Send(&closestIndex, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(&closestDiff, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return closestIndex;\n}\n",
      "\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double minDiff = std::abs(x[0] - M_PI);\n    size_t closestIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < minDiff) {\n            minDiff = diff;\n            closestIndex = i;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < MPI_Comm_size(MPI_COMM_WORLD); ++i) {\n            size_t recvIndex;\n            MPI_Recv(&recvIndex, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            double recvDiff = std::abs(x[recvIndex] - M_PI);\n            if (recvDiff < minDiff) {\n                minDiff = recvDiff;\n                closestIndex = recvIndex;\n            }\n        }\n    } else {\n        MPI_Send(&closestIndex, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return closestIndex;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double closestDiff = std::abs(x[0] - M_PI);\n    size_t closestIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < closestDiff) {\n            closestDiff = diff;\n            closestIndex = i;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            size_t recvIndex;\n            double recvDiff;\n            MPI_Recv(&recvIndex, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Recv(&recvDiff, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            if (recvDiff < closestDiff) {\n                closestDiff = recvDiff;\n                closestIndex = recvIndex;\n            }\n        }\n    } else {\n        MPI_Send(&closestIndex, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(&closestDiff, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return closestIndex;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double closestDiff = std::abs(x[0] - M_PI);\n    size_t closestIndex = 0;\n\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < closestDiff) {\n            closestDiff = diff;\n            closestIndex = i;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            size_t recvIndex;\n            double recvDiff;\n            MPI_Recv(&recvIndex, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Recv(&recvDiff, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            if (recvDiff < closestDiff) {\n                closestDiff = recvDiff;\n                closestIndex = recvIndex;\n            }\n        }\n    } else {\n        MPI_Send(&closestIndex, 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(&closestDiff, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n\n    return closestIndex;\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "37_search_find_the_closest_number_to_pi",
    "parallelism_model": "cuda",
    "prompt": "/* Find the index of the value in the vector x that is closest to the math constant PI. Store the index in closestToPiIndex.\n   Use M_PI for the value of PI.\n   Use CUDA to search in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\n__global__ void findClosestToPi(const double *x, size_t N, size_t *closestToPiIndex) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = fabs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = fabs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double minDiff = abs(x[tid] - M_PI);\n        size_t minIndex = tid;\n        for (size_t i = tid + blockDim.x; i < N; i += blockDim.x) {\n            double diff = abs(x[i] - M_PI);\n            if (diff < minDiff) {\n                minDiff = diff;\n                minIndex = i;\n            }\n        }\n        atomicMin(closestToPiIndex, minIndex);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "37_search_find_the_closest_number_to_pi",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Use MPI and OpenMP to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < min_diff) {\n                min_diff = diff;\n                local_index = i;\n            }\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < min_diff) {\n                min_diff = diff;\n                local_index = i;\n            }\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < min_diff) {\n                min_diff = diff;\n                local_index = i;\n            }\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < min_diff) {\n                min_diff = diff;\n                local_index = i;\n            }\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;  // or any other value since only rank 0 returns the result\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < min_diff) {\n                min_diff = diff;\n                local_index = i;\n            }\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < min_diff) {\n                min_diff = diff;\n                local_index = i;\n            }\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < min_diff) {\n                min_diff = diff;\n                local_index = i;\n            }\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < min_diff) {\n                min_diff = diff;\n                local_index = i;\n            }\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < min_diff) {\n                min_diff = diff;\n                local_index = i;\n            }\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < min_diff) {\n                min_diff = diff;\n                local_index = i;\n            }\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;  // Return a default value on other ranks\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            local_index = i;\n        }\n    }\n\n    size_t global_index;\n    double global_min_diff;\n\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&min_diff, &global_min_diff, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < min_diff) {\n                min_diff = diff;\n                local_index = i;\n            }\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < min_diff) {\n                min_diff = diff;\n                local_index = i;\n            }\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < min_diff) {\n                min_diff = diff;\n                local_index = i;\n            }\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < x.size(); ++i) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            local_index = i;\n        }\n    }\n\n    size_t global_index;\n    double global_min_diff;\n\n    MPI_Allreduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n    MPI_Allreduce(&min_diff, &global_min_diff, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < min_diff) {\n                min_diff = diff;\n                local_index = i;\n            }\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            local_index = i;\n        }\n    }\n\n    size_t global_index;\n    MPI_Allreduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_index;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        double diff = std::abs(x[i] - M_PI);\n        if (diff < min_diff) {\n            min_diff = diff;\n            local_index = i;\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < min_diff) {\n                min_diff = diff;\n                local_index = i;\n            }\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_index = 0;\n    double min_diff = std::abs(x[0] - M_PI);\n\n    #pragma omp parallel for\n    for (size_t i = rank; i < x.size(); i += size) {\n        double diff = std::abs(x[i] - M_PI);\n        #pragma omp critical\n        {\n            if (diff < min_diff) {\n                min_diff = diff;\n                local_index = i;\n            }\n        }\n    }\n\n    size_t global_index;\n    MPI_Reduce(&local_index, &global_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_index;\n    } else {\n        return 0;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "08_fft_split_fft",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(Kokkos::View<Kokkos::complex<double>*> const& x, Kokkos::View<double*> &r, Kokkos::View<double*> &i) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    double sumr = 0.0;\n    double sumi = 0.0;\n    for (int n = 0; n < x.extent(0); n++) {\n      double angle = 2 * M_PI * k * n / x.extent(0);\n      sumr += x(n).real() * cos(angle) + x(n).imag() * sin(angle);\n      sumi += x(n).imag() * cos(angle) - x(n).real() * sin(angle);\n    }\n    r(k) = sumr;\n    i(k) = sumi;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    double sum_r = 0.0;\n    double sum_i = 0.0;\n    for (int n = 0; n < x.extent(0); ++n) {\n      double angle = 2 * M_PI * k * n / x.extent(0);\n      sum_r += x(n).real() * cos(angle) + x(n).imag() * sin(angle);\n      sum_i += x(n).imag() * cos(angle) - x(n).real() * sin(angle);\n    }\n    r(k) = sum_r;\n    i(k) = sum_i;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    double sumr = 0.0;\n    double sumi = 0.0;\n    for (int n = 0; n < x.extent(0); n++) {\n      double angle = 2 * M_PI * k * n / x.extent(0);\n      sumr += x(n).real() * cos(angle) + x(n).imag() * sin(angle);\n      sumi += x(n).imag() * cos(angle) - x(n).real() * sin(angle);\n    }\n    r(k) = sumr;\n    i(k) = sumi;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    double sum_r = 0.0;\n    double sum_i = 0.0;\n    for (int n = 0; n < x.extent(0); n++) {\n      double angle = 2 * M_PI * k * n / x.extent(0);\n      sum_r += x(n).real() * cos(angle) + x(n).imag() * sin(angle);\n      sum_i += -x(n).real() * sin(angle) + x(n).imag() * cos(angle);\n    }\n    r(k) = sum_r;\n    i(k) = sum_i;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    double sumr = 0.0;\n    double sumi = 0.0;\n    for (int n = 0; n < x.extent(0); n++) {\n      double angle = 2 * M_PI * k * n / x.extent(0);\n      sumr += x(n).real() * cos(angle) + x(n).imag() * sin(angle);\n      sumi += x(n).imag() * cos(angle) - x(n).real() * sin(angle);\n    }\n    r(k) = sumr;\n    i(k) = sumi;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    double sumr = 0.0;\n    double sumi = 0.0;\n    for (int n = 0; n < x.extent(0); n++) {\n      double angle = 2 * M_PI * k * n / x.extent(0);\n      sumr += x(n).real() * cos(angle) + x(n).imag() * sin(angle);\n      sumi += x(n).imag() * cos(angle) - x(n).real() * sin(angle);\n    }\n    r(k) = sumr;\n    i(k) = sumi;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    int N = x.extent(0);\n    r(k) = 0;\n    i(k) = 0;\n    for (int n = 0; n < N; n++) {\n      r(k) += x(n).real() * cos(2 * M_PI * n * k / N) + x(n).imag() * sin(2 * M_PI * n * k / N);\n      i(k) += x(n).imag() * cos(2 * M_PI * n * k / N) - x(n).real() * sin(2 * M_PI * n * k / N);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    double sum_r = 0.0;\n    double sum_i = 0.0;\n    for (int n = 0; n < x.extent(0); n++) {\n      double angle = 2 * M_PI * k * n / x.extent(0);\n      sum_r += x(n).real() * cos(angle) + x(n).imag() * sin(angle);\n      sum_i += x(n).imag() * cos(angle) - x(n).real() * sin(angle);\n    }\n    r(k) = sum_r;\n    i(k) = sum_i;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"fft\", x.extent(0), KOKKOS_LAMBDA(int k) {\n    int N = x.extent(0);\n    r(k) = 0.0;\n    i(k) = 0.0;\n    for (int n = 0; n < N; n++) {\n      double angle = 2 * M_PI * k * n / N;\n      r(k) += x(n).real() * cos(angle) + x(n).imag() * sin(angle);\n      i(k) += x(n).imag() * cos(angle) - x(n).real() * sin(angle);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    int N = x.extent(0);\n    r(k) = 0;\n    i(k) = 0;\n    for (int n = 0; n < N; n++) {\n      double angle = 2 * M_PI * k * n / N;\n      r(k) += x(n).real() * cos(angle) + x(n).imag() * sin(angle);\n      i(k) += x(n).imag() * cos(angle) - x(n).real() * sin(angle);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    double sum_r = 0.0;\n    double sum_i = 0.0;\n    for (int n = 0; n < x.extent(0); n++) {\n      double angle = 2 * M_PI * k * n / x.extent(0);\n      sum_r += x(n).real() * cos(angle) + x(n).imag() * sin(angle);\n      sum_i += x(n).imag() * cos(angle) - x(n).real() * sin(angle);\n    }\n    r(k) = sum_r;\n    i(k) = sum_i;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    double sumr = 0.0;\n    double sumi = 0.0;\n    for (int j = 0; j < x.extent(0); j++) {\n      double angle = 2 * M_PI * i * j / x.extent(0);\n      sumr += x(j).real() * cos(angle) + x(j).imag() * sin(angle);\n      sumi += x(j).imag() * cos(angle) - x(j).real() * sin(angle);\n    }\n    r(i) = sumr;\n    i(i) = sumi;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    double sumr = 0.0;\n    double sumi = 0.0;\n    for (int n = 0; n < x.extent(0); n++) {\n      double angle = 2 * M_PI * n * k / x.extent(0);\n      sumr += x(n).real() * cos(angle) + x(n).imag() * sin(angle);\n      sumi += x(n).imag() * cos(angle) - x(n).real() * sin(angle);\n    }\n    r(k) = sumr;\n    i(k) = sumi;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    int N = x.extent(0);\n    r(k) = 0;\n    i(k) = 0;\n    for (int n = 0; n < N; n++) {\n      r(k) += x(n).real() * cos(2 * M_PI * n * k / N) + x(n).imag() * sin(2 * M_PI * n * k / N);\n      i(k) += x(n).imag() * cos(2 * M_PI * n * k / N) - x(n).real() * sin(2 * M_PI * n * k / N);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    double sumr = 0.0;\n    double sumi = 0.0;\n    for (int n = 0; n < x.extent(0); n++) {\n      double angle = 2 * M_PI * k * n / x.extent(0);\n      sumr += x(n).real() * cos(angle) + x(n).imag() * sin(angle);\n      sumi += x(n).imag() * cos(angle) - x(n).real() * sin(angle);\n    }\n    r(k) = sumr;\n    i(k) = sumi;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    double sumr = 0.0;\n    double sumi = 0.0;\n    for (int n = 0; n < x.extent(0); n++) {\n      double angle = -2 * M_PI * k * n / x.extent(0);\n      sumr += x(n).real() * cos(angle) - x(n).imag() * sin(angle);\n      sumi += x(n).real() * sin(angle) + x(n).imag() * cos(angle);\n    }\n    r(k) = sumr;\n    i(k) = sumi;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    double sumr = 0.0;\n    double sumi = 0.0;\n    for (int n = 0; n < x.extent(0); n++) {\n      double angle = 2 * M_PI * n * k / x.extent(0);\n      sumr += x(n).real() * cos(angle) + x(n).imag() * sin(angle);\n      sumi += x(n).imag() * cos(angle) - x(n).real() * sin(angle);\n    }\n    r(k) = sumr;\n    i(k) = sumi;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    double sumr = 0.0;\n    double sumi = 0.0;\n    for (int n = 0; n < x.extent(0); n++) {\n      double angle = 2 * M_PI * n * k / x.extent(0);\n      sumr += x(n).real() * cos(angle) + x(n).imag() * sin(angle);\n      sumi += x(n).imag() * cos(angle) - x(n).real() * sin(angle);\n    }\n    r(k) = sumr;\n    i(k) = sumi;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    double sumreal = 0;\n    double sumimag = 0;\n    for (int n = 0; n < x.extent(0); n++) {\n      double angle = 2 * M_PI * n * k / x.extent(0);\n      sumreal += x(n).real() * cos(angle) + x(n).imag() * sin(angle);\n      sumimag += -x(n).real() * sin(angle) + x(n).imag() * cos(angle);\n    }\n    r(k) = sumreal;\n    i(k) = sumimag;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    double sumreal = 0;\n    double sumimag = 0;\n    for (int n = 0; n < x.extent(0); n++) {\n      double angle = 2 * M_PI * k * n / x.extent(0);\n      sumreal += x(n).real() * cos(angle) + x(n).imag() * sin(angle);\n      sumimag += -x(n).real() * sin(angle) + x(n).imag() * cos(angle);\n    }\n    r(k) = sumreal;\n    i(k) = sumimag;\n  });\n  Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "08_fft_split_fft",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        double sumreal = 0;\n        double sumimag = 0;\n        for (int t = 0; t < N; ++t) {\n            double angle = 2 * M_PI * t * k / N;\n            sumreal += x[t].real() * cos(angle) + x[t].imag() * sin(angle);\n            sumimag += -x[t].real() * sin(angle) + x[t].imag() * cos(angle);\n        }\n        r[k] = sumreal;\n        i[k] = sumimag;\n    }\n}\n",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        double sumreal = 0;\n        double sumimag = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sumreal += x[n].real() * cos(angle) + x[n].imag() * sin(angle);\n            sumimag += -x[n].real() * sin(angle) + x[n].imag() * cos(angle);\n        }\n        r[k] = sumreal;\n        i[k] = sumimag;\n    }\n}\n",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        double sumreal = 0;\n        double sumimag = 0;\n        for (int t = 0; t < N; ++t) {\n            double angle = 2 * M_PI * t * k / N;\n            sumreal +=  x[t].real() * cos(angle) + x[t].imag() * sin(angle);\n            sumimag += -x[t].real() * sin(angle) + x[t].imag() * cos(angle);\n        }\n        r[k] = sumreal;\n        i[k] = sumimag;\n    }\n}\n",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        double sumreal = 0;\n        double sumimag = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sumreal += x[n].real() * cos(angle) + x[n].imag() * sin(angle);\n            sumimag += -x[n].real() * sin(angle) + x[n].imag() * cos(angle);\n        }\n        r[k] = sumreal;\n        i[k] = sumimag;\n    }\n}\n",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        double sumreal = 0;\n        double sumimag = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sumreal += x[n].real() * cos(angle) + x[n].imag() * sin(angle);\n            sumimag += -x[n].real() * sin(angle) + x[n].imag() * cos(angle);\n        }\n        r[k] = sumreal;\n        i[k] = sumimag;\n    }\n}\n",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        double sumreal = 0;\n        double sumimag = 0;\n        for (int t = 0; t < N; ++t) {\n            double angle = 2 * M_PI * t * k / N;\n            sumreal += x[t].real() * cos(angle) + x[t].imag() * sin(angle);\n            sumimag += -x[t].real() * sin(angle) + x[t].imag() * cos(angle);\n        }\n        r[k] = sumreal;\n        i[k] = sumimag;\n    }\n}\n",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        double sumreal = 0;\n        double sumimag = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * n * k / N;\n            sumreal += x[n].real() * cos(angle) + x[n].imag() * sin(angle);\n            sumimag += -x[n].real() * sin(angle) + x[n].imag() * cos(angle);\n        }\n        r[k] = sumreal;\n        i[k] = sumimag;\n    }\n}\n",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        double sumreal = 0;\n        double sumimag = 0;\n        for (int t = 0; t < N; ++t) {\n            double angle = 2 * M_PI * t * k / N;\n            sumreal += x[t].real() * cos(angle) + x[t].imag() * sin(angle);\n            sumimag += -x[t].real() * sin(angle) + x[t].imag() * cos(angle);\n        }\n        r[k] = sumreal;\n        i[k] = sumimag;\n    }\n}\n",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        double sumreal = 0;\n        double sumimag = 0;\n        for (int t = 0; t < N; ++t) {\n            double angle = 2 * M_PI * t * k / N;\n            sumreal += x[t].real() * cos(angle) + x[t].imag() * sin(angle);\n            sumimag += -x[t].real() * sin(angle) + x[t].imag() * cos(angle);\n        }\n        r[k] = sumreal;\n        i[k] = sumimag;\n    }\n}\n",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        double sumreal = 0;\n        double sumimag = 0;\n        for (int t = 0; t < N; ++t) {\n            double angle = 2 * M_PI * t * k / N;\n            sumreal += x[t].real() * cos(angle) + x[t].imag() * sin(angle);\n            sumimag += -x[t].real() * sin(angle) + x[t].imag() * cos(angle);\n        }\n        r[k] = sumreal;\n        i[k] = sumimag;\n    }\n}\n",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        double sumreal = 0;\n        double sumimag = 0;\n        for (int t = 0; t < N; ++t) {\n            double angle = 2 * M_PI * t * k / N;\n            sumreal += x[t].real() * cos(angle) + x[t].imag() * sin(angle);\n            sumimag += -x[t].real() * sin(angle) + x[t].imag() * cos(angle);\n        }\n        r[k] = sumreal;\n        i[k] = sumimag;\n    }\n}\n",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        double sumreal = 0;\n        double sumimag = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sumreal += x[n].real() * cos(angle) + x[n].imag() * sin(angle);\n            sumimag += -x[n].real() * sin(angle) + x[n].imag() * cos(angle);\n        }\n        r[k] = sumreal;\n        i[k] = sumimag;\n    }\n}\n",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        double sumreal = 0;\n        double sumimag = 0;\n        for (int t = 0; t < N; ++t) {\n            double angle = 2 * M_PI * t * k / N;\n            sumreal += x[t].real() * cos(angle) + x[t].imag() * sin(angle);\n            sumimag += -x[t].real() * sin(angle) + x[t].imag() * cos(angle);\n        }\n        r[k] = sumreal;\n        i[k] = sumimag;\n    }\n}\n",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        double sumreal = 0;\n        double sumimag = 0;\n        for (int t = 0; t < N; ++t) {\n            double angle = 2 * M_PI * t * k / N;\n            sumreal += x[t].real() * cos(angle) + x[t].imag() * sin(angle);\n            sumimag += -x[t].real() * sin(angle) + x[t].imag() * cos(angle);\n        }\n        r[k] = sumreal;\n        i[k] = sumimag;\n    }\n}\n",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        double sumreal = 0;\n        double sumimag = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * n * k / N;\n            sumreal += x[n].real() * cos(angle) + x[n].imag() * sin(angle);\n            sumimag += -x[n].real() * sin(angle) + x[n].imag() * cos(angle);\n        }\n        r[k] = sumreal;\n        i[k] = sumimag;\n    }\n}\n",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        double sumreal = 0;\n        double sumimag = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * n * k / N;\n            sumreal +=  x[n].real() * cos(angle) + x[n].imag() * sin(angle);\n            sumimag += -x[n].real() * sin(angle) + x[n].imag() * cos(angle);\n        }\n        r[k] = sumreal;\n        i[k] = sumimag;\n    }\n}\n",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        double sumreal = 0;\n        double sumimag = 0;\n        for (int t = 0; t < N; ++t) {\n            double angle = 2 * M_PI * t * k / N;\n            sumreal += x[t].real() * cos(angle) + x[t].imag() * sin(angle);\n            sumimag += -x[t].real() * sin(angle) + x[t].imag() * cos(angle);\n        }\n        r[k] = sumreal;\n        i[k] = sumimag;\n    }\n}",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        double sumreal = 0;\n        double sumimag = 0;\n        for (int t = 0; t < N; ++t) {\n            double angle = 2 * M_PI * t * k / N;\n            sumreal += x[t].real() * cos(angle) + x[t].imag() * sin(angle);\n            sumimag += -x[t].real() * sin(angle) + x[t].imag() * cos(angle);\n        }\n        r[k] = sumreal;\n        i[k] = sumimag;\n    }\n}",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        double sumreal = 0;\n        double sumimag = 0;\n        for (int t = 0; t < N; ++t) {\n            double angle = 2 * M_PI * t * k / N;\n            sumreal += x[t].real() * cos(angle) + x[t].imag() * sin(angle);\n            sumimag += -x[t].real() * sin(angle) + x[t].imag() * cos(angle);\n        }\n        r[k] = sumreal;\n        i[k] = sumimag;\n    }\n}",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        double sumreal = 0;\n        double sumimag = 0;\n        for (int t = 0; t < N; ++t) {\n            double angle = 2 * M_PI * t * k / N;\n            sumreal += x[t].real() * cos(angle) + x[t].imag() * sin(angle);\n            sumimag += -x[t].real() * sin(angle) + x[t].imag() * cos(angle);\n        }\n        r[k] = sumreal;\n        i[k] = sumimag;\n    }\n}"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "08_fft_split_fft",
    "parallelism_model": "serial",
    "prompt": "/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int n = x.size();\n    if (n <= 1) {\n        r = std::vector<double>(x.begin(), x.end());\n        i = std::vector<double>(n, 0);\n    } else {\n        std::vector<std::complex<double>> even(n/2);\n        std::vector<std::complex<double>> odd(n/2);\n        for (int k = 0; k < n/2; ++k) {\n            even[k] = x[2*k];\n            odd[k] = x[2*k + 1];\n        }\n        std::vector<double> r_even, i_even, r_odd, i_odd;\n        fft(even, r_even, i_even);\n        fft(odd, r_odd, i_odd);\n        r = std::vector<double>(n, 0);\n        i = std::vector<double>(n, 0);\n        for (int k = 0; k < n/2; ++k) {\n            double angle = -2 * M_PI * k / n;\n            std::complex<double> w(std::cos(angle), std::sin(angle));\n            std::complex<double> t = w * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + t.real();\n            i[k] = i_even[k] + t.imag();\n            r[k + n/2] = r_even[k] - t.real();\n            i[k + n/2] = i_even[k] - t.imag();\n        }\n    }\n}\n",
      "\n    int N = x.size();\n    if (N <= 1) {\n        r = std::vector<double>(x.begin(), x.end());\n        i = std::vector<double>(N, 0);\n    } else {\n        std::vector<std::complex<double>> even(N/2), odd(N/2);\n        for (int k = 0; k < N/2; ++k) {\n            even[k] = x[2*k];\n            odd[k] = x[2*k + 1];\n        }\n        std::vector<double> r_even, i_even, r_odd, i_odd;\n        fft(even, r_even, i_even);\n        fft(odd, r_odd, i_odd);\n        r.resize(N);\n        i.resize(N);\n        for (int k = 0; k < N/2; ++k) {\n            double t = -2 * M_PI * k / N;\n            std::complex<double> w = std::polar(1.0, t);\n            std::complex<double> z = w * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + z.real();\n            i[k] = i_even[k] + z.imag();\n            r[k + N/2] = r_even[k] - z.real();\n            i[k + N/2] = i_even[k] - z.imag();\n        }\n    }\n}\n",
      "\n    int N = x.size();\n    if (N <= 1) {\n        r = std::vector<double>(x.begin(), x.end());\n        i = std::vector<double>(N, 0);\n    } else {\n        std::vector<std::complex<double>> even(N/2);\n        std::vector<std::complex<double>> odd(N/2);\n        for (int k = 0; k < N/2; ++k) {\n            even[k] = x[2*k];\n            odd[k] = x[2*k + 1];\n        }\n        std::vector<double> r_even, i_even, r_odd, i_odd;\n        fft(even, r_even, i_even);\n        fft(odd, r_odd, i_odd);\n        r = std::vector<double>(N, 0);\n        i = std::vector<double>(N, 0);\n        for (int k = 0; k < N/2; ++k) {\n            double angle = -2 * M_PI * k / N;\n            std::complex<double> w(std::cos(angle), std::sin(angle));\n            std::complex<double> t = w * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + t.real();\n            i[k] = i_even[k] + t.imag();\n            r[k + N/2] = r_even[k] - t.real();\n            i[k + N/2] = i_even[k] - t.imag();\n        }\n    }\n}\n",
      "\n    int N = x.size();\n    if (N <= 1) {\n        r = std::vector<double>(x.begin(), x.end());\n        i = std::vector<double>(N, 0);\n    } else {\n        std::vector<std::complex<double>> even(N/2), odd(N/2);\n        for (int k = 0; k < N/2; ++k) {\n            even[k] = x[2*k];\n            odd[k] = x[2*k + 1];\n        }\n        std::vector<double> r_even, i_even, r_odd, i_odd;\n        fft(even, r_even, i_even);\n        fft(odd, r_odd, i_odd);\n        r.resize(N);\n        i.resize(N);\n        for (int k = 0; k < N/2; ++k) {\n            double t = -2 * M_PI * k / N;\n            std::complex<double> w(std::cos(t), std::sin(t));\n            std::complex<double> z = w * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + z.real();\n            i[k] = i_even[k] + z.imag();\n            r[k + N/2] = r_even[k] - z.real();\n            i[k + N/2] = i_even[k] - z.imag();\n        }\n    }\n```\n}",
      "\n    int N = x.size();\n    if (N <= 1) {\n        r = std::vector<double>(x.begin(), x.end());\n        i = std::vector<double>(N, 0);\n    } else {\n        std::vector<std::complex<double>> even(N/2);\n        std::vector<std::complex<double>> odd(N/2);\n        for (int k = 0; k < N/2; k++) {\n            even[k] = x[2*k];\n            odd[k] = x[2*k + 1];\n        }\n        std::vector<double> r_even, i_even, r_odd, i_odd;\n        fft(even, r_even, i_even);\n        fft(odd, r_odd, i_odd);\n        r = std::vector<double>(N, 0);\n        i = std::vector<double>(N, 0);\n        for (int k = 0; k < N/2; k++) {\n            double kth = -2 * M_PI * k / N;\n            std::complex<double> wk(std::cos(kth), std::sin(kth));\n            std::complex<double> t = wk * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + t.real();\n            i[k] = i_even[k] + t.imag();\n            r[k + N/2] = r_even[k] - t.real();\n            i[k + N/2] = i_even[k] - t.imag();\n        }\n    }\n}\n",
      "\n    int N = x.size();\n    if (N <= 1) {\n        r = x[0].real();\n        i = x[0].imag();\n    }\n\n    std::vector<std::complex<double>> even(N/2), odd(N/2);\n    for (int k = 0; k < N/2; ++k) {\n        even[k] = x[2*k];\n        odd[k] = x[2*k + 1];\n    }\n\n    std::vector<double> r_even, i_even, r_odd, i_odd;\n    fft(even, r_even, i_even);\n    fft(odd, r_odd, i_odd);\n\n    for (int k = 0; k < N/2; ++k) {\n        double theta = -2 * M_PI * k / N;\n        std::complex<double> t = std::polar(1.0, theta) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + t.real();\n        i[k] = i_even[k] + t.imag();\n        r[k + N/2] = r_even[k] - t.real();\n        i[k + N/2] = i_even[k] - t.imag();\n    }\n}\n",
      "\n    int N = x.size();\n    if (N <= 1) {\n        r = std::vector<double>(x.begin(), x.end());\n        i = std::vector<double>(N, 0);\n    } else {\n        std::vector<std::complex<double>> even(N/2), odd(N/2);\n        for (int k = 0; k < N/2; ++k) {\n            even[k] = x[2*k];\n            odd[k] = x[2*k + 1];\n        }\n        std::vector<double> r_even, i_even, r_odd, i_odd;\n        fft(even, r_even, i_even);\n        fft(odd, r_odd, i_odd);\n        for (int k = 0; k < N/2; ++k) {\n            double theta = -2 * M_PI * k / N;\n            std::complex<double> t = std::polar(1.0, theta) * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + t.real();\n            i[k] = i_even[k] + t.imag();\n            r[k + N/2] = r_even[k] - t.real();\n            i[k + N/2] = i_even[k] - t.imag();\n        }\n    }\n```\n}",
      "\n    int N = x.size();\n    if (N <= 1) {\n        r = std::vector<double>(x.begin(), x.end());\n        i = std::vector<double>(N, 0);\n    } else {\n        std::vector<std::complex<double>> even(N/2);\n        std::vector<std::complex<double>> odd(N/2);\n        for (int k = 0; k < N/2; ++k) {\n            even[k] = x[2*k];\n            odd[k] = x[2*k + 1];\n        }\n        std::vector<double> r_even, i_even, r_odd, i_odd;\n        fft(even, r_even, i_even);\n        fft(odd, r_odd, i_odd);\n        r = std::vector<double>(N, 0);\n        i = std::vector<double>(N, 0);\n        for (int k = 0; k < N/2; ++k) {\n            double angle = -2 * M_PI * k / N;\n            std::complex<double> w(std::cos(angle), std::sin(angle));\n            std::complex<double> t = w * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + t.real();\n            i[k] = i_even[k] + t.imag();\n            r[k + N/2] = r_even[k] - t.real();\n            i[k + N/2] = i_even[k] - t.imag();\n        }\n    }\n}\n",
      "\n    int N = x.size();\n    if (N <= 1) {\n        r = x.real();\n        i = x.imag();\n    } else {\n        std::vector<std::complex<double>> even(N/2), odd(N/2);\n        for (int k = 0; k < N/2; ++k) {\n            even[k] = x[2*k];\n            odd[k] = x[2*k + 1];\n        }\n        std::vector<double> r_even, i_even, r_odd, i_odd;\n        fft(even, r_even, i_even);\n        fft(odd, r_odd, i_odd);\n        for (int k = 0; k < N/2; ++k) {\n            double theta = -2 * M_PI * k / N;\n            std::complex<double> t = std::polar(1.0, theta) * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + t.real();\n            i[k] = i_even[k] + t.imag();\n            r[k + N/2] = r_even[k] - t.real();\n            i[k + N/2] = i_even[k] - t.imag();\n        }\n    }\n}",
      "\n    int N = x.size();\n    if (N <= 1) {\n        r = x.real();\n        i = x.imag();\n    } else {\n        std::vector<std::complex<double>> even(N/2), odd(N/2);\n        for (int k = 0; k < N/2; ++k) {\n            even[k] = x[2*k];\n            odd[k] = x[2*k + 1];\n        }\n        std::vector<double> r_even, i_even, r_odd, i_odd;\n        fft(even, r_even, i_even);\n        fft(odd, r_odd, i_odd);\n        for (int k = 0; k < N/2; ++k) {\n            double theta = -2 * M_PI * k / N;\n            std::complex<double> t = std::polar(1.0, theta) * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + t.real();\n            i[k] = i_even[k] + t.imag();\n            r[k + N/2] = r_even[k] - t.real();\n            i[k + N/2] = i_even[k] - t.imag();\n        }\n    }\n```\n}",
      "\n    int N = x.size();\n    if (N <= 1) {\n        r = std::vector<double>(x.begin(), x.end());\n        i = std::vector<double>(N, 0);\n    } else {\n        std::vector<std::complex<double>> even(N/2);\n        std::vector<std::complex<double>> odd(N/2);\n        for (int k = 0; k < N/2; ++k) {\n            even[k] = x[2*k];\n            odd[k] = x[2*k + 1];\n        }\n        std::vector<double> r_even, i_even, r_odd, i_odd;\n        fft(even, r_even, i_even);\n        fft(odd, r_odd, i_odd);\n        r.resize(N);\n        i.resize(N);\n        for (int k = 0; k < N/2; ++k) {\n            double angle = -2 * M_PI * k / N;\n            std::complex<double> w(std::cos(angle), std::sin(angle));\n            std::complex<double> t = w * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + t.real();\n            i[k] = i_even[k] + t.imag();\n            r[k + N/2] = r_even[k] - t.real();\n            i[k + N/2] = i_even[k] - t.imag();\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    if (n <= 1) {\n        r = std::vector<double>(x.begin(), x.end());\n        i = std::vector<double>(n, 0);\n    } else {\n        std::vector<std::complex<double>> even(n/2);\n        std::vector<std::complex<double>> odd(n/2);\n        for (int k = 0; k < n/2; ++k) {\n            even[k] = x[2*k];\n            odd[k] = x[2*k + 1];\n        }\n        std::vector<double> r_even, i_even, r_odd, i_odd;\n        fft(even, r_even, i_even);\n        fft(odd, r_odd, i_odd);\n        r = std::vector<double>(n, 0);\n        i = std::vector<double>(n, 0);\n        for (int k = 0; k < n/2; ++k) {\n            double angle = -2 * M_PI * k / n;\n            std::complex<double> w(std::cos(angle), std::sin(angle));\n            std::complex<double> t = w * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + t.real();\n            i[k] = i_even[k] + t.imag();\n            r[k + n/2] = r_even[k] - t.real();\n            i[k + n/2] = i_even[k] - t.imag();\n        }\n    }\n}\n",
      "\n    int N = x.size();\n    if (N <= 1) {\n        r = std::vector<double>(x.begin(), x.end());\n        i = std::vector<double>(N, 0);\n    } else {\n        std::vector<std::complex<double>> even(N/2), odd(N/2);\n        for (int k = 0; k < N/2; ++k) {\n            even[k] = x[2*k];\n            odd[k] = x[2*k + 1];\n        }\n        std::vector<double> r_even, i_even, r_odd, i_odd;\n        fft(even, r_even, i_even);\n        fft(odd, r_odd, i_odd);\n        r.resize(N);\n        i.resize(N);\n        for (int k = 0; k < N/2; ++k) {\n            double t = -2 * M_PI * k / N;\n            std::complex<double> wk(std::cos(t), std::sin(t));\n            std::complex<double> yk = wk * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + yk.real();\n            i[k] = i_even[k] + yk.imag();\n            r[k + N/2] = r_even[k] - yk.real();\n            i[k + N/2] = i_even[k] - yk.imag();\n        }\n    }\n```\n}",
      "\n    int N = x.size();\n    if (N <= 1) {\n        r = std::vector<double>(x.begin(), x.end());\n        i = std::vector<double>(N, 0);\n    } else {\n        std::vector<std::complex<double>> even(N/2);\n        std::vector<std::complex<double>> odd(N/2);\n        for (int k = 0; k < N/2; ++k) {\n            even[k] = x[2*k];\n            odd[k] = x[2*k + 1];\n        }\n        std::vector<double> r_even, i_even, r_odd, i_odd;\n        fft(even, r_even, i_even);\n        fft(odd, r_odd, i_odd);\n        r = std::vector<double>(N, 0);\n        i = std::vector<double>(N, 0);\n        for (int k = 0; k < N/2; ++k) {\n            double angle = -2 * M_PI * k / N;\n            std::complex<double> w(std::cos(angle), std::sin(angle));\n            std::complex<double> t = w * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + t.real();\n            i[k] = i_even[k] + t.imag();\n            r[k + N/2] = r_even[k] - t.real();\n            i[k + N/2] = i_even[k] - t.imag();\n        }\n    }\n}\n",
      "\n    int N = x.size();\n    if (N <= 1) {\n        r = std::vector<double>(x.begin(), x.end());\n        i = std::vector<double>(N, 0);\n    } else {\n        std::vector<std::complex<double>> even(N/2);\n        std::vector<std::complex<double>> odd(N/2);\n        for (int k = 0; k < N/2; ++k) {\n            even[k] = x[2*k];\n            odd[k] = x[2*k + 1];\n        }\n        std::vector<double> r_even, i_even, r_odd, i_odd;\n        fft(even, r_even, i_even);\n        fft(odd, r_odd, i_odd);\n        r = std::vector<double>(N, 0);\n        i = std::vector<double>(N, 0);\n        for (int k = 0; k < N/2; ++k) {\n            double angle = -2 * M_PI * k / N;\n            std::complex<double> w(std::cos(angle), std::sin(angle));\n            std::complex<double> t = w * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + t.real();\n            i[k] = i_even[k] + t.imag();\n            r[k + N/2] = r_even[k] - t.real();\n            i[k + N/2] = i_even[k] - t.imag();\n        }\n    }\n}\n",
      "\n    int N = x.size();\n    if (N <= 1) {\n        r = std::vector<double>(x.begin(), x.end());\n        i = std::vector<double>(N, 0);\n    } else {\n        std::vector<std::complex<double>> even(N/2), odd(N/2);\n        for (int k = 0; k < N/2; ++k) {\n            even[k] = x[2*k];\n            odd[k] = x[2*k + 1];\n        }\n        std::vector<double> r_even, i_even, r_odd, i_odd;\n        fft(even, r_even, i_even);\n        fft(odd, r_odd, i_odd);\n        r.resize(N);\n        i.resize(N);\n        for (int k = 0; k < N/2; ++k) {\n            double angle = -2 * M_PI * k / N;\n            std::complex<double> w(cos(angle), sin(angle));\n            std::complex<double> z = w * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + z.real();\n            i[k] = i_even[k] + z.imag();\n            r[k + N/2] = r_even[k] - z.real();\n            i[k + N/2] = i_even[k] - z.imag();\n        }\n    }\n```\n}",
      "\n    int N = x.size();\n    if (N <= 1) {\n        r = std::vector<double>(x.begin(), x.end());\n        i = std::vector<double>(N, 0);\n    } else {\n        std::vector<std::complex<double>> even(N/2);\n        std::vector<std::complex<double>> odd(N/2);\n        for (int k = 0; k < N/2; ++k) {\n            even[k] = x[2*k];\n            odd[k] = x[2*k + 1];\n        }\n        std::vector<double> r_even, i_even, r_odd, i_odd;\n        fft(even, r_even, i_even);\n        fft(odd, r_odd, i_odd);\n        r = std::vector<double>(N, 0);\n        i = std::vector<double>(N, 0);\n        for (int k = 0; k < N/2; ++k) {\n            double angle = -2 * M_PI * k / N;\n            std::complex<double> t(std::cos(angle), std::sin(angle));\n            t *= std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + t.real();\n            i[k] = i_even[k] + t.imag();\n            r[k + N/2] = r_even[k] - t.real();\n            i[k + N/2] = i_even[k] - t.imag();\n        }\n    }\n```\n}",
      "\n    int N = x.size();\n    if (N <= 1) {\n        r = std::vector<double>(x.begin(), x.end());\n        i = std::vector<double>(N, 0);\n    } else {\n        std::vector<std::complex<double>> even(N/2);\n        std::vector<std::complex<double>> odd(N/2);\n        for (int k = 0; k < N/2; ++k) {\n            even[k] = x[2*k];\n            odd[k] = x[2*k + 1];\n        }\n        std::vector<double> r_even, i_even, r_odd, i_odd;\n        fft(even, r_even, i_even);\n        fft(odd, r_odd, i_odd);\n        r.resize(N);\n        i.resize(N);\n        for (int k = 0; k < N/2; ++k) {\n            double angle = -2 * M_PI * k / N;\n            std::complex<double> w(std::cos(angle), std::sin(angle));\n            std::complex<double> t = w * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + t.real();\n            i[k] = i_even[k] + t.imag();\n            r[k + N/2] = r_even[k] - t.real();\n            i[k + N/2] = i_even[k] - t.imag();\n        }\n    }\n}\n",
      "\n    int N = x.size();\n    if (N <= 1) {\n        r = x.real();\n        i = x.imag();\n    } else {\n        std::vector<std::complex<double>> even(N/2), odd(N/2);\n        for (int k = 0; k < N/2; ++k) {\n            even[k] = x[2*k];\n            odd[k] = x[2*k + 1];\n        }\n        std::vector<double> r_even, i_even, r_odd, i_odd;\n        fft(even, r_even, i_even);\n        fft(odd, r_odd, i_odd);\n        for (int k = 0; k < N/2; ++k) {\n            double theta = -2 * M_PI * k / N;\n            std::complex<double> t = std::polar(1.0, theta) * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + t.real();\n            i[k] = i_even[k] + t.imag();\n            r[k + N/2] = r_even[k] - t.real();\n            i[k + N/2] = i_even[k] - t.imag();\n        }\n    }\n```\n}",
      "\n    int N = x.size();\n    if (N <= 1) {\n        r = std::vector<double>(x.begin(), x.end());\n        i = std::vector<double>(N, 0);\n    } else {\n        std::vector<std::complex<double>> even(N/2);\n        std::vector<std::complex<double>> odd(N/2);\n        for (int k = 0; k < N/2; ++k) {\n            even[k] = x[2*k];\n            odd[k] = x[2*k + 1];\n        }\n        std::vector<double> r_even;\n        std::vector<double> i_even;\n        fft(even, r_even, i_even);\n        std::vector<double> r_odd;\n        std::vector<double> i_odd;\n        fft(odd, r_odd, i_odd);\n        r = std::vector<double>(N, 0);\n        i = std::vector<double>(N, 0);\n        for (int k = 0; k < N/2; ++k) {\n            double angle = -2 * M_PI * k / N;\n            std::complex<double> w(std::cos(angle), std::sin(angle));\n            std::complex<double> t = w * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + t.real();\n            i[k] = i_even[k] + t.imag();\n            r[k + N/2] = r_even[k] - t.real();\n            i[k + N/2] = i_even[k] - t.imag();\n        }\n    }\n```\n}"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "08_fft_split_fft",
    "parallelism_model": "hip",
    "prompt": "/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\n__global__ void fft(const hipDoubleComplex *x, double *r, double *i, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * M_PI * tid / N;\n        double sum_r = 0.0;\n        double sum_i = 0.0;\n        for (size_t k = 0; k < N; ++k) {\n            double cos_val = cos(angle * k);\n            double sin_val = sin(angle * k);\n            sum_r += x[k].x * cos_val - x[k].y * sin_val;\n            sum_i += x[k].x * sin_val + x[k].y * cos_val;\n        }\n        r[tid] = sum_r;\n        i[tid] = sum_i;\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Compute the Fourier transform of x and store real part in r and imaginary part in i\n        // Implementation details using AMD HIP to compute in parallel\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (tid < N) {\n        double angle = -2 * M_PI * tid / N;\n        r[tid] = 0;\n        i[tid] = 0;\n\n        for (size_t k = 0; k < N; k++) {\n            double cos_val = cos(angle * k);\n            double sin_val = sin(angle * k);\n            r[tid] += x[k].x * cos_val - x[k].y * sin_val;\n            i[tid] += x[k].x * sin_val + x[k].y * cos_val;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Compute the Fourier transform using AMD HIP\n        // Implement the Fourier transform algorithm here\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Compute the Fourier transform of x and store the real part in r and imaginary part in i\n        // Implementation details using AMD HIP to compute in parallel\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Compute the Fourier transform of x[tid] and store the real part in r[tid] and the imaginary part in i[tid]\n        // Implementation of the Fourier transform algorithm using AMD HIP\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum_r = 0.0;\n        double sum_i = 0.0;\n        for (size_t k = 0; k < N; ++k) {\n            double angle = -2 * M_PI * tid * k / N;\n            sum_r += x[k].x * cos(angle) - x[k].y * sin(angle);\n            sum_i += x[k].x * sin(angle) + x[k].y * cos(angle);\n        }\n        r[tid] = sum_r;\n        i[tid] = sum_i;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Perform the Fourier transform calculation here\n        // Use tid to access the elements of x and store the real and imaginary parts in r and i\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Compute the Fourier transform of x[tid] and store the real part in r[tid] and imaginary part in i[tid]\n        // Implementation of Fourier transform goes here\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Perform the Fourier transform calculation here\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Perform the Fourier transform calculation here\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int k = 0; k < N; ++k) {\n            r[tid] += x[k].x * cos(2 * M_PI * tid * k / N) + x[k].y * sin(2 * M_PI * tid * k / N);\n            i[tid] += x[k].y * cos(2 * M_PI * tid * k / N) - x[k].x * sin(2 * M_PI * tid * k / N);\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (tid < N) {\n        double angle = -2 * M_PI * tid / N;\n        r[tid] = 0;\n        i[tid] = 0;\n\n        for (size_t k = 0; k < N; k++) {\n            double cos_val = cos(angle * k);\n            double sin_val = sin(angle * k);\n            r[tid] += x[k].x * cos_val - x[k].y * sin_val;\n            i[tid] += x[k].x * sin_val + x[k].y * cos_val;\n        }\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Perform the FFT computation here\n        // Use tid to access the elements of x, r, and i arrays\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Your FFT implementation here\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Perform FFT computation here\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sumR = 0.0;\n        double sumI = 0.0;\n        for (size_t k = 0; k < N; ++k) {\n            double angle = -2 * M_PI * tid * k / N;\n            sumR += x[k].x * cos(angle) - x[k].y * sin(angle);\n            sumI += x[k].x * sin(angle) + x[k].y * cos(angle);\n        }\n        r[tid] = sumR;\n        i[tid] = sumI;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * M_PI * tid / N;\n        for (int k = 0; k < N; k++) {\n            r[tid] += x[k].x * cos(angle * k) - x[k].y * sin(angle * k);\n            i[tid] += x[k].x * sin(angle * k) + x[k].y * cos(angle * k);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int k = 0; k < N; k++) {\n            r[tid] += x[k].x * cos(2 * M_PI * tid * k / N) + x[k].y * sin(2 * M_PI * tid * k / N);\n            i[tid] += x[k].y * cos(2 * M_PI * tid * k / N) - x[k].x * sin(2 * M_PI * tid * k / N);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * M_PI * tid / N;\n        for (int k = 0; k < N; k++) {\n            r[tid] += x[k].x * cos(angle * k) + x[k].y * sin(angle * k);\n            i[tid] += x[k].y * cos(angle * k) - x[k].x * sin(angle * k);\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "08_fft_split_fft",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation here\n\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int j = 0; j < n; j++) {\n            r[j] = x[j].real();\n            i[j] = x[j].imag();\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local_x\n\n    // Gather results on rank 0\n    MPI_Gather(local_r.data(), local_n, MPI_DOUBLE, r.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(local_i.data(), local_n, MPI_DOUBLE, i.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local_x\n\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int j = 0; j < n; j++) {\n            r[j] = x[j].real();\n            i[j] = x[j].imag();\n        }\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local_x\n\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int j = 0; j < n; j++) {\n            r[j] = x[j].real();\n            i[j] = x[j].imag();\n        }\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local_x\n\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, r.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local_x\n\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int j = 0; j < n; j++) {\n            r[j] = x[j].real();\n            i[j] = x[j].imag();\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local_x\n\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, r.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local data\n\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, r.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    // Combine results on rank 0\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local data\n\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, r.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        // Combine results from all ranks\n        // Perform final processing to get real and imaginary parts\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local_x\n\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int j = 0; j < n; j++) {\n            r[j] = x[j].real();\n            i[j] = x[j].imag();\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local_x\n\n    MPI_Gather(local_r.data(), local_n, MPI_DOUBLE, r.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(local_i.data(), local_n, MPI_DOUBLE, i.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local_x\n\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, r.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation here\n\n    MPI_Gather(local_r.data(), local_n, MPI_DOUBLE, r.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(local_i.data(), local_n, MPI_DOUBLE, i.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation here\n\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int j = 0; j < n; j++) {\n            r[j] = x[j].real();\n            i[j] = x[j].imag();\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local_x\n\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int j = 0; j < n; j++) {\n            r[j] = x[j].real();\n            i[j] = x[j].imag();\n        }\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    // Scatter the input data to all processes\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local data\n    // ...\n\n    // Gather the results on rank 0\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, r.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local_x\n\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, r.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int size = x.size();\n    int rank, numprocs;\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<std::complex<double>> local_x(size);\n    MPI_Scatter(&x[0], size, MPI_DOUBLE_COMPLEX, &local_x[0], size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform the FFT computation here\n\n    MPI_Gather(&r[0], size, MPI_DOUBLE, &r[0], size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(&i[0], size, MPI_DOUBLE, &i[0], size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local_x\n\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, r.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local_x\n\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, r.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "08_fft_split_fft",
    "parallelism_model": "cuda",
    "prompt": "/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\n__global__ void fft(const cuDoubleComplex *x, double *r, double *i, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * M_PI * tid / N;\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double cos_val = cos(angle * k);\n            double sin_val = sin(angle * k);\n            cuDoubleComplex term = make_cuDoubleComplex(cos_val, sin_val);\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        r[tid] = cuCreal(sum);\n        i[tid] = cuCimag(sum);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * PI * tid / N;\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double cos_val = cos(angle * k);\n            double sin_val = sin(angle * k);\n            cuDoubleComplex term = make_cuDoubleComplex(cos_val, sin_val);\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        r[tid] = cuCreal(sum);\n        i[tid] = cuCimag(sum);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * 3.14159265358979323846 * tid / N;\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double cosval = cos(angle * k);\n            double sinval = sin(angle * k);\n            cuDoubleComplex term = make_cuDoubleComplex(cosval, sinval);\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        r[tid] = cuCreal(sum);\n        i[tid] = cuCimag(sum);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * 3.14159265359 * tid / N;\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double cosval = cos(angle * k);\n            double sinval = sin(angle * k);\n            cuDoubleComplex term = make_cuDoubleComplex(cosval, sinval);\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        r[tid] = cuCreal(sum);\n        i[tid] = cuCimag(sum);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * M_PI * tid / N;\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double cos_val = cos(angle * k);\n            double sin_val = sin(angle * k);\n            cuDoubleComplex term = make_cuDoubleComplex(cos_val, sin_val);\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        r[tid] = cuCreal(sum);\n        i[tid] = cuCimag(sum);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * PI * tid / N;\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double cos_val = cos(angle * k);\n            double sin_val = sin(angle * k);\n            cuDoubleComplex term = make_cuDoubleComplex(cos_val, sin_val);\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        r[tid] = cuCreal(sum);\n        i[tid] = cuCimag(sum);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * M_PI * tid / N;\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double cos_val = cos(angle * k);\n            double sin_val = sin(angle * k);\n            cuDoubleComplex term = make_cuDoubleComplex(cos_val, sin_val);\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        r[tid] = cuCreal(sum);\n        i[tid] = cuCimag(sum);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * PI * tid / N;\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double cos_val = cos(angle * k);\n            double sin_val = sin(angle * k);\n            cuDoubleComplex term = make_cuDoubleComplex(cos_val, sin_val);\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        r[tid] = cuCreal(sum);\n        i[tid] = cuCimag(sum);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * M_PI * tid / N;\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double cos_val = cos(angle * k);\n            double sin_val = sin(angle * k);\n            cuDoubleComplex term = make_cuDoubleComplex(cos_val, sin_val);\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        r[tid] = cuCreal(sum);\n        i[tid] = cuCimag(sum);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * PI * tid / N;\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double cos_val = cos(angle * k);\n            double sin_val = sin(angle * k);\n            cuDoubleComplex term = make_cuDoubleComplex(cos_val, sin_val);\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        r[tid] = cuCreal(sum);\n        i[tid] = cuCimag(sum);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * PI * tid / N;\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double cosVal = cos(angle * k);\n            double sinVal = sin(angle * k);\n            cuDoubleComplex term = make_cuDoubleComplex(cosVal, sinVal);\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        r[tid] = cuCreal(sum);\n        i[tid] = cuCimag(sum);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * PI * tid / N;\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double cos_val = cos(angle * k);\n            double sin_val = sin(angle * k);\n            cuDoubleComplex term = make_cuDoubleComplex(cos_val, sin_val);\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        r[tid] = cuCreal(sum);\n        i[tid] = cuCimag(sum);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * PI * tid / N;\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double cos_val = cos(angle * k);\n            double sin_val = sin(angle * k);\n            cuDoubleComplex term = make_cuDoubleComplex(cos_val, sin_val);\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        r[tid] = cuCreal(sum);\n        i[tid] = cuCimag(sum);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * M_PI * tid / N;\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double cosVal = cos(angle * k);\n            double sinVal = sin(angle * k);\n            cuDoubleComplex term = make_cuDoubleComplex(cosVal, sinVal);\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        r[tid] = cuCreal(sum);\n        i[tid] = cuCimag(sum);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * M_PI * tid / N;\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double cos_val = cos(angle * k);\n            double sin_val = sin(angle * k);\n            cuDoubleComplex term = make_cuDoubleComplex(cos_val, sin_val);\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        r[tid] = cuCreal(sum);\n        i[tid] = cuCimag(sum);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * 3.14159 * tid / N;\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double cosval = cos(angle * k);\n            double sinval = sin(angle * k);\n            cuDoubleComplex term = make_cuDoubleComplex(cosval, sinval);\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        r[tid] = cuCreal(sum);\n        i[tid] = cuCimag(sum);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * PI * tid / N;\n        cuDoubleComplex sum = make_cuDoubleComplex(0, 0);\n        for (int k = 0; k < N; k++) {\n            double cos_val = cos(angle * k);\n            double sin_val = sin(angle * k);\n            cuDoubleComplex term = make_cuDoubleComplex(x[k].x * cos_val - x[k].y * sin_val, x[k].x * sin_val + x[k].y * cos_val);\n            sum = cuCadd(sum, term);\n        }\n        r[tid] = sum.x;\n        i[tid] = sum.y;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * M_PI * tid / N;\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double cosval = cos(angle * k);\n            double sinval = sin(angle * k);\n            cuDoubleComplex term = make_cuDoubleComplex(cosval, sinval);\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        r[tid] = cuCreal(sum);\n        i[tid] = cuCimag(sum);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * PI * tid / N;\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double cosval = cos(k * angle);\n            double sinval = sin(k * angle);\n            cuDoubleComplex term = make_cuDoubleComplex(cosval, sinval);\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        r[tid] = cuCreal(sum);\n        i[tid] = cuCimag(sum);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double angle = -2 * M_PI * tid / N;\n        cuDoubleComplex sum = make_cuDoubleComplex(0, 0);\n        for (int k = 0; k < N; k++) {\n            double cosval = cos(angle * k);\n            double sinval = sin(angle * k);\n            cuDoubleComplex term = make_cuDoubleComplex(cosval, sinval);\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        r[tid] = cuCreal(sum);\n        i[tid] = cuCimag(sum);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "08_fft_split_fft",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    // Scatter the input data to all processes\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local data using OpenMP\n    #pragma omp parallel for\n    for (int j = 0; j < local_n; j++) {\n        // Perform FFT computation on local_x[j]\n        // Store real and imaginary parts in local_result[j]\n    }\n\n    // Gather the local results to rank 0\n    MPI_Gather(&local_result[0], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        // Extract real and imaginary parts from x and store in r and i\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    // Scatter x to all ranks\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation in parallel using OpenMP\n    #pragma omp parallel for\n    for (int k = 0; k < local_n; ++k) {\n        std::complex<double> sum = 0.0;\n        for (int j = 0; j < n; ++j) {\n            sum += local_x[j] * std::exp(-2 * M_PI * std::complex<double>(0, 1) * j * k / n);\n        }\n        local_result[k] = sum;\n    }\n\n    // Gather local results to rank 0\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, r.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        // Compute imaginary part of the result\n        for (int j = 0; j < n; ++j) {\n            i[j] = r[j].imag();\n            r[j] = r[j].real();\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    // Scatter the input data to all processes\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation in parallel using OpenMP\n    #pragma omp parallel for\n    for (int k = 0; k < local_n; ++k) {\n        std::complex<double> sum = 0;\n        for (int j = 0; j < n; ++j) {\n            sum += local_x[j] * std::polar(1.0, -2 * M_PI * rank * j / n * k);\n        }\n        local_result[k] = sum;\n    }\n\n    // Gather the local results to rank 0\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Extract real and imaginary parts from the final result\n    for (int j = 0; j < n; ++j) {\n        r[j] = x[j].real();\n        i[j] = x[j].imag();\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    // Scatter x to all processes\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local data using OpenMP\n    #pragma omp parallel for\n    for (int k = 0; k < local_n; ++k) {\n        for (int j = 0; j < n; ++j) {\n            local_result[k] += local_x[j] * std::polar(1.0, -2 * M_PI * rank * j / n * k);\n        }\n    }\n\n    // Gather local results to rank 0\n    MPI_Gather(&local_result[0], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Compute real and imaginary parts\n    if (rank == 0) {\n        r.resize(n);\n        i.resize(n);\n        for (int j = 0; j < n; ++j) {\n            r[j] = x[j].real();\n            i[j] = x[j].imag();\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    // Scatter x to all ranks\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation in parallel using OpenMP\n    #pragma omp parallel for\n    for (int j = 0; j < local_n; j++) {\n        // Compute FFT for local_x[j] and store the result in local_result[j]\n        // ...\n    }\n\n    // Gather local_result from all ranks to rank 0\n    MPI_Gather(&local_result[0], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        // Extract real and imaginary parts from x and store in r and i\n        for (int j = 0; j < n; j++) {\n            r[j] = x[j].real();\n            i[j] = x[j].imag();\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    // Scatter the input data to all ranks\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform the FFT computation in parallel using OpenMP\n    #pragma omp parallel for\n    for (int j = 0; j < local_n; j++) {\n        // Perform FFT computation for local_x[j] and store the result in local_result[j]\n        // ...\n    }\n\n    // Gather the local results to rank 0\n    MPI_Gather(&local_result[0], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        // Extract the real and imaginary parts from the final result x and store them in r and i\n        for (int j = 0; j < n; j++) {\n            r[j] = x[j].real();\n            i[j] = x[j].imag();\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    // Scatter the input data to all processes\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation in parallel using OpenMP\n    #pragma omp parallel for\n    for (int j = 0; j < local_n; j++) {\n        local_result[j] = 0;\n        for (int k = 0; k < n; k++) {\n            local_result[j] += local_x[k] * std::exp(-2 * M_PI * std::complex<double>(0, 1) * (double)j * (double)k / (double)n);\n        }\n    }\n\n    // Gather the local results to rank 0\n    MPI_Gather(&local_result[0], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Extract real and imaginary parts from the gathered results\n    if (rank == 0) {\n        r.resize(n);\n        i.resize(n);\n        for (int j = 0; j < n; j++) {\n            r[j] = x[j].real();\n            i[j] = x[j].imag();\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int k = 0; k < local_n; ++k) {\n        std::complex<double> sum = 0;\n        for (int j = 0; j < n; ++j) {\n            sum += x[j] * exp(-2 * M_PI * std::complex<double>(0, 1) * j * k / n);\n        }\n        local_result[k] = sum;\n    }\n\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int k = 0; k < n; ++k) {\n            r[k] = x[k].real();\n            i[k] = x[k].imag();\n        }\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    // Scatter the input data to all processes\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation in parallel using OpenMP\n    #pragma omp parallel for\n    for (int j = 0; j < local_n; j++) {\n        // Perform FFT computation for local_x[j]\n        // Store the real and imaginary parts of the result in local_result[j]\n    }\n\n    // Gather the local results to rank 0\n    MPI_Gather(&local_result[0], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Combine the results from all processes on rank 0\n    if (rank == 0) {\n        // Combine the results from all processes and store the real and imaginary parts in r and i\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    // Scatter the input data to all processes\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform the FFT computation in parallel using OpenMP\n    #pragma omp parallel for\n    for (int j = 0; j < local_n; j++) {\n        // Perform FFT computation for local_x[j]\n        // Store the real and imaginary parts in local_result[j]\n    }\n\n    // Gather the local results to rank 0\n    MPI_Gather(&local_result[0], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        // Extract the real and imaginary parts from x and store in r and i\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    // Scatter the input data to all processes\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation in parallel using OpenMP\n    #pragma omp parallel for\n    for (int k = 0; k < local_n; ++k) {\n        std::complex<double> sum = 0;\n        for (int j = 0; j < n; ++j) {\n            sum += local_x[j] * std::exp(-2 * M_PI * std::complex<double>(0, 1) * j * k / n);\n        }\n        local_result[k] = sum;\n    }\n\n    // Gather the local results to rank 0\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Extract real and imaginary parts from the gathered results\n    if (rank == 0) {\n        for (int j = 0; j < n; ++j) {\n            r[j] = x[j].real();\n            i[j] = x[j].imag();\n        }\n    }\n}\n",
      "\n    int size = x.size();\n    int rank, numprocs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\n    #pragma omp parallel for\n    for (int k = 0; k < size; ++k) {\n        double sumreal = 0;\n        double sumimag = 0;\n        for (int n = 0; n < size; ++n) {\n            double angle = 2 * M_PI * rank * n / size;\n            sumreal += x[n].real() * cos(angle) + x[n].imag() * sin(angle);\n            sumimag += -x[n].real() * sin(angle) + x[n].imag() * cos(angle);\n        }\n        r[k] = sumreal;\n        i[k] = sumimag;\n    }\n\n    if (rank == 0) {\n        for (int p = 1; p < numprocs; ++p) {\n            std::vector<double> recv_r(size);\n            std::vector<double> recv_i(size);\n            MPI_Recv(&recv_r[0], size, MPI_DOUBLE, p, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Recv(&recv_i[0], size, MPI_DOUBLE, p, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int k = 0; k < size; ++k) {\n                r[k] += recv_r[k];\n                i[k] += recv_i[k];\n            }\n        }\n    } else {\n        MPI_Send(&r[0], size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(&i[0], size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    // Scatter the input data to all processes\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation in parallel using OpenMP\n    #pragma omp parallel for\n    for (int j = 0; j < local_n; j++) {\n        std::complex<double> sum = 0;\n        for (int k = 0; k < n; k++) {\n            sum += local_x[k] * exp(-2 * M_PI * std::complex<double>(0, 1) * j * k / n);\n        }\n        local_result[j] = sum;\n    }\n\n    // Gather the local results to rank 0\n    MPI_Gather(&local_result[0], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Extract real and imaginary parts from the result\n    for (int j = 0; j < n; j++) {\n        r[j] = x[j].real();\n        i[j] = x[j].imag();\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int k = 0; k < local_n; ++k) {\n        std::complex<double> sum = 0.0;\n        for (int j = 0; j < n; ++j) {\n            sum += x[j] * exp(-2 * M_PI * std::complex<double>(0, 1) * j * k / n);\n        }\n        local_result[k] = sum;\n    }\n\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int k = 0; k < n; ++k) {\n            r[k] = x[k].real();\n            i[k] = x[k].imag();\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    // Scatter the input data to all ranks\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation in parallel using OpenMP\n    #pragma omp parallel for\n    for (int k = 0; k < local_n; ++k) {\n        std::complex<double> sum = 0;\n        for (int j = 0; j < n; ++j) {\n            sum += local_x[j] * std::polar(1.0, -2 * M_PI * rank * j / n);\n        }\n        local_result[k] = sum;\n    }\n\n    // Gather the local results to rank 0\n    MPI_Gather(&local_result[0], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Extract real and imaginary parts from the gathered results\n    if (rank == 0) {\n        for (int j = 0; j < n; ++j) {\n            r[j] = x[j].real();\n            i[j] = x[j].imag();\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    // Scatter the input data to all ranks\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation in parallel using OpenMP\n    #pragma omp parallel for\n    for (int k = 0; k < local_n; ++k) {\n        std::complex<double> sum = 0;\n        for (int j = 0; j < n; ++j) {\n            sum += local_x[j] * std::polar(1.0, -2 * M_PI * rank * j / n);\n        }\n        local_result[k] = sum;\n    }\n\n    // Gather the local results to rank 0\n    MPI_Gather(&local_result[0], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Extract real and imaginary parts from the gathered results\n    if (rank == 0) {\n        r.resize(n);\n        i.resize(n);\n        for (int j = 0; j < n; ++j) {\n            r[j] = x[j].real();\n            i[j] = x[j].imag();\n        }\n    }\n}\n",
      "\n    // Implement the body of the function fft here\n    // Your code goes here\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    // Scatter the input data to all processes\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation in parallel using OpenMP\n    #pragma omp parallel for\n    for (int k = 0; k < local_n; ++k) {\n        std::complex<double> sum = 0.0;\n        for (int j = 0; j < n; ++j) {\n            sum += local_x[j] * exp(-2 * M_PI * std::complex<double>(0, 1) * j * k / n);\n        }\n        local_result[k] = sum;\n    }\n\n    // Gather the local results to rank 0\n    MPI_Gather(&local_result[0], local_n, MPI_DOUBLE_COMPLEX, &local_result[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Store the real and imaginary parts of the result on rank 0\n    if (rank == 0) {\n        r.resize(n);\n        i.resize(n);\n        for (int j = 0; j < n; ++j) {\n            r[j] = local_result[j].real();\n            i[j] = local_result[j].imag();\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    // Scatter the input data to all processes\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation in parallel using OpenMP\n    #pragma omp parallel for\n    for (int j = 0; j < local_n; j++) {\n        local_result[j] = 0;\n        for (int k = 0; k < n; k++) {\n            local_result[j] += local_x[k] * exp(-2 * M_PI * I * j * k / n);\n        }\n    }\n\n    // Gather the local results to rank 0\n    MPI_Gather(&local_result[0], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Extract real and imaginary parts from the final result\n    if (rank == 0) {\n        r.resize(n);\n        i.resize(n);\n        for (int j = 0; j < n; j++) {\n            r[j] = x[j].real();\n            i[j] = x[j].imag();\n        }\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int k = 0; k < local_n; ++k) {\n        std::complex<double> sum = 0;\n        for (int j = 0; j < n; ++j) {\n            sum += x[j] * exp(-2 * M_PI * std::complex<double>(0, 1) * j * k / n);\n        }\n        local_result[k] = sum;\n    }\n\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, r.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int k = 0; k < n; ++k) {\n            i[k] = std::imag(r[k]);\n            r[k] = std::real(r[k]);\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "07_fft_fft_conjugate",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(Kokkos::View<Kokkos::complex<double>*> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::conj(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::conj(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n      x(i) = Kokkos::conj(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::conj(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::conj(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::conj(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::conj(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::conj(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::conj(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::conj(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n      x(i) = Kokkos::conj(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::conj(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::conj(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::conj(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::conj(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::conj(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::conj(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n      x(i) = Kokkos::conj(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    x(i) = Kokkos::conj(x(i));\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n      x(i) = Kokkos::conj(x(i));\n  });\n  Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "07_fft_fft_conjugate",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(std::vector<std::complex<double>> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "07_fft_fft_conjugate",
    "parallelism_model": "serial",
    "prompt": "/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(std::vector<std::complex<double>> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int n = x.size();\n    for (int i = 0; i < n; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}",
      "\n    int n = x.size();\n    for (int i = 0; i < n; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}",
      "\n    int n = x.size();\n    for (int i = 0; i < n; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}",
      "\n    int n = x.size();\n    for (int i = 0; i < n; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}",
      "\n    int n = x.size();\n    for (int i = 0; i < n; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}",
      "\n    int n = x.size();\n    for (int i = 0; i < n; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}",
      "\n    int N = x.size();\n    for (int i = 0; i < N; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}",
      "\n    int n = x.size();\n    for (int i = 0; i < n; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}",
      "\n    int N = x.size();\n    for (int i = 0; i < N; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}",
      "\n    int n = x.size();\n    for (int i = 0; i < n; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}",
      "\n    int n = x.size();\n    for (int i = 0; i < n; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}",
      "\n    int n = x.size();\n    for (int i = 0; i < n; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}",
      "\n    int n = x.size();\n    for (int i = 0; i < n; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}",
      "\n    int n = x.size();\n    for (int i = 0; i < n; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}",
      "\n    int N = x.size();\n    for (int i = 0; i < N; i++) {\n        x[i] = std::conj(x[i]);\n    }\n}",
      "\n    int n = x.size();\n    for (int i = 0; i < n; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}",
      "\n    int N = x.size();\n    for (int i = 0; i < N; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}",
      "\n    int n = x.size();\n    for (int i = 0; i < n; i++) {\n        x[i] = std::conj(x[i]);\n    }\n}",
      "\n    int n = x.size();\n    for (int i = 0; i < n; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}",
      "\n    int n = x.size();\n    for (int i = 0; i < n; ++i) {\n        x[i] = std::conj(x[i]);\n    }\n}"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "07_fft_fft_conjugate",
    "parallelism_model": "hip",
    "prompt": "/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\n__global__ void fftConjugate(hipDoubleComplex *x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "07_fft_fft_conjugate",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(std::vector<std::complex<double>> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT on local_x\n\n    // Compute the imaginary conjugate of each value\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = std::conj(local_x[i]);\n    }\n\n    MPI_Gather(&local_x[0], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT on local_x\n\n    // Compute the imaginary conjugate of each value\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = std::conj(local_x[i]);\n    }\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform the Fourier transform on local_x\n\n    // Compute the imaginary conjugate of each value\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = std::conj(local_x[i]);\n    }\n\n    MPI_Gather(&local_x[0], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local_x\n\n    // Compute the imaginary conjugate of each value\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = std::conj(local_x[i]);\n    }\n\n    MPI_Gather(&local_x[0], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT on local_x\n\n    // Compute the imaginary conjugate of each value\n    for (int i = 0; i < local_n; i++) {\n        local_result[i] = std::conj(local_x[i]);\n    }\n\n    MPI_Gather(local_result.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT on local_x\n\n    // Compute the imaginary conjugate of each value\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = std::conj(local_x[i]);\n    }\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT on local_x\n\n    // Compute the imaginary conjugate of each value\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = std::conj(local_x[i]);\n    }\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform the FFT on local_x\n\n    // Compute the imaginary conjugate of each value\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = std::conj(local_x[i]);\n    }\n\n    MPI_Gather(&local_x[0], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<std::complex<double>> localX(n);\n    MPI_Scatter(x.data(), n / size, MPI_DOUBLE_COMPLEX, localX.data(), n / size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform the Fourier transform on localX\n\n    // Compute the imaginary conjugate of each value\n    for (int i = 0; i < n / size; i++) {\n        localX[i] = std::conj(localX[i]);\n    }\n\n    MPI_Gather(localX.data(), n / size, MPI_DOUBLE_COMPLEX, x.data(), n / size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT on local_x\n\n    // Compute the imaginary conjugate of each value\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = std::conj(local_x[i]);\n    }\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform the Fourier transform on local_x\n\n    // Compute the imaginary conjugate of each value\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = std::conj(local_x[i]);\n    }\n\n    MPI_Gather(&local_x[0], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    // Your code here\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_result(local_n);\n\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT on local_x\n\n    // Compute the imaginary conjugate of each value\n    for (int i = 0; i < local_n; i++) {\n        local_result[i] = std::conj(local_x[i]);\n    }\n\n    MPI_Gather(&local_result[0], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<std::complex<double>> local_x(n);\n    MPI_Scatter(x.data(), n / size, MPI_DOUBLE_COMPLEX, local_x.data(), n / size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT on local_x\n\n    // Compute the imaginary conjugate of each value\n    for (int i = 0; i < local_x.size(); i++) {\n        local_x[i] = std::conj(local_x[i]);\n    }\n\n    MPI_Gather(local_x.data(), n / size, MPI_DOUBLE_COMPLEX, x.data(), n / size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, size;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &size);\n\n    int n = x.size();\n    std::vector<std::complex<double>> local_x(n);\n    MPI_Scatter(x.data(), n / size, MPI_DOUBLE_COMPLEX, local_x.data(), n / size, MPI_DOUBLE_COMPLEX, 0, comm);\n\n    // Perform the Fourier transform on local_x\n\n    // Compute the imaginary conjugate of each value\n    for (int i = 0; i < local_x.size(); i++) {\n        local_x[i] = std::conj(local_x[i]);\n    }\n\n    MPI_Gather(local_x.data(), n / size, MPI_DOUBLE_COMPLEX, x.data(), n / size, MPI_DOUBLE_COMPLEX, 0, comm);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT on local_x\n\n    // Compute the imaginary conjugate of each value\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = std::conj(local_x[i]);\n    }\n\n    MPI_Gather(&local_x[0], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform the Fourier transform on local_x\n\n    // Compute the imaginary conjugate of each value\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = std::conj(local_x[i]);\n    }\n\n    MPI_Gather(&local_x[0], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT on local_x\n\n    // Compute the imaginary conjugate of each value\n    for (int i = 0; i < local_n; ++i) {\n        local_x[i] = std::conj(local_x[i]);\n    }\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform the FFT computation on local_x\n\n    MPI_Gather(&local_x[0], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < n; i++) {\n            x[i] = std::conj(x[i]);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform the FFT on local_x\n\n    // Compute the imaginary conjugate of each value\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = std::conj(local_x[i]);\n    }\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "07_fft_fft_conjugate",
    "parallelism_model": "cuda",
    "prompt": "/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\n__global__ void fftConjugate(cuDoubleComplex *x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx].y = -x[idx].y;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "07_fft_fft_conjugate",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(std::vector<std::complex<double>> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; k++) {\n        x[k] = std::conj(x[k]);\n    }\n\n    MPI_Gather(&x[0] + rank * local_n, local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int k = 0; k < n; k++) {\n            x[k] = std::conj(x[k]);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); k++) {\n        x[k] = std::conj(x[k]);\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, result.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int k = 0; k < local_n; k++) {\n        local_x[k] = std::conj(local_x[k]);\n    }\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; k++) {\n        x[k] = std::conj(x[k]);\n    }\n\n    MPI_Gather(&x[rank * local_n], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int k = 0; k < n; k++) {\n            x[k] = std::conj(x[k]);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    #pragma omp parallel for\n    for (int k = 0; k < local_n; k++) {\n        x[k] = std::conj(x[k]);\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(n);\n        MPI_Gather(x.data(), local_n, MPI_DOUBLE_COMPLEX, recvbuf.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), local_n, MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        x[k] = std::conj(x[k]);\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, result.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<std::complex<double>> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE_COMPLEX, local_x.data(), local_size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_size; i++) {\n        local_x[i] = std::conj(local_x[i]);\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE_COMPLEX, x.data(), local_size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    MPI_Gather(&x[rank * local_n], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < n; i++) {\n            x[i] = std::conj(x[i]);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    #pragma omp parallel for\n    for (int i = rank * local_n; i < (rank + 1) * local_n; i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(n);\n        MPI_Gather(x.data(), local_n, MPI_DOUBLE_COMPLEX, recvbuf.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), local_n, MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<std::complex<double>> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE_COMPLEX, local_x.data(), local_size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_size; i++) {\n        local_x[i] = std::conj(local_x[i]);\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE_COMPLEX, x.data(), local_size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    MPI_Gather(&x[0] + rank * local_n, local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&x[0] + i * local_n, local_n, MPI_DOUBLE_COMPLEX, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    // Perform parallel FFT using OpenMP\n    #pragma omp parallel for\n    for (int i = rank * local_n; i < (rank + 1) * local_n; i++) {\n        // Perform FFT on x[i]\n        x[i] = std::conj(x[i]);\n    }\n\n    // Gather results on rank 0\n    std::vector<std::complex<double>> local_x = x;\n    if (rank == 0) {\n        std::vector<std::vector<std::complex<double>>> recv_buf(size);\n        MPI_Gather(&local_x[0], local_n, MPI_DOUBLE_COMPLEX, &recv_buf[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        for (int i = 1; i < size; i++) {\n            std::copy(recv_buf[i].begin(), recv_buf[i].end(), std::back_inserter(local_x));\n        }\n        x = local_x;\n    } else {\n        MPI_Gather(&local_x[0], local_n, MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = std::conj(local_x[i]);\n    }\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int k = 0; k < local_n; ++k) {\n        local_x[k] = std::conj(local_x[k]);\n    }\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    MPI_Gather(&x[rank * local_n], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < n; i++) {\n            x[i] = std::conj(x[i]);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        x[k] = std::conj(x[k]);\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, result.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    #pragma omp parallel for\n    for (int k = 0; k < local_n; k++) {\n        x[k] = std::conj(x[k]);\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(n);\n        MPI_Gather(x.data(), local_n, MPI_DOUBLE_COMPLEX, recvbuf.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), local_n, MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] = std::conj(local_x[i]);\n    }\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    #pragma omp parallel for\n    for (int k = 0; k < local_n; k++) {\n        x[k] = std::conj(x[k]);\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(n);\n        MPI_Gather(&x[0], local_n, MPI_DOUBLE_COMPLEX, &recvbuf[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = recvbuf;\n    } else {\n        MPI_Gather(&x[0], local_n, MPI_DOUBLE_COMPLEX, NULL, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    #pragma omp parallel for\n    for (int k = 0; k < local_n; k++) {\n        x[k] = std::conj(x[k]);\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(n);\n        MPI_Gather(x.data(), local_n, MPI_DOUBLE_COMPLEX, recvbuf.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), local_n, MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "05_fft_inverse_fft",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* forward declare fft. computes fourier transform in-place */\nvoid fft(Kokkos::View<Kokkos::complex<double>*> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(Kokkos::View<Kokkos::complex<double>*> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // Compute the forward Fourier transform of the input\n    fft(x);\n\n    // Compute the inverse Fourier transform by conjugating the input and then computing the forward transform\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) = conj(x(i));\n    });\n    fft(x);\n\n    // Scale the result by the size of the input\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) /= x.extent(0);\n    });\n}\n",
      "\n    // Compute the forward Fourier transform of the input\n    fft(x);\n\n    // Compute the inverse Fourier transform by conjugating the complex numbers and dividing by the size of the input\n    const int N = x.extent(0);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        x(i) = conj(x(i)) / N;\n    });\n    Kokkos::fence();\n}\n",
      "\n    // Compute the forward Fourier transform of the input\n    fft(x);\n\n    // Compute the inverse Fourier transform by conjugating the complex numbers and dividing by the size of the input\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        x(i) = std::conj(x(i)) / static_cast<double>(x.size());\n    });\n    Kokkos::fence();\n}",
      "\n    // Take the conjugate of the input\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) = Kokkos::conj(x(i));\n    });\n\n    // Perform the forward FFT\n    fft(x);\n\n    // Take the conjugate of the result and scale by 1/N\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) = Kokkos::conj(x(i)) / Kokkos::complex<double>(x.extent(0), 0);\n    });\n}\n",
      "\n    // Compute the forward Fourier transform of the input\n    fft(x);\n\n    // Compute the inverse Fourier transform by conjugating the complex numbers and dividing by the size of the input\n    const int N = x.extent(0);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        x(i) = conj(x(i)) / N;\n    });\n    Kokkos::fence();\n}\n",
      "\n    // Compute the forward Fourier transform\n    fft(x);\n\n    // Compute the inverse Fourier transform\n    for (int i = 0; i < x.extent(0); ++i) {\n        x(i) = Kokkos::conj(x(i)) / x.extent(0);\n    }\n}",
      "\n    // Compute the forward Fourier transform of x\n    fft(x);\n\n    // Compute the inverse Fourier transform of x by conjugating the complex numbers and dividing by the size of x\n    int N = x.extent(0);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n        x(i) = conj(x(i)) / N;\n    });\n    Kokkos::fence();\n}\n",
      "\n    // Compute the forward Fourier transform of x\n    fft(x);\n\n    // Compute the inverse Fourier transform by conjugating the complex numbers and then performing the forward Fourier transform\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) = conj(x(i));\n    });\n    fft(x);\n\n    // Scale the result by 1/N\n    const double scale = 1.0 / x.extent(0);\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) *= scale;\n    });\n}\n",
      "\n    // Compute the forward Fourier transform of the input\n    fft(x);\n\n    // Compute the inverse Fourier transform by conjugating the input and then computing the forward transform\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) = conj(x(i));\n    });\n    fft(x);\n\n    // Scale the result by the size of the input\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) /= x.extent(0);\n    });\n}\n",
      "\n    // Compute the forward Fourier transform of the input\n    fft(x);\n\n    // Compute the inverse Fourier transform by conjugating the input and then computing the forward transform\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) = conj(x(i));\n    });\n    fft(x);\n\n    // Normalize the result by dividing by the size of the input\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) /= x.extent(0);\n    });\n}\n",
      "\n    // Compute the forward Fourier transform\n    fft(x);\n\n    // Compute the inverse Fourier transform\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) = conj(x(i)) / x.extent(0);\n    });\n    Kokkos::fence();\n}",
      "\n    // Compute the forward FFT\n    fft(x);\n\n    // Compute the inverse FFT by conjugating the input and then computing the forward FFT\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) = conj(x(i));\n    });\n    fft(x);\n\n    // Scale the result by 1/N\n    const double N = static_cast<double>(x.extent(0));\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) /= N;\n    });\n}\n",
      "\n    // Take the conjugate of the input\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) = Kokkos::conj(x(i));\n    });\n\n    // Perform the forward FFT\n    fft(x);\n\n    // Take the conjugate of the result and divide by the size of the array\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) = Kokkos::conj(x(i)) / Kokkos::complex<double>(x.extent(0), 0);\n    });\n}\n",
      "\n    // Take the conjugate of the input\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) = Kokkos::conj(x(i));\n    });\n\n    // Perform the forward FFT\n    fft(x);\n\n    // Take the conjugate of the result and scale by 1/N\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) = Kokkos::conj(x(i)) / Kokkos::complex<double>(x.extent(0), 0);\n    });\n}\n",
      "\n    // Take the conjugate of the input\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) = Kokkos::conj(x(i));\n    });\n\n    // Compute the forward FFT\n    fft(x);\n\n    // Take the conjugate of the result and scale by 1/N\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) = Kokkos::conj(x(i)) / Kokkos::complex<double>(x.extent(0), 0);\n    });\n```\n}",
      "\n    // Take the conjugate of the input\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) = Kokkos::conj(x(i));\n    });\n\n    // Perform the forward FFT\n    fft(x);\n\n    // Take the conjugate of the result and scale by 1/N\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) = Kokkos::conj(x(i)) / Kokkos::complex<double>(x.extent(0), 0);\n    });\n}\n",
      "\n    // Compute the forward Fourier transform of x\n    fft(x);\n\n    // Compute the inverse Fourier transform of x by conjugating the complex numbers and dividing by the size of x\n    int N = x.extent(0);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        x(i) = conj(x(i)) / N;\n    });\n    Kokkos::fence();\n}\n",
      "\n    // Compute the forward Fourier transform of x\n    fft(x);\n\n    // Compute the inverse Fourier transform of x by conjugating the complex numbers and dividing by the size of x\n    int N = x.size();\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        x(i) = conj(x(i)) / N;\n    });\n    Kokkos::fence();\n}",
      "\n    // Compute the forward Fourier transform of x\n    fft(x);\n\n    // Compute the inverse Fourier transform by conjugating the complex numbers and dividing by the size of x\n    int N = x.extent(0);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(int i) {\n        x(i) = conj(x(i)) / N;\n    });\n    Kokkos::fence();\n}",
      "\n    // Compute the forward FFT\n    fft(x);\n\n    // Compute the inverse FFT by conjugating the input, performing the forward FFT, and then conjugating the result\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) = conj(x(i));\n    });\n    fft(x);\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        x(i) = conj(x(i)) / x.extent(0);\n    });\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "05_fft_inverse_fft",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // Take the conjugate of the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Perform forward FFT\n    fft(x);\n\n    // Take the conjugate of the result and scale by 1/N\n    double N = static_cast<double>(x.size());\n    for (auto &val : x) {\n        val = std::conj(val) / N;\n    }\n}",
      "\n    // Take the conjugate of the input\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    // Perform forward FFT\n    fft(x);\n\n    // Take the conjugate of the result and scale by 1/N\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]) / static_cast<double>(x.size());\n    }\n}\n",
      "\n    // Take the conjugate of the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Perform the forward FFT\n    fft(x);\n\n    // Take the conjugate of the result and scale by 1/N\n    double N = static_cast<double>(x.size());\n    for (auto &val : x) {\n        val = std::conj(val) / N;\n    }\n}",
      "\n    // Take the conjugate of the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Perform the forward FFT\n    fft(x);\n\n    // Take the conjugate of the result and scale by 1/N\n    double N = static_cast<double>(x.size());\n    for (auto &val : x) {\n        val = std::conj(val) / N;\n    }\n}",
      "\n    // Compute the forward FFT\n    fft(x);\n\n    // Compute the inverse FFT\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]) / x.size();\n    }\n}",
      "\n    // Take the conjugate of the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Perform forward FFT\n    fft(x);\n\n    // Take the conjugate of the result and scale by 1/N\n    double N = static_cast<double>(x.size());\n    for (auto &val : x) {\n        val = std::conj(val) / N;\n    }\n}",
      "\n    // Take the conjugate of the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Perform the forward FFT\n    fft(x);\n\n    // Take the conjugate of the result and divide by the size of the input\n    double size = static_cast<double>(x.size());\n    for (auto &val : x) {\n        val = std::conj(val) / size;\n    }\n}",
      "\n    // Take the conjugate of the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Perform the forward FFT\n    fft(x);\n\n    // Take the conjugate of the result and scale by 1/N\n    double N = static_cast<double>(x.size());\n    for (auto &val : x) {\n        val = std::conj(val) / N;\n    }\n}",
      "\n    // Compute the forward FFT\n    fft(x);\n\n    // Compute the inverse FFT\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]) / x.size();\n    }\n}",
      "\n    // Take the conjugate of the input vector\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Perform FFT\n    fft(x);\n\n    // Take the conjugate of the result and scale by 1/N\n    double N = static_cast<double>(x.size());\n    for (auto &val : x) {\n        val = std::conj(val) / N;\n    }\n}",
      "\n    // Compute the forward FFT\n    fft(x);\n\n    // Compute the inverse FFT\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]) / x.size();\n    }\n}",
      "\n    // Compute the forward FFT\n    fft(x);\n\n    // Compute the complex conjugate of each element\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    // Scale by the size of the array\n    double size = static_cast<double>(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= size;\n    }\n}",
      "\n    // Take the conjugate of the input vector\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Perform the forward FFT\n    fft(x);\n\n    // Take the conjugate of the result and divide by the size of the vector\n    double size = static_cast<double>(x.size());\n    for (auto &val : x) {\n        val = std::conj(val) / size;\n    }\n}",
      "\n    // Take the conjugate of the input vector\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Perform the forward FFT\n    fft(x);\n\n    // Take the conjugate of the result and divide by the size of the input\n    double size = static_cast<double>(x.size());\n    for (auto &val : x) {\n        val = std::conj(val) / size;\n    }\n}",
      "\n    // Take the conjugate of the input vector\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Perform FFT\n    fft(x);\n\n    // Take the conjugate of the result and divide by the size of the vector\n    double size = static_cast<double>(x.size());\n    for (auto &val : x) {\n        val = std::conj(val) / size;\n    }\n}",
      "\n    // Take the conjugate of the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Perform FFT\n    fft(x);\n\n    // Take the conjugate of the result and scale by 1/N\n    double N = static_cast<double>(x.size());\n    for (auto &val : x) {\n        val = std::conj(val) / N;\n    }\n}",
      "\n    // Take the conjugate of the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Perform FFT\n    fft(x);\n\n    // Take the conjugate of the result and divide by the size of the input\n    double size = static_cast<double>(x.size());\n    for (auto &val : x) {\n        val = std::conj(val) / size;\n    }\n}",
      "\n    // Take the conjugate of the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Perform the forward FFT\n    fft(x);\n\n    // Take the conjugate of the result and scale by 1/N\n    double N = static_cast<double>(x.size());\n    for (auto &val : x) {\n        val = std::conj(val) / N;\n    }\n}",
      "\n    // Take the conjugate of the input vector\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        x[i] = std::conj(x[i]);\n    }\n\n    // Perform forward FFT\n    fft(x);\n\n    // Take the conjugate of the result and scale by 1/N\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        x[i] = std::conj(x[i]) / static_cast<double>(x.size());\n    }\n}\n",
      "\n    // Compute the forward FFT\n    fft(x);\n\n    // Compute the complex conjugate of each element\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    // Scale by the size of the array\n    double scale = 1.0 / x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] *= scale;\n    }\n}"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "05_fft_inverse_fft",
    "parallelism_model": "serial",
    "prompt": "/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // conjugate the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // forward fft\n    fft(x);\n\n    // conjugate the result and scale\n    for (auto &val : x) {\n        val = std::conj(val) / static_cast<double>(x.size());\n    }\n}",
      "\n    // conjugate the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // forward FFT\n    fft(x);\n\n    // conjugate the result and scale\n    for (auto &val : x) {\n        val = std::conj(val) / static_cast<double>(x.size());\n    }\n}",
      "\n    // conjugate the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // forward fft\n    fft(x);\n\n    // conjugate the result and scale\n    for (auto &val : x) {\n        val = std::conj(val) / static_cast<double>(x.size());\n    }\n}",
      "\n    // conjugate the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // forward FFT\n    fft(x);\n\n    // conjugate the result and scale\n    for (auto &val : x) {\n        val = std::conj(val) / static_cast<double>(x.size());\n    }\n}",
      "\n    // conjugate the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // forward FFT\n    fft(x);\n\n    // conjugate the result and scale\n    for (auto &val : x) {\n        val = std::conj(val) / static_cast<double>(x.size());\n    }\n}",
      "\n    // Compute the forward FFT\n    fft(x);\n    \n    // Compute the complex conjugate of each element\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n    \n    // Scale by 1/N\n    double N = x.size();\n    for (auto &elem : x) {\n        elem /= N;\n    }\n}",
      "\n    // conjugate the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // forward fft\n    fft(x);\n\n    // conjugate the result and scale\n    for (auto &val : x) {\n        val = std::conj(val) / static_cast<double>(x.size());\n    }\n}",
      "\n    // conjugate the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // forward fft\n    fft(x);\n\n    // conjugate the result and scale\n    for (auto &val : x) {\n        val = std::conj(val) / static_cast<double>(x.size());\n    }\n}",
      "\n    // conjugate the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // forward FFT\n    fft(x);\n\n    // conjugate the result and scale\n    for (auto &val : x) {\n        val = std::conj(val) / static_cast<double>(x.size());\n    }\n}",
      "\n    // Take the conjugate of the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Perform forward FFT\n    fft(x);\n\n    // Take the conjugate of the result and scale by 1/N\n    double N = static_cast<double>(x.size());\n    for (auto &val : x) {\n        val = std::conj(val) / N;\n    }\n}",
      "\n    // Take the conjugate of the input vector\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Perform forward FFT\n    fft(x);\n\n    // Take the conjugate of the result and scale by 1/N\n    double N = static_cast<double>(x.size());\n    for (auto &val : x) {\n        val = std::conj(val) / N;\n    }\n}",
      "\n    // conjugate the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // forward fft\n    fft(x);\n\n    // conjugate the result and scale\n    for (auto &val : x) {\n        val = std::conj(val) / static_cast<double>(x.size());\n    }\n}",
      "\n    // conjugate the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // forward fft\n    fft(x);\n\n    // conjugate the result and scale\n    for (auto &val : x) {\n        val = std::conj(val) / static_cast<double>(x.size());\n    }\n}",
      "\n    // conjugate the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // forward fft\n    fft(x);\n\n    // conjugate the result and scale\n    for (auto &val : x) {\n        val = std::conj(val) / static_cast<double>(x.size());\n    }\n}",
      "\n    // conjugate the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // forward fft\n    fft(x);\n\n    // conjugate the result and scale\n    for (auto &val : x) {\n        val = std::conj(val) / static_cast<double>(x.size());\n    }\n}",
      "\n    // Take the conjugate of the input vector\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Perform forward FFT\n    fft(x);\n\n    // Take the conjugate of the result and divide by the size of the vector\n    double size = static_cast<double>(x.size());\n    for (auto &val : x) {\n        val = std::conj(val) / size;\n    }\n}",
      "\n    // conjugate the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // forward FFT\n    fft(x);\n\n    // conjugate the result and scale\n    for (auto &val : x) {\n        val = std::conj(val) / static_cast<double>(x.size());\n    }\n}",
      "\n    // conjugate the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // forward FFT\n    fft(x);\n\n    // conjugate the result and scale\n    for (auto &val : x) {\n        val = std::conj(val) / static_cast<double>(x.size());\n    }\n}",
      "\n    // Take the conjugate of the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Perform forward FFT\n    fft(x);\n\n    // Take the conjugate of the result and scale by 1/N\n    double N = static_cast<double>(x.size());\n    for (auto &val : x) {\n        val = std::conj(val) / N;\n    }\n}",
      "\n    // conjugate the input\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // forward FFT\n    fft(x);\n\n    // conjugate the output and scale\n    for (auto &val : x) {\n        val = std::conj(val) / static_cast<double>(x.size());\n    }\n}"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "05_fft_inverse_fft",
    "parallelism_model": "hip",
    "prompt": "/* Compute the inverse fourier transform of x in-place.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\n__global__ void ifft(hipDoubleComplex *x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        hipDoubleComplex sum = {0, 0};\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            hipDoubleComplex twiddle = {cos(angle), sin(angle)};\n            hipDoubleComplex product = hipCmplxMul(x[k], twiddle);\n            sum = hipCmplxAdd(sum, product);\n        }\n        x[tid] = hipCmplxDivReal(sum, N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        hipDoubleComplex sum = {0.0, 0.0};\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            hipDoubleComplex twiddle = {cos(angle), sin(angle)};\n            hipDoubleComplex product = hipCmplxMul(x[k], twiddle);\n            sum = hipCmplxAdd(sum, product);\n        }\n        x[tid] = hipCmplxDivide(sum, N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        hipDoubleComplex sum = {0, 0};\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            hipDoubleComplex twiddle = {cos(angle), sin(angle)};\n            hipDoubleComplex product = hipCmplxMul(x[k], twiddle);\n            sum = hipCmplxAdd(sum, product);\n        }\n        x[tid] = hipCmplxDivReal(sum, N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        hipDoubleComplex sum = {0.0, 0.0};\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            hipDoubleComplex twiddle = {cos(angle), sin(angle)};\n            hipDoubleComplex product = hipCmul(x[k], twiddle);\n            sum = hipCadd(sum, product);\n        }\n        x[tid] = hipCdiv(sum, N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        hipDoubleComplex sum = {0, 0};\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            hipDoubleComplex twiddle = {cos(angle), sin(angle)};\n            hipDoubleComplex product = hipCmplxMul(x[k], twiddle);\n            sum = hipCmplxAdd(sum, product);\n        }\n        x[tid] = hipCmplxDivReal(sum, N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        hipDoubleComplex sum = make_hipDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            hipDoubleComplex twiddle = make_hipDoubleComplex(cos(angle), sin(angle));\n            sum = hipCadd(sum, hipCmul(x[k], twiddle));\n        }\n        x[tid] = hipCdiv(sum, make_hipDoubleComplex(N, 0.0));\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        hipDoubleComplex sum = {0, 0};\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            hipDoubleComplex twiddle = {cos(angle), sin(angle)};\n            hipDoubleComplex product = hipCmplxMul(x[k], twiddle);\n            sum = hipCmplxAdd(sum, product);\n        }\n        x[tid] = hipCmplxDivReal(sum, N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        hipDoubleComplex sum = {0.0, 0.0};\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            hipDoubleComplex twiddle = {cos(angle), sin(angle)};\n            hipDoubleComplex product = hipCmul(x[k], twiddle);\n            sum = hipCadd(sum, product);\n        }\n        x[tid] = hipCdiv(sum, N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        hipDoubleComplex sum = {0, 0};\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            hipDoubleComplex twiddle = {cos(angle), sin(angle)};\n            hipDoubleComplex product = hipCmplxMul(x[k], twiddle);\n            sum = hipCmplxAdd(sum, product);\n        }\n        x[tid] = hipCmplxDivReal(sum, N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        hipDoubleComplex sum = make_hipDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            hipDoubleComplex twiddle = make_hipDoubleComplex(cos(angle), sin(angle));\n            sum = hipCadd(sum, hipCmul(x[k], twiddle));\n        }\n        x[tid] = hipCdiv(sum, make_hipDoubleComplex(N, 0.0));\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        hipDoubleComplex sum = {0, 0};\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            hipDoubleComplex twiddle = {cos(angle), sin(angle)};\n            hipDoubleComplex product = hipCmplxMul(x[k], twiddle);\n            sum = hipCmplxAdd(sum, product);\n        }\n        x[tid] = hipCmplxDivReal(sum, N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        hipDoubleComplex sum = {0, 0};\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            hipDoubleComplex twiddle = {cos(angle), sin(angle)};\n            hipDoubleComplex product = hipCmplxMul(x[k], twiddle);\n            sum = hipCmplxAdd(sum, product);\n        }\n        x[tid] = hipCmplxDivide(sum, N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        hipDoubleComplex sum = {0.0, 0.0};\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            hipDoubleComplex twiddle = {cos(angle), sin(angle)};\n            hipDoubleComplex product = hipCmplxMul(x[k], twiddle);\n            sum = hipCmplxAdd(sum, product);\n        }\n        x[tid] = hipCmplxDivd(sum, N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        hipDoubleComplex sum = {0, 0};\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            hipDoubleComplex twiddle = {cos(angle), sin(angle)};\n            hipDoubleComplex product = hipCmplxMul(x[k], twiddle);\n            sum = hipCmplxAdd(sum, product);\n        }\n        x[tid] = hipCmplxDivReal(sum, N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        hipDoubleComplex sum = {0, 0};\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            hipDoubleComplex twiddle = {cos(angle), sin(angle)};\n            sum = hipCadd(sum, hipCmul(x[k], twiddle));\n        }\n        x[tid] = hipCdiv(sum, N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        hipDoubleComplex sum = {0, 0};\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            hipDoubleComplex twiddle = {cos(angle), sin(angle)};\n            hipDoubleComplex product = hipCmul(x[k], twiddle);\n            sum = hipCadd(sum, product);\n        }\n        x[tid] = hipCdiv(sum, N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        hipDoubleComplex sum = {0.0, 0.0};\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            hipDoubleComplex twiddle = {cos(angle), sin(angle)};\n            hipDoubleComplex product = hipCmul(x[k], twiddle);\n            sum = hipCadd(sum, product);\n        }\n        x[tid] = hipCdiv(sum, N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        hipDoubleComplex sum = {0.0, 0.0};\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            hipDoubleComplex twiddle = {cos(angle), sin(angle)};\n            hipDoubleComplex product = hipCmul(x[k], twiddle);\n            sum = hipCadd(sum, product);\n        }\n        x[tid] = hipCdiv(sum, N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (tid < N) {\n        hipDoubleComplex sum = {0.0, 0.0};\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            hipDoubleComplex twiddle = {cos(angle), sin(angle)};\n            hipDoubleComplex product = hipCmplxMul(x[k], twiddle);\n            sum = hipCmplxAdd(sum, product);\n        }\n        x[tid] = hipCmplxDivReal(sum, N);\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        hipDoubleComplex sum = {0, 0};\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            hipDoubleComplex twiddle = {cos(angle), sin(angle)};\n            hipDoubleComplex product = hipCmplxMul(x[k], twiddle);\n            sum = hipCmplxAdd(sum, product);\n        }\n        x[tid] = hipCmplxDivReal(sum, N);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "05_fft_inverse_fft",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // Compute the forward FFT\n    fft(x);\n\n    // Conjugate the complex numbers\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Scale by the size of the array\n    int size = x.size();\n    for (auto &val : x) {\n        val /= size;\n    }\n\n    // Reverse the order of the array\n    std::reverse(x.begin() + 1, x.end());\n}",
      "\n    // Compute the forward FFT\n    fft(x);\n\n    // Conjugate the complex numbers\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Scale by the size of the array\n    double size = static_cast<double>(x.size());\n    for (auto &val : x) {\n        val /= size;\n    }\n\n    // Reverse the array\n    std::reverse(x.begin() + 1, x.end());\n}",
      "\n    // Compute the forward Fourier transform\n    fft(x);\n\n    // Compute the complex conjugate of each element\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Divide each element by the size of the array\n    int size = x.size();\n    for (auto &elem : x) {\n        elem /= size;\n    }\n\n    // Reverse the order of the elements\n    std::reverse(x.begin() + 1, x.end());\n}",
      "\n    // Perform the inverse Fourier transform using the forward Fourier transform and scaling\n    fft(x); // Compute the forward Fourier transform\n\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size); // Get the total number of processes\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank); // Get the rank of the current process\n\n    int n = x.size();\n    int local_n = n / size; // Calculate the local size of the input array for each process\n\n    std::vector<std::complex<double>> local_x(local_n); // Create a local array to store the input for each process\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD); // Scatter the input array to all processes\n\n    // Perform the inverse Fourier transform on the local input array\n    for (int k = 0; k < local_n; ++k) {\n        local_x[k] /= n; // Scale by 1/n for the inverse transform\n    }\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD); // Gather the results back to rank 0\n}\n",
      "\n    // Perform the inverse Fourier transform using the forward Fourier transform and scaling\n    fft(x); // Compute the forward Fourier transform of x\n\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    for (int k = 0; k < n; ++k) {\n        x[k] = std::conj(x[k]); // Take the complex conjugate of each element\n    }\n\n    if (rank == 0) {\n        for (int k = 0; k < n; ++k) {\n            x[k] /= size; // Scale the result by the number of processes\n        }\n    }\n\n    MPI_Bcast(&x[0], n * 2, MPI_DOUBLE, 0, MPI_COMM_WORLD); // Broadcast the scaled result to all processes\n    for (int k = 0; k < n; ++k) {\n        x[k] = std::conj(x[k]); // Take the complex conjugate again to obtain the inverse transform\n    }\n}\n",
      "\n    // Compute the forward Fourier transform\n    fft(x);\n\n    // Compute the complex conjugate of each element\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    // Scale by the size of the array\n    double scale = 1.0 / x.size();\n    for (int i = 0; i < x.size(); i++) {\n        x[i] *= scale;\n    }\n}",
      "\n    // Compute the forward FFT of x\n    fft(x);\n\n    // Take the complex conjugate of each element\n    for (auto &element : x) {\n        element = std::conj(element);\n    }\n\n    // Divide each element by the size of x\n    int size = x.size();\n    for (auto &element : x) {\n        element /= size;\n    }\n\n    // Reverse the order of the elements\n    std::reverse(x.begin() + 1, x.end());\n}",
      "\n    // Compute the forward Fourier transform\n    fft(x);\n\n    // Conjugate the complex numbers\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Divide by the size of the array\n    int size = x.size();\n    for (auto &val : x) {\n        val /= size;\n    }\n\n    // Reverse the array\n    std::reverse(x.begin() + 1, x.end());\n}",
      "\n    // Compute the forward Fourier transform\n    fft(x);\n\n    // Compute the complex conjugate of each element\n    for (auto &element : x) {\n        element = std::conj(element);\n    }\n\n    // Divide each element by the size of the array\n    int size = x.size();\n    for (auto &element : x) {\n        element /= size;\n    }\n\n    // Reverse the order of the elements\n    std::reverse(x.begin() + 1, x.end());\n}\n",
      "\n    // Compute the forward FFT\n    fft(x);\n\n    // Conjugate the complex numbers\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Divide by the size of the array\n    int size = x.size();\n    for (auto &val : x) {\n        val /= size;\n    }\n\n    // Reverse the array\n    std::reverse(x.begin() + 1, x.end());\n}",
      "\n    // Compute the forward FFT\n    fft(x);\n\n    // Conjugate the complex numbers\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Scale by 1/N\n    double scale = 1.0 / x.size();\n    for (auto &val : x) {\n        val *= scale;\n    }\n\n    // Reverse the order of the elements\n    std::reverse(x.begin() + 1, x.end());\n}",
      "\n    // Perform the inverse Fourier transform\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (rank == 0) {\n        // Perform the inverse Fourier transform on rank 0\n        fft(x);\n    }\n}",
      "\n    // Compute the forward FFT\n    fft(x);\n\n    // Conjugate the complex numbers\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Scale by the size of the array\n    double size = static_cast<double>(x.size());\n    for (auto &val : x) {\n        val /= size;\n    }\n\n    // Reverse the array\n    std::reverse(x.begin() + 1, x.end());\n}",
      "\n    // Compute the forward Fourier transform\n    fft(x);\n\n    // Compute the complex conjugate of each element\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Divide each element by the size of the array\n    int size = x.size();\n    for (auto &elem : x) {\n        elem /= size;\n    }\n\n    // Reverse the order of the elements\n    std::reverse(x.begin() + 1, x.end());\n}",
      "\n    // Compute the forward Fourier transform\n    fft(x);\n\n    // Compute the complex conjugate of each element\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    // Divide each element by the size of the array\n    double size = static_cast<double>(x.size());\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= size;\n    }\n\n    // Reverse the order of the elements\n    std::reverse(x.begin() + 1, x.end());\n}\n",
      "\n    // Perform the inverse Fourier transform using the forward Fourier transform and scaling\n    fft(x); // Compute the forward Fourier transform\n    std::transform(x.begin(), x.end(), x.begin(), [&](std::complex<double> &c) { return c / static_cast<double>(x.size()); }); // Scale by 1/N\n}",
      "\n    // Compute the forward FFT\n    fft(x);\n\n    // Compute the complex conjugate of each element\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    // Scale by 1/N\n    double N = x.size();\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= N;\n    }\n\n    // Reverse the order of the elements\n    std::reverse(x.begin() + 1, x.end());\n}",
      "\n    // Compute the forward FFT\n    fft(x);\n\n    // Compute the complex conjugate of each element\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    // Scale by the size of the array\n    double size = static_cast<double>(x.size());\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= size;\n    }\n\n    // Reverse the order of the elements\n    std::reverse(x.begin() + 1, x.end());\n}\n",
      "\n    // Compute the forward FFT\n    fft(x);\n\n    // Conjugate the complex numbers\n    for (auto &val : x) {\n        val = std::conj(val);\n    }\n\n    // Scale by the size of the array\n    int size = x.size();\n    for (auto &val : x) {\n        val /= size;\n    }\n\n    // Reverse the array\n    std::reverse(x.begin() + 1, x.end());\n}",
      "\n    // Compute the forward FFT\n    fft(x);\n\n    // Compute the complex conjugate of the elements\n    for (int i = 0; i < x.size(); ++i) {\n        x[i] = std::conj(x[i]);\n    }\n\n    // Broadcast the conjugated array to all ranks\n    MPI_Bcast(&x[0], x.size() * 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Compute the forward FFT again\n    fft(x);\n\n    // Scale the result by 1/N\n    double scale = 1.0 / x.size();\n    for (int i = 0; i < x.size(); ++i) {\n        x[i] *= scale;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "05_fft_inverse_fft",
    "parallelism_model": "cuda",
    "prompt": "/* Compute the inverse fourier transform of x in-place.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\n__global__ void ifft(cuDoubleComplex *x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            cuDoubleComplex twiddle = make_cuDoubleComplex(cos(angle), sin(angle));\n            cuDoubleComplex term = cuCmul(x[k], twiddle);\n            sum = cuCadd(sum, term);\n        }\n        x[tid] = cuCdiv(sum, make_cuDoubleComplex(N, 0.0));\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            cuDoubleComplex twiddle = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], twiddle));\n        }\n        x[tid] = cuCdiv(sum, make_cuDoubleComplex(N, 0.0));\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            cuDoubleComplex twiddle = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], twiddle));\n        }\n        x[tid] = cuCdiv(sum, make_cuDoubleComplex(N, 0.0));\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            cuDoubleComplex twiddle = make_cuDoubleComplex(cos(angle), sin(angle));\n            cuDoubleComplex product = cuCmul(x[k], twiddle);\n            sum = cuCadd(sum, product);\n        }\n        x[tid] = cuCdiv(sum, make_cuDoubleComplex(N, 0.0));\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            cuDoubleComplex twiddle = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], twiddle));\n        }\n        x[tid] = cuCdiv(sum, make_cuDoubleComplex(N, 0.0));\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            cuDoubleComplex twiddle = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], twiddle));\n        }\n        x[tid] = cuCdiv(sum, make_cuDoubleComplex(N, 0.0));\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            cuDoubleComplex twiddle = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], twiddle));\n        }\n        x[tid] = cuCdiv(sum, make_cuDoubleComplex(N, 0.0));\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            cuDoubleComplex twiddle = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], twiddle));\n        }\n        x[tid] = cuCdiv(sum, make_cuDoubleComplex(N, 0.0));\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            cuDoubleComplex twiddle = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], twiddle));\n        }\n        x[tid] = cuCdiv(sum, make_cuDoubleComplex(N, 0.0));\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            cuDoubleComplex twiddle = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], twiddle));\n        }\n        x[tid] = cuCdiv(sum, make_cuDoubleComplex(N, 0.0));\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            cuDoubleComplex twiddle = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], twiddle));\n        }\n        x[tid] = cuCdiv(sum, make_cuDoubleComplex(N, 0.0));\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            cuDoubleComplex twiddle = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], twiddle));\n        }\n        x[tid] = cuCdiv(sum, make_cuDoubleComplex(N, 0.0));\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            cuDoubleComplex twiddle = make_cuDoubleComplex(cos(angle), sin(angle));\n            cuDoubleComplex product = cuCmul(x[k], twiddle);\n            sum = cuCadd(sum, product);\n        }\n        x[tid] = cuCdiv(sum, make_cuDoubleComplex(N, 0.0));\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            cuDoubleComplex twiddle = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], twiddle));\n        }\n        x[tid] = cuCdiv(sum, make_cuDoubleComplex(N, 0.0));\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            cuDoubleComplex twiddle = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], twiddle));\n        }\n        x[tid] = cuCdiv(sum, make_cuDoubleComplex(N, 0.0));\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            cuDoubleComplex twiddle = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], twiddle));\n        }\n        x[tid] = cuCdiv(sum, make_cuDoubleComplex(N, 0.0));\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            cuDoubleComplex twiddle = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], twiddle));\n        }\n        x[tid] = cuCdiv(sum, make_cuDoubleComplex(N, 0.0));\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            cuDoubleComplex twiddle = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], twiddle));\n        }\n        x[tid] = cuCdiv(sum, make_cuDoubleComplex(N, 0.0));\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            cuDoubleComplex twiddle = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], twiddle));\n        }\n        x[tid] = cuCdiv(sum, make_cuDoubleComplex(N, 0.0));\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = 2 * M_PI * tid * k / N;\n            cuDoubleComplex twiddle = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], twiddle));\n        }\n        x[tid] = cuCdiv(sum, make_cuDoubleComplex(N, 0.0));\n    }\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "05_fft_inverse_fft",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // Perform the inverse Fourier transform using the forward Fourier transform and scaling\n    fft(x); // Compute the forward Fourier transform\n\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_size = x.size() / size;\n    std::vector<std::complex<double>> local_result(local_size);\n\n    // Compute local inverse Fourier transform using OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < local_size; i++) {\n        local_result[i] = x[i * size + rank];\n    }\n\n    // Gather local results to rank 0\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size());\n        MPI_Gather(local_result.data(), local_size, MPI_DOUBLE_COMPLEX, result.data(), local_size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(local_result.data(), local_size, MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n\n    // Scale the result\n    if (rank == 0) {\n        double scale = 1.0 / x.size();\n        for (int i = 0; i < x.size(); i++) {\n            x[i] *= scale;\n        }\n    }\n}\n",
      "\n    // Perform the inverse Fourier transform using the forward Fourier transform and scaling\n    fft(x); // Compute the forward Fourier transform\n\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_size = x.size() / size;\n    std::vector<std::complex<double>> local_result(local_size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_size; i++) {\n        local_result[i] = x[rank * local_size + i];\n    }\n\n    MPI_Gather(&local_result[0], local_size, MPI_DOUBLE_COMPLEX, &x[0], local_size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        // Scale the result by 1/N\n        double scale = 1.0 / x.size();\n        for (int i = 0; i < x.size(); i++) {\n            x[i] *= scale;\n        }\n    }\n}\n",
      "\n    // Perform the inverse Fourier transform using the forward Fourier transform and scaling\n    fft(x); // Compute the forward Fourier transform\n\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_size = x.size() / size;\n    std::vector<std::complex<double>> local_result(local_size);\n\n    // Compute local inverse Fourier transform using OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < local_size; i++) {\n        local_result[i] = x[i * size + rank];\n    }\n\n    // Gather local results to rank 0\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size());\n        MPI_Gather(local_result.data(), local_size, MPI_C_DOUBLE_COMPLEX, result.data(), local_size, MPI_C_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(local_result.data(), local_size, MPI_C_DOUBLE_COMPLEX, nullptr, 0, MPI_C_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n\n    // Scale the result by 1/N\n    if (rank == 0) {\n        double scale = 1.0 / x.size();\n        for (auto &val : x) {\n            val *= scale;\n        }\n    }\n}\n",
      "\n    // Perform the inverse Fourier transform using the forward Fourier transform and scaling\n    fft(x); // Compute the forward Fourier transform\n\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_size = x.size() / size;\n    std::vector<std::complex<double>> local_result(local_size);\n\n    // Compute local inverse Fourier transform using OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < local_size; i++) {\n        local_result[i] = x[i * size + rank];\n    }\n\n    // Gather local results to rank 0\n    std::vector<std::complex<double>> global_result(x.size());\n    MPI_Gather(&local_result[0], local_size, MPI_DOUBLE_COMPLEX, &global_result[0], local_size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Scale the result by 1/N\n    if (rank == 0) {\n        double scale = 1.0 / x.size();\n        for (int i = 0; i < x.size(); i++) {\n            global_result[i] *= scale;\n        }\n        x = global_result; // Update the input vector with the inverse Fourier transform result\n    }\n```\n}",
      "\n    // Perform the inverse Fourier transform using the forward Fourier transform and scaling\n    fft(x); // Compute the forward Fourier transform\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank); // Get the rank of the process\n    MPI_Comm_size(MPI_COMM_WORLD, &size); // Get the total number of processes\n\n    int local_size = x.size() / size; // Calculate the local size for each process\n    std::vector<std::complex<double>> local_result(local_size); // Create a local result vector for each process\n\n    // Perform the inverse Fourier transform in parallel using OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < local_size; i++) {\n        local_result[i] = x[i * size + rank]; // Calculate the local result for each process\n    }\n\n    // Gather all local results to rank 0\n    std::vector<std::complex<double>> global_result(x.size());\n    MPI_Gather(local_result.data(), local_size, MPI_DOUBLE_COMPLEX, global_result.data(), local_size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Scale the global result by 1/N\n    if (rank == 0) {\n        double scale = 1.0 / x.size();\n        for (int i = 0; i < x.size(); i++) {\n            global_result[i] *= scale;\n        }\n        x = global_result; // Update the input vector with the final result\n    }\n}\n",
      "\n    // Perform the inverse Fourier transform using the forward Fourier transform and scaling\n    fft(x); // Compute the forward Fourier transform of x\n\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size); // Get the number of processes\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank); // Get the rank of the current process\n\n    int local_n = x.size() / size; // Calculate the local size of the array for each process\n    std::vector<std::complex<double>> local_x(local_n); // Create a local array for each process\n\n    // Scatter the data to all processes\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform the inverse Fourier transform in parallel using OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] /= size; // Scale the local data by the number of processes\n    }\n\n    // Gather the results from all processes to rank 0\n    MPI_Gather(&local_x[0], local_n, MPI_DOUBLE_COMPLEX, &x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    // Perform the inverse Fourier transform using the forward Fourier transform function\n    fft(x);\n\n    // Conjugate the complex numbers\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    // Divide by the size of the array\n    double size = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= size;\n    }\n\n    // Gather the results to rank 0 using MPI\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (rank != 0) {\n        MPI_Send(x.data(), x.size() * sizeof(std::complex<double>), MPI_BYTE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<std::complex<double>> recv_buffer(x.size());\n        for (int i = 1; i < MPI_COMM_WORLD.size(); i++) {\n            MPI_Recv(recv_buffer.data(), recv_buffer.size() * sizeof(std::complex<double>), MPI_BYTE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            x.insert(x.end(), recv_buffer.begin(), recv_buffer.end());\n        }\n    }\n}",
      "\n    // Perform the inverse Fourier transform using the forward Fourier transform and scaling\n    fft(x); // Compute the forward Fourier transform\n\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_size = x.size() / size;\n    std::vector<std::complex<double>> local_result(local_size);\n\n    // Compute local inverse Fourier transform using OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < local_size; i++) {\n        local_result[i] = x[i * size + rank];\n    }\n\n    // Gather local results to rank 0\n    std::vector<std::complex<double>> global_result(x.size());\n    MPI_Gather(local_result.data(), local_size, MPI_DOUBLE_COMPLEX, global_result.data(), local_size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Scale the result by 1/N\n    if (rank == 0) {\n        double scale = 1.0 / x.size();\n        for (int i = 0; i < x.size(); i++) {\n            global_result[i] *= scale;\n        }\n        x = global_result; // Update the input vector with the inverse Fourier transform\n    }\n```\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    fft(x);\n\n    if (rank == 0) {\n        for (int i = 0; i < x.size(); i++) {\n            x[i] = std::conj(x[i]) / size;\n        }\n    }\n\n    MPI_Bcast(&x[0], x.size() * 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    // Perform the inverse Fourier transform using the forward Fourier transform and scaling\n    fft(x); // Compute the forward Fourier transform\n\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_size = x.size() / size;\n    std::vector<std::complex<double>> local_result(local_size);\n\n    // Compute local inverse Fourier transform using OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < local_size; i++) {\n        local_result[i] = x[i * size + rank];\n    }\n\n    // Gather local results to rank 0\n    std::vector<std::complex<double>> global_result(x.size());\n    MPI_Gather(local_result.data(), local_size, MPI_DOUBLE_COMPLEX, global_result.data(), local_size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Scale the result by 1/N\n    if (rank == 0) {\n        double scale = 1.0 / x.size();\n        for (int i = 0; i < x.size(); i++) {\n            global_result[i] *= scale;\n        }\n        x = global_result; // Update the input vector with the inverse Fourier transform\n    }\n```\n}",
      "\n    // Perform the inverse Fourier transform using the forward Fourier transform and scaling\n    fft(x); // Compute the forward Fourier transform\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_x(local_n);\n\n    // Scatter the data to all processes\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform the inverse Fourier transform in parallel using OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] /= size; // Scale the result\n    }\n\n    // Gather the results back to rank 0\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<std::complex<double>> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE_COMPLEX, local_x.data(), local_size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel\n    {\n        fft(local_x);\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE_COMPLEX, x.data(), local_size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        fft(x);\n        std::transform(x.begin(), x.end(), x.begin(), [size](std::complex<double> &c) { return c / size; });\n    }\n}\n",
      "\n    // Perform the inverse Fourier transform using the forward Fourier transform and scaling\n    fft(x); // Compute the forward Fourier transform\n\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_size = x.size() / size;\n    std::vector<std::complex<double>> local_result(local_size);\n\n    // Compute local inverse Fourier transform using OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < local_size; i++) {\n        local_result[i] = x[i * size + rank];\n    }\n\n    // Gather local results to rank 0\n    std::vector<std::complex<double>> global_result(x.size());\n    MPI_Gather(local_result.data(), local_size, MPI_DOUBLE_COMPLEX, global_result.data(), local_size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Scale the result by 1/N\n    if (rank == 0) {\n        double scale = 1.0 / x.size();\n        for (int i = 0; i < x.size(); i++) {\n            global_result[i] *= scale;\n        }\n        x = global_result; // Update the input vector with the inverse Fourier transform\n    }\n```\n}",
      "\n    // Perform the inverse Fourier transform using the forward Fourier transform and scaling\n    fft(x); // Compute the forward Fourier transform\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank); // Get the rank of the current process\n    MPI_Comm_size(MPI_COMM_WORLD, &size); // Get the total number of processes\n\n    int local_n = x.size() / size; // Calculate the local size of the array for each process\n    std::vector<std::complex<double>> local_x(local_n); // Create a local array for each process\n\n    // Scatter the input array x to all processes\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform the inverse Fourier transform in parallel using OpenMP\n    #pragma omp parallel for\n    for (int i = 0; i < local_n; i++) {\n        local_x[i] /= size; // Scale the local array by the total number of processes\n    }\n\n    // Gather the local results from all processes to rank 0\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<std::complex<double>> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE_COMPLEX, local_x.data(), local_size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel\n    {\n        fft(local_x);\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE_COMPLEX, x.data(), local_size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        fft(x);\n        std::transform(x.begin(), x.end(), x.begin(), [size](std::complex<double> &c) { return c / size; });\n    }\n}\n",
      "\n    // Perform the inverse Fourier transform\n    fft(x); // Compute the forward Fourier transform\n    std::reverse(x.begin() + 1, x.end()); // Reverse the order of the elements\n    for (auto &val : x) {\n        val /= x.size(); // Scale by the size of the input\n    }\n}",
      "\n    // Perform inverse Fourier transform\n    fft(x); // Compute forward Fourier transform\n\n    // Conjugate the complex numbers\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    // Divide by the size of the array\n    double size = static_cast<double>(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= size;\n    }\n\n    // Gather the results to rank 0\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    \n    std::vector<std::complex<double>> local_result = x;\n    std::vector<std::complex<double>> global_result(x.size() * size);\n\n    MPI_Gather(&local_result[0], x.size(), MPI_DOUBLE_COMPLEX, &global_result[0], x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        x = global_result;\n    }\n}\n",
      "\n    // Perform the inverse Fourier transform using the FFT algorithm\n    fft(x);\n\n    // Reverse the order of the elements in the vector\n    std::reverse(x.begin() + 1, x.end());\n\n    // Divide each element by the size of the vector\n    int size = x.size();\n    for (int i = 0; i < size; ++i) {\n        x[i] /= size;\n    }\n}\n",
      "\n    // Perform inverse FFT\n    fft(x);\n\n    // Conjugate the complex numbers\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    // Divide by the size of the array\n    double size = static_cast<double>(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= size;\n    }\n\n    // Gather results to rank 0 using MPI\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (rank != 0) {\n        MPI_Send(x.data(), x.size() * sizeof(std::complex<double>), MPI_BYTE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<std::complex<double>> recv_buffer(x.size());\n        for (int i = 1; i < MPI_COMM_WORLD.size(); i++) {\n            MPI_Recv(recv_buffer.data(), recv_buffer.size() * sizeof(std::complex<double>), MPI_BYTE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            // Merge received data into x\n            for (int j = 0; j < recv_buffer.size(); j++) {\n                x[i * recv_buffer.size() + j] = recv_buffer[j];\n            }\n        }\n    }\n}",
      "\n    // Perform inverse Fourier transform\n    fft(x); // Compute forward Fourier transform\n\n    // Conjugate the complex numbers\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    // Divide by the size of the array\n    double size = static_cast<double>(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= size;\n    }\n\n    // Gather results to rank 0 using MPI\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (rank != 0) {\n        MPI_Send(x.data(), x.size() * sizeof(std::complex<double>), MPI_BYTE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<std::complex<double>> recv_buffer(x.size());\n        for (int i = 1; i < MPI_Comm_size(MPI_COMM_WORLD, &size); i++) {\n            MPI_Recv(recv_buffer.data(), recv_buffer.size() * sizeof(std::complex<double>), MPI_BYTE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < recv_buffer.size(); j++) {\n                x[j] += recv_buffer[j];\n            }\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "06_fft_dft",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(Kokkos::View<const double*> &x, Kokkos::View<Kokkos::complex<double>*> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  const int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int k) {\n    output(k) = Kokkos::complex<double>(0, 0);\n    for (int n = 0; n < N; ++n) {\n      double angle = -2 * M_PI * k * n / N;\n      output(k) += Kokkos::complex<double>(cos(angle), sin(angle)) * x(n);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    output(i) = Kokkos::complex<double>(0.0, 0.0);\n    for (int j = 0; j < N; j++) {\n      double angle = -2 * M_PI * i * j / N;\n      output(i) += x(j) * Kokkos::complex<double>(cos(angle), sin(angle));\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int k) {\n    Kokkos::complex<double> sum = Kokkos::complex<double>(0.0, 0.0);\n    for (int n = 0; n < N; n++) {\n      double angle = -2.0 * M_PI * k * n / N;\n      sum += x(n) * Kokkos::complex<double>(cos(angle), sin(angle));\n    }\n    output(k) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int k) {\n    output(k) = Kokkos::complex<double>(0.0, 0.0);\n    for (int n = 0; n < N; n++) {\n      double angle = 2 * M_PI * k * n / N;\n      output(k) += x(n) * Kokkos::complex<double>(cos(angle), -sin(angle));\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  const int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int k) {\n    Kokkos::complex<double> sum = 0;\n    for (int n = 0; n < N; ++n) {\n      double angle = -2 * M_PI * k * n / N;\n      sum += x(n) * Kokkos::complex<double>(cos(angle), sin(angle));\n    }\n    output(k) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int k) {\n    Kokkos::complex<double> sum = 0;\n    for (int n = 0; n < N; n++) {\n      double angle = -2 * M_PI * k * n / N;\n      sum += x(n) * Kokkos::complex<double>(cos(angle), sin(angle));\n    }\n    output(k) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  const int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int k) {\n    output(k) = Kokkos::complex<double>(0.0, 0.0);\n    for (int n = 0; n < N; ++n) {\n      double angle = -2.0 * M_PI * k * n / N;\n      output(k) += Kokkos::complex<double>(cos(angle), sin(angle)) * x(n);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  const int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int k) {\n    Kokkos::complex<double> sum = 0;\n    for (int n = 0; n < N; ++n) {\n      double angle = -2 * M_PI * k * n / N;\n      sum += x(n) * Kokkos::complex<double>(cos(angle), sin(angle));\n    }\n    output(k) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int k) {\n    Kokkos::complex<double> sum = Kokkos::complex<double>(0.0, 0.0);\n    for (int n = 0; n < N; ++n) {\n      double angle = -2.0 * M_PI * k * n / N;\n      sum += x(n) * Kokkos::complex<double>(cos(angle), sin(angle));\n    }\n    output(k) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int k) {\n    output(k) = Kokkos::complex<double>(0.0, 0.0);\n    for (int n = 0; n < N; n++) {\n      double angle = 2 * M_PI * k * n / N;\n      output(k) += Kokkos::complex<double>(x(n) * cos(angle), -x(n) * sin(angle));\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  const int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int k) {\n    output(k) = Kokkos::complex<double>(0.0, 0.0);\n    for (int n = 0; n < N; ++n) {\n      double angle = -2 * M_PI * k * n / N;\n      output(k) += x(n) * Kokkos::complex<double>(cos(angle), sin(angle));\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int k) {\n    output(k) = Kokkos::complex<double>(0.0, 0.0);\n    for (int n = 0; n < N; n++) {\n      double angle = 2 * M_PI * k * n / N;\n      output(k) += x(n) * Kokkos::complex<double>(cos(angle), -sin(angle));\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    output(i) = Kokkos::complex<double>(0.0, 0.0);\n    for (int j = 0; j < N; j++) {\n      double angle = -2 * M_PI * i * j / N;\n      output(i) += x(j) * Kokkos::complex<double>(cos(angle), sin(angle));\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::complex<double> sum = 0.0;\n    for (int j = 0; j < N; j++) {\n      double angle = -2.0 * M_PI * i * j / N;\n      sum += x(j) * Kokkos::complex<double>(cos(angle), sin(angle));\n    }\n    output(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int k) {\n    output(k) = Kokkos::complex<double>(0, 0);\n    for (int n = 0; n < N; n++) {\n      double angle = 2 * M_PI * k * n / N;\n      output(k) += x(n) * Kokkos::complex<double>(cos(angle), -sin(angle));\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  const int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int k) {\n    output(k) = Kokkos::complex<double>(0.0, 0.0);\n    for (int n = 0; n < N; ++n) {\n      double angle = -2.0 * M_PI * k * n / N;\n      output(k) += Kokkos::complex<double>(cos(angle), sin(angle)) * x(n);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.size();\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int k) {\n    output(k) = Kokkos::complex<double>(0.0, 0.0);\n    for (int n = 0; n < N; n++) {\n      double angle = 2 * M_PI * k * n / N;\n      output(k) += x(n) * Kokkos::complex<double>(cos(angle), -sin(angle));\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  const int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int k) {\n    output(k) = Kokkos::complex<double>(0.0, 0.0);\n    for (int n = 0; n < N; n++) {\n      double angle = 2 * M_PI * k * n / N;\n      output(k) += x(n) * Kokkos::complex<double>(cos(angle), -sin(angle));\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    output(i) = Kokkos::complex<double>(0.0, 0.0);\n    for (int j = 0; j < x.extent(0); j++) {\n      double angle = -2 * M_PI * i * j / x.extent(0);\n      output(i) += Kokkos::complex<double>(cos(angle), sin(angle)) * x(j);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    output(i) = Kokkos::complex<double>(0.0, 0.0);\n    for (int k = 0; k < N; ++k) {\n      double angle = -2.0 * M_PI * i * k / N;\n      output(i) += x(k) * Kokkos::complex<double>(cos(angle), sin(angle));\n    }\n  });\n  Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "06_fft_dft",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        output[k] = {0.0, 0.0};\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = -2 * M_PI * k * n / x.size();\n            output[k] += std::polar(x[n], angle);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        output[k] = {0.0, 0.0};\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = -2 * M_PI * k * n / x.size();\n            output[k] += std::polar(x[n], angle);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        output[k] = {0.0, 0.0};\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = 2 * M_PI * k * n / x.size();\n            output[k] += std::polar(x[n], -angle);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        output[k] = {0, 0};\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = 2 * M_PI * k * n / x.size();\n            output[k] += std::polar(x[n], -angle);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        output[k] = {0.0, 0.0};\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = 2 * M_PI * k * n / x.size();\n            output[k] += std::polar(x[n], -angle);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        output[k] = {0, 0};\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = -2 * M_PI * k * n / x.size();\n            output[k] += std::polar(x[n], angle);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        std::complex<double> sum = 0.0;\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = -2 * M_PI * k * n / x.size();\n            sum += std::polar(1.0, angle) * x[n];\n        }\n        output[k] = sum;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        output[k] = {0.0, 0.0};\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = -2 * M_PI * k * n / x.size();\n            output[k] += std::polar(x[n], angle);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        output[k] = {0.0, 0.0};\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = 2 * M_PI * k * n / x.size();\n            output[k] += std::polar(x[n], -angle);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        output[k] = {0, 0};\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = -2 * M_PI * k * n / x.size();\n            output[k] += std::polar(x[n], angle);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        output[k] = {0.0, 0.0};\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = -2 * M_PI * k * n / x.size();\n            output[k] += std::polar(x[n], angle);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        output[k] = {0.0, 0.0};\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = -2 * M_PI * k * n / x.size();\n            output[k] += std::polar(x[n], angle);\n        }\n    }\n}",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        output[k] = {0.0, 0.0};\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            output[k] += std::polar(x[n], -angle);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        output[k] = {0.0, 0.0};\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = -2 * M_PI * k * n / x.size();\n            output[k] += std::polar(x[n], angle);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        output[k] = {0, 0};\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = -2 * M_PI * k * n / x.size();\n            output[k] += std::polar(x[n], angle);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        output[k] = {0.0, 0.0};\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = -2 * M_PI * k * n / x.size();\n            output[k] += std::polar(x[n], angle);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        output[k] = {0.0, 0.0};\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = 2 * M_PI * k * n / x.size();\n            output[k] += std::polar(x[n], -angle);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        output[k] = {0.0, 0.0};\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = -2 * M_PI * k * n / x.size();\n            output[k] += std::polar(x[n], angle);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        output[k] = {0.0, 0.0};\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = -2 * M_PI * k * n / x.size();\n            output[k] += std::polar(x[n], angle);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        output[k] = {0.0, 0.0};\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = -2 * M_PI * k * n / x.size();\n            output[k] += std::polar(x[n], angle);\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "06_fft_dft",
    "parallelism_model": "serial",
    "prompt": "/* Compute the discrete fourier transform of x. Store the result in output.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int N = x.size();\n    output.resize(N);\n\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += std::complex<double>(cos(angle), -sin(angle)) * x[n];\n        }\n        output[k] = sum;\n    }\n}",
      "\n    int N = x.size();\n    output.resize(N);\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += std::polar(1.0, -angle) * x[n];\n        }\n        output[k] = sum;\n    }\n}",
      "\n    int N = x.size();\n    output.resize(N);\n\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += std::polar(1.0, -angle) * x[n];\n        }\n        output[k] = sum;\n    }\n}",
      "\n    int N = x.size();\n    output.resize(N);\n\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += x[n] * std::polar(1.0, -angle);\n        }\n        output[k] = sum;\n    }\n}",
      "\n    int N = x.size();\n    output.resize(N);\n\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += x[n] * std::polar(1.0, -angle);\n        }\n        output[k] = sum;\n    }\n}",
      "\n    int N = x.size();\n    output.resize(N);\n\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            std::complex<double> term(x[n] * cos(angle), -x[n] * sin(angle));\n            sum += term;\n        }\n        output[k] = sum;\n    }\n}",
      "\n    int N = x.size();\n    output.resize(N);\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += std::complex<double>(cos(angle), -sin(angle)) * x[n];\n        }\n        output[k] = sum;\n    }\n}",
      "\n    int N = x.size();\n    output.resize(N);\n\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += std::complex<double>(cos(angle), -sin(angle)) * x[n];\n        }\n        output[k] = sum;\n    }\n}",
      "\n    int N = x.size();\n    output.resize(N);\n\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += std::complex<double>(cos(angle), -sin(angle)) * x[n];\n        }\n        output[k] = sum;\n    }\n}",
      "\n    int N = x.size();\n    output.resize(N);\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += std::complex<double>(cos(angle), -sin(angle)) * x[n];\n        }\n        output[k] = sum;\n    }\n}",
      "\n    int N = x.size();\n    output.resize(N);\n\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += x[n] * std::polar(1.0, -angle);\n        }\n        output[k] = sum;\n    }\n}",
      "\n    int N = x.size();\n    output.resize(N);\n\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += std::complex<double>(cos(angle), -sin(angle)) * x[n];\n        }\n        output[k] = sum;\n    }\n}",
      "\n    int N = x.size();\n    output.resize(N);\n\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += std::polar(1.0, -angle) * x[n];\n        }\n        output[k] = sum;\n    }\n}",
      "\n    int N = x.size();\n    output.resize(N);\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += std::complex<double>(cos(angle), -sin(angle)) * x[n];\n        }\n        output[k] = sum;\n    }\n}",
      "\n    int N = x.size();\n    output.resize(N);\n\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += std::complex<double>(cos(angle), -sin(angle)) * x[n];\n        }\n        output[k] = sum;\n    }\n}",
      "\n    int N = x.size();\n    output.resize(N);\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += std::polar(x[n], -angle);\n        }\n        output[k] = sum;\n    }\n}",
      "\n    int N = x.size();\n    output.resize(N);\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += std::complex<double>(cos(angle), -sin(angle)) * x[n];\n        }\n        output[k] = sum;\n    }\n}",
      "\n    int N = x.size();\n    output.resize(N);\n\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += std::polar(1.0, -angle) * x[n];\n        }\n        output[k] = sum;\n    }\n}",
      "\n    int N = x.size();\n    output.resize(N);\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += std::polar(x[n], -angle);\n        }\n        output[k] = sum;\n    }\n}",
      "\n    int N = x.size();\n    output.resize(N);\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum = 0;\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += std::polar(1.0, -angle) * x[n];\n        }\n        output[k] = sum;\n    }\n}"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "06_fft_dft",
    "parallelism_model": "hip",
    "prompt": "/* Compute the discrete fourier transform of x. Store the result in output.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\n__global__ void dft(const double *x, cuDoubleComplex *output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(term, make_cuDoubleComplex(x[k], 0.0)));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(term, make_cuDoubleComplex(x[k], 0.0)));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(make_cuDoubleComplex(x[k], 0.0), term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(term, make_cuDoubleComplex(x[k], 0.0)));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(make_cuDoubleComplex(x[k], 0.0), term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(term, make_cuDoubleComplex(x[k], 0.0)));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(term, make_cuDoubleComplex(x[k], 0.0)));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(make_cuDoubleComplex(x[k], 0.0), term));\n    }\n    output[tid] = sum;\n}"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "06_fft_dft",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int k = rank; k < n; k += size) {\n        output[k] = {0.0, 0.0};\n        for (int t = 0; t < n; t++) {\n            double angle = 2 * M_PI * k * t / n;\n            output[k] += {x[t] * cos(angle), -x[t] * sin(angle)};\n        }\n    }\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], n * 2, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int k = rank; k < n; k += size) {\n        output[k] = {0.0, 0.0};\n        for (int t = 0; t < n; t++) {\n            double angle = 2 * M_PI * k * t / n;\n            output[k] += {x[t] * cos(angle), -x[t] * sin(angle)};\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(output.data(), n, MPI_C_DOUBLE_COMPLEX, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            std::vector<std::complex<double>> temp(n);\n            MPI_Recv(temp.data(), n, MPI_C_DOUBLE_COMPLEX, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                output[j] += temp[j];\n            }\n        }\n    }\n```\n}",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int k = rank; k < n; k += size) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; j++) {\n            double angle = 2 * M_PI * k * j / n;\n            output[k] += {x[j] * cos(angle), -x[j] * sin(angle)};\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(output.data(), n, MPI_DOUBLE_COMPLEX, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            std::vector<std::complex<double>> temp(n);\n            MPI_Recv(temp.data(), n, MPI_DOUBLE_COMPLEX, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                output[j] += temp[j];\n            }\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int k = rank; k < n; k += size) {\n        output[k] = {0, 0};\n        for (int t = 0; t < n; t++) {\n            double angle = 2 * M_PI * k * t / n;\n            output[k] += {x[t] * cos(angle), -x[t] * sin(angle)};\n        }\n    }\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], n * 2, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int k = rank; k < n; k += size) {\n        std::complex<double> sum = 0;\n        for (int t = 0; t < n; t++) {\n            double angle = 2 * M_PI * k * t / n;\n            sum += x[t] * std::polar(1.0, -angle);\n        }\n        output[k] = sum;\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], n, MPI_CXX_DOUBLE_COMPLEX, MPI_SUM, MPI_COMM_WORLD);\n}",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int k = rank; k < n; k += size) {\n        std::complex<double> sum = 0.0;\n        for (int t = 0; t < n; t++) {\n            double angle = 2 * M_PI * k * t / n;\n            sum += x[t] * std::polar(1.0, -angle);\n        }\n        output[k] = sum;\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, output.data(), n, MPI_DOUBLE_COMPLEX, MPI_SUM, MPI_COMM_WORLD);\n}",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int k = rank; k < n; k += size) {\n        std::complex<double> sum = 0;\n        for (int t = 0; t < n; t++) {\n            double angle = 2 * M_PI * k * t / n;\n            sum += x[t] * std::polar(1.0, -angle);\n        }\n        output[k] = sum;\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], n, MPI_CXX_DOUBLE_COMPLEX, MPI_SUM, MPI_COMM_WORLD);\n}",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int k = rank; k < n; k += size) {\n        output[k] = {0.0, 0.0};\n        for (int t = 0; t < n; t++) {\n            double angle = 2 * M_PI * k * t / n;\n            output[k] += {x[t] * cos(angle), -x[t] * sin(angle)};\n        }\n    }\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], n * 2, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int k = rank; k < n; k += size) {\n        std::complex<double> sum = 0;\n        for (int t = 0; t < n; t++) {\n            double angle = 2 * M_PI * k * t / n;\n            sum += x[t] * std::polar(1.0, -angle);\n        }\n        output[k] = sum;\n    }\n\n    MPI_Allgather(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, output.data(), n / size, MPI_COMPLEX, MPI_COMM_WORLD);\n}",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int k = rank; k < n; k += size) {\n        output[k] = {0.0, 0.0};\n        for (int t = 0; t < n; t++) {\n            double angle = 2 * M_PI * k * t / n;\n            output[k] += {x[t] * cos(angle), -x[t] * sin(angle)};\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(output.data(), n, MPI_DOUBLE_COMPLEX, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            std::vector<std::complex<double>> temp(n);\n            MPI_Recv(temp.data(), n, MPI_DOUBLE_COMPLEX, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                output[j] += temp[j];\n            }\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int k = rank; k < n; k += size) {\n        output[k] = {0.0, 0.0};\n        for (int t = 0; t < n; t++) {\n            double angle = 2 * M_PI * k * t / n;\n            output[k] += {x[t] * cos(angle), -x[t] * sin(angle)};\n        }\n    }\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], n, MPI_C_DOUBLE_COMPLEX, MPI_SUM, MPI_COMM_WORLD);\n}",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int k = rank; k < n; k += size) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; j++) {\n            double angle = 2 * M_PI * k * j / n;\n            output[k] += {x[j] * cos(angle), -x[j] * sin(angle)};\n        }\n    }\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], n * 2, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int k = rank; k < n; k += size) {\n        output[k] = {0.0, 0.0};\n        for (int t = 0; t < n; t++) {\n            double angle = 2 * M_PI * k * t / n;\n            output[k] += {x[t] * cos(angle), -x[t] * sin(angle)};\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(output.data(), n, MPI_C_DOUBLE_COMPLEX, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            std::vector<std::complex<double>> temp(n);\n            MPI_Recv(temp.data(), n, MPI_C_DOUBLE_COMPLEX, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                output[j] += temp[j];\n            }\n        }\n    }\n```\n}",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int k = rank; k < n; k += size) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; j++) {\n            double angle = 2 * M_PI * k * j / n;\n            output[k] += std::polar(x[j], -angle);\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(output.data(), n, MPI_DOUBLE_COMPLEX, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            std::vector<std::complex<double>> temp(n);\n            MPI_Recv(temp.data(), n, MPI_DOUBLE_COMPLEX, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                output[j] += temp[j];\n            }\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int k = rank; k < n; k += size) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; j++) {\n            double angle = 2 * M_PI * k * j / n;\n            output[k] += std::polar(x[j], -angle);\n        }\n    }\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], n * 2, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int k = rank; k < n; k += size) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; j++) {\n            double angle = 2 * M_PI * k * j / n;\n            output[k] += {x[j] * cos(angle), -x[j] * sin(angle)};\n        }\n    }\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], n * 2, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int k = rank; k < n; k += size) {\n        output[k] = {0, 0};\n        for (int t = 0; t < n; t++) {\n            double angle = 2 * M_PI * k * t / n;\n            output[k] += {x[t] * cos(angle), -x[t] * sin(angle)};\n        }\n    }\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], n * 2, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int k = rank; k < n; k += size) {\n        std::complex<double> sum = 0;\n        for (int t = 0; t < n; t++) {\n            double angle = 2 * M_PI * k * t / n;\n            sum += x[t] * std::polar(1.0, -angle);\n        }\n        output[k] = sum;\n    }\n\n    if (rank != 0) {\n        MPI_Send(output.data(), n, MPI_DOUBLE_COMPLEX, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            std::vector<std::complex<double>> temp(n);\n            MPI_Recv(temp.data(), n, MPI_DOUBLE_COMPLEX, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                output[j] += temp[j];\n            }\n        }\n    }\n```\n}",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int k = rank; k < n; k += size) {\n        output[k] = {0, 0};\n        for (int t = 0; t < n; t++) {\n            double angle = 2 * M_PI * k * t / n;\n            output[k] += {x[t] * cos(angle), -x[t] * sin(angle)};\n        }\n    }\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], n * 2, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int k = rank; k < n; k += size) {\n        std::complex<double> sum = 0;\n        for (int t = 0; t < n; t++) {\n            double angle = 2 * M_PI * k * t / n;\n            sum += std::polar(1.0, -angle) * x[t];\n        }\n        output[k] = sum;\n    }\n    MPI_Allgather(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, output.data(), n / size, MPI_COMPLEX, MPI_COMM_WORLD);\n}"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "06_fft_dft",
    "parallelism_model": "cuda",
    "prompt": "/* Compute the discrete fourier transform of x. Store the result in output.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\n__global__ void dft(const double *x, cuDoubleComplex *output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2.0 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        cuDoubleComplex value = make_cuDoubleComplex(x[k], 0.0);\n        sum = cuCadd(sum, cuCmul(value, term));\n    }\n    output[tid] = sum;\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(make_cuDoubleComplex(x[k], 0.0), term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n    for (int k = 0; k < N; k++) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n        sum = cuCadd(sum, cuCmul(x[k], term));\n    }\n    output[tid] = sum;\n}"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "06_fft_dft",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; ++k) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; ++j) {\n            double angle = 2 * M_PI * rank * j / size;\n            output[k] += {x[j] * cos(angle), -x[j] * sin(angle)};\n        }\n    }\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], 2 * n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; ++k) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; ++j) {\n            double angle = 2 * M_PI * k * j / n;\n            output[k] += {x[j] * cos(angle), -x[j] * sin(angle)};\n        }\n    }\n}",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; ++k) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; ++j) {\n            double angle = 2 * M_PI * rank * j / size;\n            output[k] += {x[j] * cos(angle), -x[j] * sin(angle)};\n        }\n    }\n\n    std::vector<std::complex<double>> local_output(n);\n    MPI_Allreduce(output.data(), local_output.data(), n, MPI_DOUBLE_COMPLEX, MPI_SUM, MPI_COMM_WORLD);\n    output = local_output;\n}",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = rank; k < n; k += size) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; j++) {\n            double angle = 2 * M_PI * k * j / n;\n            output[k] += {x[j] * cos(angle), -x[j] * sin(angle)};\n        }\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], 2 * n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; ++k) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; ++j) {\n            double angle = 2 * M_PI * rank * j / size;\n            output[k] += {x[j] * cos(angle * k), -x[j] * sin(angle * k)};\n        }\n    }\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], 2 * n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; ++k) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; ++j) {\n            double angle = 2 * M_PI * rank * j / size;\n            output[k] += {x[j] * cos(angle), -x[j] * sin(angle)};\n        }\n    }\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], 2 * n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = rank; k < n; k += size) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; j++) {\n            double angle = 2 * M_PI * k * j / n;\n            output[k] += {x[j] * cos(angle), -x[j] * sin(angle)};\n        }\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], 2 * n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; ++k) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; ++j) {\n            double angle = 2 * M_PI * rank * j / size;\n            output[k] += {x[j] * cos(angle * k), -x[j] * sin(angle * k)};\n        }\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], 2 * n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = rank; k < n; k += size) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; j++) {\n            double angle = 2 * M_PI * k * j / n;\n            output[k] += {x[j] * cos(angle), -x[j] * sin(angle)};\n        }\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], 2 * n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; ++k) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; ++j) {\n            double angle = 2 * M_PI * rank * j / size;\n            output[k] += {x[j] * cos(angle * k), -x[j] * sin(angle * k)};\n        }\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], 2 * n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; ++k) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; ++j) {\n            double angle = 2 * M_PI * rank * j / size;\n            output[k] += {x[j] * cos(angle * k), -x[j] * sin(angle * k)};\n        }\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], 2 * n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = rank; k < n; k += size) {\n        output[k] = {0, 0};\n        for (int j = 0; j < n; j++) {\n            double angle = 2 * M_PI * k * j / n;\n            output[k] += {x[j] * cos(angle), -x[j] * sin(angle)};\n        }\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], 2 * n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; ++k) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; ++j) {\n            double angle = 2 * M_PI * j * k / n;\n            output[k] += {x[j] * cos(angle), -x[j] * sin(angle)};\n        }\n    }\n    MPI_Allreduce(MPI_IN_PLACE, output.data(), 2 * n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; ++k) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; ++j) {\n            double angle = 2 * M_PI * rank * j / size;\n            output[k] += {x[j] * cos(angle * k), -x[j] * sin(angle * k)};\n        }\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], 2 * n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; ++k) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; ++j) {\n            double angle = 2 * M_PI * rank * j / size;\n            output[k] += {x[j] * cos(angle), -x[j] * sin(angle)};\n        }\n    }\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], 2 * n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; ++k) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; ++j) {\n            double angle = 2 * M_PI * rank * j / size;\n            output[k] += {x[j] * cos(angle * k), -x[j] * sin(angle * k)};\n        }\n    }\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], 2 * n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; ++k) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; ++j) {\n            double angle = 2 * M_PI * rank * j / size;\n            output[k] += std::polar(x[j], -angle * k);\n        }\n    }\n\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], 2 * n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; ++k) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; ++j) {\n            double angle = 2 * M_PI * rank * j / size;\n            output[k] += {x[j] * cos(angle * k), -x[j] * sin(angle * k)};\n        }\n    }\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], 2 * n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; ++k) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; ++j) {\n            double angle = 2 * M_PI * rank * j / size;\n            output[k] += {x[j] * cos(angle * k), -x[j] * sin(angle * k)};\n        }\n    }\n    MPI_Allreduce(MPI_IN_PLACE, &output[0], 2 * n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n}",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < n; ++k) {\n        output[k] = {0.0, 0.0};\n        for (int j = 0; j < n; ++j) {\n            double angle = 2 * M_PI * k * j / n;\n            output[k] += {x[j] * cos(angle), -x[j] * sin(angle)};\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "09_fft_fft_out_of_place",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the fourier transform of x. Store the result in output.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(Kokkos::View<const Kokkos::complex<double>*> &x, Kokkos::View<Kokkos::complex<double>*> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    Kokkos::complex<double> sum = 0;\n    for (int n = 0; n < x.extent(0); n++) {\n      sum += x(n) * exp(Kokkos::complex<double>(0, -2 * M_PI * k * n / x.extent(0)));\n    }\n    output(k) = sum;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    Kokkos::complex<double> sum = 0;\n    int N = x.extent(0);\n    for (int n = 0; n < N; ++n) {\n      sum += x(n) * exp(Kokkos::complex<double>(0, -2 * M_PI * k * n / N));\n    }\n    output(k) = sum;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    Kokkos::complex<double> sum = 0;\n    int N = x.extent(0);\n    for (int n = 0; n < N; ++n) {\n      sum += x(n) * exp(Kokkos::complex<double>(0, -2 * M_PI * k * n / N));\n    }\n    output(k) = sum;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n      Kokkos::complex<double> sum = 0;\n      int N = x.extent(0);\n      for (int n = 0; n < N; ++n) {\n          sum += x(n) * exp(Kokkos::complex<double>(0, -2 * M_PI * k * n / N));\n      }\n      output(k) = sum;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n      Kokkos::complex<double> sum = 0;\n      for (int n = 0; n < x.extent(0); ++n) {\n          sum += x(n) * exp(Kokkos::complex<double>(0, -2 * M_PI * k * n / x.extent(0)));\n      }\n      output(k) = sum;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    Kokkos::complex<double> sum = 0;\n    int N = x.extent(0);\n    for (int n = 0; n < N; n++) {\n      sum += x(n) * exp(Kokkos::complex<double>(0, -2 * M_PI * k * n / N));\n    }\n    output(k) = sum;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::complex<double> I(0.0, 1.0);\n  int N = x.extent(0);\n  for (int k = 0; k < N; k++) {\n    output[k] = 0;\n    for (int n = 0; n < N; n++) {\n      output[k] += x[n] * exp(-2 * M_PI * I * (double)k * (double)n / (double)N);\n    }\n  }\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    Kokkos::complex<double> sum = 0;\n    for (int j = 0; j < x.extent(0); j++) {\n      sum += x(j) * exp(Kokkos::complex<double>(0, -2 * M_PI * i * j / x.extent(0)));\n    }\n    output(i) = sum;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::complex<double> I(0.0, 1.0);\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int k) {\n    output[k] = 0;\n    for (int n = 0; n < N; ++n) {\n      output[k] += x[n] * exp(-2 * M_PI * I * k * n / N);\n    }\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    Kokkos::complex<double> sum = 0;\n    int N = x.extent(0);\n    for (int n = 0; n < N; ++n) {\n      sum += x(n) * exp(Kokkos::complex<double>(0, -2 * M_PI * k * n / N));\n    }\n    output(k) = sum;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    Kokkos::complex<double> sum = 0;\n    for (int n = 0; n < x.extent(0); ++n) {\n      sum += x(n) * exp(Kokkos::complex<double>(0, -2 * M_PI * k * n / x.extent(0)));\n    }\n    output(k) = sum;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    Kokkos::complex<double> sum = 0;\n    for (int n = 0; n < x.extent(0); n++) {\n      sum += x(n) * exp(Kokkos::complex<double>(0, -2 * M_PI * k * n / x.extent(0)));\n    }\n    output(k) = sum;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    Kokkos::complex<double> sum = 0;\n    for (int k = 0; k < x.extent(0); k++) {\n      sum += x(k) * exp(Kokkos::complex<double>(0, -2 * M_PI * i * k / x.extent(0)));\n    }\n    output(i) = sum;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    Kokkos::complex<double> sum = 0;\n    for (int n = 0; n < x.extent(0); ++n) {\n      sum += x(n) * exp(Kokkos::complex<double>(0, -2 * M_PI * k * n / x.extent(0)));\n    }\n    output(k) = sum;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    Kokkos::complex<double> sum = 0;\n    for (int j = 0; j < x.extent(0); j++) {\n      sum += x(j) * exp(Kokkos::complex<double>(0, -2 * M_PI * i * j / x.extent(0)));\n    }\n    output(i) = sum;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    Kokkos::complex<double> sum = 0;\n    int N = x.extent(0);\n    for (int n = 0; n < N; ++n) {\n      sum += x(n) * exp(Kokkos::complex<double>(0, -2 * M_PI * k * n / N));\n    }\n    output(k) = sum;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    Kokkos::complex<double> sum = 0;\n    for (int k = 0; k < x.extent(0); ++k) {\n      sum += x(k) * exp(Kokkos::complex<double>(0, -2 * M_PI * i * k / x.extent(0)));\n    }\n    output(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int k) {\n    Kokkos::complex<double> sum = 0;\n    int N = x.extent(0);\n    for (int n = 0; n < N; ++n) {\n      sum += x(n) * exp(Kokkos::complex<double>(0, -2 * M_PI * k * n / N));\n    }\n    output(k) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n      Kokkos::complex<double> sum = 0;\n      for (int k = 0; k < x.extent(0); k++) {\n          sum += x(k) * exp(Kokkos::complex<double>(0, -2 * M_PI * i * k / x.extent(0)));\n      }\n      output(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::complex<double> I(0.0, 1.0);\n  int N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(int k) {\n    output[k] = Kokkos::complex<double>(0.0, 0.0);\n    for (int n = 0; n < N; n++) {\n      output[k] += x[n] * exp(-2.0 * M_PI * I * (double)k * (double)n / (double)N);\n    }\n  });\n  Kokkos::fence();\n```\n}"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "09_fft_fft_out_of_place",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the fourier transform of x. Store the result in output.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int N = x.size();\n    output.resize(N);\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += x[n] * std::polar(1.0, -angle);\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int N = x.size();\n    output.resize(N);\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += x[n] * std::polar(1.0, -angle);\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int N = x.size();\n    output.resize(N);\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            std::complex<double> term(std::cos(angle), -std::sin(angle));\n            sum += x[n] * term;\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int N = x.size();\n    output.resize(N);\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0, 0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            std::complex<double> term(std::cos(angle), -std::sin(angle));\n            sum += x[n] * term;\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0, 0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            std::complex<double> term(std::cos(angle), -std::sin(angle));\n            sum += x[n] * term;\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int N = x.size();\n    output.resize(N);\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += x[n] * std::polar(1.0, -angle);\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int N = x.size();\n    output.resize(N);\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += x[n] * std::polar(1.0, -angle);\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int N = x.size();\n    output.resize(N);\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            std::complex<double> term(std::cos(angle), -std::sin(angle));\n            sum += x[n] * term;\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int N = x.size();\n    output.resize(N);\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += x[n] * std::polar(1.0, -angle);\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int N = x.size();\n    output.resize(N);\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            std::complex<double> term(std::cos(angle), -std::sin(angle));\n            sum += x[n] * term;\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int N = x.size();\n    output.resize(N);\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += x[n] * std::polar(1.0, -angle);\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int N = x.size();\n    output.resize(N);\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += x[n] * std::polar(1.0, -angle);\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int N = x.size();\n    output.resize(N);\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            std::complex<double> term(std::cos(angle), -std::sin(angle));\n            sum += x[n] * term;\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int N = x.size();\n    output.resize(N);\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += x[n] * std::polar(1.0, -angle);\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int N = x.size();\n    output.resize(N);\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += x[n] * std::polar(1.0, -angle);\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int N = x.size();\n    output.resize(N);\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            std::complex<double> term(std::cos(angle), -std::sin(angle));\n            sum += x[n] * term;\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0, 0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * n * k / N;\n            sum += x[n] * std::polar(1.0, -angle);\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int N = x.size();\n    output.resize(N);\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            std::complex<double> term(std::cos(angle), -std::sin(angle));\n            sum += x[n] * term;\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0, 0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += x[n] * std::polar(1.0, -angle);\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int N = x.size();\n    #pragma omp parallel for\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0, 0);\n        for (int n = 0; n < N; ++n) {\n            double angle = 2 * M_PI * k * n / N;\n            sum += x[n] * std::polar(1.0, -angle);\n        }\n        output[k] = sum;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "09_fft_fft_out_of_place",
    "parallelism_model": "serial",
    "prompt": "/* Compute the fourier transform of x. Store the result in output.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    output.resize(x.size());\n    std::copy(x.begin(), x.end(), output.begin());\n    int N = x.size();\n    if (N <= 1) return;\n\n    std::vector<std::complex<double>> even(N/2), odd(N/2);\n    for (int i = 0; i < N/2; ++i) {\n        even[i] = x[i*2];\n        odd[i] = x[i*2 + 1];\n    }\n\n    fft(even, even);\n    fft(odd, odd);\n\n    for (int k = 0; k < N/2; ++k) {\n        std::complex<double> t = std::polar(1.0, -2 * M_PI * k / N) * odd[k];\n        output[k] = even[k] + t;\n        output[k + N/2] = even[k] - t;\n    }\n}",
      "\n    int N = x.size();\n    if (N <= 1) {\n        output = x;\n    } else {\n        std::vector<std::complex<double>> even(N/2), odd(N/2);\n        for (int i = 0; i < N/2; ++i) {\n            even[i] = x[2*i];\n            odd[i] = x[2*i + 1];\n        }\n        std::vector<std::complex<double>> evenResult, oddResult;\n        fft(even, evenResult);\n        fft(odd, oddResult);\n        output.resize(N);\n        for (int k = 0; k < N/2; ++k) {\n            std::complex<double> t = std::polar(1.0, -2 * M_PI * k / N) * oddResult[k];\n            output[k] = evenResult[k] + t;\n            output[k + N/2] = evenResult[k] - t;\n        }\n    }\n```\n}",
      "\n    int N = x.size();\n    if (N <= 1) {\n        output = x;\n    } else {\n        std::vector<std::complex<double>> even(N/2);\n        std::vector<std::complex<double>> odd(N/2);\n        for (int i = 0; i < N/2; ++i) {\n            even[i] = x[2*i];\n            odd[i] = x[2*i + 1];\n        }\n        fft(even, even);\n        fft(odd, odd);\n        for (int k = 0; k < N/2; ++k) {\n            std::complex<double> t = std::polar(1.0, -2 * M_PI * k / N) * odd[k];\n            output[k] = even[k] + t;\n            output[k + N/2] = even[k] - t;\n        }\n    }\n```\n}",
      "\n    int N = x.size();\n    if (N <= 1) {\n        output = x;\n    } else {\n        std::vector<std::complex<double>> even(N/2);\n        std::vector<std::complex<double>> odd(N/2);\n        for (int i = 0; i < N/2; ++i) {\n            even[i] = x[i*2];\n            odd[i] = x[i*2 + 1];\n        }\n        fft(even, even);\n        fft(odd, odd);\n        for (int k = 0; k < N/2; ++k) {\n            std::complex<double> t = std::polar(1.0, -2 * M_PI * k / N) * odd[k];\n            output[k] = even[k] + t;\n            output[k + N/2] = even[k] - t;\n        }\n    }\n}",
      "\n    int N = x.size();\n    if (N <= 1) {\n        output = x;\n    } else {\n        std::vector<std::complex<double>> even(N/2);\n        std::vector<std::complex<double>> odd(N/2);\n        for (int i = 0; i < N/2; ++i) {\n            even[i] = x[i*2];\n            odd[i] = x[i*2 + 1];\n        }\n        fft(even, even);\n        fft(odd, odd);\n        for (int k = 0; k < N/2; ++k) {\n            std::complex<double> t = std::polar(1.0, -2 * M_PI * k / N) * odd[k];\n            output[k] = even[k] + t;\n            output[k + N/2] = even[k] - t;\n        }\n    }\n```\n}",
      "\n    output.resize(x.size());\n    std::copy(x.begin(), x.end(), output.begin());\n    int n = x.size();\n    if (n <= 1) {\n        return;\n    }\n\n    std::vector<std::complex<double>> even(n / 2), odd(n / 2);\n    for (int i = 0; i < n / 2; ++i) {\n        even[i] = x[i * 2];\n        odd[i] = x[i * 2 + 1];\n    }\n\n    fft(even, even);\n    fft(odd, odd);\n\n    for (int k = 0; k < n / 2; ++k) {\n        std::complex<double> t = std::polar(1.0, -2 * M_PI * k / n) * odd[k];\n        output[k] = even[k] + t;\n        output[k + n / 2] = even[k] - t;\n    }\n```\n}",
      "\n    output.resize(x.size());\n    std::copy(x.begin(), x.end(), output.begin());\n    int N = x.size();\n    if (N <= 1) return;\n\n    std::vector<std::complex<double>> even(N/2), odd(N/2);\n    for (int i = 0; i < N/2; ++i) {\n        even[i] = x[i*2];\n        odd[i] = x[i*2 + 1];\n    }\n\n    fft(even, even);\n    fft(odd, odd);\n\n    for (int k = 0; k < N/2; ++k) {\n        std::complex<double> t = std::polar(1.0, -2 * M_PI * k / N) * odd[k];\n        output[k] = even[k] + t;\n        output[k + N/2] = even[k] - t;\n    }\n}",
      "\n    int N = x.size();\n    if (N <= 1) {\n        output = x;\n    } else {\n        std::vector<std::complex<double>> even(N/2), odd(N/2);\n        for (int i = 0; i < N/2; ++i) {\n            even[i] = x[i*2];\n            odd[i] = x[i*2 + 1];\n        }\n        fft(even, even);\n        fft(odd, odd);\n        for (int k = 0; k < N/2; ++k) {\n            std::complex<double> t = std::polar(1.0, -2 * M_PI * k / N) * odd[k];\n            output[k] = even[k] + t;\n            output[k + N/2] = even[k] - t;\n        }\n    }\n}",
      "\n    int N = x.size();\n    if (N <= 1) {\n        output = x;\n    } else {\n        std::vector<std::complex<double>> even(N/2);\n        std::vector<std::complex<double>> odd(N/2);\n        for (int i = 0; i < N/2; ++i) {\n            even[i] = x[2*i];\n            odd[i] = x[2*i + 1];\n        }\n        fft(even, even);\n        fft(odd, odd);\n        for (int k = 0; k < N/2; ++k) {\n            std::complex<double> t = std::polar(1.0, -2 * M_PI * k / N) * odd[k];\n            output[k] = even[k] + t;\n            output[k + N/2] = even[k] - t;\n        }\n    }\n```\n}",
      "\n    int N = x.size();\n    if (N <= 1) {\n        output = x;\n    } else {\n        std::vector<std::complex<double>> even(N/2), odd(N/2);\n        for (int i = 0; i < N/2; ++i) {\n            even[i] = x[i*2];\n            odd[i] = x[i*2 + 1];\n        }\n        fft(even, even);\n        fft(odd, odd);\n        for (int k = 0; k < N/2; ++k) {\n            std::complex<double> t = std::polar(1.0, -2 * M_PI * k / N) * odd[k];\n            output[k] = even[k] + t;\n            output[k + N/2] = even[k] - t;\n        }\n    }\n}",
      "\n    output.resize(x.size());\n    std::copy(x.begin(), x.end(), output.begin());\n    int N = x.size();\n    if (N <= 1) return;\n\n    std::vector<std::complex<double>> even(N/2), odd(N/2);\n    for (int i = 0; i < N/2; ++i) {\n        even[i] = x[i*2];\n        odd[i] = x[i*2+1];\n    }\n\n    fft(even, even);\n    fft(odd, odd);\n\n    for (int k = 0; k < N/2; ++k) {\n        std::complex<double> t = std::polar(1.0, -2 * M_PI * k / N) * odd[k];\n        output[k] = even[k] + t;\n        output[k + N/2] = even[k] - t;\n    }\n}",
      "\n    output.resize(x.size());\n    std::copy(x.begin(), x.end(), output.begin());\n    std::transform(output.begin(), output.end(), output.begin(), [](std::complex<double> val) { return val / std::sqrt(x.size()); });\n    std::transform(output.begin(), output.end(), output.begin(), [](std::complex<double> val) { return std::conj(val); });\n    std::transform(output.begin(), output.end(), output.begin(), [](std::complex<double> val) { return std::conj(val); });\n    std::transform(output.begin(), output.end(), output.begin(), [](std::complex<double> val) { return std::conj(val); });\n    std::transform(output.begin(), output.end(), output.begin(), [](std::complex<double> val) { return std::conj(val); });\n    std::transform(output.begin(), output.end(), output.begin(), [](std::complex<double> val) { return std::conj(val); });\n    std::transform(output.begin(), output.end(), output.begin(), [](std::complex<double> val) { return std::conj(val); });\n    std::transform(output.begin(), output.end(), output.begin(), [](std::complex<double> val) { return std::conj(val); });\n    std::transform(output.begin(), output.end(), output.begin(), [](std::complex<double> val) { return std::conj(val); });\n    std::transform(output.begin(), output.end(), output.begin(), [](std::complex<double> val) { return std::conj(val); });\n    std::transform(output.begin(), output.end(), output.begin(), [](std::complex<double> val) { return std::conj(val); });\n}",
      "\n    int N = x.size();\n    if (N <= 1) {\n        output = x;\n    } else {\n        std::vector<std::complex<double>> even(N/2);\n        std::vector<std::complex<double>> odd(N/2);\n        for (int i = 0; i < N/2; ++i) {\n            even[i] = x[i*2];\n            odd[i] = x[i*2 + 1];\n        }\n        fft(even, even);\n        fft(odd, odd);\n        for (int k = 0; k < N/2; ++k) {\n            std::complex<double> t = std::polar(1.0, -2 * M_PI * k / N) * odd[k];\n            output[k] = even[k] + t;\n            output[k + N/2] = even[k] - t;\n        }\n    }\n```\n}",
      "\n    output.resize(x.size());\n    std::copy(x.begin(), x.end(), output.begin());\n    int N = x.size();\n    if (N <= 1) return;\n\n    std::vector<std::complex<double>> even(N/2), odd(N/2);\n    for (int i = 0; i < N/2; ++i) {\n        even[i] = x[i*2];\n        odd[i] = x[i*2 + 1];\n    }\n\n    fft(even, even);\n    fft(odd, odd);\n\n    for (int k = 0; k < N/2; ++k) {\n        std::complex<double> t = std::polar(1.0, -2 * M_PI * k / N) * odd[k];\n        output[k] = even[k] + t;\n        output[k + N/2] = even[k] - t;\n    }\n}",
      "\n    int N = x.size();\n    if (N <= 1) {\n        output = x;\n    } else {\n        std::vector<std::complex<double>> even(N/2);\n        std::vector<std::complex<double>> odd(N/2);\n        for (int i = 0; i < N/2; ++i) {\n            even[i] = x[2*i];\n            odd[i] = x[2*i + 1];\n        }\n        fft(even, even);\n        fft(odd, odd);\n        for (int k = 0; k < N/2; ++k) {\n            std::complex<double> t = std::polar(1.0, -2 * M_PI * k / N) * odd[k];\n            output[k] = even[k] + t;\n            output[k + N/2] = even[k] - t;\n        }\n    }\n```\n}",
      "\n    int N = x.size();\n    if (N <= 1) {\n        output = x;\n    } else {\n        std::vector<std::complex<double>> even(N/2), odd(N/2);\n        for (int i = 0; i < N/2; ++i) {\n            even[i] = x[i*2];\n            odd[i] = x[i*2 + 1];\n        }\n        fft(even, even);\n        fft(odd, odd);\n        for (int k = 0; k < N/2; ++k) {\n            std::complex<double> t = std::polar(1.0, -2 * M_PI * k / N) * odd[k];\n            output[k] = even[k] + t;\n            output[k + N/2] = even[k] - t;\n        }\n    }\n}",
      "\n    int N = x.size();\n    if (N <= 1) {\n        output = x;\n    } else {\n        std::vector<std::complex<double>> even(N/2), odd(N/2);\n        for (int i = 0; i < N/2; ++i) {\n            even[i] = x[i*2];\n            odd[i] = x[i*2+1];\n        }\n        fft(even, even);\n        fft(odd, odd);\n        for (int k = 0; k < N/2; ++k) {\n            std::complex<double> t = std::polar(1.0, -2 * M_PI * k / N) * odd[k];\n            output[k] = even[k] + t;\n            output[k + N/2] = even[k] - t;\n        }\n    }\n}",
      "\n    int N = x.size();\n    if (N <= 1) {\n        output = x;\n    } else {\n        std::vector<std::complex<double>> even(N/2), odd(N/2);\n        for (int i = 0; i < N/2; ++i) {\n            even[i] = x[i*2];\n            odd[i] = x[i*2 + 1];\n        }\n        fft(even, even);\n        fft(odd, odd);\n        for (int k = 0; k < N/2; ++k) {\n            std::complex<double> t = std::polar(1.0, -2 * M_PI * k / N) * odd[k];\n            output[k] = even[k] + t;\n            output[k + N/2] = even[k] - t;\n        }\n    }\n}",
      "\n    int N = x.size();\n    if (N <= 1) {\n        output = x;\n    } else {\n        std::vector<std::complex<double>> even(N/2), odd(N/2);\n        for (int i = 0; i < N/2; ++i) {\n            even[i] = x[i*2];\n            odd[i] = x[i*2 + 1];\n        }\n        fft(even, even);\n        fft(odd, odd);\n        for (int k = 0; k < N/2; ++k) {\n            std::complex<double> t = std::polar(1.0, -2 * M_PI * k / N) * odd[k];\n            output[k] = even[k] + t;\n            output[k + N/2] = even[k] - t;\n        }\n    }\n}",
      "\n    int N = x.size();\n    if (N <= 1) {\n        output = x;\n    } else {\n        std::vector<std::complex<double>> even(N/2), odd(N/2);\n        for (int i = 0; i < N/2; ++i) {\n            even[i] = x[i*2];\n            odd[i] = x[i*2 + 1];\n        }\n        fft(even, even);\n        fft(odd, odd);\n        for (int k = 0; k < N/2; ++k) {\n            std::complex<double> t = std::polar(1.0, -2 * M_PI * k / N) * odd[k];\n            output[k] = even[k] + t;\n            output[k + N/2] = even[k] - t;\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "09_fft_fft_out_of_place",
    "parallelism_model": "hip",
    "prompt": "/* Compute the fourier transform of x. Store the result in output.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\n__global__ void fft(const hipDoubleComplex *x, hipDoubleComplex *output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Perform the FFT computation here\n        // Use tid to access the elements of x and output arrays\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Compute the Fourier transform of x[tid] and store the result in output[tid]\n        // Use AMD HIP to compute in parallel\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Perform the Fourier transform calculation here\n        // using the input x and store the result in output\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Perform the Fourier transform calculation here\n        // Use tid to access the input array x and store the result in the output array\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Perform the Fourier transform computation here\n        // using AMD HIP for parallel processing\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Perform the Fourier transform computation here\n        // Use tid to access the input array x and store the result in the output array\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (tid < N) {\n        for (int k = 0; k < N; k++) {\n            output[tid].x += x[k].x * cos(2 * M_PI * tid * k / N) + x[k].y * sin(2 * M_PI * tid * k / N);\n            output[tid].y += -x[k].x * sin(2 * M_PI * tid * k / N) + x[k].y * cos(2 * M_PI * tid * k / N);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Perform the Fourier transform calculation here\n        // Use tid to access the elements of x and output arrays\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (tid < N) {\n        for (int k = 0; k < N; k++) {\n            output[tid].x += x[k].x * cos(2 * M_PI * tid * k / N) + x[k].y * sin(2 * M_PI * tid * k / N);\n            output[tid].y += -x[k].x * sin(2 * M_PI * tid * k / N) + x[k].y * cos(2 * M_PI * tid * k / N);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Perform the Fourier transform calculation here\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Perform the Fourier transform calculation here\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Perform the Fourier transform calculation here\n        // Use tid to access the input array x and store the result in the output array\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Perform the Fourier transform calculation here\n        // Use tid to access the elements of x and output arrays\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Perform the FFT computation here\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Perform FFT computation here\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Perform the FFT computation here\n        // Use tid to access the input array x and store the result in the output array\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Compute the fourier transform of x[tid] and store the result in output[tid]\n        // Use AMD HIP to compute in parallel\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (tid < N) {\n        for (int k = 0; k < N; k++) {\n            output[tid].x += x[k].x * cos(2 * M_PI * tid * k / N) + x[k].y * sin(2 * M_PI * tid * k / N);\n            output[tid].y += -x[k].x * sin(2 * M_PI * tid * k / N) + x[k].y * cos(2 * M_PI * tid * k / N);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Compute the fourier transform of x[tid] and store the result in output[tid]\n        // Use AMD HIP to compute in parallel\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Compute the fourier transform of x[tid] and store the result in output[tid]\n        // Use AMD HIP to compute in parallel\n    }\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "09_fft_fft_out_of_place",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the fourier transform of x. Store the result in output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_output(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform the FFT computation on local_x\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform the FFT computation here\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_output(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform the FFT computation here\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_output(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation here\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_output(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation here\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_output(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation here\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_output(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation here\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_output(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation here\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_output(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation here\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_output(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation here\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_output(local_n);\n\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE_COMPLEX, &local_x[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local_x\n\n    MPI_Gather(&local_output[0], local_n, MPI_DOUBLE_COMPLEX, &output[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local_x\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_output(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation here\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_output(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local_x\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_output(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation on local_x\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_output(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform the FFT computation here\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_output(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation here\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_output(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation here\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_output(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation here\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_output(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Perform FFT computation here\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "09_fft_fft_out_of_place",
    "parallelism_model": "cuda",
    "prompt": "/* Compute the fourier transform of x. Store the result in output.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\n__global__ void fft(const cuDoubleComplex *x, cuDoubleComplex *output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = -2 * M_PI * tid * k / N;\n            cuDoubleComplex w = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], w));\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int k = 0; k < N; k++) {\n            cuDoubleComplex w = make_cuDoubleComplex(cos(2 * M_PI * tid * k / N), -sin(2 * M_PI * tid * k / N));\n            cuCadd(output[tid], cuCmul(x[k], w), output[tid]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = -2 * 3.14159265358979323846 * tid * k / N;\n            cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (tid < N) {\n        for (int k = 0; k < N; k++) {\n            cuDoubleComplex W = make_cuDoubleComplex(cos(-2 * M_PI * tid * k / N), sin(-2 * M_PI * tid * k / N));\n            output[tid] = cuCadd(output[tid], cuCmul(x[k], W));\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = -2 * M_PI * tid * k / N;\n            cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = -2 * 3.14159265358979323846 * tid * k / N;\n            cuDoubleComplex w = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], w));\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = -2 * 3.14159265358979323846 * tid * k / N;\n            cuDoubleComplex w = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], w));\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = -2 * M_PI * tid * k / N;\n            cuDoubleComplex w = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], w));\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = -2 * M_PI * tid * k / N;\n            cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = -2 * 3.14159265359 * tid * k / N;\n            cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = -2 * M_PI * tid * k / N;\n            cuDoubleComplex w = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], w));\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = -2 * 3.14159265358979323846 * tid * k / N;\n            cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = -2 * 3.14159265358979323846 * tid * k / N;\n            cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = -2 * 3.14159265358979323846 * tid * k / N;\n            cuDoubleComplex w = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], w));\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = -2 * 3.14159265358979323846 * tid * k / N;\n            cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = -2 * 3.14159 * tid * k / N;\n            cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = -2 * PI * tid * k / N;\n            cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (tid < N) {\n        for (int k = 0; k < N; k++) {\n            cuDoubleComplex W = make_cuDoubleComplex(cos(2 * M_PI * tid * k / N), -sin(2 * M_PI * tid * k / N));\n            cuCadd(output[tid], cuCmul(x[k], W), output[tid]);\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        cuDoubleComplex sum = make_cuDoubleComplex(0.0, 0.0);\n        for (int k = 0; k < N; k++) {\n            double angle = -2 * M_PI * tid * k / N;\n            cuDoubleComplex term = make_cuDoubleComplex(cos(angle), sin(angle));\n            sum = cuCadd(sum, cuCmul(x[k], term));\n        }\n        output[tid] = sum;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int k = 0; k < N; k++) {\n            cuDoubleComplex W = make_cuDoubleComplex(cos(2 * M_PI * tid * k / N), -sin(2 * M_PI * tid * k / N));\n            cuCadd(output[tid], cuCmul(x[k], W), output[tid]);\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "09_fft_fft_out_of_place",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_output(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int k = 0; k < local_n; ++k) {\n        std::complex<double> sum = 0;\n        for (int j = 0; j < n; ++j) {\n            sum += local_x[j] * std::polar(1.0, -2 * M_PI * rank * j / n * k);\n        }\n        local_output[k] = sum;\n    }\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = 2 * M_PI * n * k / x.size();\n            std::complex<double> term(x[n].real() * cos(angle), x[n].real() * sin(angle));\n            sum += term;\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    std::vector<std::complex<double>> local_output(local_n);\n    #pragma omp parallel for\n    for (int k = local_start; k < local_end; ++k) {\n        std::complex<double> sum = 0;\n        for (int j = 0; j < n; ++j) {\n            sum += x[j] * std::polar(1.0, -2 * M_PI * k * j / n);\n        }\n        local_output[k - local_start] = sum;\n    }\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    std::vector<std::complex<double>> local_output(local_n);\n\n    #pragma omp parallel for\n    for (int k = local_start; k < local_end; ++k) {\n        std::complex<double> sum = 0;\n        for (int j = 0; j < n; ++j) {\n            sum += x[j] * exp(std::complex<double>(0, -2 * M_PI * k * j / n));\n        }\n        local_output[k - local_start] = sum;\n    }\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        std::complex<double> sum = 0;\n        for (int n = 0; n < x.size(); ++n) {\n            sum += x[n] * exp(std::complex<double>(0, -2 * M_PI * k * n / x.size()));\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    std::vector<std::complex<double>> local_output(local_n);\n\n    #pragma omp parallel for\n    for (int k = local_start; k < local_end; ++k) {\n        std::complex<double> sum = 0;\n        for (int j = 0; j < n; ++j) {\n            sum += x[j] * std::polar(1.0, -2 * M_PI * k * j / n);\n        }\n        local_output[k - local_start] = sum;\n    }\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = 2 * M_PI * n * k / x.size();\n            std::complex<double> term(x[n].real() * cos(angle) + x[n].imag() * sin(angle),\n                                      -x[n].real() * sin(angle) + x[n].imag() * cos(angle));\n            sum += term;\n        }\n        output[k] = sum;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        std::complex<double> sum = 0;\n        for (int n = 0; n < x.size(); ++n) {\n            sum += x[n] * exp(std::complex<double>(0, -2 * M_PI * rank * n / size * k));\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    std::vector<std::complex<double>> local_output(local_n);\n\n    #pragma omp parallel for\n    for (int k = local_start; k < local_end; ++k) {\n        std::complex<double> sum = 0;\n        for (int j = 0; j < n; ++j) {\n            double angle = 2 * M_PI * k * j / n;\n            sum += x[j] * std::polar(1.0, -angle);\n        }\n        local_output[k - local_start] = sum;\n    }\n\n    MPI_Gather(&local_output[0], local_n, MPI_DOUBLE_COMPLEX, &output[0], local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    std::vector<std::complex<double>> local_output(local_n);\n\n    #pragma omp parallel for\n    for (int k = local_start; k < local_end; ++k) {\n        std::complex<double> sum = 0;\n        for (int j = 0; j < n; ++j) {\n            double angle = 2 * M_PI * k * j / n;\n            sum += x[j] * std::polar(1.0, -angle);\n        }\n        local_output[k - local_start] = sum;\n    }\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    std::vector<std::complex<double>> local_output(local_n);\n\n    #pragma omp parallel for\n    for (int k = local_start; k < local_end; ++k) {\n        std::complex<double> sum = 0;\n        for (int j = 0; j < n; ++j) {\n            sum += x[j] * exp(std::complex<double>(0, -2 * M_PI * k * j / n));\n        }\n        local_output[k - local_start] = sum;\n    }\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    std::vector<std::complex<double>> local_output(local_n);\n\n    #pragma omp parallel for\n    for (int k = local_start; k < local_end; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int j = 0; j < n; ++j) {\n            double angle = 2 * M_PI * k * j / n;\n            sum += x[j] * std::polar(1.0, -angle);\n        }\n        local_output[k - local_start] = sum;\n    }\n\n    MPI_Gather(&local_output[0], local_n, MPI_CXX_DOUBLE_COMPLEX, &output[0], local_n, MPI_CXX_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = 2 * M_PI * n * k / x.size();\n            std::complex<double> term(x[n].real() * cos(angle), x[n].real() * sin(angle));\n            sum += term;\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = 2 * M_PI * n * k / x.size();\n            std::complex<double> exp_term(cos(angle), -sin(angle));\n            sum += x[n] * exp_term;\n        }\n        output[k] = sum;\n    }\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    std::vector<std::complex<double>> local_output(local_n);\n\n    #pragma omp parallel for\n    for (int k = local_start; k < local_end; ++k) {\n        std::complex<double> sum = 0;\n        for (int j = 0; j < n; ++j) {\n            sum += x[j] * std::polar(1.0, -2 * M_PI * k * j / n);\n        }\n        local_output[k - local_start] = sum;\n    }\n\n    MPI_Gather(local_output.data(), local_n, MPI_DOUBLE_COMPLEX, output.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = 2 * M_PI * rank * n / size;\n            std::complex<double> term = x[n] * exp(std::complex<double>(0, -angle * k));\n            sum += term;\n        }\n        output[k] = sum;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    #pragma omp parallel for\n    for (int k = local_start; k < local_end; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int j = 0; j < n; ++j) {\n            double angle = 2 * M_PI * k * j / n;\n            std::complex<double> w(std::cos(angle), -std::sin(angle));\n            sum += x[j] * w;\n        }\n        output[k] = sum;\n    }\n\n    MPI_Allgather(&output[local_start], local_n, MPI_CXX_DOUBLE_COMPLEX, &output[0], local_n, MPI_CXX_DOUBLE_COMPLEX, MPI_COMM_WORLD);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int local_n = n / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    std::vector<std::complex<double>> local_output(local_n);\n\n    #pragma omp parallel for\n    for (int k = local_start; k < local_end; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int j = 0; j < n; ++j) {\n            double angle = 2 * M_PI * k * j / n;\n            sum += x[j] * std::polar(1.0, -angle);\n        }\n        local_output[k - local_start] = sum;\n    }\n\n    MPI_Allgather(&local_output[0], local_n, MPI_DOUBLE_COMPLEX, &output[0], local_n, MPI_DOUBLE_COMPLEX, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int local_n = n / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    #pragma omp parallel for\n    for (int k = start; k < end; ++k) {\n        std::complex<double> sum(0, 0);\n        for (int t = 0; t < n; ++t) {\n            double angle = 2 * M_PI * k * t / n;\n            sum += x[t] * std::polar(1.0, -angle);\n        }\n        output[k] = sum;\n    }\n\n    MPI_Allgather(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int k = 0; k < x.size(); ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < x.size(); ++n) {\n            double angle = 2 * M_PI * n * k / x.size();\n            std::complex<double> term(std::cos(angle), -std::sin(angle));\n            sum += x[n] * term;\n        }\n        output[k] = sum;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "03_dense_la_axpy",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, Kokkos::View<const double*> &x, Kokkos::View<const double*> &y, Kokkos::View<double*> &z) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(\"axpy\", z.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(i) = alpha * x(i) + y(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"axpy\", z.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(i) = alpha * x(i) + y(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"axpy\", z.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(i) = alpha * x(i) + y(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"axpy\", z.size(), KOKKOS_LAMBDA(const int i) {\n    z(i) = alpha * x(i) + y(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"axpy\", z.extent(0), KOKKOS_LAMBDA (const int i) {\n    z(i) = alpha * x(i) + y(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"axpy\", z.extent(0), KOKKOS_LAMBDA (const int i) {\n    z(i) = alpha * x(i) + y(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"axpy\", z.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(i) = alpha * x(i) + y(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    z(i) = alpha * x(i) + y(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"axpy\", z.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(i) = alpha * x(i) + y(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"axpy\", z.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(i) = alpha * x(i) + y(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"axpy\", z.size(), KOKKOS_LAMBDA(const int i) {\n    z(i) = alpha * x(i) + y(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"axpy\", z.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(i) = alpha * x(i) + y(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(i) = alpha * x(i) + y(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"axpy\", z.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(i) = alpha * x(i) + y(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"axpy\", z.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(i) = alpha * x(i) + y(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"axpy\", z.extent(0), KOKKOS_LAMBDA(const int i) {\n    z(i) = alpha * x(i) + y(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    z(i) = alpha * x(i) + y(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"axpy\", z.extent(0), KOKKOS_LAMBDA (const int i) {\n    z(i) = alpha * x(i) + y(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    z(i) = alpha * x(i) + y(i);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    z(i) = alpha * x(i) + y(i);\n  });\n  Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "03_dense_la_axpy",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "03_dense_la_axpy",
    "parallelism_model": "serial",
    "prompt": "/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); ++i) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
      "\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "03_dense_la_axpy",
    "parallelism_model": "hip",
    "prompt": "/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\n__global__ void axpy(double alpha, const double *x, const double *y, double *z, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        z[tid] = alpha * x[tid] + y[tid];\n    }\n}\n",
      "\n    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        z[idx] = alpha * x[idx] + y[idx];\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        z[tid] = alpha * x[tid] + y[tid];\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n",
      "\n    size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        z[index] = alpha * x[index] + y[index];\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        z[tid] = alpha * x[tid] + y[tid];\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        z[tid] = alpha * x[tid] + y[tid];\n    }\n}\n",
      "\n    size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        z[index] = alpha * x[index] + y[index];\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        z[tid] = alpha * x[tid] + y[tid];\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        z[tid] = alpha * x[tid] + y[tid];\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n",
      "\n    size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        z[index] = alpha * x[index] + y[index];\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n",
      "\n    size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        z[index] = alpha * x[index] + y[index];\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "03_dense_la_axpy",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Store the result in z on rank 0.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_z(local_n);\n\n    for (int i = 0; i < local_n; i++) {\n        local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n    }\n\n    if (rank == 0) {\n        z.resize(x.size());\n    }\n\n    MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_z(local_n);\n\n    for (int i = 0; i < local_n; i++) {\n        local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n    }\n\n    if (rank == 0) {\n        z.resize(x.size());\n    }\n\n    MPI_Gather(local_z.data(), local_n, MPI_DOUBLE, z.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_z(local_n);\n\n    for (int i = 0; i < local_n; i++) {\n        local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n    }\n\n    if (rank == 0) {\n        z.resize(x.size());\n    }\n\n    MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_z(local_n);\n\n    for (int i = 0; i < local_n; i++) {\n        local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n    }\n\n    if (rank == 0) {\n        z.resize(x.size());\n    }\n\n    MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_z(local_n);\n\n    for (int i = 0; i < local_n; i++) {\n        local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n    }\n\n    if (rank == 0) {\n        z.resize(x.size());\n    }\n\n    MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_z(local_n);\n\n    for (int i = 0; i < local_n; i++) {\n        local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n    }\n\n    if (rank == 0) {\n        z.resize(x.size());\n    }\n\n    MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_z(local_n);\n\n    for (int i = 0; i < local_n; i++) {\n        local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n    }\n\n    if (rank == 0) {\n        z.resize(x.size());\n    }\n\n    MPI_Gather(local_z.data(), local_n, MPI_DOUBLE, z.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_z(local_n);\n\n    for (int i = 0; i < local_n; i++) {\n        local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n    }\n\n    if (rank == 0) {\n        z.resize(x.size());\n    }\n\n    MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        z.resize(x.size());\n        for (int i = 0; i < x.size(); ++i) {\n            z[i] = alpha * x[i] + y[i];\n        }\n    }\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_z(local_n);\n\n    for (int i = 0; i < local_n; i++) {\n        local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n    }\n\n    if (rank == 0) {\n        z.resize(x.size());\n        std::vector<double> recvbuf(local_n);\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&recvbuf[0], local_n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < local_n; j++) {\n                z[i * local_n + j] = recvbuf[j];\n            }\n        }\n        for (int i = 0; i < local_n; i++) {\n            z[i] = local_z[i];\n        }\n    } else {\n        MPI_Send(&local_z[0], local_n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_z(local_n);\n\n    for (int i = 0; i < local_n; i++) {\n        local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n    }\n\n    if (rank == 0) {\n        z.resize(x.size());\n    }\n\n    MPI_Gather(local_z.data(), local_n, MPI_DOUBLE, z.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_z(local_n);\n\n    for (int i = 0; i < local_n; i++) {\n        local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n    }\n\n    if (rank == 0) {\n        z.resize(x.size());\n    }\n\n    MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_z(local_n);\n\n    for (int i = 0; i < local_n; i++) {\n        local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n    }\n\n    if (rank == 0) {\n        z.resize(x.size());\n    }\n\n    MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_z(local_n);\n\n    for (int i = 0; i < local_n; i++) {\n        local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n    }\n\n    if (rank == 0) {\n        z.resize(x.size());\n    }\n\n    MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_z(local_n);\n\n    for (int i = 0; i < local_n; i++) {\n        local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n    }\n\n    if (rank == 0) {\n        z.resize(x.size());\n    }\n\n    MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_z(local_n);\n\n    for (int i = 0; i < local_n; i++) {\n        local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n    }\n\n    if (rank == 0) {\n        z.resize(x.size());\n        MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, NULL, 0, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_z(local_n);\n\n    for (int i = 0; i < local_n; i++) {\n        local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n    }\n\n    if (rank == 0) {\n        z.resize(x.size());\n    }\n\n    MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_z(local_n);\n\n    for (int i = 0; i < local_n; i++) {\n        local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n    }\n\n    if (rank == 0) {\n        z.resize(x.size());\n    }\n\n    MPI_Gather(local_z.data(), local_n, MPI_DOUBLE, z.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_z(local_n);\n\n    for (int i = 0; i < local_n; i++) {\n        local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n    }\n\n    if (rank == 0) {\n        z.resize(x.size());\n    }\n\n    MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_z(local_n);\n\n    for (int i = 0; i < local_n; i++) {\n        local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n    }\n\n    if (rank == 0) {\n        z.resize(x.size());\n    }\n\n    MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "03_dense_la_axpy",
    "parallelism_model": "cuda",
    "prompt": "/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\n__global__ void axpy(double alpha, const double *x, const double *y, double *z, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n",
      "\n    size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        z[index] = alpha * x[index] + y[index];\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        z[tid] = alpha * x[tid] + y[tid];\n    }\n}\n",
      "\n    size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        z[index] = alpha * x[index] + y[index];\n    }\n}\n",
      "\n    size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        z[index] = alpha * x[index] + y[index];\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n",
      "\n    size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        z[index] = alpha * x[index] + y[index];\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n",
      "\n    size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        z[index] = alpha * x[index] + y[index];\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n",
      "\n    size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        z[index] = alpha * x[index] + y[index];\n    }\n}\n",
      "\n    size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        z[index] = alpha * x[index] + y[index];\n    }\n}\n",
      "\n    size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        z[index] = alpha * x[index] + y[index];\n    }\n}\n",
      "\n    size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        z[index] = alpha * x[index] + y[index];\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        z[tid] = alpha * x[tid] + y[tid];\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "03_dense_la_axpy",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Store the result in z on rank 0.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::vector<double> temp_z(x.size());\n            MPI_Recv(temp_z.data(), x.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                z[j] += temp_z[j];\n            }\n        }\n    } else {\n        MPI_Send(z.data(), x.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::vector<double> temp_z(x.size());\n            MPI_Recv(temp_z.data(), x.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                z[j] += temp_z[j];\n            }\n        }\n    } else {\n        MPI_Send(z.data(), x.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_n;\n\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&z[0] + i * local_n, local_n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&z[local_start], local_n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&z[i * local_n], local_n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&z[start], local_n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::vector<double> temp_z(x.size());\n            MPI_Recv(temp_z.data(), x.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                z[j] += temp_z[j];\n            }\n        }\n    } else {\n        MPI_Send(z.data(), x.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n\n    if (rank == 0) {\n        for (int i = local_n * size; i < x.size(); i++) {\n            z[i] = alpha * x[i] + y[i];\n        }\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&z[i * local_n], local_n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&z[start], local_n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n\n    if (rank == 0) {\n        for (int i = local_size * size; i < x.size(); i++) {\n            z[i] = alpha * x[i] + y[i];\n        }\n        for (int i = 1; i < size; i++) {\n            std::vector<double> temp(local_size);\n            MPI_Recv(&temp[0], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < local_size; j++) {\n                z[i * local_size + j] = temp[j];\n            }\n        }\n    } else {\n        MPI_Send(&z[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&z[i * local_n], local_n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&z[start], local_n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::vector<double> temp(x.size());\n            MPI_Recv(temp.data(), x.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                z[j] += temp[j];\n            }\n        }\n    } else {\n        MPI_Send(z.data(), x.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? x.size() : (rank + 1) * local_n;\n\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&z[0] + i * local_n, local_n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&z[local_start], local_end - local_start, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::vector<double> temp_z(x.size());\n            MPI_Recv(temp_z.data(), x.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                z[j] += temp_z[j];\n            }\n        }\n    } else {\n        MPI_Send(z.data(), x.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&z[i * local_n], local_n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&z[start], local_n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&z[i * local_n], local_n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&z[start], local_n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n\n    if (rank == 0) {\n        for (int i = local_n * size; i < x.size(); i++) {\n            z[i] = alpha * x[i] + y[i];\n        }\n        for (int r = 1; r < size; r++) {\n            std::vector<double> temp(local_n);\n            MPI_Recv(&temp[0], local_n, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int i = 0; i < local_n; i++) {\n                z[r * local_n + i] = temp[i];\n            }\n        }\n    } else {\n        MPI_Send(&z[start], local_n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n\n    if (rank == 0) {\n        for (int i = local_n * size; i < x.size(); i++) {\n            z[i] = alpha * x[i] + y[i];\n        }\n        for (int i = 1; i < size; i++) {\n            std::vector<double> temp(local_n);\n            MPI_Recv(&temp[0], local_n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < local_n; j++) {\n                z[i * local_n + j] = temp[j];\n            }\n        }\n    } else {\n        MPI_Send(&z[start], local_n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&z[i * local_n], local_n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&z[start], local_n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::vector<double> tempZ(x.size());\n            MPI_Recv(tempZ.data(), x.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                z[j] += tempZ[j];\n            }\n        }\n    } else {\n        MPI_Send(z.data(), x.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::vector<double> tempZ(x.size());\n            MPI_Recv(tempZ.data(), x.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                z[j] += tempZ[j];\n            }\n        }\n    } else {\n        MPI_Send(z.data(), x.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::vector<double> tempZ(x.size());\n            MPI_Recv(tempZ.data(), x.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                z[j] += tempZ[j];\n            }\n        }\n    } else {\n        MPI_Send(z.data(), x.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        z[i] = alpha * x[i] + y[i];\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::vector<double> tempZ(x.size());\n            MPI_Recv(tempZ.data(), x.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                z[j] += tempZ[j];\n            }\n        }\n    } else {\n        MPI_Send(z.data(), x.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "01_dense_la_solve",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix. x and b have N elements.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(Kokkos::View<const double**> &A, Kokkos::View<const double*> &b, Kokkos::View<double*> &x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::View<double*> temp(\"temp\", N);\n  KokkosBlas::gemv(\"N\", 1.0, A, b, 0.0, temp);\n  Kokkos::deep_copy(x, temp);\n}",
      "\n  Kokkos::View<double*> tmp(\"tmp\", N);\n  KokkosBlas::gemv(\"N\", 1.0, A, b, 0.0, tmp);\n  Kokkos::deep_copy(x, tmp);\n}",
      "\n  Kokkos::View<double*> tmp(\"tmp\", N);\n  KokkosBlas::gemv(\"N\", 1.0, A, b, 0.0, tmp);\n  Kokkos::deep_copy(x, tmp);\n}",
      "\n    Kokkos::View<double**> A_transpose(\"A_transpose\", N, N);\n    KokkosBlas::transpose(A_transpose, A);\n\n    Kokkos::View<double*> b_copy(\"b_copy\", N);\n    Kokkos::deep_copy(b_copy, b);\n\n    Kokkos::View<double*> x_temp(\"x_temp\", N);\n    Kokkos::View<double*> x_copy(\"x_copy\", N);\n    Kokkos::deep_copy(x_copy, x);\n\n    Kokkos::View<double*> y(\"y\", N);\n    KokkosBlas::gemv(\"N\", 1.0, A_transpose, b_copy, 0.0, y);\n\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < i; ++j) {\n            sum += A_transpose(i, j) * x_temp(j);\n        }\n        x_temp(i) = (y(i) - sum) / A_transpose(i, i);\n    }\n\n    KokkosBlas::transpose(x, x_temp);\n}",
      "\n  Kokkos::View<double*> temp(\"temp\", N);\n  KokkosBlas::gemv(\"N\", 1.0, A, b, 0.0, temp);\n  Kokkos::deep_copy(x, temp);\n}",
      "\n  Kokkos::View<double**> A_copy(\"A_copy\", N, N);\n  Kokkos::deep_copy(A_copy, A);\n\n  Kokkos::View<double*> b_copy(\"b_copy\", N);\n  Kokkos::deep_copy(b_copy, b);\n\n  Kokkos::View<double*> x_copy(\"x_copy\", N);\n\n  Kokkos::View<double*> temp(\"temp\", N);\n\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    double pivot = A_copy(i, i);\n    for (int j = i + 1; j < N; j++) {\n      double ratio = A_copy(j, i) / pivot;\n      for (int k = i; k < N; k++) {\n        A_copy(j, k) -= ratio * A_copy(i, k);\n      }\n      b_copy(j) -= ratio * b_copy(i);\n    }\n  });\n\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    temp(i) = b_copy(i);\n    for (int j = 0; j < i; j++) {\n      temp(i) -= A_copy(i, j) * temp(j);\n    }\n  });\n\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    x_copy(i) = temp(i);\n    for (int j = N - 1; j > i; j--) {\n      x_copy(i) -= A_copy(i, j) * x_copy(j);\n    }\n    x_copy(i) /= A_copy(i, i);\n  });\n\n  Kokkos::deep_copy(x, x_copy);\n}\n",
      "\n    Kokkos::View<double*> tmp(\"tmp\", N);\n    KokkosBlas::gemv(\"N\", 1.0, A, b, 0.0, tmp);\n    Kokkos::deep_copy(x, tmp);\n}",
      "\n    Kokkos::View<double*> tmp(\"tmp\", N);\n    KokkosBlas::gemv(\"N\", 1.0, A, b, 0.0, tmp);\n    Kokkos::deep_copy(x, tmp);\n}",
      "\n  Kokkos::View<double*> tmp(\"tmp\", N);\n  KokkosBlas::gemv(\"N\", 1.0, A, b, 0.0, tmp);\n  Kokkos::deep_copy(x, tmp);\n}",
      "\n    Kokkos::View<double*> tmp(\"tmp\", N);\n    KokkosBlas::gemv(\"N\", 1.0, A, b, 0.0, tmp);\n    Kokkos::deep_copy(x, tmp);\n}",
      "\n  Kokkos::View<double*> tmp(\"tmp\", N);\n  KokkosBlas::gemv(\"N\", 1.0, A, b, 0.0, tmp);\n  Kokkos::deep_copy(x, tmp);\n}",
      "\n    Kokkos::View<double*> tmp(\"tmp\", N);\n    KokkosBlas::gemv(\"N\", 1.0, A, b, 0.0, tmp);\n    Kokkos::deep_copy(x, tmp);\n}",
      "\n    Kokkos::View<double*> tmp(\"tmp\", N);\n    KokkosBlas::gemv(\"N\", 1.0, A, b, 0.0, tmp);\n    Kokkos::deep_copy(x, tmp);\n}",
      "\n  Kokkos::View<double*> tmp(\"tmp\", N);\n  KokkosBlas::gemv(\"N\", 1.0, A, b, 0.0, tmp);\n  Kokkos::deep_copy(x, tmp);\n}",
      "\n  Kokkos::View<double*> tmp(\"tmp\", N);\n  KokkosBlas::gemv(\"N\", 1.0, A, b, 0.0, tmp);\n  Kokkos::deep_copy(x, tmp);\n}",
      "\n    Kokkos::View<double*> tmp(\"tmp\", N);\n    KokkosBlas::gemv(\"N\", 1.0, A, b, 0.0, tmp);\n    Kokkos::deep_copy(x, tmp);\n}",
      "\n  Kokkos::View<double*> tmp(\"tmp\", N);\n  KokkosBlas::gemv(\"N\", 1.0, A, b, 0.0, tmp);\n  Kokkos::deep_copy(x, tmp);\n```\n}",
      "\n  Kokkos::View<double*> temp(\"temp\", N);\n  Kokkos::deep_copy(temp, b);\n\n  for (size_t k = 0; k < N; k++) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t i) {\n      if (i > k) {\n        double ratio = A(i, k) / A(k, k);\n        for (size_t j = 0; j < N; j++) {\n          A(i, j) -= ratio * A(k, j);\n        }\n        temp(i) -= ratio * temp(k);\n      }\n    });\n  }\n\n  for (int i = N - 1; i >= 0; i--) {\n    double sum = 0.0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const size_t j, double &local_sum) {\n      if (j > i) {\n        local_sum += A(i, j) * x(j);\n      }\n    }, sum);\n    x(i) = (temp(i) - sum) / A(i, i);\n  }\n}\n",
      "\n  Kokkos::View<double*> tmp(\"tmp\", N);\n  KokkosBlas::gemv(\"N\", 1.0, A, b, 0.0, tmp);\n  Kokkos::deep_copy(x, tmp);\n```\n}",
      "\n  Kokkos::View<double*> temp(\"temp\", N);\n  KokkosBlas::gemv(\"N\", 1.0, A, b, 0.0, temp);\n  Kokkos::deep_copy(x, temp);\n}"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "01_dense_la_solve",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            x[i] += A[i * N + j] * b[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            x[i] += A[i * N + j] * b[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            x[i] += A[i * N + j] * b[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            x[i] += A[i * N + j] * b[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            x[i] += A[i * N + j] * b[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            x[i] += A[i * N + j] * b[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            x[i] += A[i * N + j] * b[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            x[i] += A[i * N + j] * b[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            x[i] += A[i * N + j] * b[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            x[i] += A[i * N + j] * b[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            x[i] += A[i * N + j] * b[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            x[i] += A[i * N + j] * b[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            x[i] += A[i * N + j] * b[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            x[i] += A[i * N + j] * b[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            x[i] += A[i * N + j] * b[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            x[i] += A[i * N + j] * b[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            x[i] += A[i * N + j] * b[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            x[i] += A[i * N + j] * b[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        x[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            x[i] += A[i * N + j] * b[j];\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "01_dense_la_solve",
    "parallelism_model": "serial",
    "prompt": "/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // Create a copy of A to avoid modifying the original matrix\n    std::vector<double> A_copy = A;\n\n    // Perform Gaussian elimination to transform A into an upper triangular matrix\n    for (size_t k = 0; k < N - 1; ++k) {\n        for (size_t i = k + 1; i < N; ++i) {\n            double factor = A_copy[i * N + k] / A_copy[k * N + k];\n            for (size_t j = k; j < N; ++j) {\n                A_copy[i * N + j] -= factor * A_copy[k * N + j];\n            }\n            b[i] -= factor * b[k];\n        }\n    }\n\n    // Back substitution to solve for x\n    for (int i = N - 1; i >= 0; --i) {\n        x[i] = b[i];\n        for (size_t j = i + 1; j < N; ++j) {\n            x[i] -= A_copy[i * N + j] * x[j];\n        }\n        x[i] /= A_copy[i * N + i];\n    }\n}\n",
      "\n    // Create a copy of A and b to avoid modifying the original vectors\n    std::vector<double> Acopy = A;\n    std::vector<double> bcopy = b;\n\n    // Perform Gaussian elimination to transform A into an upper triangular matrix\n    for (size_t k = 0; k < N-1; ++k) {\n        for (size_t i = k+1; i < N; ++i) {\n            double factor = Acopy[i*N+k] / Acopy[k*N+k];\n            for (size_t j = k; j < N; ++j) {\n                Acopy[i*N+j] -= factor * Acopy[k*N+j];\n            }\n            bcopy[i] -= factor * bcopy[k];\n        }\n    }\n\n    // Back substitution to solve for x\n    for (int i = N-1; i >= 0; --i) {\n        x[i] = bcopy[i];\n        for (size_t j = i+1; j < N; ++j) {\n            x[i] -= Acopy[i*N+j] * x[j];\n        }\n        x[i] /= Acopy[i*N+i];\n    }\n}\n",
      "\n    // Create a copy of A to avoid modifying the original matrix\n    std::vector<double> A_copy = A;\n\n    // Perform Gaussian elimination to transform A into an upper triangular matrix\n    for (size_t k = 0; k < N - 1; ++k) {\n        for (size_t i = k + 1; i < N; ++i) {\n            double factor = A_copy[i * N + k] / A_copy[k * N + k];\n            for (size_t j = k; j < N; ++j) {\n                A_copy[i * N + j] -= factor * A_copy[k * N + j];\n            }\n            b[i] -= factor * b[k];\n        }\n    }\n\n    // Back substitution to solve for x\n    for (int i = N - 1; i >= 0; --i) {\n        x[i] = b[i];\n        for (size_t j = i + 1; j < N; ++j) {\n            x[i] -= A_copy[i * N + j] * x[j];\n        }\n        x[i] /= A_copy[i * N + i];\n    }\n}\n",
      "\n    // Create a copy of A to avoid modifying the original matrix\n    std::vector<double> A_copy = A;\n\n    // Perform Gaussian elimination to transform A into an upper triangular matrix\n    for (size_t k = 0; k < N - 1; ++k) {\n        for (size_t i = k + 1; i < N; ++i) {\n            double factor = A_copy[i * N + k] / A_copy[k * N + k];\n            for (size_t j = k; j < N; ++j) {\n                A_copy[i * N + j] -= factor * A_copy[k * N + j];\n            }\n            b[i] -= factor * b[k];\n        }\n    }\n\n    // Back substitution to solve for x\n    for (int i = N - 1; i >= 0; --i) {\n        x[i] = b[i];\n        for (int j = i + 1; j < N; ++j) {\n            x[i] -= A_copy[i * N + j] * x[j];\n        }\n        x[i] /= A_copy[i * N + i];\n    }\n}\n",
      "\n    // Create a copy of A to avoid modifying the original matrix\n    std::vector<double> A_copy = A;\n\n    // Gaussian elimination\n    for (size_t i = 0; i < N; ++i) {\n        // Find the pivot row\n        size_t max_row = i;\n        for (size_t j = i + 1; j < N; ++j) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[max_row * N + i])) {\n                max_row = j;\n            }\n        }\n\n        // Swap the current row with the pivot row\n        if (max_row != i) {\n            for (size_t k = 0; k < N; ++k) {\n                std::swap(A_copy[i * N + k], A_copy[max_row * N + k]);\n            }\n            std::swap(b[i], b[max_row]);\n        }\n\n        // Perform row operations to create upper triangular matrix\n        for (size_t j = i + 1; j < N; ++j) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = i; k < N; ++k) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b[j] -= factor * b[i];\n        }\n    }\n\n    // Back substitution to solve for x\n    for (int i = N - 1; i >= 0; --i) {\n        x[i] = b[i];\n        for (size_t j = i + 1; j < N; ++j) {\n            x[i] -= A_copy[i * N + j] * x[j];\n        }\n        x[i] /= A_copy[i * N + i];\n    }\n}\n",
      "\n    // Create a copy of A to avoid modifying the original matrix\n    std::vector<double> A_copy = A;\n\n    // Perform Gaussian elimination to transform A into an upper triangular matrix\n    for (size_t k = 0; k < N - 1; ++k) {\n        for (size_t i = k + 1; i < N; ++i) {\n            double factor = A_copy[i * N + k] / A_copy[k * N + k];\n            for (size_t j = k; j < N; ++j) {\n                A_copy[i * N + j] -= factor * A_copy[k * N + j];\n            }\n            b[i] -= factor * b[k];\n        }\n    }\n\n    // Perform back substitution to solve for x\n    for (int i = N - 1; i >= 0; --i) {\n        x[i] = b[i];\n        for (size_t j = i + 1; j < N; ++j) {\n            x[i] -= A_copy[i * N + j] * x[j];\n        }\n        x[i] /= A_copy[i * N + i];\n    }\n}\n",
      "\n    // Create a copy of A to avoid modifying the original matrix\n    std::vector<double> A_copy = A;\n\n    // Gaussian elimination\n    for (size_t i = 0; i < N; ++i) {\n        // Find pivot\n        size_t max_row = i;\n        for (size_t j = i + 1; j < N; ++j) {\n            if (std::abs(A_copy[j*N + i]) > std::abs(A_copy[max_row*N + i])) {\n                max_row = j;\n            }\n        }\n        if (max_row != i) {\n            // Swap rows\n            for (size_t k = 0; k < N; ++k) {\n                std::swap(A_copy[i*N + k], A_copy[max_row*N + k]);\n            }\n            std::swap(b[i], b[max_row]);\n        }\n\n        // Perform elimination\n        for (size_t j = i + 1; j < N; ++j) {\n            double factor = A_copy[j*N + i] / A_copy[i*N + i];\n            for (size_t k = i; k < N; ++k) {\n                A_copy[j*N + k] -= factor * A_copy[i*N + k];\n            }\n            b[j] -= factor * b[i];\n        }\n    }\n\n    // Back substitution\n    for (int i = N - 1; i >= 0; --i) {\n        x[i] = b[i];\n        for (size_t j = i + 1; j < N; ++j) {\n            x[i] -= A_copy[i*N + j] * x[j];\n        }\n        x[i] /= A_copy[i*N + i];\n    }\n}\n",
      "\n    // Create a copy of A to avoid modifying the original matrix\n    std::vector<double> A_copy = A;\n\n    // Gaussian elimination\n    for (size_t i = 0; i < N; ++i) {\n        // Find pivot\n        size_t max_row = i;\n        for (size_t k = i + 1; k < N; ++k) {\n            if (std::abs(A_copy[k*N + i]) > std::abs(A_copy[max_row*N + i])) {\n                max_row = k;\n            }\n        }\n\n        // Swap rows to make the pivot the largest element\n        if (max_row != i) {\n            for (size_t j = 0; j < N; ++j) {\n                std::swap(A_copy[i*N + j], A_copy[max_row*N + j]);\n            }\n            std::swap(b[i], b[max_row]);\n        }\n\n        // Perform row operations to eliminate the elements below the pivot\n        for (size_t k = i + 1; k < N; ++k) {\n            double factor = A_copy[k*N + i] / A_copy[i*N + i];\n            for (size_t j = i; j < N; ++j) {\n                A_copy[k*N + j] -= factor * A_copy[i*N + j];\n            }\n            b[k] -= factor * b[i];\n        }\n    }\n\n    // Back substitution\n    for (int i = N - 1; i >= 0; --i) {\n        x[i] = b[i];\n        for (size_t j = i + 1; j < N; ++j) {\n            x[i] -= A_copy[i*N + j] * x[j];\n        }\n        x[i] /= A_copy[i*N + i];\n    }\n}\n",
      "\n    // Create a copy of A to avoid modifying the original matrix\n    std::vector<double> A_copy = A;\n\n    // Gaussian elimination\n    for (size_t i = 0; i < N; ++i) {\n        // Find pivot row and swap\n        size_t max_row = i;\n        for (size_t k = i + 1; k < N; ++k) {\n            if (std::abs(A_copy[k*N + i]) > std::abs(A_copy[max_row*N + i])) {\n                max_row = k;\n            }\n        }\n        if (max_row != i) {\n            for (size_t j = 0; j < N; ++j) {\n                std::swap(A_copy[i*N + j], A_copy[max_row*N + j]);\n            }\n            std::swap(b[i], b[max_row]);\n        }\n\n        // Perform forward elimination\n        for (size_t k = i + 1; k < N; ++k) {\n            double factor = A_copy[k*N + i] / A_copy[i*N + i];\n            for (size_t j = i; j < N; ++j) {\n                A_copy[k*N + j] -= factor * A_copy[i*N + j];\n            }\n            b[k] -= factor * b[i];\n        }\n    }\n\n    // Back substitution\n    for (int i = N - 1; i >= 0; --i) {\n        x[i] = b[i];\n        for (size_t j = i + 1; j < N; ++j) {\n            x[i] -= A_copy[i*N + j] * x[j];\n        }\n        x[i] /= A_copy[i*N + i];\n    }\n}\n",
      "\n    // Create a copy of A to avoid modifying the original matrix\n    std::vector<double> A_copy = A;\n\n    // Gaussian elimination\n    for (size_t i = 0; i < N; ++i) {\n        // Find pivot row and swap\n        size_t max_row = i;\n        for (size_t k = i + 1; k < N; ++k) {\n            if (std::abs(A_copy[k * N + i]) > std::abs(A_copy[max_row * N + i])) {\n                max_row = k;\n            }\n        }\n        if (max_row != i) {\n            for (size_t j = 0; j < N; ++j) {\n                std::swap(A_copy[i * N + j], A_copy[max_row * N + j]);\n            }\n            std::swap(b[i], b[max_row]);\n        }\n\n        // Perform row operations\n        for (size_t k = i + 1; k < N; ++k) {\n            double factor = A_copy[k * N + i] / A_copy[i * N + i];\n            for (size_t j = i; j < N; ++j) {\n                A_copy[k * N + j] -= factor * A_copy[i * N + j];\n            }\n            b[k] -= factor * b[i];\n        }\n    }\n\n    // Back substitution\n    for (int i = N - 1; i >= 0; --i) {\n        x[i] = b[i];\n        for (size_t j = i + 1; j < N; ++j) {\n            x[i] -= A_copy[i * N + j] * x[j];\n        }\n        x[i] /= A_copy[i * N + i];\n    }\n}\n",
      "\n    // Create a matrix A_copy as a copy of A\n    std::vector<double> A_copy = A;\n\n    // Perform Gaussian elimination to transform A_copy into an upper triangular matrix\n    for (size_t k = 0; k < N - 1; ++k) {\n        for (size_t i = k + 1; i < N; ++i) {\n            double factor = A_copy[i * N + k] / A_copy[k * N + k];\n            for (size_t j = k; j < N; ++j) {\n                A_copy[i * N + j] -= factor * A_copy[k * N + j];\n            }\n            b[i] -= factor * b[k];\n        }\n    }\n\n    // Back substitution to solve for x\n    for (int i = N - 1; i >= 0; --i) {\n        x[i] = b[i] / A_copy[i * N + i];\n        for (int j = i + 1; j < N; ++j) {\n            x[i] -= A_copy[i * N + j] / A_copy[i * N + i] * x[j];\n        }\n    }\n}\n",
      "\n    // Create a copy of A and b to avoid modifying the original vectors\n    std::vector<double> Acopy = A;\n    std::vector<double> bcopy = b;\n\n    // Perform Gaussian elimination to transform A into an upper triangular matrix\n    for (size_t k = 0; k < N-1; ++k) {\n        for (size_t i = k+1; i < N; ++i) {\n            double factor = Acopy[i*N+k] / Acopy[k*N+k];\n            for (size_t j = k; j < N; ++j) {\n                Acopy[i*N+j] -= factor * Acopy[k*N+j];\n            }\n            bcopy[i] -= factor * bcopy[k];\n        }\n    }\n\n    // Perform back substitution to solve for x\n    for (int i = N-1; i >= 0; --i) {\n        x[i] = bcopy[i];\n        for (size_t j = i+1; j < N; ++j) {\n            x[i] -= Acopy[i*N+j] * x[j];\n        }\n        x[i] /= Acopy[i*N+i];\n    }\n}\n",
      "\n    // Create a copy of A to avoid modifying the original matrix\n    std::vector<double> A_copy = A;\n\n    // Perform Gaussian elimination to transform A into an upper triangular matrix\n    for (size_t k = 0; k < N - 1; ++k) {\n        for (size_t i = k + 1; i < N; ++i) {\n            double factor = A_copy[i * N + k] / A_copy[k * N + k];\n            for (size_t j = k; j < N; ++j) {\n                A_copy[i * N + j] -= factor * A_copy[k * N + j];\n            }\n            b[i] -= factor * b[k];\n        }\n    }\n\n    // Back substitution to solve for x\n    for (int i = N - 1; i >= 0; --i) {\n        x[i] = b[i];\n        for (size_t j = i + 1; j < N; ++j) {\n            x[i] -= A_copy[i * N + j] * x[j];\n        }\n        x[i] /= A_copy[i * N + i];\n    }\n}\n",
      "\n    // Create a copy of A to avoid modifying the original matrix\n    std::vector<double> A_copy = A;\n\n    // Perform Gaussian elimination to transform A into an upper triangular matrix\n    for (size_t k = 0; k < N - 1; ++k) {\n        for (size_t i = k + 1; i < N; ++i) {\n            double factor = A_copy[i * N + k] / A_copy[k * N + k];\n            for (size_t j = k; j < N; ++j) {\n                A_copy[i * N + j] -= factor * A_copy[k * N + j];\n            }\n            b[i] -= factor * b[k];\n        }\n    }\n\n    // Perform back substitution to solve for x\n    for (int i = N - 1; i >= 0; --i) {\n        x[i] = b[i];\n        for (size_t j = i + 1; j < N; ++j) {\n            x[i] -= A_copy[i * N + j] * x[j];\n        }\n        x[i] /= A_copy[i * N + i];\n    }\n}\n",
      "\n    // Create a copy of A to avoid modifying the original matrix\n    std::vector<double> A_copy = A;\n\n    // Gaussian elimination\n    for (size_t i = 0; i < N; ++i) {\n        // Find pivot row and swap\n        size_t max_row = i;\n        for (size_t k = i + 1; k < N; ++k) {\n            if (std::abs(A_copy[k*N + i]) > std::abs(A_copy[max_row*N + i])) {\n                max_row = k;\n            }\n        }\n        if (max_row != i) {\n            for (size_t j = 0; j < N; ++j) {\n                std::swap(A_copy[i*N + j], A_copy[max_row*N + j]);\n            }\n            std::swap(b[i], b[max_row]);\n        }\n\n        // Perform forward elimination\n        for (size_t k = i + 1; k < N; ++k) {\n            double factor = A_copy[k*N + i] / A_copy[i*N + i];\n            for (size_t j = i; j < N; ++j) {\n                A_copy[k*N + j] -= factor * A_copy[i*N + j];\n            }\n            b[k] -= factor * b[i];\n        }\n    }\n\n    // Perform back substitution\n    for (int i = N - 1; i >= 0; --i) {\n        x[i] = b[i];\n        for (size_t j = i + 1; j < N; ++j) {\n            x[i] -= A_copy[i*N + j] * x[j];\n        }\n        x[i] /= A_copy[i*N + i];\n    }\n}\n",
      "\n    // Create a copy of A to avoid modifying the original matrix\n    std::vector<double> A_copy = A;\n\n    // Gaussian elimination\n    for (size_t i = 0; i < N; ++i) {\n        // Find pivot row and swap\n        size_t max_row = i;\n        for (size_t k = i + 1; k < N; ++k) {\n            if (std::abs(A_copy[k * N + i]) > std::abs(A_copy[max_row * N + i])) {\n                max_row = k;\n            }\n        }\n        if (max_row != i) {\n            for (size_t j = 0; j < N; ++j) {\n                std::swap(A_copy[i * N + j], A_copy[max_row * N + j]);\n            }\n            std::swap(b[i], b[max_row]);\n        }\n\n        // Perform row operations\n        for (size_t k = i + 1; k < N; ++k) {\n            double factor = A_copy[k * N + i] / A_copy[i * N + i];\n            for (size_t j = i; j < N; ++j) {\n                A_copy[k * N + j] -= factor * A_copy[i * N + j];\n            }\n            b[k] -= factor * b[i];\n        }\n    }\n\n    // Back substitution\n    for (int i = N - 1; i >= 0; --i) {\n        x[i] = b[i];\n        for (size_t j = i + 1; j < N; ++j) {\n            x[i] -= A_copy[i * N + j] * x[j];\n        }\n        x[i] /= A_copy[i * N + i];\n    }\n}\n",
      "\n    // Create a copy of A to avoid modifying the original matrix\n    std::vector<double> A_copy = A;\n\n    // Gaussian elimination\n    for (size_t i = 0; i < N; ++i) {\n        // Find the pivot row\n        size_t max_row = i;\n        for (size_t j = i + 1; j < N; ++j) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[max_row * N + i])) {\n                max_row = j;\n            }\n        }\n\n        // Swap the current row with the pivot row\n        if (max_row != i) {\n            for (size_t k = 0; k < N; ++k) {\n                std::swap(A_copy[i * N + k], A_copy[max_row * N + k]);\n            }\n            std::swap(b[i], b[max_row]);\n        }\n\n        // Perform row operations to create upper triangular matrix\n        for (size_t j = i + 1; j < N; ++j) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = i; k < N; ++k) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b[j] -= factor * b[i];\n        }\n    }\n\n    // Back substitution to solve for x\n    for (int i = N - 1; i >= 0; --i) {\n        x[i] = b[i];\n        for (size_t j = i + 1; j < N; ++j) {\n            x[i] -= A_copy[i * N + j] * x[j];\n        }\n        x[i] /= A_copy[i * N + i];\n    }\n}\n",
      "\n    // Create a copy of A to avoid modifying the original matrix\n    std::vector<double> A_copy = A;\n\n    // Gaussian elimination\n    for (size_t i = 0; i < N; ++i) {\n        // Find the pivot row\n        size_t max_row = i;\n        for (size_t j = i + 1; j < N; ++j) {\n            if (std::abs(A_copy[j*N + i]) > std::abs(A_copy[max_row*N + i])) {\n                max_row = j;\n            }\n        }\n\n        // Swap the current row with the pivot row\n        if (max_row != i) {\n            for (size_t k = 0; k < N; ++k) {\n                std::swap(A_copy[i*N + k], A_copy[max_row*N + k]);\n            }\n            std::swap(b[i], b[max_row]);\n        }\n\n        // Perform row operations to create upper triangular matrix\n        for (size_t j = i + 1; j < N; ++j) {\n            double factor = A_copy[j*N + i] / A_copy[i*N + i];\n            for (size_t k = i; k < N; ++k) {\n                A_copy[j*N + k] -= factor * A_copy[i*N + k];\n            }\n            b[j] -= factor * b[i];\n        }\n    }\n\n    // Back substitution to solve for x\n    for (int i = N - 1; i >= 0; --i) {\n        x[i] = b[i];\n        for (size_t j = i + 1; j < N; ++j) {\n            x[i] -= A_copy[i*N + j] * x[j];\n        }\n        x[i] /= A_copy[i*N + i];\n    }\n}\n",
      "\n    // Create a copy of A and b to avoid modifying the input vectors\n    std::vector<double> Acopy = A;\n    std::vector<double> bcopy = b;\n\n    // Gaussian elimination\n    for (size_t i = 0; i < N; ++i) {\n        // Find pivot for column i\n        size_t max_row = i;\n        for (size_t k = i + 1; k < N; ++k) {\n            if (std::abs(Acopy[k * N + i]) > std::abs(Acopy[max_row * N + i])) {\n                max_row = k;\n            }\n        }\n\n        // Swap the max_row and i-th row\n        for (size_t k = i; k < N; ++k) {\n            std::swap(Acopy[i * N + k], Acopy[max_row * N + k]);\n        }\n        std::swap(bcopy[i], bcopy[max_row]);\n\n        // Make all rows below this one 0 in current column\n        for (size_t k = i + 1; k < N; ++k) {\n            double factor = -Acopy[k * N + i] / Acopy[i * N + i];\n            for (size_t j = i; j < N; ++j) {\n                if (i == j) {\n                    Acopy[k * N + j] = 0;\n                } else {\n                    Acopy[k * N + j] += factor * Acopy[i * N + j];\n                }\n            }\n            bcopy[k] += factor * bcopy[i];\n        }\n    }\n\n    // Back substitution\n    x.resize(N);\n    for (int i = N - 1; i >= 0; --i) {\n        x[i] = bcopy[i] / Acopy[i * N + i];\n        for (int k = i - 1; k >= 0; --k) {\n            bcopy[k] -= Acopy[k * N + i] * x[i];\n        }\n    }\n}\n",
      "\n    // Create a copy of A to avoid modifying the original matrix\n    std::vector<double> A_copy = A;\n\n    // Perform Gaussian elimination to transform A into an upper triangular matrix\n    for (size_t k = 0; k < N - 1; ++k) {\n        for (size_t i = k + 1; i < N; ++i) {\n            double factor = A_copy[i * N + k] / A_copy[k * N + k];\n            for (size_t j = k; j < N; ++j) {\n                A_copy[i * N + j] -= factor * A_copy[k * N + j];\n            }\n            b[i] -= factor * b[k];\n        }\n    }\n\n    // Back substitution to solve for x\n    for (int i = N - 1; i >= 0; --i) {\n        x[i] = b[i];\n        for (size_t j = i + 1; j < N; ++j) {\n            x[i] -= A_copy[i * N + j] * x[j];\n        }\n        x[i] /= A_copy[i * N + i];\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "01_dense_la_solve",
    "parallelism_model": "hip",
    "prompt": "/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\n__global__ void solveLinearSystem(const double *A, const double *b, double *x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; j++) {\n            sum += A[tid * N + j] * x[j];\n        }\n        x[tid] = (b[tid] - sum) / A[tid * N + tid];\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[idx * N + j] * x[j];\n        }\n        x[idx] = (b[idx] - sum) / A[idx * N + idx];\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[idx * N + j] * x[j];\n        }\n        x[idx] = (b[idx] - sum) / A[idx * N + idx];\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[idx * N + j] * x[j];\n        }\n        x[idx] = (b[idx] - sum) / A[idx * N + idx];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[tid * N + j] * x[j];\n        }\n        x[tid] = (b[tid] - sum) / A[tid * N + tid];\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[idx * N + j] * x[j];\n        }\n        x[idx] = (b[idx] - sum) / A[idx * N + idx];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[tid * N + j] * x[j];\n        }\n        x[tid] = (b[tid] - sum) / A[tid * N + tid];\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[idx * N + j] * x[j];\n        }\n        x[idx] = (b[idx] - sum) / A[idx * N + idx];\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[tid * N + j] * x[j];\n        }\n        x[tid] = (b[tid] - sum) / A[tid * N + tid];\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[idx * N + j] * x[j];\n        }\n        x[idx] = (b[idx] - sum) / A[idx * N + idx];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[tid * N + j] * x[j];\n        }\n        x[tid] = (b[tid] - sum) / A[tid * N + tid];\n    }\n}\n",
      "\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[idx * N + j] * x[j];\n        }\n        x[idx] = (b[idx] - sum) / A[idx * N + idx];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; j++) {\n            sum += A[tid * N + j] * x[j];\n        }\n        x[tid] = (b[tid] - sum) / A[tid * N + tid];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[tid * N + j] * x[j];\n        }\n        x[tid] = (b[tid] - sum) / A[tid * N + tid];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; j++) {\n            sum += A[tid * N + j] * x[j];\n        }\n        x[tid] = (b[tid] - sum) / A[tid * N + tid];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[tid * N + j] * x[j];\n        }\n        x[tid] = (b[tid] - sum) / A[tid * N + tid];\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        double sum = 0.0;\n        for (int i = 0; i < N; ++i) {\n            sum += A[tid * N + i] * x[i];\n        }\n        x[tid] = (b[tid] - sum) / A[tid * N + tid];\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; j++) {\n            sum += A[idx * N + j] * x[j];\n        }\n        x[idx] = (b[idx] - sum) / A[idx * N + idx];\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[tid * N + j] * x[j];\n        }\n        x[tid] = (b[tid] - sum) / A[tid * N + tid];\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "01_dense_la_solve",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and b. Store the result in x on rank 0.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the calculations for x on rank 0\n        // ...\n    } else {\n        // Other ranks do nothing\n    }\n}\n",
      "\n    // Implement the function body here\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation and store the result in x\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the serial computation for rank 0\n        // ...\n    } else {\n        // Perform the parallel computation for other ranks\n        // ...\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        // ...\n        // Store the result in x\n    }\n}\n",
      "\n    // Implement the function body here\n}",
      "\n    // TODO: Implement the function body here\n}",
      "\n    // Implement the function body here\n}",
      "\n    // Implement the code to solve the linear system using MPI\n    // This code will involve distributing the work among MPI ranks and then gathering the results on rank 0\n    // The specific implementation will depend on the parallel algorithm used\n    // It may involve matrix operations such as LU decomposition, Gaussian elimination, or other parallel linear system solvers\n    // The result should be stored in the vector x on rank 0\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the linear system solving on rank 0\n        // and store the result in x\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation and store the result in x\n    }\n}\n",
      "\n    // Add your code here\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        // ...\n    } else {\n        // Other ranks do nothing\n    }\n}\n",
      "\n    // Implement the function body here\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the serial computation on rank 0\n        // ...\n    } else {\n        // Perform the parallel computation on other ranks\n        // ...\n    }\n}\n",
      "\n    // Your code here\n}",
      "\n  // TODO: Write the body of the function here\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        // ...\n        // Store the result in x\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        // ...\n    } else {\n        // Other ranks do nothing\n    }\n}\n",
      "\n  // Implement the solution here using MPI\n  // Only rank 0 will store the result in x\n  if (MPI::COMM_WORLD.Get_rank() == 0) {\n      // Perform the computation and store the result in x\n  }\n}"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "01_dense_la_solve",
    "parallelism_model": "cuda",
    "prompt": "/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\n__global__ void solveLinearSystem(const double *A, const double *b, double *x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[idx * N + j] * x[j];\n        }\n        x[idx] = (b[idx] - sum) / A[idx * N + idx];\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0.0;\n        for (int i = 0; i < N; ++i) {\n            sum += A[tid * N + i] * x[i];\n        }\n        x[tid] = (b[tid] - sum) / A[tid * N + tid];\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[idx * N + j] * x[j];\n        }\n        x[idx] = (b[idx] - sum) / A[idx * N + idx];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0.0;\n        for (int i = 0; i < N; ++i) {\n            sum += A[tid * N + i] * x[i];\n        }\n        x[tid] = (b[tid] - sum) / A[tid * N + tid];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0.0;\n        for (int i = 0; i < N; i++) {\n            sum += A[tid * N + i] * x[i];\n        }\n        x[tid] = (b[tid] - sum) / A[tid * N + tid];\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[idx * N + j] * x[j];\n        }\n        x[idx] = (b[idx] - sum) / A[idx * N + idx];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0.0;\n        for (int i = 0; i < N; i++) {\n            sum += A[tid * N + i] * x[i];\n        }\n        x[tid] = (b[tid] - sum) / A[tid * N + tid];\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[idx * N + j] * x[j];\n        }\n        x[idx] = (b[idx] - sum) / A[idx * N + idx];\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[idx * N + j] * x[j];\n        }\n        x[idx] = (b[idx] - sum) / A[idx * N + idx];\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[idx * N + j] * x[j];\n        }\n        x[idx] = (b[idx] - sum) / A[idx * N + idx];\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0.0;\n        for (int i = 0; i < N; ++i) {\n            sum += A[tid * N + i] * x[i];\n        }\n        x[tid] = (b[tid] - sum) / A[tid * N + tid];\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        x[i] = (b[i] - sum) / A[i * N + i];\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[idx * N + j] * x[j];\n        }\n        x[idx] = (b[idx] - sum) / A[idx * N + idx];\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[idx * N + j] * x[j];\n        }\n        x[idx] = (b[idx] - sum) / A[idx * N + idx];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[tid * N + j] * x[j];\n        }\n        x[tid] = (b[tid] - sum) / A[tid * N + tid];\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[idx * N + j] * x[j];\n        }\n        x[idx] = (b[idx] - sum) / A[idx * N + idx];\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[idx * N + j] * x[j];\n        }\n        x[idx] = (b[idx] - sum) / A[idx * N + idx];\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[tid * N + j] * x[j];\n        }\n        x[tid] = (b[tid] - sum) / A[tid * N + tid];\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "01_dense_la_solve",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and b. Store the result in x on rank 0.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                sum += A[i * N + j] * x[j];\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        x.resize(N);\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * b[j];\n        }\n        if (rank == 0) {\n            x[i] = sum;\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(x.data(), N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int r = 1; r < size; ++r) {\n            std::vector<double> temp(N);\n            MPI_Recv(temp.data(), N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t i = 0; i < N; ++i) {\n                x[i] += temp[i];\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                sum += A[i * N + j] * x[j];\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                sum += A[i * N + j] * x[j];\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        x.resize(N);\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * b[j];\n        }\n        if (rank == 0) {\n            x[i] = sum;\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(x.data(), N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int r = 1; r < size; ++r) {\n            std::vector<double> recv_x(N);\n            MPI_Recv(recv_x.data(), N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t i = 0; i < N; ++i) {\n                x[i] += recv_x[i];\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                sum += A[i * N + j] * x[j];\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                sum += A[i * N + j] * x[j];\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        x.resize(N);\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * b[j];\n        }\n        if (rank == 0) {\n            x[i] = sum;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> partial_result(N);\n            MPI_Recv(partial_result.data(), N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < N; ++j) {\n                x[j] += partial_result[j];\n            }\n        }\n    } else {\n        MPI_Send(x.data(), N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        x.resize(N);\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * b[j];\n        }\n        if (rank == 0) {\n            x[i] = sum;\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(x.data(), N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> temp(N);\n            MPI_Recv(temp.data(), N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < N; ++j) {\n                x[j] += temp[j];\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                sum += A[i * N + j] * x[j];\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                sum += A[i * N + j] * x[j];\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        x.resize(N);\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * b[j];\n        }\n        if (rank == 0) {\n            x[i] = sum;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> recvBuf(N);\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(recvBuf.data(), N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < N; ++j) {\n                x[j] += recvBuf[j];\n            }\n        }\n    } else {\n        MPI_Send(x.data(), N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                sum += A[i * N + j] * x[j];\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                sum += A[i * N + j] * x[j];\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                sum += A[i * N + j] * x[j];\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                sum += A[i * N + j] * x[j];\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                sum += A[i * N + j] * x[j];\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        x.resize(N);\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * b[j];\n        }\n        if (rank == 0) {\n            x[i] = sum;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> partial_result(N);\n            MPI_Recv(partial_result.data(), N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < N; ++j) {\n                x[j] += partial_result[j];\n            }\n        }\n    } else {\n        MPI_Send(x.data(), N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                sum += A[i * N + j] * x[j];\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        // Perform the computation on rank 0\n        #pragma omp parallel for\n        for (size_t i = 0; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t j = 0; j < N; ++j) {\n                sum += A[i * N + j] * x[j];\n            }\n            x[i] = (b[i] - sum) / A[i * N + i];\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "02_dense_la_gemm",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(Kokkos::View<const double**> &A, Kokkos::View<const double**> &B, Kokkos::View<double**> &C, size_t M, size_t K, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(\"gemm\", M, K, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      C(i, j) = 0;\n      for (size_t k = 0; k < K; ++k) {\n        C(i, j) += A(i, k) * B(k, j);\n      }\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", M, K, KOKKOS_LAMBDA (const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      C(i, j) = 0;\n      for (size_t k = 0; k < K; ++k) {\n        C(i, j) += A(i, k) * B(k, j);\n      }\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", M, K, KOKKOS_LAMBDA (const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      C(i, j) = 0;\n      for (size_t k = 0; k < K; ++k) {\n        C(i, j) += A(i, k) * B(k, j);\n      }\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", M, K, KOKKOS_LAMBDA(const int i, const int j) {\n    for (size_t k = 0; k < N; ++k) {\n      C(i, j) += A(i, k) * B(k, j);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", M, K, KOKKOS_LAMBDA (const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      C(i, j) = 0;\n      for (size_t k = 0; k < K; ++k) {\n        C(i, j) += A(i, k) * B(k, j);\n      }\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", Kokkos::MDRangePolicy<Kokkos::Rank<2>>({0,0}, {M,N}), KOKKOS_LAMBDA (const int i, const int j) {\n    double sum = 0.0;\n    for (size_t k = 0; k < K; ++k) {\n      sum += A(i, k) * B(k, j);\n    }\n    C(i, j) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", M, K, KOKKOS_LAMBDA (const int i, const int j) {\n    for (size_t k = 0; k < N; ++k) {\n      C(i, k) += A(i, j) * B(j, k);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", M, K, KOKKOS_LAMBDA(const int i, const int j) {\n    double sum = 0.0;\n    for (int k = 0; k < N; ++k) {\n      sum += A(i, k) * B(k, j);\n    }\n    C(i, j) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"gemm\", M, K, KOKKOS_LAMBDA (const int i, const int j) {\n    for (size_t k = 0; k < N; ++k) {\n      C(i, k) += A(i, j) * B(j, k);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", M, K, KOKKOS_LAMBDA (const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      C(i, j) = 0;\n      for (size_t k = 0; k < K; ++k) {\n        C(i, j) += A(i, k) * B(k, j);\n      }\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", M, K, KOKKOS_LAMBDA(const int i, const int j) {\n    for (size_t k = 0; k < N; ++k) {\n      C(i, j) += A(i, k) * B(k, j);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"gemm\", M, K, KOKKOS_LAMBDA (const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      C(i, j) = 0;\n      for (size_t k = 0; k < K; ++k) {\n        C(i, j) += A(i, k) * B(k, j);\n      }\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", M, K, KOKKOS_LAMBDA (const int i) {\n    for (size_t j = 0; j < N; j++) {\n      C(i, j) = 0;\n      for (size_t k = 0; k < K; k++) {\n        C(i, j) += A(i, k) * B(k, j);\n      }\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", M, K, KOKKOS_LAMBDA (const int i, const int j) {\n    for (size_t k = 0; k < N; ++k) {\n      C(i, k) += A(i, j) * B(j, k);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", M, K, KOKKOS_LAMBDA(const int i, const int j) {\n    for (size_t k = 0; k < N; ++k) {\n      C(i, j) += A(i, k) * B(k, j);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", M, K, KOKKOS_LAMBDA (const int i, const int j) {\n    for (size_t k = 0; k < N; ++k) {\n      C(i, j) += A(i, k) * B(k, j);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", M, K, KOKKOS_LAMBDA(const int i, const int j) {\n    for (size_t k = 0; k < N; ++k) {\n      C(i, k) += A(i, j) * B(j, k);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", M, K, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      C(i, j) = 0;\n      for (size_t k = 0; k < K; ++k) {\n        C(i, j) += A(i, k) * B(k, j);\n      }\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", M, K, KOKKOS_LAMBDA (const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      C(i, j) = 0;\n      for (size_t k = 0; k < K; ++k) {\n        C(i, j) += A(i, k) * B(k, j);\n      }\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", M, K, KOKKOS_LAMBDA (const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      C(i, j) = 0;\n      for (size_t k = 0; k < K; ++k) {\n        C(i, j) += A(i, k) * B(k, j);\n      }\n    }\n  });\n  Kokkos::fence();\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "02_dense_la_gemm",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < M; i++) {\n    for (size_t j = 0; j < N; j++) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; k++) {\n        sum += A[i * K + k] * B[k * N + j];\n      }\n      C[i * N + j] = sum;\n    }\n  }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < M; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; ++k) {\n        sum += A[i * K + k] * B[k * N + j];\n      }\n      C[i * N + j] = sum;\n    }\n  }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < M; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; ++k) {\n        sum += A[i * K + k] * B[k * N + j];\n      }\n      C[i * N + j] = sum;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < M; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; ++k) {\n        sum += A[i * K + k] * B[k * N + j];\n      }\n      C[i * N + j] = sum;\n    }\n  }\n}",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < M; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        double sum = 0.0;\n        for (size_t k = 0; k < K; ++k) {\n            sum += A[i * K + k] * B[k * N + j];\n        }\n        C[i * N + j] = sum;\n    }\n}\n}",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < M; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; ++k) {\n        sum += A[i * K + k] * B[k * N + j];\n      }\n      C[i * N + j] = sum;\n    }\n  }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < M; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; ++k) {\n        sum += A[i * K + k] * B[k * N + j];\n      }\n      C[i * N + j] = sum;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < M; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; ++k) {\n        sum += A[i * K + k] * B[k * N + j];\n      }\n      C[i * N + j] = sum;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < M; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; ++k) {\n        sum += A[i * K + k] * B[k * N + j];\n      }\n      C[i * N + j] = sum;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < M; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      double sum = 0.0;\n      for (size_t k = 0; k < K; ++k) {\n        sum += A[i * K + k] * B[k * N + j];\n      }\n      C[i * N + j] = sum;\n    }\n  }\n}"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "02_dense_la_gemm",
    "parallelism_model": "serial",
    "prompt": "/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            C[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                C[i * N + j] += A[i * K + k] * B[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            C[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                C[i * N + j] += A[i * K + k] * B[k * N + j];\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            C[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                C[i * N + j] += A[i * K + k] * B[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            C[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                C[i * N + j] += A[i * K + k] * B[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            C[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                C[i * N + j] += A[i * K + k] * B[k * N + j];\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            C[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                C[i * N + j] += A[i * K + k] * B[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            C[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                C[i * N + j] += A[i * K + k] * B[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            C[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                C[i * N + j] += A[i * K + k] * B[k * N + j];\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            C[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                C[i * N + j] += A[i * K + k] * B[k * N + j];\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            C[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                C[i * N + j] += A[i * K + k] * B[k * N + j];\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            C[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                C[i * N + j] += A[i * K + k] * B[k * N + j];\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            C[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                C[i * N + j] += A[i * K + k] * B[k * N + j];\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            C[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                C[i * N + j] += A[i * K + k] * B[k * N + j];\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "02_dense_la_gemm",
    "parallelism_model": "hip",
    "prompt": "/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use AMD HIP to compute in parallel. The kernel is launched on an MxN grid of threads.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\n__global__ void gemm(const double *A, const double *B, double *C, size_t M, size_t K, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int i = 0; i < K; ++i) {\n            sum += A[row * K + i] * B[i * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int i = 0; i < K; ++i) {\n            sum += A[row * K + i] * B[i * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int i = 0; i < K; i++) {\n            sum += A[row * K + i] * B[i * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int i = 0; i < K; ++i) {\n            sum += A[row * K + i] * B[i * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int i = 0; i < K; ++i) {\n            sum += A[row * K + i] * B[i * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int i = 0; i < K; i++) {\n            sum += A[row * K + i] * B[i * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int i = 0; i < K; ++i) {\n            sum += A[row * K + i] * B[i * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int i = 0; i < K; ++i) {\n            sum += A[row * K + i] * B[i * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int i = 0; i < K; ++i) {\n            sum += A[row * K + i] * B[i * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "02_dense_la_gemm",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and B. Store the result in C on rank 0.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                C[i * N + j] = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    C[i * N + j] += A[i * K + k] * B[k * N + j];\n                }\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                C[i * N + j] = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    C[i * N + j] += A[i * K + k] * B[k * N + j];\n                }\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                C[i * N + j] = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    C[i * N + j] += A[i * K + k] * B[k * N + j];\n                }\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                C[i * N + j] = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    C[i * N + j] += A[i * K + k] * B[k * N + j];\n                }\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                C[i * N + j] = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    C[i * N + j] += A[i * K + k] * B[k * N + j];\n                }\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                C[i * N + j] = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    C[i * N + j] += A[i * K + k] * B[k * N + j];\n                }\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                C[i * N + j] = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    C[i * N + j] += A[i * K + k] * B[k * N + j];\n                }\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                C[i * N + j] = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    C[i * N + j] += A[i * K + k] * B[k * N + j];\n                }\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                C[i * N + j] = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    C[i * N + j] += A[i * K + k] * B[k * N + j];\n                }\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                C[i * N + j] = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    C[i * N + j] += A[i * K + k] * B[k * N + j];\n                }\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                C[i * N + j] = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    C[i * N + j] += A[i * K + k] * B[k * N + j];\n                }\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                C[i * N + j] = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    C[i * N + j] += A[i * K + k] * B[k * N + j];\n                }\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                C[i * N + j] = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    C[i * N + j] += A[i * K + k] * B[k * N + j];\n                }\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                C[i * N + j] = 0.0;\n                for (size_t k = 0; k < K; ++k) {\n                    C[i * N + j] += A[i * K + k] * B[k * N + j];\n                }\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                C[i * N + j] = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    C[i * N + j] += A[i * K + k] * B[k * N + j];\n                }\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                C[i * N + j] = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    C[i * N + j] += A[i * K + k] * B[k * N + j];\n                }\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                C[i * N + j] = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    C[i * N + j] += A[i * K + k] * B[k * N + j];\n                }\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                C[i * N + j] = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    C[i * N + j] += A[i * K + k] * B[k * N + j];\n                }\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                C[i * N + j] = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    C[i * N + j] += A[i * K + k] * B[k * N + j];\n                }\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                C[i * N + j] = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    C[i * N + j] += A[i * K + k] * B[k * N + j];\n                }\n            }\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "02_dense_la_gemm",
    "parallelism_model": "cuda",
    "prompt": "/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an MxN grid of threads.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\n__global__ void gemm(const double *A, const double *B, double *C, size_t M, size_t K, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M && col < N) {\n        double sum = 0.0;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "02_dense_la_gemm",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and B. Store the result in C on rank 0.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            std::vector<double> tempC(M * N);\n            MPI_Recv(tempC.data(), M * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t i = 0; i < M * N; ++i) {\n                C[i] += tempC[i];\n            }\n        }\n    } else {\n        MPI_Send(C.data(), M * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; i++) {\n        for (size_t j = 0; j < N; j++) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; k++) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            std::vector<double> tempC(M * N);\n            MPI_Recv(tempC.data(), M * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t i = 0; i < M; i++) {\n                for (size_t j = 0; j < N; j++) {\n                    C[i * N + j] += tempC[i * N + j];\n                }\n            }\n        }\n    } else {\n        MPI_Send(C.data(), M * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> tempC(M * N);\n            MPI_Recv(tempC.data(), M * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < M * N; ++j) {\n                C[j] += tempC[j];\n            }\n        }\n    } else {\n        MPI_Send(C.data(), M * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> tempC(M * N);\n        MPI_Gather(C.data(), M * N, MPI_DOUBLE, tempC.data(), M * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        C = tempC;\n    } else {\n        MPI_Gather(C.data(), M * N, MPI_DOUBLE, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double temp = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                temp += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = temp;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> recvbuf(M * N);\n            MPI_Recv(recvbuf.data(), M * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < M * N; ++j) {\n                C[j] += recvbuf[j];\n            }\n        }\n    } else {\n        MPI_Send(C.data(), M * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> tempC(M * N);\n        MPI_Gather(C.data(), M * N, MPI_DOUBLE, tempC.data(), M * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        C = tempC;\n    } else {\n        MPI_Gather(C.data(), M * N, MPI_DOUBLE, nullptr, 0, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double temp = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                temp += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = temp;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> tempC(M * N);\n            MPI_Recv(&tempC[0], M * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < M * N; ++j) {\n                C[j] += tempC[j];\n            }\n        }\n    } else {\n        MPI_Send(&C[0], M * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double temp = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                temp += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = temp;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> tempC(M * N);\n            MPI_Recv(tempC.data(), M * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < M * N; ++j) {\n                C[j] += tempC[j];\n            }\n        }\n    } else {\n        MPI_Send(C.data(), M * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double temp = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                temp += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = temp;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> tempC(M * N);\n        MPI_Gather(C.data(), M * N, MPI_DOUBLE, tempC.data(), M * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        C = tempC;\n    } else {\n        MPI_Gather(C.data(), M * N, MPI_DOUBLE, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double temp = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                temp += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = temp;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> tempC(M * N);\n            MPI_Recv(tempC.data(), M * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < M * N; ++j) {\n                C[j] += tempC[j];\n            }\n        }\n    } else {\n        MPI_Send(C.data(), M * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; i++) {\n        for (size_t j = 0; j < N; j++) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; k++) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::vector<double> tempC(M * N);\n            MPI_Recv(tempC.data(), M * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < M * N; j++) {\n                C[j] += tempC[j];\n            }\n        }\n    } else {\n        MPI_Send(C.data(), M * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double temp = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                temp += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = temp;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> tempC(M * N);\n            MPI_Recv(tempC.data(), M * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < M * N; ++j) {\n                C[j] += tempC[j];\n            }\n        }\n    } else {\n        MPI_Send(C.data(), M * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> tempC(M * N);\n        MPI_Gather(C.data(), M * N, MPI_DOUBLE, tempC.data(), M * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        C = tempC;\n    } else {\n        MPI_Gather(C.data(), M * N, MPI_DOUBLE, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> tempC(M * N);\n            MPI_Recv(tempC.data(), M * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < M * N; ++j) {\n                C[j] += tempC[j];\n            }\n        }\n    } else {\n        MPI_Send(C.data(), M * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; i++) {\n        for (size_t j = 0; j < N; j++) {\n            double temp = 0.0;\n            for (size_t k = 0; k < K; k++) {\n                temp += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = temp;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> recvbuf(M * N);\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(recvbuf.data(), M * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < M * N; j++) {\n                C[j] += recvbuf[j];\n            }\n        }\n    } else {\n        MPI_Send(C.data(), M * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double temp = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                temp += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = temp;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> tempC(M * N);\n            MPI_Recv(tempC.data(), M * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < M * N; ++j) {\n                C[j] += tempC[j];\n            }\n        }\n    } else {\n        MPI_Send(C.data(), M * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> tempC(M * N);\n        MPI_Gather(C.data(), M * N, MPI_DOUBLE, tempC.data(), M * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        C = tempC;\n    } else {\n        MPI_Gather(C.data(), M * N, MPI_DOUBLE, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; i++) {\n        for (size_t j = 0; j < N; j++) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; k++) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> tempC(M * N);\n        MPI_Reduce(C.data(), tempC.data(), M * N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n        C = tempC;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> tempC(M * N);\n            MPI_Recv(tempC.data(), M * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < M * N; ++j) {\n                C[j] += tempC[j];\n            }\n        }\n    } else {\n        MPI_Send(C.data(), M * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            std::vector<double> tempC(M * N);\n            MPI_Recv(tempC.data(), M * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t i = 0; i < M; ++i) {\n                for (size_t j = 0; j < N; ++j) {\n                    C[i * N + j] += tempC[i * N + j];\n                }\n            }\n        }\n    } else {\n        MPI_Send(C.data(), M * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "04_dense_la_gemv",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix, x has N elements, and y has M elements.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(Kokkos::View<const double**> &A, Kokkos::View<const double*> &x, Kokkos::View<double*> &y, size_t M, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int i) {\n    y(i) = 0;\n    for (int j = 0; j < N; j++) {\n      y(i) += A(i, j) * x(j);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int i) {\n    y(i) = 0.0;\n    for (size_t j = 0; j < N; ++j) {\n      y(i) += A(i, j) * x(j);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"gemv\", M, KOKKOS_LAMBDA(const int i) {\n    double sum = 0.0;\n    for (size_t j = 0; j < N; ++j) {\n      sum += A(i, j) * x(j);\n    }\n    y(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int i) {\n    double sum = 0.0;\n    for (size_t j = 0; j < N; ++j) {\n      sum += A(i, j) * x(j);\n    }\n    y(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int i) {\n    y(i) = 0.0;\n    for (size_t j = 0; j < N; ++j) {\n      y(i) += A(i, j) * x(j);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int i) {\n    y(i) = 0;\n    for (size_t j = 0; j < N; ++j) {\n      y(i) += A(i, j) * x(j);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int i) {\n    y(i) = 0;\n    for (size_t j = 0; j < N; ++j) {\n      y(i) += A(i, j) * x(j);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int i) {\n    double sum = 0.0;\n    for (size_t j = 0; j < N; ++j) {\n      sum += A(i, j) * x(j);\n    }\n    y(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int i) {\n    double sum = 0.0;\n    for (size_t j = 0; j < N; ++j) {\n      sum += A(i, j) * x(j);\n    }\n    y(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int i) {\n    double sum = 0.0;\n    for (size_t j = 0; j < N; ++j) {\n      sum += A(i, j) * x(j);\n    }\n    y(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int i) {\n    y(i) = 0;\n    for (size_t j = 0; j < N; ++j) {\n      y(i) += A(i, j) * x(j);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int i) {\n    double sum = 0.0;\n    for (size_t j = 0; j < N; ++j) {\n      sum += A(i, j) * x(j);\n    }\n    y(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int i) {\n    double sum = 0.0;\n    for (size_t j = 0; j < N; ++j) {\n      sum += A(i, j) * x(j);\n    }\n    y(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int i) {\n    y(i) = 0;\n    for (int j = 0; j < N; j++) {\n      y(i) += A(i, j) * x(j);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int i) {\n    double sum = 0.0;\n    for (size_t j = 0; j < N; ++j) {\n      sum += A(i, j) * x(j);\n    }\n    y(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int i) {\n    y(i) = 0;\n    for (int j = 0; j < N; j++) {\n      y(i) += A(i, j) * x(j);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int i) {\n    y(i) = 0;\n    for (size_t j = 0; j < N; ++j) {\n      y(i) += A(i, j) * x(j);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int i) {\n    y(i) = 0.0;\n    for (size_t j = 0; j < N; ++j) {\n      y(i) += A(i, j) * x(j);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int i) {\n    double sum = 0.0;\n    for (size_t j = 0; j < N; ++j) {\n      sum += A(i, j) * x(j);\n    }\n    y(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int i) {\n    double sum = 0.0;\n    for (size_t j = 0; j < N; ++j) {\n      sum += A(i, j) * x(j);\n    }\n    y(i) = sum;\n  });\n  Kokkos::fence();\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "04_dense_la_gemv",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "04_dense_la_gemv",
    "parallelism_model": "serial",
    "prompt": "/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "04_dense_la_gemv",
    "parallelism_model": "hip",
    "prompt": "/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least M threads.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\n__global__ void gemv(const double *A, const double *x, double *y, size_t M, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        y[i] = 0;\n        for (int j = 0; j < N; j++) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        y[i] = 0;\n        for (int j = 0; j < N; j++) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        y[i] = 0;\n        for (int j = 0; j < N; j++) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "04_dense_la_gemv",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and x. Store the result in y on rank 0.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            local_y[i - local_start] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        y = local_y;\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> recv(local_M);\n            MPI_Recv(recv.data(), local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            y.insert(y.end(), recv.begin(), recv.end());\n        }\n    } else {\n        MPI_Send(local_y.data(), local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            local_y[i - local_start] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        y.resize(M, 0.0);\n    }\n\n    MPI_Gather(&local_y[0], local_M, MPI_DOUBLE, &y[0], local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            local_y[i - local_start] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        y = local_y;\n        for (int r = 1; r < size; ++r) {\n            std::vector<double> recv_buf(local_M);\n            MPI_Recv(recv_buf.data(), local_M, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            y.insert(y.end(), recv_buf.begin(), recv_buf.end());\n        }\n    } else {\n        MPI_Send(local_y.data(), local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            local_y[i - local_start] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        y = local_y;\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> recv(local_M);\n            MPI_Recv(recv.data(), local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            y.insert(y.end(), recv.begin(), recv.end());\n        }\n    } else {\n        MPI_Send(local_y.data(), local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            local_y[i - local_start] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        y = local_y;\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> recv_buf(local_M);\n            MPI_Recv(recv_buf.data(), local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            y.insert(y.end(), recv_buf.begin(), recv_buf.end());\n        }\n    } else {\n        MPI_Send(local_y.data(), local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            local_y[i - local_start] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        y = local_y;\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> recv(local_M);\n            MPI_Recv(recv.data(), local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            y.insert(y.end(), recv.begin(), recv.end());\n        }\n    } else {\n        MPI_Send(local_y.data(), local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            local_y[i - local_start] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        y.resize(M, 0.0);\n    }\n\n    MPI_Gather(local_y.data(), local_M, MPI_DOUBLE, y.data(), local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            local_y[i - local_start] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        y.resize(M, 0.0);\n    }\n\n    MPI_Gather(local_y.data(), local_M, MPI_DOUBLE, y.data(), local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            local_y[i - local_start] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        y = local_y;\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> recv(local_M, 0.0);\n            MPI_Recv(recv.data(), local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            y.insert(y.end(), recv.begin(), recv.end());\n        }\n    } else {\n        MPI_Send(local_y.data(), local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            local_y[i - local_start] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        y = local_y;\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> recv_buf(local_M);\n            MPI_Recv(recv_buf.data(), local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            y.insert(y.end(), recv_buf.begin(), recv_buf.end());\n        }\n    } else {\n        MPI_Send(local_y.data(), local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            local_y[i - local_start] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        y = local_y;\n        for (int r = 1; r < size; ++r) {\n            std::vector<double> recv_y(local_M, 0.0);\n            MPI_Recv(recv_y.data(), local_M, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            y.insert(y.end(), recv_y.begin(), recv_y.end());\n        }\n    } else {\n        MPI_Send(local_y.data(), local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            local_y[i - local_start] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        y.resize(M);\n    }\n\n    MPI_Gather(local_y.data(), local_M, MPI_DOUBLE, y.data(), local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            local_y[i - local_start] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        y = local_y;\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> recv_buf(local_M);\n            MPI_Recv(recv_buf.data(), local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            y.insert(y.end(), recv_buf.begin(), recv_buf.end());\n        }\n    } else {\n        MPI_Send(local_y.data(), local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            local_y[i - local_start] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        y = local_y;\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> recv_y(local_M, 0.0);\n            MPI_Recv(recv_y.data(), local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            y.insert(y.end(), recv_y.begin(), recv_y.end());\n        }\n    } else {\n        MPI_Send(local_y.data(), local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            local_y[i - local_start] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        y = local_y;\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> recv(local_M);\n            MPI_Recv(recv.data(), local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            y.insert(y.end(), recv.begin(), recv.end());\n        }\n    } else {\n        MPI_Send(local_y.data(), local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            local_y[i - local_start] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        y.resize(M, 0.0);\n    }\n\n    MPI_Gather(local_y.data(), local_M, MPI_DOUBLE, y.data(), local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            local_y[i - local_start] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        y.resize(M, 0.0);\n    }\n\n    MPI_Gather(local_y.data(), local_M, MPI_DOUBLE, y.data(), local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            local_y[i - local_start] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        y.resize(M);\n    }\n\n    MPI_Gather(&local_y[0], local_M, MPI_DOUBLE, &y[0], local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            local_y[i - local_start] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        y.resize(M, 0.0);\n    }\n\n    MPI_Gather(local_y.data(), local_M, MPI_DOUBLE, y.data(), local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "04_dense_la_gemv",
    "parallelism_model": "cuda",
    "prompt": "/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use CUDA to compute in parallel. The kernel is launched with at least M threads.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\n__global__ void gemv(const double *A, const double *x, double *y, size_t M, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        y[i] = 0;\n        for (int j = 0; j < N; j++) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        y[i] = 0;\n        for (int j = 0; j < N; j++) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; j++) {\n            sum += A[tid * N + j] * x[j];\n        }\n        y[tid] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        y[i] = 0;\n        for (int j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[tid * N + j] * x[j];\n        }\n        y[tid] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        y[i] = 0;\n        for (int j = 0; j < N; j++) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        y[i] = 0;\n        for (int j = 0; j < N; j++) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        y[i] = 0;\n        for (int j = 0; j < N; j++) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        y[i] = 0;\n        for (int j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        y[i] = 0;\n        for (int j = 0; j < N; j++) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        y[i] = 0;\n        for (int j = 0; j < N; j++) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        y[i] = 0;\n        for (int j = 0; j < N; j++) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        y[i] = 0;\n        for (int j = 0; j < N; j++) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < M) {\n        double sum = 0.0;\n        for (int j = 0; j < N; j++) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "04_dense_la_gemv",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and x. Store the result in y on rank 0.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    #pragma omp parallel for\n    for (size_t i = local_start; i < local_end; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> recvbuf(M);\n        MPI_Gather(y.data() + local_start, local_M, MPI_DOUBLE, recvbuf.data(), local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        y = recvbuf;\n    } else {\n        MPI_Gather(y.data() + local_start, local_M, MPI_DOUBLE, nullptr, 0, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> temp(M, 0.0);\n        MPI_Reduce(&y[0], &temp[0], M, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n        y = temp;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; i++) {\n        y[i] = 0.0;\n        for (size_t j = 0; j < N; j++) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> recvbuf(M, 0.0);\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(recvbuf.data(), M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < M; j++) {\n                y[j] += recvbuf[j];\n            }\n        }\n    } else {\n        MPI_Send(y.data(), M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> temp(M);\n        MPI_Reduce(&y[0], &temp[0], M, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n        y = temp;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    #pragma omp parallel for\n    for (size_t i = local_start; i < local_end; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> recvbuf(M);\n        MPI_Gather(y.data() + local_start, local_M, MPI_DOUBLE, recvbuf.data(), local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        y = recvbuf;\n    } else {\n        MPI_Gather(y.data() + local_start, local_M, MPI_DOUBLE, nullptr, 0, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    #pragma omp parallel for\n    for (size_t i = local_start; i < local_end; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&y[i * local_M], local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&y[local_start], local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> temp(M, 0.0);\n        MPI_Reduce(&y[0], &temp[0], M, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n        y = temp;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> temp(M, 0.0);\n        MPI_Reduce(&y[0], &temp[0], M, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n        y = temp;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    #pragma omp parallel for\n    for (size_t i = local_start; i < local_end; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> recvbuf(M);\n        MPI_Gather(y.data() + local_start, local_M, MPI_DOUBLE, recvbuf.data(), local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        y = recvbuf;\n    } else {\n        MPI_Gather(y.data() + local_start, local_M, MPI_DOUBLE, nullptr, 0, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> recvbuf(M);\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&recvbuf[0], M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < M; ++j) {\n                y[j] += recvbuf[j];\n            }\n        }\n    } else {\n        MPI_Send(&y[0], M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    #pragma omp parallel for\n    for (size_t i = local_start; i < local_end; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> recvbuf(M);\n        MPI_Gather(y.data() + local_start, local_M, MPI_DOUBLE, recvbuf.data(), local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        y = recvbuf;\n    } else {\n        MPI_Gather(y.data() + local_start, local_M, MPI_DOUBLE, nullptr, 0, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> temp_y(M, 0.0);\n        MPI_Reduce(&y[0], &temp_y[0], M, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n        y = temp_y;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    #pragma omp parallel for\n    for (size_t i = local_start; i < local_end; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> recvbuf(M);\n        MPI_Gather(y.data() + local_start, local_M, MPI_DOUBLE, recvbuf.data(), local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        y = recvbuf;\n    } else {\n        MPI_Gather(y.data() + local_start, local_M, MPI_DOUBLE, nullptr, 0, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> temp(M, 0.0);\n        MPI_Reduce(&y[0], &temp[0], M, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n        y = temp;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; i++) {\n        y[i] = 0.0;\n        for (size_t j = 0; j < N; j++) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> temp(M, 0.0);\n        MPI_Reduce(y.data(), temp.data(), M, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n        y = temp;\n    } else {\n        MPI_Reduce(y.data(), nullptr, M, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> temp(M, 0.0);\n        MPI_Reduce(&y[0], &temp[0], M, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n        y = temp;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> temp(M, 0.0);\n        MPI_Reduce(&y[0], &temp[0], M, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n        y = temp;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    #pragma omp parallel for\n    for (size_t i = local_start; i < local_end; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> recvbuf(M);\n        MPI_Reduce(y.data(), recvbuf.data(), M, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n        y = recvbuf;\n    } else {\n        MPI_Reduce(y.data(), nullptr, M, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    #pragma omp parallel for\n    for (size_t i = local_start; i < local_end; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i * N + j] * x[j];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> recvbuf(M);\n        MPI_Gather(y.data() + local_start, local_M, MPI_DOUBLE, recvbuf.data(), local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        y = recvbuf;\n    } else {\n        MPI_Gather(y.data() + local_start, local_M, MPI_DOUBLE, nullptr, 0, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "00_dense_la_lu_decomp",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(Kokkos::View<double**> &A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  for (size_t k = 0; k < N; k++) {\n    for (size_t i = k + 1; i < N; i++) {\n      A(i, k) /= A(k, k);\n      for (size_t j = k + 1; j < N; j++) {\n        A(i, j) -= A(i, k) * A(k, j);\n      }\n    }\n  }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A(i, k) /= A(k, k);\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) -= A(i, k) * A(k, j);\n            }\n        }\n    }\n}\n",
      "\nfor (size_t k = 0; k < N; k++) {\n    for (size_t i = k + 1; i < N; i++) {\n        A(i, k) /= A(k, k);\n        for (size_t j = k + 1; j < N; j++) {\n            A(i, j) -= A(i, k) * A(k, j);\n        }\n    }\n}\n}",
      "\nfor (size_t k = 0; k < N; k++) {\n    for (size_t i = k + 1; i < N; i++) {\n        A(i, k) /= A(k, k);\n        for (size_t j = k + 1; j < N; j++) {\n            A(i, j) -= A(i, k) * A(k, j);\n        }\n    }\n}\n}",
      "\nfor (size_t k = 0; k < N; k++) {\n    for (size_t i = k + 1; i < N; i++) {\n        A(i, k) /= A(k, k);\n        for (size_t j = k + 1; j < N; j++) {\n            A(i, j) -= A(i, k) * A(k, j);\n        }\n    }\n}\n}",
      "\nfor (size_t k = 0; k < N; k++) {\n    for (size_t i = k + 1; i < N; i++) {\n        A(i, k) /= A(k, k);\n        for (size_t j = k + 1; j < N; j++) {\n            A(i, j) -= A(i, k) * A(k, j);\n        }\n    }\n}\n}",
      "\nfor (size_t k = 0; k < N; k++) {\n    for (size_t i = k + 1; i < N; i++) {\n        A(i, k) /= A(k, k);\n        for (size_t j = k + 1; j < N; j++) {\n            A(i, j) -= A(i, k) * A(k, j);\n        }\n    }\n}\n}",
      "\n  for (size_t k = 0; k < N; k++) {\n    for (size_t i = k + 1; i < N; i++) {\n      A(i, k) /= A(k, k);\n      for (size_t j = k + 1; j < N; j++) {\n        A(i, j) -= A(i, k) * A(k, j);\n      }\n    }\n  }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A(i, k) /= A(k, k);\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) -= A(i, k) * A(k, j);\n            }\n        }\n    }\n}\n",
      "\nfor (size_t k = 0; k < N; k++) {\n    for (size_t i = k + 1; i < N; i++) {\n        A(i, k) /= A(k, k);\n        for (size_t j = k + 1; j < N; j++) {\n            A(i, j) -= A(i, k) * A(k, j);\n        }\n    }\n}\n}",
      "\nfor (size_t k = 0; k < N; k++) {\n    for (size_t i = k + 1; i < N; i++) {\n        A(i, k) /= A(k, k);\n        for (size_t j = k + 1; j < N; j++) {\n            A(i, j) -= A(i, k) * A(k, j);\n        }\n    }\n}\n}",
      "\nfor (size_t k = 0; k < N; k++) {\n    for (size_t i = k + 1; i < N; i++) {\n        A(i, k) /= A(k, k);\n        for (size_t j = k + 1; j < N; j++) {\n            A(i, j) -= A(i, k) * A(k, j);\n        }\n    }\n}\n}",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A(i, k) /= A(k, k);\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) -= A(i, k) * A(k, j);\n            }\n        }\n    }\n}\n",
      "\n  for (size_t k = 0; k < N; k++) {\n    for (size_t i = k + 1; i < N; i++) {\n      A(i, k) /= A(k, k);\n      for (size_t j = k + 1; j < N; j++) {\n        A(i, j) -= A(i, k) * A(k, j);\n      }\n    }\n  }\n}\n",
      "\n  for (size_t k = 0; k < N; k++) {\n    for (size_t i = k + 1; i < N; i++) {\n      A(i, k) /= A(k, k);\n      for (size_t j = k + 1; j < N; j++) {\n        A(i, j) -= A(i, k) * A(k, j);\n      }\n    }\n  }\n}\n",
      "\nfor (size_t k = 0; k < N; k++) {\n    for (size_t i = k + 1; i < N; i++) {\n        A(i, k) /= A(k, k);\n        for (size_t j = k + 1; j < N; j++) {\n            A(i, j) -= A(i, k) * A(k, j);\n        }\n    }\n}\n}",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A(i, k) /= A(k, k);\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) -= A(i, k) * A(k, j);\n            }\n        }\n    }\n}\n",
      "\nfor (size_t k = 0; k < N; k++) {\n    for (size_t i = k + 1; i < N; i++) {\n        A(i, k) /= A(k, k);\n        for (size_t j = k + 1; j < N; j++) {\n            A(i, j) -= A(i, k) * A(k, j);\n        }\n    }\n}\n}",
      "\n  for (size_t k = 0; k < N; k++) {\n    for (size_t i = k + 1; i < N; i++) {\n      A(i, k) /= A(k, k);\n      for (size_t j = k + 1; j < N; j++) {\n        A(i, j) -= A(i, k) * A(k, j);\n      }\n    }\n  }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A(i, k) /= A(k, k);\n            for (size_t j = k + 1; j < N; j++) {\n                A(i, j) -= A(i, k) * A(k, j);\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "00_dense_la_lu_decomp",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        #pragma omp parallel for\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        #pragma omp parallel for\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        #pragma omp parallel for\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "00_dense_la_lu_decomp",
    "parallelism_model": "serial",
    "prompt": "/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[N * i + k] /= A[N * k + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[N * i + j] -= A[N * i + k] * A[N * k + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[N * i + k] /= A[N * k + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[N * i + j] -= A[N * i + k] * A[N * k + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = i + 1; j < N; j++) {\n            A[N * i + j] /= A[N * i + i];\n            for (size_t k = i + 1; k < N; k++) {\n                A[N * j + k] -= A[N * i + k] * A[N * j + i];\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = i + 1; j < N; j++) {\n            A[N * i + j] /= A[N * i + i];\n            for (size_t k = i + 1; k < N; k++) {\n                A[N * j + k] -= A[N * i + k] * A[N * j + i];\n            }\n        }\n    }\n}",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; ++k) {\n        for (size_t i = k + 1; i < N; ++i) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; ++j) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = i + 1; j < N; j++) {\n            A[N * i + j] /= A[N * i + i];\n            for (size_t k = i + 1; k < N; k++) {\n                A[N * j + k] -= A[N * i + k] * A[N * j + i];\n            }\n        }\n    }\n}",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; ++k) {\n        for (size_t i = k + 1; i < N; ++i) {\n            A[N * i + k] /= A[N * k + k];\n            for (size_t j = k + 1; j < N; ++j) {\n                A[N * i + j] -= A[N * i + k] * A[N * k + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; ++k) {\n        for (size_t i = k + 1; i < N; ++i) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; ++j) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; ++k) {\n        for (size_t i = k + 1; i < N; ++i) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; ++j) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; ++k) {\n        for (size_t i = k + 1; i < N; ++i) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; ++j) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "00_dense_la_lu_decomp",
    "parallelism_model": "hip",
    "prompt": "/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\n__global__ void luFactorize(double *A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (col < N && row < N) {\n        if (col <= row) {\n            for (int k = 0; k < col; k++) {\n                A[row * N + col] -= A[row * N + k] * A[k * N + col];\n            }\n        } else {\n            A[row * N + col] /= A[col * N + col];\n            for (int k = 0; k < row; k++) {\n                A[row * N + col] -= A[row * N + k] * A[k * N + col];\n            }\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "00_dense_la_lu_decomp",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. Store the result in A on rank 0.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "00_dense_la_lu_decomp",
    "parallelism_model": "cuda",
    "prompt": "/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\n__global__ void luFactorize(double *A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (col < N && row < N) {\n        if (col <= row) {\n            for (int k = 0; k < col; k++) {\n                A[row * N + col] -= A[row * N + k] * A[k * N + col];\n            }\n        } else {\n            A[row * N + col] /= A[col * N + col];\n            for (int k = 0; k < col; k++) {\n                A[row * N + col] -= A[row * N + k] * A[k * N + col];\n            }\n        }\n    }\n}\n",
      "\n    int col = threadIdx.x + blockIdx.x * blockDim.x;\n    int row = threadIdx.y + blockIdx.y * blockDim.y;\n\n    if (col < N && row < N) {\n        if (col < row) {\n            return;\n        }\n\n        for (int k = 0; k < row; k++) {\n            A[row * N + col] -= A[row * N + k] * A[k * N + col];\n        }\n\n        if (col == row) {\n            for (int k = 0; k < row; k++) {\n                A[row * N + col] /= A[row * N + k];\n            }\n        } else {\n            A[row * N + col] /= A[row * N + row];\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int col = threadIdx.x + blockIdx.x * blockDim.x;\n    int row = threadIdx.y + blockIdx.y * blockDim.y;\n\n    if (col < N && row < N) {\n        if (col < row) {\n            return;\n        }\n\n        for (int k = 0; k < row; k++) {\n            A[row * N + col] -= A[row * N + k] * A[k * N + col];\n        }\n\n        if (col == row) {\n            for (int k = 0; k < row; k++) {\n                A[row * N + col] /= A[row * N + k];\n            }\n        } else {\n            A[row * N + col] /= A[row * N + row];\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int col = threadIdx.x + blockIdx.x * blockDim.x;\n    int row = threadIdx.y + blockIdx.y * blockDim.y;\n\n    if (col < N && row < N) {\n        if (col < row) {\n            return;\n        }\n\n        for (int k = 0; k < row; k++) {\n            A[row * N + col] -= A[row * N + k] * A[k * N + col];\n        }\n\n        if (col == row) {\n            for (int k = 0; k < row; k++) {\n                A[row * N + col] /= A[row * N + k];\n            }\n        } else {\n            A[row * N + col] /= A[row * N + row];\n        }\n    }\n}\n",
      "\n    int col = threadIdx.x + blockIdx.x * blockDim.x;\n    int row = threadIdx.y + blockIdx.y * blockDim.y;\n\n    if (col < N && row < N) {\n        if (col < row) {\n            return;\n        }\n\n        for (int k = 0; k < row; k++) {\n            A[row * N + col] -= A[row * N + k] * A[k * N + col];\n        }\n\n        if (col == row) {\n            for (int k = 0; k < row; k++) {\n                A[row * N + col] /= A[row * N + k];\n            }\n        } else {\n            A[row * N + col] /= A[row * N + row];\n        }\n    }\n}\n",
      "\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (col < N && row < N) {\n        if (col <= row) {\n            for (int k = 0; k < col; k++) {\n                A[row * N + col] -= A[row * N + k] * A[k * N + col];\n            }\n        } else {\n            A[row * N + col] /= A[col * N + col];\n            for (int k = 0; k < row; k++) {\n                A[row * N + col] -= A[row * N + k] * A[k * N + col];\n            }\n        }\n    }\n}\n",
      "\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (col < N && row < N) {\n        if (col <= row) {\n            for (int k = 0; k < col; k++) {\n                A[row * N + col] -= A[row * N + k] * A[k * N + col];\n            }\n            if (col == row) {\n                for (int k = 0; k < col; k++) {\n                    A[row * N + col] = A[row * N + col] / A[col * N + col];\n                }\n            }\n        } else {\n            for (int k = 0; k < row; k++) {\n                A[row * N + col] -= A[row * N + k] * A[k * N + col];\n            }\n        }\n    }\n}\n",
      "\n    int col = threadIdx.x + blockIdx.x * blockDim.x;\n    int row = threadIdx.y + blockIdx.y * blockDim.y;\n\n    if (col < N && row < N) {\n        if (col < row) {\n            return;\n        }\n\n        for (int k = 0; k < row; k++) {\n            A[row * N + col] -= A[row * N + k] * A[k * N + col];\n        }\n\n        if (col == row) {\n            for (int k = 0; k < row; k++) {\n                A[row * N + col] /= A[row * N + k];\n            }\n        } else {\n            A[row * N + col] /= A[row * N + row];\n        }\n    }\n}\n",
      "\n    int i = threadIdx.x + blockIdx.x * blockDim.x;\n    int j = threadIdx.y + blockIdx.y * blockDim.y;\n\n    if (i < N && j < N) {\n        if (i > j) {\n            for (int k = 0; k < j; k++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n            A[i * N + j] /= A[j * N + j];\n        } else if (i <= j) {\n            for (int k = 0; k < i; k++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (col < N && row < N) {\n        if (col <= row) {\n            for (int k = 0; k < col; k++) {\n                A[row * N + col] -= A[row * N + k] * A[k * N + col];\n            }\n        } else {\n            A[row * N + col] /= A[col * N + col];\n            for (int k = 0; k < row; k++) {\n                A[row * N + col] -= A[row * N + k] * A[k * N + col];\n            }\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (col < N && row < N) {\n        if (col <= row) {\n            for (int k = 0; k < col; k++) {\n                A[row * N + col] -= A[row * N + k] * A[k * N + col];\n            }\n        } else {\n            A[row * N + col] /= A[col * N + col];\n            for (int k = 0; k < col; k++) {\n                A[row * N + col] -= A[row * N + k] * A[k * N + col];\n            }\n        }\n    }\n}\n",
      "\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (col < N && row < N) {\n        if (col <= row) {\n            for (int k = 0; k < col; k++) {\n                A[row * N + col] -= A[row * N + k] * A[k * N + col];\n            }\n        } else {\n            A[row * N + col] /= A[col * N + col];\n            for (int k = 0; k < row; k++) {\n                A[row * N + col] -= A[row * N + k] * A[k * N + col];\n            }\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int col = threadIdx.x + blockIdx.x * blockDim.x;\n    int row = threadIdx.y + blockIdx.y * blockDim.y;\n    \n    if (col < N && row < N) {\n        for (int k = 0; k < N; ++k) {\n            if (k < row) {\n                A[row * N + col] -= A[row * N + k] * A[k * N + col];\n            } else {\n                A[row * N + col] /= A[row * N + row];\n            }\n            __syncthreads();\n        }\n    }\n}\n",
      "\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (col < N && row < N) {\n        if (col < row) {\n            return;\n        }\n\n        for (int k = 0; k < row; k++) {\n            A[row * N + col] -= A[row * N + k] * A[k * N + col];\n        }\n\n        if (col == row) {\n            for (int k = 0; k < row; k++) {\n                A[row * N + col] /= A[row * N + k];\n            }\n        } else {\n            A[row * N + col] /= A[row * N + row];\n        }\n    }\n}\n",
      "\n    int i, j, k;\n    for (k = 0; k < N; k++) {\n        for (i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "00_dense_la_lu_decomp",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. Store the result in A on rank 0.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (size_t k = 0; k < N; k++) {\n        #pragma omp parallel for\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        #pragma omp parallel for\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        #pragma omp parallel for\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        #pragma omp parallel for\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        #pragma omp parallel for\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        #pragma omp parallel for\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        #pragma omp parallel for\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = N / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    for (int k = 0; k < N; k++) {\n        if (k % size == rank) {\n            for (int j = k + 1; j < N; j++) {\n                A[k * N + j] /= A[k * N + k];\n            }\n            for (int i = k + 1; i < N; i++) {\n                for (int j = k + 1; j < N; j++) {\n                    A[i * N + j] -= A[i * N + k] * A[k * N + j];\n                }\n            }\n        }\n        MPI_Bcast(&A[k * N], N, MPI_DOUBLE, k % size, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        #pragma omp parallel for\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        #pragma omp parallel for\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        #pragma omp parallel for\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = N / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    #pragma omp parallel for\n    for (int k = 0; k < N; k++) {\n        if (rank == 0) {\n            for (int j = k + 1; j < N; j++) {\n                A[k * N + j] /= A[k * N + k];\n            }\n        }\n\n        MPI_Bcast(&A[k * N + k], N - k, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n        #pragma omp parallel for\n        for (int i = local_start + 1; i < local_end; i++) {\n            for (int j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localN = N / size;\n    int localStart = rank * localN;\n    int localEnd = localStart + localN;\n\n    // Perform LU factorization in parallel using OpenMP\n    #pragma omp parallel for\n    for (int k = 0; k < N; k++) {\n        if (k >= localStart && k < localEnd) {\n            for (int i = k + 1; i < N; i++) {\n                if (i >= localStart && i < localEnd) {\n                    A[i * N + k] /= A[k * N + k];\n                }\n            }\n            for (int i = k + 1; i < N; i++) {\n                if (i >= localStart && i < localEnd) {\n                    for (int j = k + 1; j < N; j++) {\n                        if (j >= localStart && j < localEnd) {\n                            A[i * N + j] -= A[i * N + k] * A[k * N + j];\n                        }\n                    }\n                }\n            }\n        }\n        MPI_Bcast(&A[k * N], N, MPI_DOUBLE, rank, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        #pragma omp parallel for\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    int local_start = rank * local_N;\n    int local_end = local_start + local_N;\n\n    // Perform LU factorization in parallel using OpenMP\n    #pragma omp parallel for\n    for (int k = 0; k < N; k++) {\n        if (k >= local_start && k < local_end) {\n            for (int i = k + 1; i < N; i++) {\n                A[i * N + k] /= A[k * N + k];\n                for (int j = k + 1; j < N; j++) {\n                    A[i * N + j] -= A[i * N + k] * A[k * N + j];\n                }\n            }\n        }\n        MPI_Bcast(&A[k * N], N, MPI_DOUBLE, rank, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        #pragma omp parallel for\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    int local_start = rank * local_N;\n    int local_end = local_start + local_N;\n\n    for (int k = 0; k < N; k++) {\n        if (k % size == rank) {\n            for (int j = k + 1; j < N; j++) {\n                A[k * N + j] /= A[k * N + k];\n            }\n            for (int i = k + 1; i < N; i++) {\n                for (int j = k + 1; j < N; j++) {\n                    A[i * N + j] -= A[i * N + k] * A[k * N + j];\n                }\n            }\n        }\n        MPI_Bcast(&A[k * N], N, MPI_DOUBLE, k % size, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        #pragma omp parallel for\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        #pragma omp parallel for\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n",
      "\n    for (size_t k = 0; k < N; k++) {\n        #pragma omp parallel for\n        for (size_t i = k + 1; i < N; i++) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; j++) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "42_sort_sorted_ranks",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* For each value in the array x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(Kokkos::View<const float*> const& x, Kokkos::View<size_t*> &ranks) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::View<float*> sorted_x(\"sorted_x\", x.extent(0));\n  Kokkos::deep_copy(sorted_x, x);\n\n  Kokkos::View<size_t*> indices(\"indices\", x.extent(0));\n  Kokkos::View<size_t*> sorted_indices(\"sorted_indices\", x.extent(0));\n\n  Kokkos::parallel_for(\"SortIndices\", x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n    indices(i) = i;\n  });\n\n  Kokkos::sort_pair(sorted_x, indices);\n\n  Kokkos::parallel_for(\"ComputeRanks\", x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n    ranks(indices(i)) = i;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::View<float*> sorted_x(\"sorted_x\", x.extent(0));\n  Kokkos::deep_copy(sorted_x, x);\n  Kokkos::sort(sorted_x);\n  Kokkos::View<size_t*> indices(\"indices\", x.extent(0));\n  Kokkos::parallel_for(\"compute_ranks\", x.extent(0), KOKKOS_LAMBDA(const int i) {\n    auto it = Kokkos::find(sorted_x, x(i));\n    ranks(i) = it - sorted_x.data();\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::View<float*, Kokkos::HostSpace> sorted_x(\"sorted_x\", x.extent(0));\n  Kokkos::View<size_t*, Kokkos::HostSpace> permutation(\"permutation\", x.extent(0));\n\n  Kokkos::deep_copy(sorted_x, x);\n  Kokkos::sort_permutation(sorted_x, permutation);\n\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    ranks(permutation(i)) = i;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::View<float*, Kokkos::HostSpace> sorted_x(\"sorted_x\", x.extent(0));\n  Kokkos::View<size_t*, Kokkos::HostSpace> permutation(\"permutation\", x.extent(0));\n\n  Kokkos::deep_copy(sorted_x, x);\n  Kokkos::sort_permutation(sorted_x, permutation);\n\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    ranks(permutation(i)) = i;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::View<float*, Kokkos::HostSpace> sorted_x(\"sorted_x\", x.extent(0));\n  Kokkos::View<size_t*, Kokkos::HostSpace> permutation(\"permutation\", x.extent(0));\n\n  Kokkos::deep_copy(sorted_x, x);\n  Kokkos::sort_permutation(sorted_x, permutation);\n\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    ranks(permutation(i)) = i;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::View<float*> sorted_x(\"sorted_x\", x.extent(0));\n  Kokkos::deep_copy(sorted_x, x);\n\n  Kokkos::View<size_t*> sorted_indices(\"sorted_indices\", x.extent(0));\n  for (size_t i = 0; i < x.extent(0); i++) {\n    sorted_indices(i) = i;\n  }\n\n  Kokkos::sort_pair(sorted_x, sorted_indices);\n\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n    ranks(sorted_indices(i)) = i;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::View<float*, Kokkos::HostSpace> x_sorted(\"x_sorted\", x.extent(0));\n  Kokkos::View<size_t*, Kokkos::HostSpace> indices(\"indices\", x.extent(0));\n\n  // Fill indices with 0, 1, 2, ..., n-1\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    indices(i) = i;\n  });\n\n  // Sort x and update indices accordingly\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    x_sorted(i) = x(i);\n  });\n  Kokkos::sort(indices, Kokkos::View<const float*, Kokkos::HostSpace>(x_sorted.data(), x_sorted.extent(0)));\n\n  // Fill ranks with the sorted indices\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    ranks(i) = indices(i);\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::View<size_t*> sorted_indices(\"sorted_indices\", x.extent(0));\n  Kokkos::View<float*, Kokkos::HostSpace, Kokkos::MemoryTraits<Kokkos::Unmanaged>> x_host(x.data(), x.extent(0));\n  Kokkos::View<size_t*, Kokkos::HostSpace, Kokkos::MemoryTraits<Kokkos::Unmanaged>> ranks_host(ranks.data(), ranks.extent(0));\n\n  Kokkos::RangePolicy policy(0, x.extent(0));\n  Kokkos::parallel_for(\"compute_ranks\", policy, KOKKOS_LAMBDA(const int i) {\n    sorted_indices(i) = i;\n  });\n\n  Kokkos::sort(sorted_indices, Kokkos::View<float*, Kokkos::HostSpace, Kokkos::MemoryTraits<Kokkos::Unmanaged>>(x_host.data(), x_host.extent(0)), x.extent(0));\n\n  Kokkos::parallel_for(\"fill_ranks\", policy, KOKKOS_LAMBDA(const int i) {\n    ranks_host(sorted_indices(i)) = i;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::View<float*> sorted_x(\"sorted_x\", x.extent(0));\n  Kokkos::deep_copy(sorted_x, x);\n\n  Kokkos::View<size_t*> permutation(\"permutation\", x.extent(0));\n  Kokkos::View<size_t*> inverse_permutation(\"inverse_permutation\", x.extent(0));\n\n  Kokkos::sort_permute(&sorted_x[0], &permutation[0], x.extent(0));\n\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    ranks(permutation(i)) = i;\n  });\n\n  Kokkos::fence();\n}",
      "\n  Kokkos::View<float*, Kokkos::HostSpace> x_sorted(\"x_sorted\", x.extent(0));\n  Kokkos::View<size_t*, Kokkos::HostSpace> indices(\"indices\", x.extent(0));\n\n  // Fill indices with 0, 1, 2, ..., n-1\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    indices(i) = i;\n  });\n\n  // Sort x and update indices accordingly\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    x_sorted(i) = x(i);\n  });\n  Kokkos::sort(x_sorted, indices);\n\n  // Fill ranks with the sorted indices\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    ranks(indices(i)) = i;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::View<float*, Kokkos::HostSpace> x_sorted(\"x_sorted\", x.extent(0));\n  Kokkos::View<size_t*, Kokkos::HostSpace> indices(\"indices\", x.extent(0));\n\n  // Fill indices with 0, 1, 2, ..., n-1\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    indices(i) = i;\n  });\n\n  // Sort the indices based on the values in x\n  Kokkos::sort(indices, Kokkos::ALL, [&x](int i, int j) {\n    return x(i) < x(j);\n  });\n\n  // Fill ranks with the sorted indices\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    ranks(indices(i)) = i;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::View<float*, Kokkos::HostSpace> sorted_x(\"sorted_x\", x.extent(0));\n  Kokkos::View<size_t*, Kokkos::HostSpace> permutation(\"permutation\", x.extent(0));\n\n  Kokkos::deep_copy(sorted_x, x);\n  Kokkos::sort_permutation(sorted_x, permutation);\n\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    ranks(permutation(i)) = i;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::View<float*> sorted_x(\"sorted_x\", x.extent(0));\n  Kokkos::deep_copy(sorted_x, x);\n\n  Kokkos::View<size_t*> permutation(\"permutation\", x.extent(0));\n  Kokkos::View<size_t*> inverse_permutation(\"inverse_permutation\", x.extent(0));\n\n  Kokkos::sort_permutation(permutation, sorted_x);\n  Kokkos::sort_permutation(inverse_permutation, permutation);\n\n  Kokkos::deep_copy(ranks, inverse_permutation);\n}",
      "\n  Kokkos::View<float*, Kokkos::HostSpace> x_sorted(\"x_sorted\", x.extent(0));\n  Kokkos::View<size_t*, Kokkos::HostSpace> indices(\"indices\", x.extent(0));\n\n  // Fill indices with 0, 1, 2, ..., n-1\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    indices(i) = i;\n  });\n\n  // Sort indices based on values in x\n  Kokkos::sort(indices, Kokkos::ALL, [&x](int i, int j) {\n    return x(i) < x(j);\n  });\n\n  // Fill ranks with the sorted indices\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    ranks(indices(i)) = i;\n  });\n\n  Kokkos::fence();\n}",
      "\n  Kokkos::View<float*, Kokkos::HostSpace> sorted_x(\"sorted_x\", x.extent(0));\n  Kokkos::View<size_t*, Kokkos::HostSpace> permutation(\"permutation\", x.extent(0));\n\n  Kokkos::deep_copy(sorted_x, x);\n  Kokkos::sort_permutation(sorted_x, permutation);\n\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    ranks(permutation(i)) = i;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::View<float*, Kokkos::HostSpace> x_sorted(\"x_sorted\", x.extent(0));\n  Kokkos::View<size_t*, Kokkos::HostSpace> indices(\"indices\", x.extent(0));\n\n  // Fill indices with 0, 1, 2, ..., n-1\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    indices(i) = i;\n  });\n\n  // Sort x and update indices accordingly\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    x_sorted(i) = x(i);\n  });\n  Kokkos::sort(x_sorted, indices);\n\n  // Fill ranks with the sorted indices\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    ranks(indices(i)) = i;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::View<float*> sorted_x(\"sorted_x\", x.extent(0));\n  Kokkos::deep_copy(sorted_x, x);\n\n  Kokkos::View<size_t*> indices(\"indices\", x.extent(0));\n  Kokkos::View<size_t*> ranks_count(\"ranks_count\", x.extent(0));\n\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n    indices(i) = i;\n  });\n\n  Kokkos::sort_pair_indices(sorted_x, indices);\n\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n    ranks_count(indices(i)) = i;\n  });\n\n  Kokkos::deep_copy(ranks, ranks_count);\n}",
      "\n  Kokkos::View<float*, Kokkos::HostSpace> sorted_x(\"sorted_x\", x.extent(0));\n  Kokkos::View<size_t*, Kokkos::HostSpace> permutation(\"permutation\", x.extent(0));\n\n  Kokkos::deep_copy(sorted_x, x);\n  Kokkos::sort_permutation(sorted_x, permutation);\n\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    ranks(permutation(i)) = i;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::View<float*, Kokkos::HostSpace> sorted_x(\"sorted_x\", x.extent(0));\n  Kokkos::View<size_t*, Kokkos::HostSpace> sorted_indices(\"sorted_indices\", x.extent(0));\n\n  // Fill sorted_x with the values from x\n  Kokkos::deep_copy(sorted_x, x);\n\n  // Fill sorted_indices with the original indices\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    sorted_indices(i) = i;\n  });\n\n  // Sort sorted_x and sorted_indices in parallel\n  Kokkos::sort_pair(sorted_x, sorted_indices);\n\n  // Fill ranks with the sorted indices\n  Kokkos::deep_copy(ranks, sorted_indices);\n}\n",
      "\n  Kokkos::View<float*, Kokkos::HostSpace> sorted_x(\"sorted_x\", x.extent(0));\n  Kokkos::View<size_t*, Kokkos::HostSpace> permutation(\"permutation\", x.extent(0));\n\n  Kokkos::deep_copy(sorted_x, x);\n  Kokkos::sort_permutation(sorted_x, permutation);\n\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    ranks(permutation(i)) = i;\n  });\n  Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "42_sort_sorted_ranks",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}\n",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}\n",
      "\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = 0;\n        for (size_t j = 0; j < x.size(); ++j) {\n            if (x[j] < x[i] || (x[j] == x[i] && j < i)) {\n                #pragma omp atomic\n                ++ranks[i];\n            }\n        }\n    }\n}\n",
      "\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t count = 0;\n        for (size_t j = 0; j < x.size(); ++j) {\n            if (x[j] < x[i] || (x[j] == x[i] && j < i)) {\n                count++;\n            }\n        }\n        ranks[i] = count;\n    }\n}\n",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}\n",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}\n",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}\n",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}\n",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}\n",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}\n",
      "\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t rank = 0;\n        for (size_t j = 0; j < x.size(); ++j) {\n            if (x[j] < x[i]) {\n                rank++;\n            }\n        }\n        ranks[i] = rank;\n    }\n}",
      "\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = 0;\n        for (size_t j = 0; j < x.size(); ++j) {\n            if (x[j] < x[i] || (x[j] == x[i] && j < i)) {\n                #pragma omp atomic\n                ++ranks[i];\n            }\n        }\n    }\n}\n",
      "\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = 0;\n        for (size_t j = 0; j < x.size(); ++j) {\n            if (x[j] < x[i] || (x[j] == x[i] && j < i)) {\n                #pragma omp atomic\n                ranks[i]++;\n            }\n        }\n    }\n}\n",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}\n",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}\n",
      "\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = 0;\n        for (size_t j = 0; j < x.size(); ++j) {\n            if (x[j] < x[i]) {\n                #pragma omp atomic\n                ++ranks[i];\n            }\n        }\n    }\n}\n",
      "\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t rank = 0;\n        for (size_t j = 0; j < x.size(); ++j) {\n            if (x[j] < x[i]) {\n                rank++;\n            }\n        }\n        ranks[i] = rank;\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}\n",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}\n",
      "\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = 0;\n        for (size_t j = 0; j < x.size(); ++j) {\n            if (x[j] < x[i]) {\n                #pragma omp atomic\n                ++ranks[i];\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "42_sort_sorted_ranks",
    "parallelism_model": "serial",
    "prompt": "/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.clear();\n    for (float value : x) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), value);\n        ranks.push_back(std::distance(sorted_x.begin(), it));\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.clear();\n    for (float value : x) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), value);\n        ranks.push_back(std::distance(sorted_x.begin(), it));\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.clear();\n    for (float value : x) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), value);\n        ranks.push_back(std::distance(sorted_x.begin(), it));\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    \n    ranks.clear();\n    for (float val : x) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), val);\n        ranks.push_back(std::distance(sorted_x.begin(), it));\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.clear();\n    for (float value : x) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), value);\n        ranks.push_back(std::distance(sorted_x.begin(), it));\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.clear();\n    for (auto const& value : x) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), value);\n        ranks.push_back(std::distance(sorted_x.begin(), it));\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.clear();\n    for (float value : x) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), value);\n        ranks.push_back(std::distance(sorted_x.begin(), it));\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.clear();\n    for (float val : x) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), val);\n        ranks.push_back(std::distance(sorted_x.begin(), it));\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.clear();\n    for (float value : x) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), value);\n        ranks.push_back(std::distance(sorted_x.begin(), it));\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.clear();\n    for (float value : x) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), value);\n        ranks.push_back(std::distance(sorted_x.begin(), it));\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.clear();\n    for (float value : x) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), value);\n        ranks.push_back(std::distance(sorted_x.begin(), it));\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.clear();\n    for (float value : x) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), value);\n        ranks.push_back(std::distance(sorted_x.begin(), it));\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.clear();\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks.push_back(std::distance(sorted_x.begin(), it));\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.clear();\n    for (float value : x) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), value);\n        ranks.push_back(std::distance(sorted_x.begin(), it));\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.clear();\n    for (float value : x) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), value);\n        ranks.push_back(std::distance(sorted_x.begin(), it));\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.clear();\n    for (float value : x) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), value);\n        ranks.push_back(std::distance(sorted_x.begin(), it));\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.clear();\n    for (float value : x) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), value);\n        ranks.push_back(std::distance(sorted_x.begin(), it));\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.clear();\n    for (float value : x) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), value);\n        ranks.push_back(std::distance(sorted_x.begin(), it));\n    }\n}"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "42_sort_sorted_ranks",
    "parallelism_model": "hip",
    "prompt": "/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use AMD HIP to compute in parallel. The kernel will be launched with at least as many threads as elements in x.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\n__global__ void ranks(const float *x, size_t *ranks, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        size_t r = 0;\n        for (size_t i = 0; i < N; ++i) {\n            if (x[tid] > x[i]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; i++) {\n            if (x[tid] > x[i]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        size_t r = 0;\n        for (size_t i = 0; i < N; ++i) {\n            if (x[tid] > x[i]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        size_t count = 0;\n        for (size_t i = 0; i < N; ++i) {\n            if (x[tid] > x[i] || (x[tid] == x[i] && tid > i)) {\n                count++;\n            }\n        }\n        ranks[tid] = count;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int my_rank = 0;\n        for (int i = 0; i < N; i++) {\n            if (x[tid] > x[i] || (x[tid] == x[i] && tid > i)) {\n                my_rank++;\n            }\n        }\n        ranks[tid] = my_rank;\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        size_t count = 0;\n        for (size_t i = 0; i < N; ++i) {\n            if (x[tid] > x[i] || (x[tid] == x[i] && tid > i)) {\n                count++;\n            }\n        }\n        ranks[tid] = count;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; ++i) {\n            if (x[tid] > x[i]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        size_t count = 0;\n        for (size_t i = 0; i < N; ++i) {\n            if (x[tid] > x[i] || (x[tid] == x[i] && tid > i)) {\n                count++;\n            }\n        }\n        ranks[tid] = count;\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        size_t count = 0;\n        for (size_t i = 0; i < N; ++i) {\n            if (x[tid] > x[i] || (x[tid] == x[i] && tid > i)) {\n                count++;\n            }\n        }\n        ranks[tid] = count;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; i++) {\n            if (x[tid] > x[i]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; i++) {\n            if (x[tid] > x[i]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        size_t count = 0;\n        for (size_t i = 0; i < N; ++i) {\n            if (x[tid] > x[i] || (x[tid] == x[i] && tid > i)) {\n                count++;\n            }\n        }\n        ranks[tid] = count;\n    }\n}\n",
      "\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        size_t r = 0;\n        for (size_t i = 0; i < N; ++i) {\n            if (x[tid] > x[i]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; i++) {\n            if (x[tid] > x[i]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; i++) {\n            if (x[tid] > x[i]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; i++) {\n            if (x[tid] > x[i]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int id = blockIdx.x * blockDim.x + threadIdx.x;\n    if (id < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (x[i] < x[id] || (x[i] == x[id] && i < id)) {\n                count++;\n            }\n        }\n        ranks[id] = count;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; i++) {\n            if (x[tid] > x[i]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (x[i] < x[tid] || (x[i] == x[tid] && i < tid)) {\n                count++;\n            }\n        }\n        ranks[tid] = count;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; i++) {\n            if (x[tid] > x[i]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "42_sort_sorted_ranks",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every process has a complete copy of x. Store the result in ranks on process 0.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}",
      "\n    std::vector<float> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    ranks.resize(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        auto it = std::lower_bound(sorted_x.begin(), sorted_x.end(), x[i]);\n        ranks[i] = std::distance(sorted_x.begin(), it);\n    }\n}"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "42_sort_sorted_ranks",
    "parallelism_model": "cuda",
    "prompt": "/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use CUDA to compute in parallel. The kernel will be launched with at least as many threads as elements in x.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\n__global__ void ranks(const float *x, size_t *ranks, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; ++i) {\n            if (x[tid] > x[i]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; ++i) {\n            if (x[i] < x[tid]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; ++i) {\n            if (x[i] < x[tid]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; ++i) {\n            if (x[i] < x[tid]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; ++i) {\n            if (x[tid] > x[i]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; ++i) {\n            if (x[i] < x[tid]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; ++i) {\n            if (x[tid] > x[i]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; ++i) {\n            if (x[tid] > x[i]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; ++i) {\n            if (x[i] < x[tid] || (x[i] == x[tid] && i < tid)) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; ++i) {\n            if (x[i] < x[tid]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; i++) {\n            if (x[tid] > x[i]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; i++) {\n            if (x[tid] > x[i]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; ++i) {\n            if (x[i] < x[tid]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; i++) {\n            if (x[tid] > x[i]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; ++i) {\n            if (x[tid] > x[i]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; ++i) {\n            if (x[tid] > x[i]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; ++i) {\n            if (x[i] < x[tid]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; i++) {\n            if (x[tid] > x[i]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; ++i) {\n            if (x[i] < x[tid]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int r = 0;\n        for (int i = 0; i < N; ++i) {\n            if (x[i] < x[tid]) {\n                r++;\n            }\n        }\n        ranks[tid] = r;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "42_sort_sorted_ranks",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every process has a complete copy of x. Store the result in ranks on process 0.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_n;\n\n    std::vector<size_t> local_ranks(local_end - local_start);\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; i++) {\n        local_ranks[i - local_start] = std::distance(x.begin(), std::find(x.begin(), x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::vector<float> sorted_x = x;\n        std::sort(sorted_x.begin(), sorted_x.end());\n        for (int i = 0; i < x.size(); i++) {\n            ranks[i] = std::distance(sorted_x.begin(), std::find(sorted_x.begin(), sorted_x.end(), x[i]));\n        }\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_size = x.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? x.size() : start + local_size;\n\n    std::vector<float> local_x(x.begin() + start, x.begin() + end);\n    std::vector<size_t> local_ranks(local_x.size());\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_x.size(); ++i) {\n        local_ranks[i] = std::distance(x.begin(), std::lower_bound(x.begin(), x.end(), local_x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::vector<std::pair<float, size_t>> sorted_x;\n        for (size_t i = 0; i < x.size(); ++i) {\n            sorted_x.emplace_back(x[i], ranks[i]);\n        }\n        std::sort(sorted_x.begin(), sorted_x.end());\n        for (size_t i = 0; i < x.size(); ++i) {\n            ranks[i] = sorted_x[i].second;\n        }\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = local_start + local_n;\n\n    if (rank == size - 1) {\n        local_end = x.size();\n    }\n\n    std::vector<float> local_x(x.begin() + local_start, x.begin() + local_end);\n    std::vector<size_t> local_ranks(local_x.size());\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_x.size(); ++i) {\n        local_ranks[i] = 0;\n        for (int j = 0; j < x.size(); ++j) {\n            if (local_x[i] > x[j] || (local_x[i] == x[j] && j < local_start)) {\n                local_ranks[i]++;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_size = x.size() / size;\n    int local_start = rank * local_size;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_size;\n\n    std::vector<size_t> local_ranks(local_end - local_start);\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; ++i) {\n        local_ranks[i - local_start] = std::distance(x.begin(), std::find(x.begin(), x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::vector<std::pair<float, size_t>> sorted_x;\n        for (size_t i = 0; i < x.size(); ++i) {\n            sorted_x.push_back({x[i], ranks[i]});\n        }\n        std::sort(sorted_x.begin(), sorted_x.end());\n\n        for (size_t i = 0; i < x.size(); ++i) {\n            ranks[i] = sorted_x[i].second;\n        }\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_size = x.size() / size;\n    int local_start = rank * local_size;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_size;\n\n    std::vector<size_t> local_ranks(local_end - local_start);\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; ++i) {\n        local_ranks[i - local_start] = std::distance(x.begin(), std::find(x.begin(), x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::vector<std::pair<float, size_t>> sorted_x;\n        for (size_t i = 0; i < x.size(); ++i) {\n            sorted_x.push_back(std::make_pair(x[i], ranks[i]));\n        }\n        std::sort(sorted_x.begin(), sorted_x.end());\n\n        for (size_t i = 0; i < sorted_x.size(); ++i) {\n            ranks[sorted_x[i].second] = i;\n        }\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_size = x.size() / size;\n    int local_start = rank * local_size;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_size;\n\n    std::vector<size_t> local_ranks(local_end - local_start);\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; ++i) {\n        local_ranks[i - local_start] = std::distance(x.begin(), std::find(x.begin(), x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::vector<std::pair<float, size_t>> sorted_x;\n        for (size_t i = 0; i < x.size(); ++i) {\n            sorted_x.push_back({x[i], ranks[i]});\n        }\n        std::sort(sorted_x.begin(), sorted_x.end());\n\n        for (size_t i = 0; i < x.size(); ++i) {\n            ranks[i] = sorted_x[i].second;\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_n;\n\n    std::vector<size_t> local_ranks(local_end - local_start);\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; ++i) {\n        local_ranks[i - local_start] = std::distance(x.begin(), std::find(x.begin(), x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_n, MPI_UNSIGNED_LONG, ranks.data(), local_n, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = size * local_n; i < x.size(); ++i) {\n            ranks[i] = std::distance(x.begin(), std::find(x.begin(), x.end(), x[i]));\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t count = 0;\n        for (size_t j = 0; j < x.size(); ++j) {\n            if (x[j] < x[i] || (x[j] == x[i] && j < i)) {\n                count++;\n            }\n        }\n        ranks[i] = count;\n    }\n\n    if (rank == 0) {\n        std::vector<size_t> recv_counts(size);\n        std::vector<size_t> displs(size, 0);\n        MPI_Gather(&x[0], x.size(), MPI_FLOAT, &ranks[0], x.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_n;\n\n    std::vector<size_t> local_ranks(local_end - local_start);\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; ++i) {\n        local_ranks[i - local_start] = std::distance(x.begin(), std::find(x.begin(), x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::vector<float> sorted_x = x;\n        std::sort(sorted_x.begin(), sorted_x.end());\n        for (int i = 0; i < x.size(); ++i) {\n            ranks[i] = std::distance(sorted_x.begin(), std::find(sorted_x.begin(), sorted_x.end(), x[i]));\n        }\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_n;\n\n    std::vector<size_t> local_ranks(local_end - local_start);\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; i++) {\n        local_ranks[i - local_start] = std::distance(x.begin(), std::find(x.begin(), x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::vector<float> sorted_x = x;\n        std::sort(sorted_x.begin(), sorted_x.end());\n        for (int i = 0; i < x.size(); i++) {\n            ranks[i] = std::distance(sorted_x.begin(), std::find(sorted_x.begin(), sorted_x.end(), x[i]));\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_n;\n\n    std::vector<size_t> local_ranks(local_end - local_start);\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; i++) {\n        local_ranks[i - local_start] = std::distance(x.begin(), std::find(x.begin(), x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::vector<std::pair<float, size_t>> sorted_x;\n        for (size_t i = 0; i < x.size(); i++) {\n            sorted_x.push_back(std::make_pair(x[i], ranks[i]));\n        }\n        std::sort(sorted_x.begin(), sorted_x.end());\n\n        for (size_t i = 0; i < sorted_x.size(); i++) {\n            ranks[sorted_x[i].second] = i;\n        }\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<float> local_x(local_n);\n    std::vector<size_t> local_ranks(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_FLOAT, local_x.data(), local_n, MPI_FLOAT, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_n; ++i) {\n        local_ranks[i] = 0;\n        for (int j = 0; j < x.size(); ++j) {\n            if (local_x[i] > x[j] || (local_x[i] == x[j] && j < rank * local_n + i)) {\n                local_ranks[i]++;\n            }\n        }\n    }\n\n    MPI_Gather(local_ranks.data(), local_n, MPI_UNSIGNED_LONG, ranks.data(), local_n, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_n;\n\n    std::vector<size_t> local_ranks(local_end - local_start);\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; ++i) {\n        local_ranks[i - local_start] = std::distance(x.begin(), std::find(x.begin(), x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::vector<std::pair<float, size_t>> sorted_x;\n        for (size_t i = 0; i < x.size(); ++i) {\n            sorted_x.push_back({x[i], ranks[i]});\n        }\n        std::sort(sorted_x.begin(), sorted_x.end());\n\n        for (size_t i = 0; i < sorted_x.size(); ++i) {\n            ranks[sorted_x[i].second] = i;\n        }\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_size = x.size() / size;\n    int local_start = rank * local_size;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_size;\n\n    std::vector<float> local_x(x.begin() + local_start, x.begin() + local_end);\n    std::vector<size_t> local_ranks(local_x.size());\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_x.size(); ++i) {\n        local_ranks[i] = std::distance(x.begin(), std::find(x.begin(), x.end(), local_x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_n;\n\n    std::vector<size_t> local_ranks(local_end - local_start);\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; ++i) {\n        local_ranks[i - local_start] = std::distance(x.begin(), std::find(x.begin(), x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_n, MPI_UNSIGNED_LONG, ranks.data(), local_n, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_n;\n\n    std::vector<size_t> local_ranks(local_end - local_start);\n    #pragma omp parallel for\n    for (int i = local_start; i < local_end; ++i) {\n        local_ranks[i - local_start] = std::distance(x.begin(), std::find(x.begin(), x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::vector<float> sorted_x = x;\n        std::sort(sorted_x.begin(), sorted_x.end());\n        for (int i = 0; i < x.size(); ++i) {\n            ranks[i] = std::distance(sorted_x.begin(), std::find(sorted_x.begin(), sorted_x.end(), x[i]));\n        }\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<size_t> local_ranks(local_n);\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_n; ++i) {\n        local_ranks[i] = 0;\n        for (int j = 0; j < x.size(); ++j) {\n            if (x[j] < x[rank * local_n + i]) {\n                local_ranks[i]++;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_n, MPI_UNSIGNED_LONG, ranks.data(), local_n, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_n;\n\n    std::vector<float> local_x(x.begin() + local_start, x.begin() + local_end);\n    std::vector<size_t> local_ranks(local_x.size());\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_x.size(); ++i) {\n        local_ranks[i] = 0;\n        for (int j = 0; j < x.size(); ++j) {\n            if (local_x[i] > x[j] || (local_x[i] == x[j] && j < local_start)) {\n                local_ranks[i]++;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_size = x.size() / size;\n    int local_start = rank * local_size;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_size;\n\n    std::vector<float> local_x(x.begin() + local_start, x.begin() + local_end);\n    std::vector<size_t> local_ranks(local_x.size());\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_x.size(); ++i) {\n        local_ranks[i] = std::distance(x.begin(), std::lower_bound(x.begin(), x.end(), local_x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::vector<std::pair<float, size_t>> sorted_x;\n        for (size_t i = 0; i < x.size(); ++i) {\n            sorted_x.emplace_back(x[i], ranks[i]);\n        }\n        std::sort(sorted_x.begin(), sorted_x.end());\n\n        for (size_t i = 0; i < x.size(); ++i) {\n            ranks[i] = sorted_x[i].second;\n        }\n    }\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_size = x.size() / size;\n    int local_start = rank * local_size;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_size;\n\n    std::vector<float> local_x(x.begin() + local_start, x.begin() + local_end);\n    std::vector<size_t> local_ranks(local_x.size());\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_x.size(); ++i) {\n        local_ranks[i] = 0;\n        for (int j = 0; j < x.size(); ++j) {\n            if (local_x[i] > x[j] || (local_x[i] == x[j] && j < local_start)) {\n                local_ranks[i]++;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "44_sort_sort_non-zero_elements",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Sort the array x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use Kokkos to sort in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(Kokkos::View<int*> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\nKokkos::sort(x, Kokkos::View<int*>::HostMirror(x));\n}",
      "\nKokkos::sort(x, [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return a < b;\n    }\n});\n}",
      "\nKokkos::sort(x, Kokkos::View<int*>::HostMirror(x));\n}",
      "\nKokkos::sort(x, [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return a < b;\n    }\n});\n}",
      "\nKokkos::sort(x, [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return a < b;\n    }\n});\n}",
      "\nKokkos::sort(x, Kokkos::View<int*>::HostMirror(x));\n}",
      "\nKokkos::sort(x, Kokkos::View<int*>::HostMirror(x));\n}",
      "\nKokkos::sort(x, Kokkos::View<int*>::HostMirror(x));\n}",
      "\nKokkos::sort(x, [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return false;\n    }\n});\n}",
      "\nKokkos::sort(x, Kokkos::View<int*>::HostMirror(x));\n}",
      "\nKokkos::View<int*>::HostMirror x_host = Kokkos::create_mirror_view(x);\nKokkos::deep_copy(x_host, x);\n\nstd::vector<int> nonZeroElements;\nfor (int i = 0; i < x_host.size(); ++i) {\n    if (x_host(i) != 0) {\n        nonZeroElements.push_back(x_host(i));\n    }\n}\n\nstd::sort(nonZeroElements.begin(), nonZeroElements.end());\n\nint nonZeroIndex = 0;\nfor (int i = 0; i < x_host.size(); ++i) {\n    if (x_host(i) != 0) {\n        x_host(i) = nonZeroElements[nonZeroIndex];\n        ++nonZeroIndex;\n    }\n}\n\nKokkos::deep_copy(x, x_host);\n}",
      "\n  Kokkos::sort(x, Kokkos::View<int*>::HostMirror(x));\n  int numZeros = 0;\n  for (int i = 0; i < x.extent(0); i++) {\n    if (x(i) == 0) {\n      numZeros++;\n    }\n  }\n  Kokkos::View<int*> sortedZeros(\"sortedZeros\", numZeros);\n  int j = 0;\n  for (int i = 0; i < x.extent(0); i++) {\n    if (x(i) != 0) {\n      x(j) = x(i);\n      j++;\n    } else {\n      sortedZeros(j - x.extent(0) + numZeros) = 0;\n    }\n  }\n  Kokkos::sort(x);\n  j = 0;\n  for (int i = x.extent(0) - numZeros; i < x.extent(0); i++) {\n    x(i) = sortedZeros(j);\n    j++;\n  }\n```\n}",
      "\nKokkos::sort(x, Kokkos::View<int*>::HostMirror(x));\n}",
      "\nKokkos::sort(x, Kokkos::View<int*>::HostMirror(x));\n}",
      "\nKokkos::sort(x, Kokkos::View<int*>::HostMirror(x));\n}",
      "\nKokkos::sort(x, Kokkos::View<int*>::HostMirror(x));\n}",
      "\nKokkos::sort(x, Kokkos::View<int*>::HostMirror(x));\n}",
      "\nKokkos::sort(x, Kokkos::View<int*>::HostMirror(x));\n}",
      "\nKokkos::sort(x, Kokkos::View<int*>::HostMirror(x));\n}",
      "\nKokkos::sort(x, Kokkos::View<int*>::HostMirror(x));\n}"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "44_sort_sort_non-zero_elements",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use OpenMP to sort x in parallel. \n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (x[j] != 0 && x[j] < x[i]) {\n                int temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (x[j] != 0 && x[j] < x[i]) {\n                int temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (x[j] != 0 && x[j] < x[i]) {\n                int temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (x[j] != 0 && x[j] < x[i]) {\n                int temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (x[j] != 0 && x[j] < x[i]) {\n                int temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); ++i) {\n    if (x[i] != 0) {\n        for (int j = i + 1; j < x.size(); ++j) {\n            if (x[j] != 0 && x[j] < x[i]) {\n                int temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (x[j] != 0 && x[j] < x[i]) {\n                int temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (x[j] != 0 && x[j] < x[i]) {\n                int temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n        #pragma omp critical\n        {\n            for (int j = i + 1; j < x.size(); j++) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    std::swap(x[i], x[j]);\n                }\n            }\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (x[j] != 0 && x[j] < x[i]) {\n                int temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (x[j] != 0 && x[j] < x[i]) {\n                int temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (x[j] != 0 && x[j] < x[i]) {\n                int temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (x[j] != 0 && x[j] < x[i]) {\n                int temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n        #pragma omp critical\n        {\n            for (int j = i + 1; j < x.size(); j++) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    std::swap(x[i], x[j]);\n                }\n            }\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (x[j] != 0 && x[j] < x[i]) {\n                int temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (x[j] != 0 && x[j] < x[i]) {\n                int temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (x[j] != 0 && x[j] < x[i]) {\n                int temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (x[j] != 0 && x[j] < x[i]) {\n                int temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (x[j] != 0 && x[j] < x[i]) {\n                int temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); ++i) {\n    if (x[i] != 0) {\n        for (int j = i + 1; j < x.size(); ++j) {\n            if (x[j] != 0 && x[j] < x[i]) {\n                int temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}\n}"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "44_sort_sort_non-zero_elements",
    "parallelism_model": "serial",
    "prompt": "/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place.\n   Example:\n\n\t input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\nstd::vector<int> nonZeroElements;\nfor (int num : x) {\n    if (num != 0) {\n        nonZeroElements.push_back(num);\n    }\n}\nstd::sort(nonZeroElements.begin(), nonZeroElements.end());\nauto it = nonZeroElements.begin();\nfor (int &num : x) {\n    if (num != 0) {\n        num = *it;\n        ++it;\n    }\n}\n}",
      "\nstd::sort(x.begin(), x.end(), [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return a < b;\n    }\n});\n}",
      "\nstd::sort(x.begin(), x.end(), [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return a < b;\n    }\n});\n}",
      "\nstd::sort(x.begin(), x.end(), [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return a < b;\n    }\n});\n}",
      "\nstd::sort(x.begin(), x.end(), [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return a < b;\n    }\n});\n}",
      "\nstd::sort(x.begin(), x.end(), [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return a < b;\n    }\n});\n}",
      "\nstd::sort(x.begin(), x.end(), [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return a < b;\n    }\n});\n}",
      "\nstd::sort(x.begin(), x.end(), [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return a < b;\n    }\n});\n}",
      "\nstd::sort(x.begin(), x.end(), [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return a < b;\n    }\n});\n}",
      "\nstd::sort(x.begin(), x.end(), [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return a < b;\n    }\n});\n}",
      "\nstd::sort(x.begin(), x.end(), [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return a < b;\n    }\n});\n}",
      "\nstd::sort(x.begin(), x.end(), [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return a < b;\n    }\n});\n}",
      "\nstd::sort(x.begin(), x.end(), [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return a < b;\n    }\n});\n}",
      "\nstd::sort(x.begin(), x.end(), [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return a < b;\n    }\n});\n}",
      "\nstd::sort(x.begin(), x.end(), [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return a < b;\n    }\n});\n}",
      "\nstd::sort(x.begin(), x.end(), [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return a < b;\n    }\n});\n}",
      "\nstd::sort(x.begin(), x.end(), [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return a < b;\n    }\n});\n}",
      "\nstd::sort(x.begin(), x.end(), [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return a < b;\n    }\n});\n}",
      "\nstd::sort(x.begin(), x.end(), [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return a < b;\n    }\n});\n}",
      "\nstd::sort(x.begin(), x.end(), [](int a, int b) {\n    if (a == 0 || b == 0) {\n        return a < b;\n    } else {\n        return a < b;\n    }\n});\n}"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "44_sort_sort_non-zero_elements",
    "parallelism_model": "hip",
    "prompt": "/* Sort the array x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use AMD HIP to compute in parallel. The kernel will be launched with 1 thread per element.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\n__global__ void sortIgnoreZero(int *x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  for (int i = 0; i < N; i++) {\n      if (x[i] != 0) {\n          for (int j = i + 1; j < N; j++) {\n              if (x[j] != 0 && x[j] < x[i]) {\n                  int temp = x[i];\n                  x[i] = x[j];\n                  x[j] = temp;\n              }\n          }\n      }\n  }\n}\n",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n  for (int i = 0; i < N; i++) {\n    if (x[i] != 0) {\n      for (int j = i + 1; j < N; j++) {\n        if (x[j] != 0 && x[j] < x[i]) {\n          int temp = x[i];\n          x[i] = x[j];\n          x[j] = temp;\n        }\n      }\n    }\n  }\n}\n",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n  for (int i = 0; i < N; ++i) {\n    if (x[i] != 0) {\n      for (int j = i + 1; j < N; ++j) {\n        if (x[j] != 0 && x[j] < x[i]) {\n          int temp = x[i];\n          x[i] = x[j];\n          x[j] = temp;\n        }\n      }\n    }\n  }\n}\n",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (int i = 0; i < N; i++) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; j++) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (int i = 0; i < N; i++) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; j++) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n  for (int i = 0; i < N; ++i) {\n    if (x[i] != 0) {\n      for (int j = i + 1; j < N; ++j) {\n        if (x[j] != 0 && x[j] < x[i]) {\n          int temp = x[i];\n          x[i] = x[j];\n          x[j] = temp;\n        }\n      }\n    }\n  }\n}\n",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n  for (int i = 0; i < N; ++i) {\n    if (x[i] != 0) {\n      for (int j = i + 1; j < N; ++j) {\n        if (x[j] != 0 && x[j] < x[i]) {\n          int temp = x[i];\n          x[i] = x[j];\n          x[j] = temp;\n        }\n      }\n    }\n  }\n}\n",
      "\n    for (int i = 0; i < N; i++) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; j++) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (int i = 0; i < N; i++) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; j++) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}",
      "\n  for (int i = 0; i < N; i++) {\n    if (x[i] != 0) {\n      for (int j = i + 1; j < N; j++) {\n        if (x[j] != 0 && x[j] < x[i]) {\n          int temp = x[i];\n          x[i] = x[j];\n          x[j] = temp;\n        }\n      }\n    }\n  }\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "44_sort_sort_non-zero_elements",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use MPI to sort x in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  std::vector<int> nonZeroElements;\n  for (int i = 0; i < x.size(); ++i) {\n    if (x[i] != 0) {\n      nonZeroElements.push_back(x[i]);\n    }\n  }\n\n  std::sort(nonZeroElements.begin(), nonZeroElements.end());\n\n  int nonZeroCount = nonZeroElements.size();\n  MPI_Gather(&nonZeroCount, 1, MPI_INT, 0, 0, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (MPI::COMM_WORLD.Get_rank() == 0) {\n    std::vector<int> allNonZeroElements(x.size());\n    MPI_Gather(&nonZeroElements[0], nonZeroCount, MPI_INT, &allNonZeroElements[0], nonZeroCount, MPI_INT, 0, MPI_COMM_WORLD);\n    std::sort(allNonZeroElements.begin(), allNonZeroElements.end());\n    x = allNonZeroElements;\n  }\n```\n}",
      "\n  std::vector<int> nonZeroElements;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      nonZeroElements.push_back(x[i]);\n    }\n  }\n\n  std::sort(nonZeroElements.begin(), nonZeroElements.end());\n\n  int nonZeroCount = nonZeroElements.size();\n  MPI_Gather(&nonZeroCount, 1, MPI_INT, &nonZeroCount, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (MPI::COMM_WORLD.Get_rank() == 0) {\n    std::vector<int> sortedNonZeroElements(x.size());\n    MPI_Gatherv(nonZeroElements.data(), nonZeroCount, MPI_INT, sortedNonZeroElements.data(), nonZeroCount, MPI_INT, 0, MPI_COMM_WORLD);\n    int zeroCount = x.size() - nonZeroCount;\n    for (int i = 0; i < zeroCount; i++) {\n      sortedNonZeroElements.push_back(0);\n    }\n    x = sortedNonZeroElements;\n  }\n}",
      "\n  std::vector<int> nonZeroElements;\n  for (int i = 0; i < x.size(); ++i) {\n    if (x[i] != 0) {\n      nonZeroElements.push_back(x[i]);\n    }\n  }\n\n  std::sort(nonZeroElements.begin(), nonZeroElements.end());\n\n  int nonZeroCount = nonZeroElements.size();\n  MPI_Gather(&nonZeroCount, 1, MPI_INT, &nonZeroCount, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (MPI::COMM_WORLD.Get_rank() == 0) {\n    std::vector<int> receivedCounts(MPI::COMM_WORLD.Get_size());\n    MPI_Gather(&nonZeroCount, 1, MPI_INT, &receivedCounts[0], 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    std::vector<int> displs(MPI::COMM_WORLD.Get_size(), 0);\n    for (int i = 1; i < MPI::COMM_WORLD.Get_size(); ++i) {\n      displs[i] = displs[i - 1] + receivedCounts[i - 1];\n    }\n\n    std::vector<int> sortedNonZeroElements(displs.back() + nonZeroCount);\n    MPI_Gatherv(&nonZeroElements[0], nonZeroCount, MPI_INT, &sortedNonZeroElements[0], &receivedCounts[0], &displs[0], MPI_INT, 0, MPI_COMM_WORLD);\n\n    std::sort(x.begin(), x.end(), [](int a, int b) {\n      if (a == 0 || b == 0) {\n        return a < b;\n      } else {\n        return false;\n      }\n    });\n\n    int j = 0;\n    for (int i = 0; i < x.size(); ++i) {\n      if (x[i] != 0) {\n        x[i] = sortedNonZeroElements[j++];\n      }\n    }\n  }\n```\n}",
      "\n  std::vector<int> nonZeroElements;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      nonZeroElements.push_back(x[i]);\n    }\n  }\n\n  std::sort(nonZeroElements.begin(), nonZeroElements.end());\n\n  int nonZeroIndex = 0;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      x[i] = nonZeroElements[nonZeroIndex];\n      nonZeroIndex++;\n    }\n  }\n}",
      "\n  std::vector<int> nonZeroElements;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      nonZeroElements.push_back(x[i]);\n    }\n  }\n\n  std::sort(nonZeroElements.begin(), nonZeroElements.end());\n\n  int nonZeroCount = nonZeroElements.size();\n  MPI_Gather(&nonZeroCount, 1, MPI_INT, &nonZeroCount, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (MPI::COMM_WORLD.Get_rank() == 0) {\n    std::vector<int> sortedNonZeroElements(nonZeroCount);\n    MPI_Gather(nonZeroElements.data(), nonZeroCount, MPI_INT, sortedNonZeroElements.data(), nonZeroCount, MPI_INT, 0, MPI_COMM_WORLD);\n\n    int index = 0;\n    for (int i = 0; i < x.size(); i++) {\n      if (x[i] != 0) {\n        x[i] = sortedNonZeroElements[index];\n        index++;\n      }\n    }\n  }\n```\n}",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> local_x = x;\n\n  // Remove zero valued elements from local_x\n  local_x.erase(std::remove(local_x.begin(), local_x.end(), 0), local_x.end());\n\n  // Sort local_x\n  std::sort(local_x.begin(), local_x.end());\n\n  // Gather all non-zero elements on rank 0\n  std::vector<int> recv_counts(size);\n  std::vector<int> displs(size);\n  MPI_Gather(&local_x.size(), 1, MPI_INT, recv_counts.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    displs[0] = 0;\n    for (int i = 1; i < size; i++) {\n      displs[i] = displs[i - 1] + recv_counts[i - 1];\n    }\n  }\n\n  std::vector<int> sorted_x(x.size());\n  MPI_Gatherv(local_x.data(), local_x.size(), MPI_INT, sorted_x.data(), recv_counts.data(), displs.data(), MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    // Copy zero valued elements back to their original positions\n    for (int i = 0; i < x.size(); i++) {\n      if (x[i] == 0) {\n        sorted_x.insert(sorted_x.begin() + i, 0);\n      }\n    }\n    x = sorted_x;\n  }\n}\n",
      "\n  std::vector<int> nonZeroElements;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      nonZeroElements.push_back(x[i]);\n    }\n  }\n\n  std::sort(nonZeroElements.begin(), nonZeroElements.end());\n\n  int nonZeroCount = nonZeroElements.size();\n  MPI_Gather(&nonZeroCount, 1, MPI_INT, &nonZeroCount, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (MPI::COMM_WORLD.Get_rank() == 0) {\n    std::vector<int> sortedNonZeroElements(nonZeroCount);\n    MPI_Gather(nonZeroElements.data(), nonZeroCount, MPI_INT, sortedNonZeroElements.data(), nonZeroCount, MPI_INT, 0, MPI_COMM_WORLD);\n\n    int index = 0;\n    for (int i = 0; i < x.size(); i++) {\n      if (x[i] != 0) {\n        x[i] = sortedNonZeroElements[index++];\n      }\n    }\n  }\n```\n}",
      "\nstd::vector<int> nonZeroElements;\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n        nonZeroElements.push_back(x[i]);\n    }\n}\nstd::sort(nonZeroElements.begin(), nonZeroElements.end());\n\nint nonZeroCount = nonZeroElements.size();\nMPI_Gather(&nonZeroCount, 1, MPI_INT, &nonZeroCount, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\nif (MPI::COMM_WORLD.Get_rank() == 0) {\n    std::vector<int> sortedNonZeroElements(x.size());\n    MPI_Gatherv(nonZeroElements.data(), nonZeroCount, MPI_INT, sortedNonZeroElements.data(), nonZeroCount, MPI_INT, 0, MPI_COMM_WORLD);\n    std::sort(sortedNonZeroElements.begin(), sortedNonZeroElements.end());\n    x = sortedNonZeroElements;\n}\n}",
      "\n  std::vector<int> nonZeroElements;\n  std::vector<int> zeroIndices;\n\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      nonZeroElements.push_back(x[i]);\n    } else {\n      zeroIndices.push_back(i);\n    }\n  }\n\n  std::sort(nonZeroElements.begin(), nonZeroElements.end());\n\n  for (int i = 0; i < nonZeroElements.size(); i++) {\n    x[zeroIndices[i]] = nonZeroElements[i];\n  }\n```\n}",
      "\n  std::vector<int> nonZeroElements;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      nonZeroElements.push_back(x[i]);\n    }\n  }\n\n  std::sort(nonZeroElements.begin(), nonZeroElements.end());\n\n  int nonZeroIndex = 0;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      x[i] = nonZeroElements[nonZeroIndex];\n      nonZeroIndex++;\n    }\n  }\n}",
      "\n  std::vector<int> nonZeroElements;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      nonZeroElements.push_back(x[i]);\n    }\n  }\n\n  std::sort(nonZeroElements.begin(), nonZeroElements.end());\n\n  int nonZeroIndex = 0;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      x[i] = nonZeroElements[nonZeroIndex];\n      nonZeroIndex++;\n    }\n  }\n}",
      "\n  std::vector<int> nonZeroElements;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      nonZeroElements.push_back(x[i]);\n    }\n  }\n\n  std::sort(nonZeroElements.begin(), nonZeroElements.end());\n\n  int nonZeroCount = nonZeroElements.size();\n  MPI_Gather(&nonZeroCount, 1, MPI_INT, &nonZeroCount, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (MPI::COMM_WORLD.Get_rank() == 0) {\n    std::vector<int> sortedNonZeroElements(nonZeroCount);\n    MPI_Gather(nonZeroElements.data(), nonZeroCount, MPI_INT, sortedNonZeroElements.data(), nonZeroCount, MPI_INT, 0, MPI_COMM_WORLD);\n    std::sort(x.begin(), x.end());\n    int zeroCount = x.size() - nonZeroCount;\n    for (int i = 0; i < zeroCount; i++) {\n      sortedNonZeroElements.push_back(0);\n    }\n    x = sortedNonZeroElements;\n  }\n}",
      "\n  std::vector<int> nonZeroElements;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      nonZeroElements.push_back(x[i]);\n    }\n  }\n\n  std::sort(nonZeroElements.begin(), nonZeroElements.end());\n\n  int nonZeroCount = nonZeroElements.size();\n  MPI_Gather(&nonZeroCount, 1, MPI_INT, &nonZeroCount, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (MPI::COMM_WORLD.Get_rank() == 0) {\n    std::vector<int> sortedNonZeroElements(x.size());\n    MPI_Gatherv(nonZeroElements.data(), nonZeroCount, MPI_INT, sortedNonZeroElements.data(), nonZeroCount, MPI_INT, 0, MPI_COMM_WORLD);\n    int zeroCount = x.size() - nonZeroCount;\n    for (int i = 0; i < zeroCount; i++) {\n      sortedNonZeroElements.push_back(0);\n    }\n    x = sortedNonZeroElements;\n  }\n}",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> local_x = x;\n  std::vector<int> non_zero_elements;\n\n  for (int i = 0; i < local_x.size(); ++i) {\n    if (local_x[i] != 0) {\n      non_zero_elements.push_back(local_x[i]);\n    }\n  }\n\n  std::sort(non_zero_elements.begin(), non_zero_elements.end());\n\n  if (rank == 0) {\n    x.clear();\n    x.reserve(non_zero_elements.size() + std::count(local_x.begin(), local_x.end(), 0));\n    for (int i = 0, j = 0; i < local_x.size(); ++i) {\n      if (local_x[i] == 0) {\n        x.push_back(0);\n      } else {\n        x.push_back(non_zero_elements[j++]);\n      }\n    }\n  }\n  MPI_Bcast(&x[0], x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n}",
      "\n  std::vector<int> nonZeroElements;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      nonZeroElements.push_back(x[i]);\n    }\n  }\n  std::sort(nonZeroElements.begin(), nonZeroElements.end());\n\n  int nonZeroCount = nonZeroElements.size();\n  MPI_Gather(&nonZeroCount, 1, MPI_INT, &nonZeroCount, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (MPI::COMM_WORLD.Get_rank() == 0) {\n    std::vector<int> sortedNonZeroElements(nonZeroCount);\n    MPI_Gather(nonZeroElements.data(), nonZeroCount, MPI_INT, sortedNonZeroElements.data(), nonZeroCount, MPI_INT, 0, MPI_COMM_WORLD);\n\n    int index = 0;\n    for (int i = 0; i < x.size(); i++) {\n      if (x[i] != 0) {\n        x[i] = sortedNonZeroElements[index];\n        index++;\n      }\n    }\n  }\n```\n}",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> local_x = x;\n  std::vector<int> non_zero_elements;\n\n  for (int i = 0; i < local_x.size(); i++) {\n    if (local_x[i] != 0) {\n      non_zero_elements.push_back(local_x[i]);\n    }\n  }\n\n  std::sort(non_zero_elements.begin(), non_zero_elements.end());\n\n  if (rank == 0) {\n    x.clear();\n    int non_zero_index = 0;\n    for (int i = 0; i < local_x.size(); i++) {\n      if (local_x[i] != 0) {\n        x.push_back(non_zero_elements[non_zero_index]);\n        non_zero_index++;\n      } else {\n        x.push_back(0);\n      }\n    }\n  }\n```\n}",
      "\n  std::vector<int> nonZeroElements;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      nonZeroElements.push_back(x[i]);\n    }\n  }\n\n  std::sort(nonZeroElements.begin(), nonZeroElements.end());\n\n  int nonZeroIndex = 0;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      x[i] = nonZeroElements[nonZeroIndex];\n      nonZeroIndex++;\n    }\n  }\n```\n}",
      "\n  std::vector<int> nonZeroElements;\n  for (int i = 0; i < x.size(); i++) {\n      if (x[i] != 0) {\n          nonZeroElements.push_back(x[i]);\n      }\n  }\n\n  std::sort(nonZeroElements.begin(), nonZeroElements.end());\n\n  int nonZeroIndex = 0;\n  for (int i = 0; i < x.size(); i++) {\n      if (x[i] != 0) {\n          x[i] = nonZeroElements[nonZeroIndex];\n          nonZeroIndex++;\n      }\n  }\n}",
      "\n  std::vector<int> nonZeroElements;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      nonZeroElements.push_back(x[i]);\n    }\n  }\n\n  std::sort(nonZeroElements.begin(), nonZeroElements.end());\n\n  int nonZeroIndex = 0;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      x[i] = nonZeroElements[nonZeroIndex];\n      nonZeroIndex++;\n    }\n  }\n```\n}",
      "\n  std::vector<int> nonZeroElements;\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] != 0) {\n      nonZeroElements.push_back(x[i]);\n    }\n  }\n  std::sort(nonZeroElements.begin(), nonZeroElements.end());\n\n  int nonZeroCount = nonZeroElements.size();\n  MPI_Gather(&nonZeroCount, 1, MPI_INT, &nonZeroCount, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (MPI::COMM_WORLD.Get_rank() == 0) {\n    std::vector<int> sortedNonZeroElements(nonZeroCount);\n    MPI_Gather(nonZeroElements.data(), nonZeroCount, MPI_INT, sortedNonZeroElements.data(), nonZeroCount, MPI_INT, 0, MPI_COMM_WORLD);\n    std::sort(x.begin(), x.end());\n    int zeroCount = x.size() - nonZeroCount;\n    for (int i = 0; i < zeroCount; i++) {\n      sortedNonZeroElements.push_back(0);\n    }\n    x = sortedNonZeroElements;\n  }\n}"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "44_sort_sort_non-zero_elements",
    "parallelism_model": "cuda",
    "prompt": "/* Sort the array x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use CUDA to compute in parallel. The kernel will be launched with 1 thread per element.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\n__global__ void sortIgnoreZero(int *x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (int i = 0; i < N; i++) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; j++) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}",
      "\n    for (int i = 0; i < N; i++) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; j++) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (int i = 0; i < N; ++i) {\n        if (x[i] != 0) {\n            for (int j = i + 1; j < N; ++j) {\n                if (x[j] != 0 && x[j] < x[i]) {\n                    int temp = x[i];\n                    x[i] = x[j];\n                    x[j] = temp;\n                }\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "44_sort_sort_non-zero_elements",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use MPI and OpenMP to sort x in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            #pragma omp critical\n            {\n                std::sort(x.begin(), x.end());\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::sort(x.begin(), x.end());\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            #pragma omp critical\n            {\n                std::sort(x.begin(), x.end());\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::sort(x.begin(), x.end());\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            #pragma omp critical\n            {\n                std::sort(x.begin(), x.end());\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::sort(x.begin(), x.end());\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            #pragma omp critical\n            {\n                int min_index = i;\n                for (int j = i + 1; j < x.size(); j++) {\n                    if (x[j] != 0 && (x[j] < x[min_index] || x[min_index] == 0)) {\n                        min_index = j;\n                    }\n                }\n                if (min_index != i) {\n                    int temp = x[i];\n                    x[i] = x[min_index];\n                    x[min_index] = temp;\n                }\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<int> recvbuf(x.size() * size);\n        MPI_Gather(&x[0], x.size(), MPI_INT, &recvbuf[0], x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        std::sort(recvbuf.begin(), recvbuf.end());\n        x = recvbuf;\n    } else {\n        MPI_Gather(&x[0], x.size(), MPI_INT, NULL, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n```\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            #pragma omp critical\n            {\n                std::sort(x.begin(), x.end());\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::sort(x.begin(), x.end());\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            #pragma omp critical\n            {\n                std::sort(x.begin(), x.end());\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::sort(x.begin(), x.end());\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            #pragma omp critical\n            {\n                int min_index = i;\n                for (int j = i + 1; j < x.size(); j++) {\n                    if (x[j] != 0 && (x[j] < x[min_index] || x[min_index] == 0)) {\n                        min_index = j;\n                    }\n                }\n                if (min_index != i) {\n                    int temp = x[i];\n                    x[i] = x[min_index];\n                    x[min_index] = temp;\n                }\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<int> recvbuf(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_INT, recvbuf.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        std::sort(recvbuf.begin(), recvbuf.end());\n        std::copy(recvbuf.begin(), recvbuf.end(), x.begin());\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            #pragma omp critical\n            {\n                std::sort(x.begin(), x.end());\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::sort(x.begin(), x.end());\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            #pragma omp critical\n            {\n                int min_index = i;\n                for (int j = i + 1; j < x.size(); j++) {\n                    if (x[j] != 0 && (x[j] < x[min_index] || x[min_index] == 0)) {\n                        min_index = j;\n                    }\n                }\n                if (min_index != i) {\n                    int temp = x[i];\n                    x[i] = x[min_index];\n                    x[min_index] = temp;\n                }\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<int> recvbuf(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_INT, recvbuf.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        std::sort(recvbuf.begin(), recvbuf.end());\n        std::copy(recvbuf.begin(), recvbuf.end(), x.begin());\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n```\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            #pragma omp critical\n            {\n                int min_idx = i;\n                for (int j = i + 1; j < x.size(); j++) {\n                    if (x[j] != 0 && x[j] < x[min_idx]) {\n                        min_idx = j;\n                    }\n                }\n                if (min_idx != i) {\n                    int temp = x[i];\n                    x[i] = x[min_idx];\n                    x[min_idx] = temp;\n                }\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<int> recvbuf(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_INT, recvbuf.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        std::sort(recvbuf.begin(), recvbuf.end());\n        std::copy(recvbuf.begin(), recvbuf.end(), x.begin());\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n```\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            #pragma omp critical\n            {\n                std::sort(x.begin(), x.end());\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::sort(x.begin(), x.end());\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            #pragma omp critical\n            {\n                int min_val = x[i];\n                int min_idx = i;\n                for (int j = i + 1; j < x.size(); j++) {\n                    if (x[j] != 0 && x[j] < min_val) {\n                        min_val = x[j];\n                        min_idx = j;\n                    }\n                }\n                if (min_idx != i) {\n                    int temp = x[i];\n                    x[i] = x[min_idx];\n                    x[min_idx] = temp;\n                }\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<int> recvbuf(x.size());\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&recvbuf[0], x.size(), MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int i = 0; i < x.size(); i++) {\n                if (recvbuf[i] != 0) {\n                    x[i] = recvbuf[i];\n                }\n            }\n        }\n    } else {\n        MPI_Send(&x[0], x.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            #pragma omp critical\n            {\n                int min_index = i;\n                for (int j = i + 1; j < x.size(); j++) {\n                    if (x[j] != 0 && (x[j] < x[min_index] || x[min_index] == 0)) {\n                        min_index = j;\n                    }\n                }\n                if (min_index != i) {\n                    int temp = x[i];\n                    x[i] = x[min_index];\n                    x[min_index] = temp;\n                }\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<int> recvbuf(x.size() * size);\n        MPI_Gather(&x[0], x.size(), MPI_INT, &recvbuf[0], x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        std::sort(recvbuf.begin(), recvbuf.end());\n        x = recvbuf;\n    } else {\n        MPI_Gather(&x[0], x.size(), MPI_INT, NULL, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            #pragma omp critical\n            {\n                std::sort(x.begin(), x.end());\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::sort(x.begin(), x.end());\n    }\n    MPI_Bcast(&x[0], x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            #pragma omp critical\n            {\n                std::sort(x.begin(), x.end());\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::sort(x.begin(), x.end());\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            #pragma omp critical\n            {\n                int min_index = i;\n                for (int j = i + 1; j < x.size(); j++) {\n                    if (x[j] != 0 && (x[j] < x[min_index] || x[min_index] == 0)) {\n                        min_index = j;\n                    }\n                }\n                if (min_index != i) {\n                    int temp = x[i];\n                    x[i] = x[min_index];\n                    x[min_index] = temp;\n                }\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            #pragma omp critical\n            {\n                std::sort(x.begin(), x.end());\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::sort(x.begin(), x.end());\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            #pragma omp critical\n            {\n                std::sort(x.begin(), x.end());\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::sort(x.begin(), x.end());\n    }\n    MPI_Bcast(&x[0], x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            #pragma omp critical\n            {\n                int min_val = x[i];\n                int min_idx = i;\n                for (int j = i + 1; j < x.size(); j++) {\n                    if (x[j] != 0 && x[j] < min_val) {\n                        min_val = x[j];\n                        min_idx = j;\n                    }\n                }\n                if (min_idx != i) {\n                    int temp = x[i];\n                    x[i] = x[min_idx];\n                    x[min_idx] = temp;\n                }\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<int> recvbuf(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_INT, recvbuf.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_INT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n```\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] != 0) {\n            #pragma omp critical\n            {\n                std::sort(x.begin(), x.end());\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::sort(x.begin(), x.end());\n    }\n    MPI_Bcast(&x[0], x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n}"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "41_sort_k-th_smallest_element",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Find the k-th smallest element of the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(Kokkos::View<const int*> const& x, int k) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::View<int*>::HostMirror x_host = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(x_host, x);\n\n    std::nth_element(x_host.data(), x_host.data() + k - 1, x_host.data() + x_host.size());\n\n    return x_host(k - 1);\n}",
      "\n    Kokkos::View<int*>::HostMirror x_host = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(x_host, x);\n\n    std::nth_element(x_host.data(), x_host.data() + k - 1, x_host.data() + x_host.size());\n\n    return x_host(k - 1);\n}",
      "\n    Kokkos::View<int*>::HostMirror x_host = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(x_host, x);\n\n    std::nth_element(x_host.data(), x_host.data() + k - 1, x_host.data() + x_host.size());\n\n    return x_host(k - 1);\n}",
      "\n    Kokkos::View<int*>::HostMirror x_host = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(x_host, x);\n\n    std::nth_element(x_host.data(), x_host.data() + k - 1, x_host.data() + x_host.size());\n\n    return x_host(k - 1);\n}",
      "\n    Kokkos::View<int*>::HostMirror x_host = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(x_host, x);\n\n    std::nth_element(x_host.data(), x_host.data() + k - 1, x_host.data() + x_host.size());\n    return x_host(k - 1);\n}",
      "\n    Kokkos::View<int*>::HostMirror x_host = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(x_host, x);\n\n    std::nth_element(x_host.data(), x_host.data() + k - 1, x_host.data() + x_host.size());\n    return x_host(k - 1);\n}",
      "\n    Kokkos::View<int*>::HostMirror x_host = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(x_host, x);\n\n    std::nth_element(x_host.data(), x_host.data() + k - 1, x_host.data() + x_host.size());\n    return x_host(k - 1);\n}",
      "\n    Kokkos::View<int*>::HostMirror x_host = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(x_host, x);\n\n    std::nth_element(x_host.data(), x_host.data() + k - 1, x_host.data() + x_host.size());\n\n    return x_host(k - 1);\n}",
      "\n    Kokkos::View<int*> sorted_x(\"sorted_x\", x.extent(0));\n    Kokkos::deep_copy(sorted_x, x);\n    std::sort(sorted_x.data(), sorted_x.data() + sorted_x.extent(0));\n    return sorted_x(k - 1);\n}",
      "\n    Kokkos::View<int*> sorted_x(\"sorted_x\", x.extent(0));\n    Kokkos::deep_copy(sorted_x, x);\n\n    std::sort(sorted_x.data(), sorted_x.data() + sorted_x.extent(0));\n\n    return sorted_x(k - 1);\n}",
      "\n    Kokkos::View<int*> sorted_x(\"sorted_x\", x.extent(0));\n    Kokkos::deep_copy(sorted_x, x);\n\n    std::sort(sorted_x.data(), sorted_x.data() + sorted_x.extent(0));\n\n    return sorted_x(k - 1);\n}",
      "\n    Kokkos::View<int*>::HostMirror x_host = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(x_host, x);\n\n    std::nth_element(x_host.data(), x_host.data() + k - 1, x_host.data() + x_host.size());\n    return x_host(k - 1);\n}",
      "\n    Kokkos::View<int*>::HostMirror x_host = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(x_host, x);\n\n    std::nth_element(x_host.data(), x_host.data() + k - 1, x_host.data() + x_host.size());\n\n    return x_host(k - 1);\n}",
      "\n    Kokkos::View<int*> sorted_x(\"sorted_x\", x.extent(0));\n    Kokkos::deep_copy(sorted_x, x);\n    std::sort(sorted_x.data(), sorted_x.data() + sorted_x.extent(0));\n    return sorted_x(k - 1);\n}",
      "\n    Kokkos::View<int*> sorted_x(\"sorted_x\", x.extent(0));\n    Kokkos::deep_copy(sorted_x, x);\n    std::sort(sorted_x.data(), sorted_x.data() + sorted_x.size());\n    return sorted_x(k - 1);\n}",
      "\n    Kokkos::View<int*> sorted_x(\"sorted_x\", x.size());\n    Kokkos::deep_copy(sorted_x, x);\n\n    std::sort(sorted_x.data(), sorted_x.data() + sorted_x.size());\n\n    return sorted_x(k - 1);\n}",
      "\n    Kokkos::View<int*>::HostMirror x_host = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(x_host, x);\n\n    std::nth_element(x_host.data(), x_host.data() + k - 1, x_host.data() + x_host.size());\n\n    return x_host(k - 1);\n}",
      "\n    Kokkos::View<int*>::HostMirror x_host = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(x_host, x);\n\n    std::nth_element(x_host.data(), x_host.data() + k - 1, x_host.data() + x_host.size());\n\n    return x_host(k - 1);\n}",
      "\n    Kokkos::View<int*>::HostMirror x_host = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(x_host, x);\n\n    std::nth_element(x_host.data(), x_host.data() + k - 1, x_host.data() + x_host.size());\n\n    return x_host(k - 1);\n}",
      "\n    Kokkos::View<int*>::HostMirror x_host = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(x_host, x);\n\n    std::nth_element(x_host.data(), x_host.data() + k - 1, x_host.data() + x_host.size());\n\n    return x_host(k - 1);\n}"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "41_sort_k-th_smallest_element",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    std::vector<int> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    return sorted_x[k-1];\n}",
      "\n    std::vector<int> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    return sorted_x[k-1];\n}",
      "\n    std::vector<int> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    return sorted_x[k-1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k - 1];\n}",
      "\n    std::vector<int> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    return sorted_x[k-1];\n}",
      "\n    std::vector<int> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    return sorted_x[k - 1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k-1];\n}",
      "\n    std::vector<int> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    return sorted_x[k - 1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k-1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k - 1];\n}",
      "\n    std::vector<int> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    return sorted_x[k-1];\n}",
      "\n    std::vector<int> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    return sorted_x[k - 1];\n}",
      "\n    std::vector<int> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    return sorted_x[k - 1];\n}",
      "\n    std::vector<int> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    return sorted_x[k - 1];\n}",
      "\n    std::vector<int> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    return sorted_x[k-1];\n}",
      "\n    std::vector<int> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    return sorted_x[k-1];\n}",
      "\n    std::vector<int> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    return sorted_x[k-1];\n}",
      "\n    std::vector<int> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    return sorted_x[k-1];\n}",
      "\n    std::vector<int> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    return sorted_x[k - 1];\n}",
      "\n    std::vector<int> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n    return sorted_x[k-1];\n}"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "41_sort_k-th_smallest_element",
    "parallelism_model": "serial",
    "prompt": "/* Find the k-th smallest element of the vector x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k-1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k-1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k-1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k-1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k-1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k - 1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k-1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k-1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k-1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k - 1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k-1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k-1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k-1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k - 1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k-1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k - 1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k-1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k-1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k-1];\n}",
      "\n    std::vector<int> sortedX = x;\n    std::sort(sortedX.begin(), sortedX.end());\n    return sortedX[k - 1];\n}"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "41_sort_k-th_smallest_element",
    "parallelism_model": "hip",
    "prompt": "/* Find the k-th smallest element of the vector x.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\n__global__ void findKthSmallest(const int *x, size_t N, int k, int *kthSmallest) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    } else {\n        shared[tid] = INT_MAX;\n    }\n    \n    __syncthreads();\n    \n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            if (shared[tid] > shared[tid + s]) {\n                shared[tid] = shared[tid + s];\n            }\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    } else {\n        shared[tid] = INT_MAX;\n    }\n    \n    __syncthreads();\n    \n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            if (shared[tid + s] < shared[tid]) {\n                shared[tid] = shared[tid + s];\n            }\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        shared[tid] = x[i];\n    }\n    __syncthreads();\n\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0 && tid + s < blockDim.x) {\n            if (shared[tid] > shared[tid + s]) {\n                int temp = shared[tid];\n                shared[tid] = shared[tid + s];\n                shared[tid + s] = temp;\n            }\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}\n",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        shared[tid] = x[i];\n    }\n    __syncthreads();\n\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0 && tid + s < blockDim.x) {\n            if (shared[tid] > shared[tid + s]) {\n                int temp = shared[tid];\n                shared[tid] = shared[tid + s];\n                shared[tid + s] = temp;\n            }\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}\n",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    shared[tid] = x[i];\n    __syncthreads();\n\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            if (tid + s < blockDim.x) {\n                if (shared[tid] > shared[tid + s]) {\n                    int temp = shared[tid];\n                    shared[tid] = shared[tid + s];\n                    shared[tid + s] = temp;\n                }\n            }\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}\n",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    } else {\n        shared[tid] = INT_MAX;\n    }\n    \n    __syncthreads();\n    \n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            if (shared[tid] > shared[tid + s]) {\n                shared[tid] = shared[tid + s];\n            }\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    } else {\n        shared[tid] = INT_MAX;\n    }\n    \n    __syncthreads();\n    \n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            if (shared[tid + s] < shared[tid]) {\n                shared[tid] = shared[tid + s];\n            }\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    }\n    __syncthreads();\n\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0 && tid + s < blockDim.x) {\n            if (shared[tid] > shared[tid + s]) {\n                int temp = shared[tid];\n                shared[tid] = shared[tid + s];\n                shared[tid + s] = temp;\n            }\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}\n",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    } else {\n        shared[tid] = INT_MAX;\n    }\n    \n    __syncthreads();\n    \n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            if (shared[tid] > shared[tid + s]) {\n                shared[tid] = shared[tid + s];\n            }\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        shared[tid] = x[i];\n    }\n    __syncthreads();\n\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0 && tid + s < blockDim.x) {\n            if (shared[tid] > shared[tid + s]) {\n                int temp = shared[tid];\n                shared[tid] = shared[tid + s];\n                shared[tid + s] = temp;\n            }\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}\n",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    }\n    __syncthreads();\n\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0 && tid + s < blockDim.x) {\n            if (shared[tid] > shared[tid + s]) {\n                int temp = shared[tid];\n                shared[tid] = shared[tid + s];\n                shared[tid + s] = temp;\n            }\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}\n",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        shared[tid] = x[i];\n    }\n    __syncthreads();\n\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0 && tid + s < blockDim.x) {\n            if (shared[tid] > shared[tid + s]) {\n                int temp = shared[tid];\n                shared[tid] = shared[tid + s];\n                shared[tid + s] = temp;\n            }\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}\n",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    }\n    __syncthreads();\n\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0 && tid + s < blockDim.x) {\n            if (shared[tid] > shared[tid + s]) {\n                int temp = shared[tid];\n                shared[tid] = shared[tid + s];\n                shared[tid + s] = temp;\n            }\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}\n",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    shared[tid] = x[i];\n    __syncthreads();\n\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            if (tid + s < blockDim.x) {\n                if (shared[tid] > shared[tid + s]) {\n                    int temp = shared[tid];\n                    shared[tid] = shared[tid + s];\n                    shared[tid + s] = temp;\n                }\n            }\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}\n",
      "\n    extern __shared__ int sharedMem[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        sharedMem[tid] = x[i];\n    }\n    __syncthreads();\n    \n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0 && tid + s < blockDim.x) {\n            if (sharedMem[tid] > sharedMem[tid + s]) {\n                int temp = sharedMem[tid];\n                sharedMem[tid] = sharedMem[tid + s];\n                sharedMem[tid + s] = temp;\n            }\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = sharedMem[0];\n    }\n}\n",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    } else {\n        shared[tid] = INT_MAX;\n    }\n    \n    __syncthreads();\n    \n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            shared[tid] = min(shared[tid], shared[tid + s]);\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}",
      "\n    // Sort the input array using parallel sorting algorithm (e.g. merge sort or quicksort)\n    // After sorting, the k-th smallest element will be at index k-1\n    // Store the k-th smallest element in kthSmallest\n}",
      "\n    int tid = threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (x[i] < x[tid] || (x[i] == x[tid] && i < tid)) {\n                count++;\n            }\n        }\n        if (count == k - 1) {\n            *kthSmallest = x[tid];\n        }\n    }\n}\n",
      "\n    // Use parallel reduction to find the k-th smallest element\n    extern __shared__ int sharedMem[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        sharedMem[tid] = x[i];\n    } else {\n        sharedMem[tid] = INT_MAX;\n    }\n    __syncthreads();\n\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            sharedMem[tid] = min(sharedMem[tid], sharedMem[tid + s]);\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = sharedMem[0];\n    }\n}\n",
      "\n    int tid = threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (x[i] < x[tid] || (x[i] == x[tid] && i < tid)) {\n                count++;\n            }\n        }\n        if (count == k - 1) {\n            *kthSmallest = x[tid];\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "41_sort_k-th_smallest_element",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    std::vector<int> localCopy = x;\n    std::sort(localCopy.begin(), localCopy.end());\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<int> allValues(MPI::COMM_WORLD.Get_size());\n        MPI::COMM_WORLD.Gather(&localCopy[k-1], 1, MPI::INT, &allValues[0], 1, MPI::INT, 0);\n        std::sort(allValues.begin(), allValues.end());\n        return allValues[0];\n    } else {\n        MPI::COMM_WORLD.Gather(&localCopy[k-1], 1, MPI::INT, nullptr, 0, MPI::INT, 0);\n        return -1;  // Placeholder for non-root ranks\n    }\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    int local_size = local_x.size();\n    int global_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &global_size);\n    \n    std::vector<int> recv_counts(global_size);\n    MPI_Allgather(&local_size, 1, MPI_INT, recv_counts.data(), 1, MPI_INT, MPI_COMM_WORLD);\n    \n    std::vector<int> displs(global_size);\n    displs[0] = 0;\n    for (int i = 1; i < global_size; ++i) {\n        displs[i] = displs[i-1] + recv_counts[i-1];\n    }\n    \n    std::vector<int> global_x(displs[global_size-1] + recv_counts[global_size-1]);\n    MPI_Allgatherv(local_x.data(), local_size, MPI_INT, global_x.data(), recv_counts.data(), displs.data(), MPI_INT, MPI_COMM_WORLD);\n    \n    std::sort(global_x.begin(), global_x.end());\n    \n    return global_x[k-1];\n}\n",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    int result;\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<int> all_values(MPI::COMM_WORLD.Get_size());\n        MPI::COMM_WORLD.Gather(&local_x[k-1], 1, MPI::INT, &all_values[0], 1, MPI::INT, 0);\n        std::sort(all_values.begin(), all_values.end());\n        result = all_values[0];\n    } else {\n        MPI::COMM_WORLD.Gather(&local_x[k-1], 1, MPI::INT, nullptr, 0, MPI::INT, 0);\n    }\n    \n    return result;\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<int> all_x(MPI::COMM_WORLD.Get_size() * local_x.size());\n        MPI::COMM_WORLD.Gather(&local_x[0], local_x.size(), MPI::INT, &all_x[0], local_x.size(), MPI::INT, 0);\n        std::sort(all_x.begin(), all_x.end());\n        return all_x[k - 1];\n    } else {\n        MPI::COMM_WORLD.Gather(&local_x[0], local_x.size(), MPI::INT, NULL, 0, MPI::INT, 0);\n        return -1;  // Placeholder for non-root ranks\n    }\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<int> all_x(MPI::COMM_WORLD.Get_size() * local_x.size());\n        MPI::COMM_WORLD.Gather(&local_x[0], local_x.size(), MPI::INT, &all_x[0], local_x.size(), MPI::INT, 0);\n        std::sort(all_x.begin(), all_x.end());\n        return all_x[k - 1];\n    } else {\n        MPI::COMM_WORLD.Gather(&local_x[0], local_x.size(), MPI::INT, NULL, 0, MPI::INT, 0);\n    }\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    if (k > local_x.size()) {\n        if (MPI::COMM_WORLD.Get_rank() == 0) {\n            std::cerr << \"Error: k is larger than the size of the vector\" << std::endl;\n        }\n        return -1; // or handle the error in a different way\n    }\n    \n    std::vector<int> all_kth_elements(MPI::COMM_WORLD.Get_size());\n    MPI::COMM_WORLD.Gather(&local_x[k-1], 1, MPI::INT, &all_kth_elements[0], 1, MPI::INT, 0);\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::sort(all_kth_elements.begin(), all_kth_elements.end());\n        return all_kth_elements[0];\n    }\n    return -1; // or handle the case when rank is not 0 in a different way\n}\n",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    int result;\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<int> all_values(MPI::COMM_WORLD.Get_size());\n        MPI::COMM_WORLD.Gather(&local_x[k-1], 1, MPI::INT, &all_values[0], 1, MPI::INT, 0);\n        std::sort(all_values.begin(), all_values.end());\n        result = all_values[0];\n    } else {\n        MPI::COMM_WORLD.Gather(&local_x[k-1], 1, MPI::INT, nullptr, 0, MPI::INT, 0);\n    }\n    \n    return result;\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<int> all_x(MPI::COMM_WORLD.Get_size() * local_x.size());\n        MPI::COMM_WORLD.Gather(&local_x[0], local_x.size(), MPI::INT, &all_x[0], local_x.size(), MPI::INT, 0);\n        std::sort(all_x.begin(), all_x.end());\n        return all_x[k - 1];\n    } else {\n        MPI::COMM_WORLD.Gather(&local_x[0], local_x.size(), MPI::INT, NULL, 0, MPI::INT, 0);\n    }\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    int result;\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<int> all_values(MPI::COMM_WORLD.Get_size());\n        MPI::COMM_WORLD.Gather(&local_x[k-1], 1, MPI::INT, &all_values[0], 1, MPI::INT, 0);\n        std::sort(all_values.begin(), all_values.end());\n        result = all_values[0];\n    } else {\n        MPI::COMM_WORLD.Gather(&local_x[k-1], 1, MPI::INT, nullptr, 0, MPI::INT, 0);\n    }\n    \n    return result;\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<int> all_x(MPI::COMM_WORLD.Get_size() * local_x.size());\n        MPI::COMM_WORLD.Gather(&local_x[0], local_x.size(), MPI::INT, &all_x[0], local_x.size(), MPI::INT, 0);\n        std::sort(all_x.begin(), all_x.end());\n        return all_x[k - 1];\n    } else {\n        MPI::COMM_WORLD.Gather(&local_x[0], local_x.size(), MPI::INT, NULL, 0, MPI::INT, 0);\n        return -1;\n    }\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    int result;\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<int> all_values(MPI::COMM_WORLD.Get_size());\n        MPI::COMM_WORLD.Gather(&local_x[k-1], 1, MPI::INT, &all_values[0], 1, MPI::INT, 0);\n        std::sort(all_values.begin(), all_values.end());\n        result = all_values[0];\n    } else {\n        MPI::COMM_WORLD.Gather(&local_x[k-1], 1, MPI::INT, NULL, 0, MPI::INT, 0);\n    }\n    \n    return result;\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    int result;\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<int> all_values(MPI::COMM_WORLD.Get_size());\n        MPI::COMM_WORLD.Gather(&local_x[k-1], 1, MPI::INT, &all_values[0], 1, MPI::INT, 0);\n        std::sort(all_values.begin(), all_values.end());\n        result = all_values[0];\n    } else {\n        MPI::COMM_WORLD.Gather(&local_x[k-1], 1, MPI::INT, nullptr, 0, MPI::INT, 0);\n    }\n    \n    return result;\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    if (k > local_x.size()) {\n        if (MPI::COMM_WORLD.Get_rank() == 0) {\n            std::cerr << \"Error: k is larger than the size of the vector.\" << std::endl;\n        }\n        return -1;\n    }\n    \n    std::vector<int> all_values(MPI::COMM_WORLD.Get_size());\n    MPI::COMM_WORLD.Gather(&local_x[k-1], 1, MPI::INT, &all_values[0], 1, MPI::INT, 0);\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::sort(all_values.begin(), all_values.end());\n        return all_values[k-1];\n    } else {\n        return -1;\n    }\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<int> all_x(MPI::COMM_WORLD.Get_size() * local_x.size());\n        MPI::COMM_WORLD.Gather(&local_x[0], local_x.size(), MPI::INT, &all_x[0], local_x.size(), MPI::INT, 0);\n        std::sort(all_x.begin(), all_x.end());\n        return all_x[k - 1];\n    } else {\n        MPI::COMM_WORLD.Gather(&local_x[0], local_x.size(), MPI::INT, NULL, 0, MPI::INT, 0);\n    }\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<int> all_values(MPI::COMM_WORLD.Get_size());\n        MPI::COMM_WORLD.Gather(&local_x[k-1], 1, MPI::INT, &all_values[0], 1, MPI::INT, 0);\n        std::sort(all_values.begin(), all_values.end());\n        return all_values[0];\n    } else {\n        MPI::COMM_WORLD.Gather(&local_x[k-1], 1, MPI::INT, nullptr, 0, MPI::INT, 0);\n        return -1; // Placeholder for non-root ranks\n    }\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    int result;\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<int> all_values(MPI::COMM_WORLD.Get_size());\n        MPI::COMM_WORLD.Gather(&local_x[k-1], 1, MPI::INT, &all_values[0], 1, MPI::INT, 0);\n        std::sort(all_values.begin(), all_values.end());\n        result = all_values[0];\n    } else {\n        MPI::COMM_WORLD.Gather(&local_x[k-1], 1, MPI::INT, nullptr, 0, MPI::INT, 0);\n    }\n    \n    return result;\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<int> all_x(MPI::COMM_WORLD.Get_size() * local_x.size());\n        MPI::COMM_WORLD.Gather(&local_x[0], local_x.size(), MPI::INT, &all_x[0], local_x.size(), MPI::INT, 0);\n        std::sort(all_x.begin(), all_x.end());\n        return all_x[k - 1];\n    } else {\n        MPI::COMM_WORLD.Gather(&local_x[0], local_x.size(), MPI::INT, NULL, 0, MPI::INT, 0);\n    }\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<int> all_x(MPI::COMM_WORLD.Get_size() * local_x.size());\n        MPI::COMM_WORLD.Gather(&local_x[0], local_x.size(), MPI::INT, &all_x[0], local_x.size(), MPI::INT, 0);\n        std::sort(all_x.begin(), all_x.end());\n        return all_x[k - 1];\n    } else {\n        MPI::COMM_WORLD.Gather(&local_x[0], local_x.size(), MPI::INT, NULL, 0, MPI::INT, 0);\n    }\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<int> all_x(MPI::COMM_WORLD.Get_size() * local_x.size());\n        MPI::COMM_WORLD.Gather(&local_x[0], local_x.size(), MPI::INT, &all_x[0], local_x.size(), MPI::INT, 0);\n        std::sort(all_x.begin(), all_x.end());\n        return all_x[k - 1];\n    } else {\n        MPI::COMM_WORLD.Gather(&local_x[0], local_x.size(), MPI::INT, NULL, 0, MPI::INT, 0);\n    }\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    int result;\n    if (k <= local_x.size()) {\n        result = local_x[k - 1];\n    } else {\n        result = -1; // or handle the out-of-bounds case in a different way\n    }\n    \n    int final_result;\n    MPI_Reduce(&result, &final_result, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return final_result;\n    } else {\n        return -1; // or handle the case where the rank is not 0 in a different way\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "41_sort_k-th_smallest_element",
    "parallelism_model": "cuda",
    "prompt": "/* Find the k-th smallest element of the vector x.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\n__global__ void findKthSmallest(const int *x, size_t N, int k, int *kthSmallest) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    } else {\n        shared[tid] = INT_MAX;\n    }\n    \n    __syncthreads();\n    \n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            if (shared[tid] > shared[tid + s]) {\n                shared[tid] = shared[tid + s];\n            }\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    } else {\n        shared[tid] = INT_MAX;\n    }\n    \n    __syncthreads();\n    \n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            if (shared[tid] > shared[tid + s]) {\n                shared[tid] = shared[tid + s];\n            }\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    shared[tid] = x[i];\n    __syncthreads();\n\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            if (tid + s < blockDim.x) {\n                if (shared[tid] > shared[tid + s]) {\n                    int temp = shared[tid];\n                    shared[tid] = shared[tid + s];\n                    shared[tid + s] = temp;\n                }\n            }\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    } else {\n        shared[tid] = INT_MAX;\n    }\n    \n    __syncthreads();\n    \n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            if (shared[tid + s] < shared[tid]) {\n                shared[tid] = shared[tid + s];\n            }\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    } else {\n        shared[tid] = INT_MAX;\n    }\n    \n    __syncthreads();\n    \n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            if (shared[tid] > shared[tid + s]) {\n                shared[tid] = shared[tid + s];\n            }\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    } else {\n        shared[tid] = INT_MAX;\n    }\n    __syncthreads();\n    \n    for (int s = 1; s < blockDim.x; s *= 2) {\n        int index = 2 * s * tid;\n        if (index < blockDim.x) {\n            if (shared[index] > shared[index + s]) {\n                shared[index] = shared[index + s];\n            }\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n```\n}",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    } else {\n        shared[tid] = INT_MAX;\n    }\n    \n    __syncthreads();\n    \n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            if (shared[tid] > shared[tid + s]) {\n                shared[tid] = shared[tid + s];\n            }\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    } else {\n        shared[tid] = INT_MAX;\n    }\n    __syncthreads();\n    \n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            if (shared[tid] > shared[tid + s]) {\n                shared[tid] = shared[tid + s];\n            }\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    }\n    __syncthreads();\n\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0 && tid + s < blockDim.x) {\n            if (shared[tid] > shared[tid + s]) {\n                int temp = shared[tid];\n                shared[tid] = shared[tid + s];\n                shared[tid + s] = temp;\n            }\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}\n",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    } else {\n        shared[tid] = INT_MAX;\n    }\n    __syncthreads();\n    \n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            if (shared[tid] > shared[tid + s]) {\n                shared[tid] = shared[tid + s];\n            }\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    } else {\n        shared[tid] = INT_MAX;\n    }\n    __syncthreads();\n\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            if (shared[tid + s] < shared[tid]) {\n                shared[tid] = shared[tid + s];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}\n",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    } else {\n        shared[tid] = INT_MAX;\n    }\n    \n    __syncthreads();\n    \n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            if (shared[tid] > shared[tid + s]) {\n                shared[tid] = shared[tid + s];\n            }\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    } else {\n        shared[tid] = INT_MAX;\n    }\n    \n    __syncthreads();\n    \n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            if (shared[tid] > shared[tid + s]) {\n                shared[tid] = shared[tid + s];\n            }\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    } else {\n        shared[tid] = INT_MAX;\n    }\n    \n    __syncthreads();\n    \n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            if (shared[tid] > shared[tid + s]) {\n                shared[tid] = shared[tid + s];\n            }\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    } else {\n        shared[tid] = INT_MAX;\n    }\n    \n    __syncthreads();\n    \n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            shared[tid] = min(shared[tid], shared[tid + s]);\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    } else {\n        shared[tid] = INT_MAX;\n    }\n    \n    __syncthreads();\n    \n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            if (shared[tid] > shared[tid + s]) {\n                shared[tid] = shared[tid + s];\n            }\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    } else {\n        shared[tid] = INT_MAX;\n    }\n    \n    __syncthreads();\n    \n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            shared[tid] = min(shared[tid], shared[tid + s]);\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    }\n    __syncthreads();\n\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0 && tid + s < blockDim.x) {\n            if (shared[tid] > shared[tid + s]) {\n                int temp = shared[tid];\n                shared[tid] = shared[tid + s];\n                shared[tid + s] = temp;\n            }\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}\n",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    shared[tid] = x[i];\n    __syncthreads();\n\n    for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            if (tid + s < blockDim.x) {\n                if (shared[tid] > shared[tid + s]) {\n                    int temp = shared[tid];\n                    shared[tid] = shared[tid + s];\n                    shared[tid + s] = temp;\n                }\n            }\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}\n",
      "\n    extern __shared__ int shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N) {\n        shared[tid] = x[i];\n    }\n    __syncthreads();\n\n    for (int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0 && tid + s < blockDim.x) {\n            if (shared[tid] > shared[tid + s]) {\n                int temp = shared[tid];\n                shared[tid] = shared[tid + s];\n                shared[tid + s] = temp;\n            }\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        kthSmallest[blockIdx.x] = shared[0];\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "41_sort_k-th_smallest_element",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    int result;\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        result = local_x[k-1];\n    }\n    MPI::COMM_WORLD.Bcast(&result, 1, MPI::INT, 0);\n    return result;\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    int local_size = local_x.size();\n    int global_size;\n    MPI_Allreduce(&local_size, &global_size, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    \n    if (k > global_size) {\n        if (MPI::COMM_WORLD.Get_rank() == 0) {\n            std::cerr << \"Error: k is larger than the size of the vector.\" << std::endl;\n        }\n        MPI_Finalize();\n        exit(1);\n    }\n    \n    int local_k = k;\n    int pivot;\n    int local_count = 0;\n    \n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] < local_x[local_k - 1]) {\n            local_count++;\n        }\n    }\n    \n    MPI_Allreduce(&local_count, &pivot, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    \n    if (pivot == k) {\n        return local_x[local_k - 1];\n    } else if (pivot < k) {\n        local_k = k - pivot;\n    }\n    \n    std::vector<int> new_x;\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        new_x.resize(global_size);\n    }\n    \n    MPI_Gather(&local_x[0], local_size, MPI_INT, &new_x[0], local_size, MPI_INT, 0, MPI_COMM_WORLD);\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::sort(new_x.begin(), new_x.end());\n        return new_x[k - 1];\n    }\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    int local_size = local_x.size();\n    int global_size;\n    MPI_Allreduce(&local_size, &global_size, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    \n    int global_k = k;\n    if (global_k > global_size) {\n        global_k = global_size;\n    }\n    \n    int local_k = global_k;\n    for (int i = 0; i < local_size; ++i) {\n        if (local_x[i] == local_x[i + 1]) {\n            local_k--;\n        }\n        if (local_k == 0) {\n            global_k = local_x[i];\n            break;\n        }\n    }\n    \n    int* global_k_array = new int[MPI_COMM_WORLD];\n    MPI_Gather(&global_k, 1, MPI_INT, global_k_array, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    \n    int result;\n    if (rank == 0) {\n        std::sort(global_k_array, global_k_array + MPI_COMM_WORLD);\n        result = global_k_array[k - 1];\n    }\n    \n    delete[] global_k_array;\n    return result;\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    int result;\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        result = local_x[k-1];\n    }\n    MPI::COMM_WORLD.Reduce(&result, &result, 1, MPI::INT, MPI::MIN, 0);\n    return result;\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    int global_kth;\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<int> all_kth(MPI::COMM_WORLD.Get_size());\n        MPI::COMM_WORLD.Gather(&local_x[k - 1], 1, MPI::INT, &all_kth[0], 1, MPI::INT, 0);\n        std::sort(all_kth.begin(), all_kth.end());\n        global_kth = all_kth[k - 1];\n    } else {\n        MPI::COMM_WORLD.Gather(&local_x[k - 1], 1, MPI::INT, nullptr, 0, MPI::INT, 0);\n    }\n    \n    MPI::COMM_WORLD.Bcast(&global_kth, 1, MPI::INT, 0);\n    \n    return global_kth;\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    int result;\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        result = local_x[k-1];\n    }\n    MPI::COMM_WORLD.Bcast(&result, 1, MPI::INT, 0);\n    return result;\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    int local_size = local_x.size();\n    int global_size;\n    MPI_Allreduce(&local_size, &global_size, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    \n    int global_k = k;\n    if (global_k > global_size) {\n        global_k = global_size;\n    }\n    \n    int local_k = global_k;\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] == local_x[i + 1]) {\n            local_k--;\n        }\n        if (local_k == 0) {\n            global_k = local_x[i];\n            break;\n        }\n    }\n    \n    int* all_k = new int[MPI_COMM_WORLD];\n    MPI_Gather(&global_k, 1, MPI_INT, all_k, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    \n    int result;\n    if (rank == 0) {\n        std::sort(all_k, all_k + MPI_COMM_WORLD);\n        result = all_k[k - 1];\n    }\n    \n    delete[] all_k;\n    return result;\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    int result;\n    if (rank == 0) {\n        std::vector<int> all_values(size);\n        MPI_Gather(local_x.data(), local_x.size(), MPI_INT, all_values.data(), local_x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        std::sort(all_values.begin(), all_values.end());\n        result = all_values[k-1];\n    }\n    MPI_Bcast(&result, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    return result;\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    int result;\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        result = local_x[k - 1];\n    }\n    MPI::COMM_WORLD.Reduce(MPI::IN_PLACE, &result, 1, MPI::INT, MPI::MIN, 0);\n    return result;\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    int local_size = local_x.size();\n    int global_size;\n    MPI_Allreduce(&local_size, &global_size, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    \n    int local_k = k * local_size / global_size;\n    int global_k;\n    MPI_Scan(&local_k, &global_k, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    global_k -= local_k;\n    \n    int result;\n    MPI_Bcast(&global_k, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    result = local_x[global_k];\n    \n    return result;\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    int result;\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        result = local_x[k - 1];\n    }\n    MPI::COMM_WORLD.Reduce(MPI::IN_PLACE, &result, 1, MPI::INT, MPI::MIN, 0);\n    return result;\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    int result;\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        result = local_x[k - 1];\n    }\n    MPI::COMM_WORLD.Bcast(&result, 1, MPI::INT, 0);\n    return result;\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    int result;\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        result = local_x[k-1];\n    }\n    MPI::COMM_WORLD.Reduce(MPI::IN_PLACE, &result, 1, MPI::INT, MPI::MIN, 0);\n    return result;\n}\n",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    int local_size = local_x.size();\n    int global_size;\n    MPI_Allreduce(&local_size, &global_size, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    \n    if (k > global_size) {\n        if (MPI::COMM_WORLD.Get_rank() == 0) {\n            std::cerr << \"Error: k is larger than the size of the vector.\" << std::endl;\n        }\n        MPI_Finalize();\n        exit(1);\n    }\n    \n    int local_k = k;\n    int pivot;\n    int pivot_rank;\n    int pivot_index;\n    int count = 0;\n    for (int i = 0; i < MPI::COMM_WORLD.Get_size(); i++) {\n        int local_count = std::upper_bound(local_x.begin(), local_x.end(), local_x[i]) - local_x.begin();\n        int global_count;\n        MPI_Allreduce(&local_count, &global_count, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n        if (global_count >= local_k) {\n            pivot = local_x[i];\n            pivot_rank = MPI::COMM_WORLD.Get_rank();\n            pivot_index = i;\n            count = global_count;\n            break;\n        }\n    }\n    \n    int global_pivot_rank;\n    MPI_Allreduce(&pivot_rank, &global_pivot_rank, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);\n    int global_pivot_index;\n    MPI_Allreduce(&pivot_index, &global_pivot_index, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);\n    int global_count;\n    MPI_Allreduce(&count, &global_count, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);\n    \n    int local_less = 0;\n    for (int i = 0; i < local_x.size(); i++) {\n        if (local_x[i] < pivot) {\n            local_less++;\n        }\n    }\n    \n    int global_less;\n    MPI_Allreduce(&local_less, &global_less, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    \n    if (global_less == local_k) {\n        return pivot;\n    } else if (global_less < local_k) {\n        local_k -= global_less;\n    }\n    \n    std::vector<int> send_counts(MPI::COMM_WORLD.Get_size());\n    std::vector<int> displacements(MPI::COMM_WORLD.Get_size());\n    int local_send_count = 0;\n    for (int i = 0; i < MPI::COMM_WORLD.Get_size(); i++) {\n        send_counts[i] = local_x.size() / MPI::COMM_WORLD.Get_size();\n        if (i < local_x.size() % MPI::COMM_WORLD.Get_size()) {\n            send_counts[i]++;\n        }\n        displacements[i] = local_send_count;\n        local_send_count += send_counts[i];\n    }\n    \n    std::vector<int> local_send_buffer(send_counts[MPI::COMM_WORLD.Get_rank()]);\n    std::copy(local_x.begin(), local_x.end(), local_send_buffer.begin());\n    \n    std::vector<int> global_receive_buffer(global_size);\n    MPI_Gatherv(&local_send_buffer[0], send_counts[MPI::COMM_WORLD.Get_rank()], MPI_INT, &global_receive_buffer[0], &send_counts[0], &displacements[0], MPI_INT, 0, MPI_COMM_WORLD);\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::sort(global_receive_buffer.begin(), global_receive_buffer.end());\n        return global_receive_buffer[local_k - 1];\n    }\n    \n    return -1;\n}\n",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    int local_size = local_x.size();\n    int global_size;\n    MPI_Allreduce(&local_size, &global_size, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    \n    if (k > global_size) {\n        if (MPI::COMM_WORLD.Get_rank() == 0) {\n            std::cerr << \"Error: k is larger than the size of the vector\" << std::endl;\n        }\n        MPI_Finalize();\n        exit(1);\n    }\n    \n    int local_k = k;\n    int global_k;\n    MPI_Scan(&local_k, &global_k, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    global_k -= local_k;\n    \n    int result;\n    if (global_k < local_size) {\n        result = local_x[global_k];\n    } else {\n        result = local_x.back();\n    }\n    \n    int* all_results = new int[mpi_size];\n    MPI_Gather(&result, 1, MPI_INT, all_results, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::sort(all_results, all_results + mpi_size);\n        result = all_results[k - 1];\n    }\n    \n    delete[] all_results;\n    \n    return result;\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    int local_size = local_x.size();\n    int global_size;\n    MPI_Allreduce(&local_size, &global_size, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    \n    int local_k = k;\n    int prefix_sum = 0;\n    int global_k;\n    for (int i = 0; i < local_size; ++i) {\n        prefix_sum += (local_x[i] == local_x[i - 1]) ? 0 : 1;\n        if (prefix_sum >= local_k) {\n            local_k = i;\n            break;\n        }\n    }\n    MPI_Allreduce(&local_k, &global_k, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    \n    if (global_k >= global_size) {\n        return -1; // k is out of range\n    }\n    \n    std::vector<int> all_values(global_size);\n    MPI_Allgather(local_x.data(), local_size, MPI_INT, all_values.data(), local_size, MPI_INT, MPI_COMM_WORLD);\n    std::sort(all_values.begin(), all_values.end());\n    \n    return all_values[global_k - 1];\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    int localStart = rank * localSize;\n    int localEnd = (rank == size - 1) ? x.size() : localStart + localSize;\n\n    std::vector<int> localX(x.begin() + localStart, x.begin() + localEnd);\n    std::sort(localX.begin(), localX.end());\n\n    std::vector<int> allLocalX(size * localSize);\n    MPI_Gather(localX.data(), localSize, MPI_INT, allLocalX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::sort(allLocalX.begin(), allLocalX.end());\n        return allLocalX[k - 1];\n    } else {\n        return -1; // or any other value to indicate non-rank 0 processes\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    int localStart = rank * localSize;\n    int localEnd = (rank == size - 1) ? x.size() : localStart + localSize;\n\n    std::vector<int> localX(x.begin() + localStart, x.begin() + localEnd);\n    std::sort(localX.begin(), localX.end());\n\n    std::vector<int> allLocalX(size * localSize);\n    MPI_Gather(localX.data(), localSize, MPI_INT, allLocalX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::sort(allLocalX.begin(), allLocalX.end());\n        return allLocalX[k - 1];\n    } else {\n        return -1; // or any other suitable value for non-root ranks\n    }\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    \n    int local_kth_smallest;\n    if (k <= local_x.size()) {\n        local_kth_smallest = local_x[k - 1];\n    } else {\n        local_kth_smallest = -1;  // Indicates k is out of range\n    }\n    \n    int global_kth_smallest;\n    MPI_Reduce(&local_kth_smallest, &global_kth_smallest, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n    \n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return global_kth_smallest;\n    } else {\n        return -1;  // Indicates result is only valid on rank 0\n    }\n}",
      "\n    std::vector<int> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n    int result;\n    if (rank == 0) {\n        result = local_x[k-1];\n    }\n    return result;\n}"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "43_sort_sort_an_array_of_structs_by_key",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use Kokkos to sort in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(Kokkos::View<Result*> &results) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\nKokkos::sort(results, [](const Result& a, const Result& b) {\n    return a.startTime < b.startTime;\n});\n}",
      "\nKokkos::sort(results, [](const Result& a, const Result& b) {\n    return a.startTime < b.startTime;\n});\n}",
      "\n   Kokkos::sort(results, [](const Result& a, const Result& b) {\n       return a.startTime < b.startTime;\n   });\n}",
      "\n   Kokkos::sort(results, [](const Result& a, const Result& b) {\n       return a.startTime < b.startTime;\n   });\n}",
      "\nKokkos::sort(results, [](const Result& a, const Result& b) {\n    return a.startTime < b.startTime;\n});\n}",
      "\nKokkos::sort(results, [](const Result& a, const Result& b) {\n    return a.startTime < b.startTime;\n});\n}",
      "\nKokkos::sort(results, [](const Result& a, const Result& b) {\n    return a.startTime < b.startTime;\n});\n}",
      "\n   Kokkos::sort(results, [](const Result& a, const Result& b) {\n       return a.startTime < b.startTime;\n   });\n}",
      "\nKokkos::sort(results, [](const Result& a, const Result& b) {\n    return a.startTime < b.startTime;\n});\n}",
      "\n    Kokkos::sort(results, [](const Result &a, const Result &b) {\n        return a.startTime < b.startTime;\n    });\n}",
      "\n  Kokkos::sort(results, [](const Result& a, const Result& b) {\n      return a.startTime < b.startTime;\n  });\n}",
      "\n   Kokkos::sort(results, [](const Result& a, const Result& b) {\n       return a.startTime < b.startTime;\n   });\n}",
      "\nKokkos::sort(results, [](const Result& a, const Result& b) {\n    return a.startTime < b.startTime;\n});\n}",
      "\nKokkos::sort(results, [](const Result& a, const Result& b) {\n    return a.startTime < b.startTime;\n});\n}",
      "\nKokkos::sort(results, [](const Result& a, const Result& b) {\n    return a.startTime < b.startTime;\n});\n}",
      "\n   Kokkos::sort(results, [](const Result& a, const Result& b) {\n       return a.startTime < b.startTime;\n   });\n}",
      "\nKokkos::sort(results, [](const Result& a, const Result& b) {\n    return a.startTime < b.startTime;\n});\n}",
      "\nKokkos::sort(results, [](const Result& a, const Result& b) {\n    return a.startTime < b.startTime;\n});\n}",
      "\n   Kokkos::sort(results, [](const Result& a, const Result& b) {\n       return a.startTime < b.startTime;\n   });\n}",
      "\n   Kokkos::sort(results, [](const Result& a, const Result& b) {\n       return a.startTime < b.startTime;\n   });\n}"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "43_sort_sort_an_array_of_structs_by_key",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use OpenMP to sort in parallel.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n       for (int j = i + 1; j < results.size(); j++) {\n           if (results[i].startTime > results[j].startTime) {\n               std::swap(results[i], results[j]);\n           }\n       }\n   }\n}",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n       for (int j = i + 1; j < results.size(); j++) {\n           if (results[i].startTime > results[j].startTime) {\n               Result temp = results[i];\n               results[i] = results[j];\n               results[j] = temp;\n           }\n       }\n   }\n}",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n       for (int j = i + 1; j < results.size(); j++) {\n           if (results[i].startTime > results[j].startTime) {\n               Result temp = results[i];\n               results[i] = results[j];\n               results[j] = temp;\n           }\n       }\n   }\n}",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n       for (int j = i + 1; j < results.size(); j++) {\n           if (results[i].startTime > results[j].startTime) {\n               std::swap(results[i], results[j]);\n           }\n       }\n   }\n}",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n       for (int j = i + 1; j < results.size(); j++) {\n           if (results[i].startTime > results[j].startTime) {\n               std::swap(results[i], results[j]);\n           }\n       }\n   }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < results.size(); i++) {\n        for (int j = i + 1; j < results.size(); j++) {\n            if (results[i].startTime > results[j].startTime) {\n                std::swap(results[i], results[j]);\n            }\n        }\n    }\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < results.size(); i++) {\n    for (int j = i + 1; j < results.size(); j++) {\n        if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n        }\n    }\n}\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < results.size(); i++) {\n        for (int j = i + 1; j < results.size(); j++) {\n            if (results[i].startTime > results[j].startTime) {\n                std::swap(results[i], results[j]);\n            }\n        }\n    }\n}",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n       for (int j = i + 1; j < results.size(); j++) {\n           if (results[i].startTime > results[j].startTime) {\n               std::swap(results[i], results[j]);\n           }\n       }\n   }\n}",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n       for (int j = i + 1; j < results.size(); j++) {\n           if (results[i].startTime > results[j].startTime) {\n               Result temp = results[i];\n               results[i] = results[j];\n               results[j] = temp;\n           }\n       }\n   }\n}",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n       for (int j = i + 1; j < results.size(); j++) {\n           if (results[i].startTime > results[j].startTime) {\n               std::swap(results[i], results[j]);\n           }\n       }\n   }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < results.size(); i++) {\n        for (int j = i + 1; j < results.size(); j++) {\n            if (results[i].startTime > results[j].startTime) {\n                std::swap(results[i], results[j]);\n            }\n        }\n    }\n}",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n       for (int j = i + 1; j < results.size(); j++) {\n           if (results[i].startTime > results[j].startTime) {\n               Result temp = results[i];\n               results[i] = results[j];\n               results[j] = temp;\n           }\n       }\n   }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < results.size(); i++) {\n        for (int j = i + 1; j < results.size(); j++) {\n            if (results[i].startTime > results[j].startTime) {\n                std::swap(results[i], results[j]);\n            }\n        }\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < results.size(); i++) {\n        for (int j = i + 1; j < results.size(); j++) {\n            if (results[i].startTime > results[j].startTime) {\n                std::swap(results[i], results[j]);\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < results.size(); i++) {\n        for (int j = i + 1; j < results.size(); j++) {\n            if (results[i].startTime > results[j].startTime) {\n                std::swap(results[i], results[j]);\n            }\n        }\n    }\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < results.size(); i++) {\n    for (int j = i + 1; j < results.size(); j++) {\n        if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < results.size(); i++) {\n    for (int j = i + 1; j < results.size(); j++) {\n        if (results[i].startTime > results[j].startTime) {\n            Result temp = results[i];\n            results[i] = results[j];\n            results[j] = temp;\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < results.size(); i++) {\n    for (int j = i + 1; j < results.size(); j++) {\n        if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < results.size(); ++i) {\n    for (int j = i + 1; j < results.size(); ++j) {\n        if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n        }\n    }\n}\n}"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "43_sort_sort_an_array_of_structs_by_key",
    "parallelism_model": "serial",
    "prompt": "struct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n   std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n       return a.startTime < b.startTime;\n   });\n}",
      "\n    std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n        return a.startTime < b.startTime;\n    });\n}",
      "\n    std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n        return a.startTime < b.startTime;\n    });\n}",
      "\n   std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n       return a.startTime < b.startTime;\n   });\n}",
      "\n   std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n       return a.startTime < b.startTime;\n   });\n}",
      "\n   std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n       return a.startTime < b.startTime;\n   });\n}",
      "\n   std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n       return a.startTime < b.startTime;\n   });\n}",
      "\nstd::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n    return a.startTime < b.startTime;\n});\n}",
      "\n   std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n       return a.startTime < b.startTime;\n   });\n}",
      "\n   std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n       return a.startTime < b.startTime;\n   });\n}",
      "\n   std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n       return a.startTime < b.startTime;\n   });\n}",
      "\n    std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n        return a.startTime < b.startTime;\n    });\n}",
      "\n   std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n       return a.startTime < b.startTime;\n   });\n}",
      "\nstd::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n    return a.startTime < b.startTime;\n});\n}",
      "\n   std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n       return a.startTime < b.startTime;\n   });\n}",
      "\n   std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n       return a.startTime < b.startTime;\n   });\n}",
      "\n   std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n       return a.startTime < b.startTime;\n   });\n}",
      "\n   std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n       return a.startTime < b.startTime;\n   });\n}",
      "\n   std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n       return a.startTime < b.startTime;\n   });\n}",
      "\n    std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n        return a.startTime < b.startTime;\n    });\n}"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "43_sort_sort_an_array_of_structs_by_key",
    "parallelism_model": "hip",
    "prompt": "struct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use AMD HIP to sort in parallel. The kernel is launched with at least as many threads as there are elements.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\n__global__ void sortByStartTime(Result *results, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; ++i) {\n            for (int j = 0; j < N - i - 1; ++j) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; ++i) {\n            for (int j = 0; j < N - i - 1; ++j) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; ++i) {\n            for (int j = 0; j < N - i - 1; ++j) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; ++i) {\n            for (int j = 0; j < N - i - 1; ++j) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; ++i) {\n            for (int j = 0; j < N - i - 1; ++j) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j;\n    Result temp;\n    for (j = 0; j < N - 1; j++) {\n        if (i < N - 1 - j) {\n            if (results[i].startTime > results[i + 1].startTime) {\n                temp = results[i];\n                results[i] = results[i + 1];\n                results[i + 1] = temp;\n            }\n        }\n        __syncthreads();\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; ++i) {\n            for (int j = 0; j < N - i - 1; ++j) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "43_sort_sort_an_array_of_structs_by_key",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use MPI to sort in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of results. Store the output in results on rank 0.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    int *recvCounts = new int[size];\n    int *displs = new int[size];\n    MPI_Gather(&localSize, 1, MPI_INT, recvCounts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        displs[0] = 0;\n        for (int i = 1; i < size; i++) {\n            displs[i] = displs[i - 1] + recvCounts[i - 1];\n        }\n    }\n\n    Result *localResults = new Result[localSize];\n    for (int i = 0; i < localSize; i++) {\n        localResults[i] = results[i];\n    }\n\n    Result *sortedResults = new Result[results.size()];\n    MPI_Gatherv(localResults, localSize, MPI_BYTE, sortedResults, recvCounts, displs, MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::sort(sortedResults, sortedResults + results.size(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results.assign(sortedResults, sortedResults + results.size());\n    }\n\n    delete[] recvCounts;\n    delete[] displs;\n    delete[] localResults;\n    delete[] sortedResults;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    std::vector<Result> localResults = results;\n\n    // Gather all local results to rank 0\n    std::vector<int> recvCounts(size);\n    std::vector<int> displs(size, 0);\n    MPI_Gather(&localSize, 1, MPI_INT, recvCounts.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            displs[i] = displs[i - 1] + recvCounts[i - 1];\n        }\n    }\n\n    std::vector<Result> allResults(results.size() * size);\n    MPI_Gatherv(localResults.data(), localSize, MPI_BYTE, allResults.data(), recvCounts.data(), displs.data(), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::sort(allResults.begin(), allResults.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results = allResults;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    int *recvCounts = new int[size];\n    int *displs = new int[size];\n    MPI_Gather(&localSize, 1, MPI_INT, recvCounts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        displs[0] = 0;\n        for (int i = 1; i < size; i++) {\n            displs[i] = displs[i - 1] + recvCounts[i - 1];\n        }\n    }\n\n    Result *localResults = new Result[localSize];\n    for (int i = 0; i < localSize; i++) {\n        localResults[i] = results[i];\n    }\n\n    Result *sortedResults = new Result[results.size()];\n    MPI_Gatherv(localResults, localSize, MPI_BYTE, sortedResults, recvCounts, displs, MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::sort(sortedResults, sortedResults + results.size(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results.assign(sortedResults, sortedResults + results.size());\n    }\n\n    delete[] recvCounts;\n    delete[] displs;\n    delete[] localResults;\n    delete[] sortedResults;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    int *recvCounts = new int[size];\n    int *displs = new int[size];\n    MPI_Gather(&localSize, 1, MPI_INT, recvCounts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        displs[0] = 0;\n        for (int i = 1; i < size; i++) {\n            displs[i] = displs[i - 1] + recvCounts[i - 1];\n        }\n    }\n\n    Result *localResults = new Result[localSize];\n    for (int i = 0; i < localSize; i++) {\n        localResults[i] = results[i];\n    }\n\n    Result *sortedResults = new Result[results.size()];\n    MPI_Gatherv(localResults, localSize, MPI_BYTE, sortedResults, recvCounts, displs, MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::sort(sortedResults, sortedResults + results.size(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results.assign(sortedResults, sortedResults + results.size());\n    }\n\n    delete[] recvCounts;\n    delete[] displs;\n    delete[] localResults;\n    delete[] sortedResults;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    int *recvCounts = new int[size];\n    int *displs = new int[size];\n\n    MPI_Gather(&localSize, 1, MPI_INT, recvCounts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        displs[0] = 0;\n        for (int i = 1; i < size; i++) {\n            displs[i] = displs[i - 1] + recvCounts[i - 1];\n        }\n    }\n\n    Result *localResults = new Result[localSize];\n    for (int i = 0; i < localSize; i++) {\n        localResults[i] = results[i];\n    }\n\n    Result *sortedResults = new Result[results.size()];\n\n    MPI_Gatherv(localResults, localSize, MPI_BYTE, sortedResults, recvCounts, displs, MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::sort(sortedResults, sortedResults + results.size(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n\n        for (int i = 0; i < results.size(); i++) {\n            results[i] = sortedResults[i];\n        }\n    }\n\n    delete[] recvCounts;\n    delete[] displs;\n    delete[] localResults;\n    delete[] sortedResults;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    int *recvCounts = new int[size];\n    int *displs = new int[size];\n    MPI_Gather(&localSize, 1, MPI_INT, recvCounts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        displs[0] = 0;\n        for (int i = 1; i < size; i++) {\n            displs[i] = displs[i - 1] + recvCounts[i - 1];\n        }\n    }\n\n    Result *localResults = new Result[localSize];\n    for (int i = 0; i < localSize; i++) {\n        localResults[i] = results[i];\n    }\n\n    Result *sortedResults = new Result[results.size()];\n    MPI_Gatherv(localResults, localSize, MPI_BYTE, sortedResults, recvCounts, displs, MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::sort(sortedResults, sortedResults + results.size(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results.assign(sortedResults, sortedResults + results.size());\n    }\n\n    delete[] recvCounts;\n    delete[] displs;\n    delete[] localResults;\n    delete[] sortedResults;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    int *recvCounts = new int[size];\n    int *displs = new int[size];\n\n    MPI_Gather(&localSize, 1, MPI_INT, recvCounts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        displs[0] = 0;\n        for (int i = 1; i < size; i++) {\n            displs[i] = displs[i - 1] + recvCounts[i - 1];\n        }\n    }\n\n    Result *localResults = new Result[localSize];\n    for (int i = 0; i < localSize; i++) {\n        localResults[i] = results[i];\n    }\n\n    Result *sortedResults = new Result[results.size()];\n\n    MPI_Gatherv(localResults, localSize, MPI_BYTE, sortedResults, recvCounts, displs, MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::sort(sortedResults, sortedResults + results.size(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n\n        for (int i = 0; i < results.size(); i++) {\n            results[i] = sortedResults[i];\n        }\n    }\n\n    delete[] recvCounts;\n    delete[] displs;\n    delete[] localResults;\n    delete[] sortedResults;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    int *recvCounts = new int[size];\n    int *displs = new int[size];\n    MPI_Gather(&localSize, 1, MPI_INT, recvCounts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        displs[0] = 0;\n        for (int i = 1; i < size; i++) {\n            displs[i] = displs[i - 1] + recvCounts[i - 1];\n        }\n    }\n\n    Result *localResults = new Result[localSize];\n    for (int i = 0; i < localSize; i++) {\n        localResults[i] = results[i];\n    }\n\n    Result *sortedResults = new Result[results.size()];\n\n    MPI_Gatherv(localResults, localSize, MPI_BYTE, sortedResults, recvCounts, displs, MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::sort(sortedResults, sortedResults + results.size(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results.assign(sortedResults, sortedResults + results.size());\n    }\n\n    delete[] recvCounts;\n    delete[] displs;\n    delete[] localResults;\n    delete[] sortedResults;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    int *recvCounts = new int[size];\n    int *displs = new int[size];\n\n    MPI_Gather(&localSize, 1, MPI_INT, recvCounts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        displs[0] = 0;\n        for (int i = 1; i < size; i++) {\n            displs[i] = displs[i - 1] + recvCounts[i - 1];\n        }\n    }\n\n    Result *localResults = new Result[localSize];\n    for (int i = 0; i < localSize; i++) {\n        localResults[i] = results[i];\n    }\n\n    Result *sortedResults = new Result[results.size()];\n\n    MPI_Gatherv(localResults, localSize, MPI_BYTE, sortedResults, recvCounts, displs, MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::sort(sortedResults, sortedResults + results.size(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n\n        for (int i = 0; i < results.size(); i++) {\n            results[i] = sortedResults[i];\n        }\n    }\n\n    delete[] recvCounts;\n    delete[] displs;\n    delete[] localResults;\n    delete[] sortedResults;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    int *recvCounts = new int[size];\n    int *displs = new int[size];\n    MPI_Gather(&localSize, 1, MPI_INT, recvCounts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        displs[0] = 0;\n        for (int i = 1; i < size; i++) {\n            displs[i] = displs[i - 1] + recvCounts[i - 1];\n        }\n    }\n\n    Result *localResults = new Result[localSize];\n    for (int i = 0; i < localSize; i++) {\n        localResults[i] = results[i];\n    }\n\n    Result *sortedResults = new Result[results.size()];\n    MPI_Gatherv(localResults, localSize, MPI_BYTE, sortedResults, recvCounts, displs, MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::sort(sortedResults, sortedResults + results.size(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results.assign(sortedResults, sortedResults + results.size());\n    }\n\n    delete[] recvCounts;\n    delete[] displs;\n    delete[] localResults;\n    delete[] sortedResults;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    int *recvCounts = new int[size];\n    int *displs = new int[size];\n    MPI_Gather(&localSize, 1, MPI_INT, recvCounts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        displs[0] = 0;\n        for (int i = 1; i < size; i++) {\n            displs[i] = displs[i - 1] + recvCounts[i - 1];\n        }\n    }\n\n    Result *localResults = new Result[localSize];\n    for (int i = 0; i < localSize; i++) {\n        localResults[i] = results[i];\n    }\n\n    Result *sortedResults = new Result[results.size()];\n    MPI_Gatherv(localResults, localSize, MPI_BYTE, sortedResults, recvCounts, displs, MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::sort(sortedResults, sortedResults + results.size(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results.assign(sortedResults, sortedResults + results.size());\n    }\n\n    delete[] recvCounts;\n    delete[] displs;\n    delete[] localResults;\n    delete[] sortedResults;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    int *recvCounts = new int[size];\n    int *displs = new int[size];\n    MPI_Gather(&localSize, 1, MPI_INT, recvCounts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        displs[0] = 0;\n        for (int i = 1; i < size; i++) {\n            displs[i] = displs[i - 1] + recvCounts[i - 1];\n        }\n    }\n\n    Result *localResults = new Result[localSize];\n    for (int i = 0; i < localSize; i++) {\n        localResults[i] = results[i];\n    }\n\n    Result *sortedResults = new Result[results.size()];\n    MPI_Gatherv(localResults, localSize, MPI_BYTE, sortedResults, recvCounts, displs, MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::sort(sortedResults, sortedResults + results.size(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results.assign(sortedResults, sortedResults + results.size());\n    }\n\n    delete[] recvCounts;\n    delete[] displs;\n    delete[] localResults;\n    delete[] sortedResults;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    int *recvCounts = new int[size];\n    int *displs = new int[size];\n    MPI_Gather(&localSize, 1, MPI_INT, recvCounts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        displs[0] = 0;\n        for (int i = 1; i < size; i++) {\n            displs[i] = displs[i - 1] + recvCounts[i - 1];\n        }\n    }\n\n    Result *localResults = new Result[localSize];\n    for (int i = 0; i < localSize; i++) {\n        localResults[i] = results[i];\n    }\n\n    Result *sortedResults = new Result[results.size()];\n    MPI_Gatherv(localResults, localSize, MPI_BYTE, sortedResults, recvCounts, displs, MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::sort(sortedResults, sortedResults + results.size(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results.assign(sortedResults, sortedResults + results.size());\n    }\n\n    delete[] recvCounts;\n    delete[] displs;\n    delete[] localResults;\n    delete[] sortedResults;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    int *recvCounts = new int[size];\n    int *displs = new int[size];\n    MPI_Gather(&localSize, 1, MPI_INT, recvCounts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        displs[0] = 0;\n        for (int i = 1; i < size; i++) {\n            displs[i] = displs[i - 1] + recvCounts[i - 1];\n        }\n    }\n\n    Result *localResults = new Result[localSize];\n    for (int i = 0; i < localSize; i++) {\n        localResults[i] = results[i];\n    }\n\n    Result *sortedResults = new Result[results.size()];\n    MPI_Gatherv(localResults, localSize, MPI_BYTE, sortedResults, recvCounts, displs, MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::sort(sortedResults, sortedResults + results.size(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results.assign(sortedResults, sortedResults + results.size());\n    }\n\n    delete[] recvCounts;\n    delete[] displs;\n    delete[] localResults;\n    delete[] sortedResults;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    int *recvCounts = new int[size];\n    int *displs = new int[size];\n\n    MPI_Gather(&localSize, 1, MPI_INT, recvCounts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        displs[0] = 0;\n        for (int i = 1; i < size; i++) {\n            displs[i] = displs[i - 1] + recvCounts[i - 1];\n        }\n    }\n\n    Result *localResults = new Result[localSize];\n    for (int i = 0; i < localSize; i++) {\n        localResults[i] = results[i];\n    }\n\n    Result *sortedResults = new Result[results.size()];\n\n    MPI_Gatherv(localResults, localSize, MPI_BYTE, sortedResults, recvCounts, displs, MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::sort(sortedResults, sortedResults + results.size(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n\n        for (int i = 0; i < results.size(); i++) {\n            results[i] = sortedResults[i];\n        }\n    }\n\n    delete[] recvCounts;\n    delete[] displs;\n    delete[] localResults;\n    delete[] sortedResults;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    int *recvCounts = new int[size];\n    int *displs = new int[size];\n    MPI_Gather(&localSize, 1, MPI_INT, recvCounts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        displs[0] = 0;\n        for (int i = 1; i < size; i++) {\n            displs[i] = displs[i - 1] + recvCounts[i - 1];\n        }\n    }\n\n    Result *localResults = new Result[localSize];\n    for (int i = 0; i < localSize; i++) {\n        localResults[i] = results[i];\n    }\n\n    Result *sortedResults = new Result[results.size()];\n    MPI_Gatherv(localResults, localSize, MPI_BYTE, sortedResults, recvCounts, displs, MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::sort(sortedResults, sortedResults + results.size(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results.assign(sortedResults, sortedResults + results.size());\n    }\n\n    delete[] recvCounts;\n    delete[] displs;\n    delete[] localResults;\n    delete[] sortedResults;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    int *recvCounts = new int[size];\n    int *displs = new int[size];\n    MPI_Gather(&localSize, 1, MPI_INT, recvCounts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        displs[0] = 0;\n        for (int i = 1; i < size; i++) {\n            displs[i] = displs[i - 1] + recvCounts[i - 1];\n        }\n    }\n\n    Result *localResults = new Result[localSize];\n    for (int i = 0; i < localSize; i++) {\n        localResults[i] = results[i];\n    }\n\n    Result *sortedResults = new Result[results.size()];\n\n    MPI_Gatherv(localResults, localSize, MPI_BYTE, sortedResults, recvCounts, displs, MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::sort(sortedResults, sortedResults + results.size(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n\n        for (int i = 0; i < results.size(); i++) {\n            results[i] = sortedResults[i];\n        }\n    }\n\n    delete[] recvCounts;\n    delete[] displs;\n    delete[] localResults;\n    delete[] sortedResults;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    int *recvCounts = new int[size];\n    int *displs = new int[size];\n    MPI_Gather(&localSize, 1, MPI_INT, recvCounts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        displs[0] = 0;\n        for (int i = 1; i < size; i++) {\n            displs[i] = displs[i - 1] + recvCounts[i - 1];\n        }\n    }\n\n    Result *localResults = new Result[localSize];\n    for (int i = 0; i < localSize; i++) {\n        localResults[i] = results[i];\n    }\n\n    Result *sortedResults = new Result[results.size()];\n\n    MPI_Gatherv(localResults, localSize, MPI_BYTE, sortedResults, recvCounts, displs, MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::sort(sortedResults, sortedResults + results.size(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n\n        for (int i = 0; i < results.size(); i++) {\n            results[i] = sortedResults[i];\n        }\n    }\n\n    delete[] recvCounts;\n    delete[] displs;\n    delete[] localResults;\n    delete[] sortedResults;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    int *recvCounts = new int[size];\n    int *displs = new int[size];\n    MPI_Gather(&localSize, 1, MPI_INT, recvCounts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        displs[0] = 0;\n        for (int i = 1; i < size; i++) {\n            displs[i] = displs[i - 1] + recvCounts[i - 1];\n        }\n    }\n\n    Result *localResults = new Result[localSize];\n    for (int i = 0; i < localSize; i++) {\n        localResults[i] = results[i];\n    }\n\n    Result *sortedResults = new Result[results.size()];\n    MPI_Gatherv(localResults, localSize, MPI_BYTE, sortedResults, recvCounts, displs, MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::sort(sortedResults, sortedResults + results.size(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results.assign(sortedResults, sortedResults + results.size());\n    }\n\n    delete[] recvCounts;\n    delete[] displs;\n    delete[] localResults;\n    delete[] sortedResults;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    int *recvCounts = new int[size];\n    int *displs = new int[size];\n    MPI_Gather(&localSize, 1, MPI_INT, recvCounts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        displs[0] = 0;\n        for (int i = 1; i < size; i++) {\n            displs[i] = displs[i - 1] + recvCounts[i - 1];\n        }\n    }\n\n    Result *localResults = new Result[localSize];\n    for (int i = 0; i < localSize; i++) {\n        localResults[i] = results[i];\n    }\n\n    Result *sortedResults = new Result[results.size()];\n    MPI_Gatherv(localResults, localSize, MPI_BYTE, sortedResults, recvCounts, displs, MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::sort(sortedResults, sortedResults + results.size(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n\n        for (int i = 0; i < results.size(); i++) {\n            results[i] = sortedResults[i];\n        }\n    }\n\n    delete[] recvCounts;\n    delete[] displs;\n    delete[] localResults;\n    delete[] sortedResults;\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "43_sort_sort_an_array_of_structs_by_key",
    "parallelism_model": "cuda",
    "prompt": "struct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use CUDA to sort in parallel. The kernel is launched with at least as many threads as there are elements.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\n__global__ void sortByStartTime(Result *results, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    // Swap the elements\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; ++i) {\n            for (int j = 0; j < N - i - 1; ++j) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N - 1 - i; ++j) {\n            if (results[j].startTime > results[j + 1].startTime) {\n                Result temp = results[j];\n                results[j] = results[j + 1];\n                results[j + 1] = temp;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (results[j].startTime > results[j + 1].startTime) {\n                    Result temp = results[j];\n                    results[j] = results[j + 1];\n                    results[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (int i = 0; i < N - 1; i++) {\n        for (int j = 0; j < N - i - 1; j++) {\n            if (results[j].startTime > results[j + 1].startTime) {\n                Result temp = results[j];\n                results[j] = results[j + 1];\n                results[j + 1] = temp;\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "43_sort_sort_an_array_of_structs_by_key",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use MPI and OpenMP to sort in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of results. Store the outputs in results on rank 0.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < results.size(); i++) {\n        for (int j = 0; j < results.size() - i - 1; j++) {\n            if (results[j].startTime > results[j + 1].startTime) {\n                Result temp = results[j];\n                results[j] = results[j + 1];\n                results[j + 1] = temp;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<Result> mergedResults;\n        mergedResults.resize(results.size() * size);\n        MPI_Gather(&results[0], results.size(), MPI_BYTE, &mergedResults[0], results.size(), MPI_BYTE, 0, MPI_COMM_WORLD);\n        std::sort(mergedResults.begin(), mergedResults.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results = mergedResults;\n    } else {\n        MPI_Gather(&results[0], results.size(), MPI_BYTE, nullptr, 0, MPI_BYTE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < results.size(); i++) {\n        for (int j = i + 1; j < results.size(); j++) {\n            if (results[i].startTime > results[j].startTime) {\n                Result temp = results[i];\n                results[i] = results[j];\n                results[j] = temp;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<Result> mergedResults;\n        mergedResults.resize(results.size() * size);\n        MPI_Gather(&results[0], results.size(), MPI_INT, &mergedResults[0], results.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        std::sort(mergedResults.begin(), mergedResults.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results = mergedResults;\n    } else {\n        MPI_Gather(&results[0], results.size(), MPI_INT, NULL, 0, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < results.size(); i++) {\n        for (int j = 0; j < results.size() - 1; j++) {\n            if (results[j].startTime > results[j + 1].startTime) {\n                Result temp = results[j];\n                results[j] = results[j + 1];\n                results[j + 1] = temp;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<Result> mergedResults;\n        for (int i = 0; i < size; i++) {\n            if (i != 0) {\n                std::vector<Result> receivedResults(results.size());\n                MPI_Recv(&receivedResults[0], results.size() * sizeof(Result), MPI_BYTE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n                mergedResults.insert(mergedResults.end(), receivedResults.begin(), receivedResults.end());\n            } else {\n                mergedResults.insert(mergedResults.end(), results.begin(), results.end());\n            }\n        }\n\n        std::sort(mergedResults.begin(), mergedResults.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n\n        results = mergedResults;\n    } else {\n        MPI_Send(&results[0], results.size() * sizeof(Result), MPI_BYTE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < results.size(); i++) {\n        for (int j = i + 1; j < results.size(); j++) {\n            if (results[i].startTime > results[j].startTime) {\n                Result temp = results[i];\n                results[i] = results[j];\n                results[j] = temp;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<Result> mergedResults;\n        mergedResults.resize(results.size() * size);\n        MPI_Gather(&results[0], results.size(), MPI_BYTE, &mergedResults[0], results.size(), MPI_BYTE, 0, MPI_COMM_WORLD);\n        std::sort(mergedResults.begin(), mergedResults.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results = mergedResults;\n    } else {\n        MPI_Gather(&results[0], results.size(), MPI_BYTE, nullptr, 0, MPI_BYTE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < results.size(); i++) {\n        for (int j = 0; j < results.size() - 1; j++) {\n            if (results[j].startTime > results[j + 1].startTime) {\n                Result temp = results[j];\n                results[j] = results[j + 1];\n                results[j + 1] = temp;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<Result> mergedResults;\n        mergedResults.resize(results.size() * size);\n        MPI_Gather(&results[0], results.size(), MPI_BYTE, &mergedResults[0], results.size(), MPI_BYTE, 0, MPI_COMM_WORLD);\n        std::sort(mergedResults.begin(), mergedResults.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results = mergedResults;\n    } else {\n        MPI_Gather(&results[0], results.size(), MPI_BYTE, nullptr, 0, MPI_BYTE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < results.size(); i++) {\n        for (int j = i + 1; j < results.size(); j++) {\n            if (results[i].startTime > results[j].startTime) {\n                Result temp = results[i];\n                results[i] = results[j];\n                results[j] = temp;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<Result> mergedResults;\n        mergedResults.resize(results.size() * size);\n        MPI_Gather(&results[0], results.size(), MPI_BYTE, &mergedResults[0], results.size(), MPI_BYTE, 0, MPI_COMM_WORLD);\n        std::sort(mergedResults.begin(), mergedResults.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results = mergedResults;\n    } else {\n        MPI_Gather(&results[0], results.size(), MPI_BYTE, nullptr, 0, MPI_BYTE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    int *allSizes = new int[size];\n    MPI_Allgather(&localSize, 1, MPI_INT, allSizes, 1, MPI_INT, MPI_COMM_WORLD);\n\n    int *displs = new int[size];\n    displs[0] = 0;\n    for (int i = 1; i < size; i++) {\n        displs[i] = displs[i - 1] + allSizes[i - 1];\n    }\n\n    int totalSize = displs[size - 1] + allSizes[size - 1];\n    Result *allResults = new Result[totalSize];\n    MPI_Allgatherv(&results[0], localSize, MPI::INT, allResults, allSizes, displs, MPI::INT, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < totalSize - 1; i++) {\n        for (int j = 0; j < totalSize - i - 1; j++) {\n            if (allResults[j].startTime > allResults[j + 1].startTime) {\n                Result temp = allResults[j];\n                allResults[j] = allResults[j + 1];\n                allResults[j + 1] = temp;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        results.assign(allResults, allResults + totalSize);\n    }\n\n    delete[] allSizes;\n    delete[] displs;\n    delete[] allResults;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < results.size(); i++) {\n        for (int j = 0; j < results.size() - 1; j++) {\n            if (results[j].startTime > results[j + 1].startTime) {\n                Result temp = results[j];\n                results[j] = results[j + 1];\n                results[j + 1] = temp;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<Result> mergedResults;\n        mergedResults.resize(size * results.size());\n        MPI_Gather(&results[0], results.size(), MPI_BYTE, &mergedResults[0], results.size(), MPI_BYTE, 0, MPI_COMM_WORLD);\n        std::sort(mergedResults.begin(), mergedResults.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results = std::vector<Result>(mergedResults.begin(), mergedResults.begin() + results.size());\n    } else {\n        MPI_Gather(&results[0], results.size(), MPI_BYTE, nullptr, 0, MPI_BYTE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < results.size(); i++) {\n        for (int j = 0; j < results.size() - 1; j++) {\n            if (results[j].startTime > results[j + 1].startTime) {\n                Result temp = results[j];\n                results[j] = results[j + 1];\n                results[j + 1] = temp;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<Result> mergedResults;\n        mergedResults.resize(size * results.size());\n        MPI_Gather(&results[0], results.size(), MPI_BYTE, &mergedResults[0], results.size(), MPI_BYTE, 0, MPI_COMM_WORLD);\n        std::sort(mergedResults.begin(), mergedResults.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results = std::vector<Result>(mergedResults.begin(), mergedResults.begin() + results.size());\n    } else {\n        MPI_Gather(&results[0], results.size(), MPI_BYTE, nullptr, 0, MPI_BYTE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        int numResults = results.size();\n        std::vector<Result> sortedResults(numResults);\n\n        #pragma omp parallel for\n        for (int i = 0; i < numResults; i++) {\n            sortedResults[i] = results[i];\n        }\n\n        std::sort(sortedResults.begin(), sortedResults.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&sortedResults[0], numResults * sizeof(Result), MPI_BYTE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n\n        results = sortedResults;\n    } else {\n        int numResults = results.size();\n        MPI_Send(&results[0], numResults * sizeof(Result), MPI_BYTE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    int *allSizes = new int[size];\n    MPI_Allgather(&localSize, 1, MPI_INT, allSizes, 1, MPI_INT, MPI_COMM_WORLD);\n\n    int *displs = new int[size];\n    displs[0] = 0;\n    for (int i = 1; i < size; i++) {\n        displs[i] = displs[i - 1] + allSizes[i - 1];\n    }\n\n    int totalSize = displs[size - 1] + allSizes[size - 1];\n    Result *allResults = new Result[totalSize];\n    MPI_Allgatherv(&results[0], localSize, MPI::INT, allResults, allSizes, displs, MPI::INT, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < totalSize - 1; i++) {\n        for (int j = 0; j < totalSize - i - 1; j++) {\n            if (allResults[j].startTime > allResults[j + 1].startTime) {\n                Result temp = allResults[j];\n                allResults[j] = allResults[j + 1];\n                allResults[j + 1] = temp;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        results.assign(allResults, allResults + totalSize);\n    }\n\n    delete[] allSizes;\n    delete[] displs;\n    delete[] allResults;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < results.size(); i++) {\n        for (int j = 0; j < results.size() - 1; j++) {\n            if (results[j].startTime > results[j + 1].startTime) {\n                Result temp = results[j];\n                results[j] = results[j + 1];\n                results[j + 1] = temp;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<Result> mergedResults;\n        mergedResults.resize(size * results.size());\n        MPI_Gather(&results[0], results.size(), MPI_BYTE, &mergedResults[0], results.size(), MPI_BYTE, 0, MPI_COMM_WORLD);\n        std::sort(mergedResults.begin(), mergedResults.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results = std::vector<Result>(mergedResults.begin(), mergedResults.begin() + results.size());\n    } else {\n        MPI_Gather(&results[0], results.size(), MPI_BYTE, nullptr, 0, MPI_BYTE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < results.size(); i++) {\n        for (int j = 0; j < results.size() - 1; j++) {\n            if (results[j].startTime > results[j + 1].startTime) {\n                Result temp = results[j];\n                results[j] = results[j + 1];\n                results[j + 1] = temp;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<Result> mergedResults;\n        mergedResults.resize(results.size() * size);\n        MPI_Gather(&results[0], results.size(), MPI_BYTE, &mergedResults[0], results.size(), MPI_BYTE, 0, MPI_COMM_WORLD);\n        std::sort(mergedResults.begin(), mergedResults.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results = mergedResults;\n    } else {\n        MPI_Gather(&results[0], results.size(), MPI_BYTE, NULL, 0, MPI_BYTE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < results.size(); i++) {\n        for (int j = 0; j < results.size() - i - 1; j++) {\n            if (results[j].startTime > results[j + 1].startTime) {\n                std::swap(results[j], results[j + 1]);\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<Result> mergedResults;\n        for (int i = 0; i < size; i++) {\n            if (i != 0) {\n                std::vector<Result> receivedResults(results.size());\n                MPI_Recv(&receivedResults[0], results.size() * sizeof(Result), MPI_BYTE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n                mergedResults.insert(mergedResults.end(), receivedResults.begin(), receivedResults.end());\n            } else {\n                mergedResults.insert(mergedResults.end(), results.begin(), results.end());\n            }\n        }\n        results = mergedResults;\n    } else {\n        MPI_Send(&results[0], results.size() * sizeof(Result), MPI_BYTE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = results.size();\n    int globalSize;\n\n    MPI_Allreduce(&localSize, &globalSize, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    std::vector<Result> localResults = results;\n    std::vector<Result> globalResults(globalSize);\n\n    MPI_Allgather(&localResults[0], localSize, MPI_BYTE, &globalResults[0], localSize, MPI_BYTE, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < globalSize; i++) {\n        for (int j = i + 1; j < globalSize; j++) {\n            if (globalResults[i].startTime > globalResults[j].startTime) {\n                std::swap(globalResults[i], globalResults[j]);\n            }\n        }\n    }\n\n    if (rank == 0) {\n        results = globalResults;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < results.size(); i++) {\n        for (int j = 0; j < results.size() - i - 1; j++) {\n            if (results[j].startTime > results[j + 1].startTime) {\n                Result temp = results[j];\n                results[j] = results[j + 1];\n                results[j + 1] = temp;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<Result> mergedResults;\n        mergedResults.resize(results.size() * size);\n        MPI_Gather(&results[0], results.size(), MPI_BYTE, &mergedResults[0], results.size(), MPI_BYTE, 0, MPI_COMM_WORLD);\n        std::sort(mergedResults.begin(), mergedResults.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results = mergedResults;\n    } else {\n        MPI_Gather(&results[0], results.size(), MPI_BYTE, nullptr, 0, MPI_BYTE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < results.size(); i++) {\n        for (int j = i + 1; j < results.size(); j++) {\n            if (results[i].startTime > results[j].startTime) {\n                std::swap(results[i], results[j]);\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<Result> mergedResults;\n        mergedResults.reserve(results.size() * size);\n        for (int i = 0; i < size; i++) {\n            std::vector<Result> recvResults(results.size());\n            MPI_Recv(&recvResults[0], results.size() * sizeof(Result), MPI_BYTE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            mergedResults.insert(mergedResults.end(), recvResults.begin(), recvResults.end());\n        }\n        results = mergedResults;\n    } else {\n        MPI_Send(&results[0], results.size() * sizeof(Result), MPI_BYTE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < results.size(); i++) {\n        for (int j = 0; j < results.size() - 1; j++) {\n            if (results[j].startTime > results[j + 1].startTime) {\n                Result temp = results[j];\n                results[j] = results[j + 1];\n                results[j + 1] = temp;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<Result> mergedResults;\n        mergedResults.resize(results.size() * size);\n        MPI_Gather(&results[0], results.size(), MPI_BYTE, &mergedResults[0], results.size(), MPI_BYTE, 0, MPI_COMM_WORLD);\n        std::sort(mergedResults.begin(), mergedResults.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results = mergedResults;\n    } else {\n        MPI_Gather(&results[0], results.size(), MPI_BYTE, nullptr, 0, MPI_BYTE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < results.size(); i++) {\n        for (int j = 0; j < results.size() - 1; j++) {\n            if (results[j].startTime > results[j + 1].startTime) {\n                Result temp = results[j];\n                results[j] = results[j + 1];\n                results[j + 1] = temp;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<Result> mergedResults;\n        mergedResults.resize(results.size() * size);\n        MPI_Gather(&results[0], results.size(), MPI_BYTE, &mergedResults[0], results.size(), MPI_BYTE, 0, MPI_COMM_WORLD);\n        std::sort(mergedResults.begin(), mergedResults.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results = mergedResults;\n    } else {\n        MPI_Gather(&results[0], results.size(), MPI_BYTE, nullptr, 0, MPI_BYTE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (int i = 0; i < results.size(); i++) {\n        for (int j = 0; j < results.size() - 1; j++) {\n            if (results[j].startTime > results[j + 1].startTime) {\n                Result temp = results[j];\n                results[j] = results[j + 1];\n                results[j + 1] = temp;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<Result> tempResults;\n        for (int i = 1; i < size; i++) {\n            int count;\n            MPI_Recv(&count, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            std::vector<Result> recvResults(count);\n            MPI_Recv(recvResults.data(), count * sizeof(Result), MPI_BYTE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            tempResults.insert(tempResults.end(), recvResults.begin(), recvResults.end());\n        }\n        tempResults.insert(tempResults.end(), results.begin(), results.end());\n        std::sort(tempResults.begin(), tempResults.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n        });\n        results = tempResults;\n    } else {\n        int count = results.size();\n        MPI_Send(&count, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(results.data(), count * sizeof(Result), MPI_BYTE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "40_sort_sort_an_array_of_complex_numbers_by_magnitude",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Sort the array x of complex numbers by their magnitude in ascending order.\n   Use Kokkos to sort in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(Kokkos::View<Kokkos::complex<double>*> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\nKokkos::sort(x, [](const Kokkos::complex<double>& a, const Kokkos::complex<double>& b) {\n    return std::abs(a) < std::abs(b);\n});\n}",
      "\n    Kokkos::sort(x, [](const Kokkos::complex<double>& a, const Kokkos::complex<double>& b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\nKokkos::sort(x, [](const Kokkos::complex<double>& a, const Kokkos::complex<double>& b) {\n    return Kokkos::abs(a) < Kokkos::abs(b);\n});\n}",
      "\n  Kokkos::sort(x, [](const Kokkos::complex<double>& a, const Kokkos::complex<double>& b) {\n      return std::abs(a) < std::abs(b);\n  });\n}",
      "\n  Kokkos::sort(x, [](const Kokkos::complex<double>& a, const Kokkos::complex<double>& b) {\n      return std::abs(a) < std::abs(b);\n  });\n}",
      "\n  Kokkos::sort(x, [](const Kokkos::complex<double>& a, const Kokkos::complex<double>& b) {\n      return std::abs(a) < std::abs(b);\n  });\n}",
      "\n    Kokkos::sort(x, [](const Kokkos::complex<double>& a, const Kokkos::complex<double>& b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n  Kokkos::sort(x, [](const Kokkos::complex<double>& a, const Kokkos::complex<double>& b) {\n      return std::abs(a) < std::abs(b);\n  });\n}",
      "\n  Kokkos::sort(x, [](const Kokkos::complex<double>& a, const Kokkos::complex<double>& b) {\n      return std::abs(a) < std::abs(b);\n  });\n}",
      "\nKokkos::sort(x, [](const Kokkos::complex<double>& a, const Kokkos::complex<double>& b) {\n    return std::abs(a) < std::abs(b);\n});\n}",
      "\nKokkos::sort(x, [](const Kokkos::complex<double>& a, const Kokkos::complex<double>& b) {\n    return std::abs(a) < std::abs(b);\n});\n}",
      "\n  Kokkos::sort(x, [](const Kokkos::complex<double>& a, const Kokkos::complex<double>& b) {\n      return std::abs(a) < std::abs(b);\n  });\n}",
      "\n  Kokkos::sort(x, [](const Kokkos::complex<double>& a, const Kokkos::complex<double>& b) {\n      return std::abs(a) < std::abs(b);\n  });\n}",
      "\nKokkos::sort(x, [](const Kokkos::complex<double>& a, const Kokkos::complex<double>& b) {\n    return std::abs(a) < std::abs(b);\n});\n}",
      "\n  Kokkos::sort(x, [](const Kokkos::complex<double>& a, const Kokkos::complex<double>& b) {\n      return std::abs(a) < std::abs(b);\n  });\n}",
      "\nKokkos::sort(x, [](const Kokkos::complex<double>& a, const Kokkos::complex<double>& b) {\n    return std::abs(a) < std::abs(b);\n});\n}",
      "\n  Kokkos::sort(x, [](const Kokkos::complex<double>& a, const Kokkos::complex<double>& b) {\n      return std::abs(a) < std::abs(b);\n  });\n}",
      "\n    Kokkos::sort(x, [](const Kokkos::complex<double>& a, const Kokkos::complex<double>& b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    Kokkos::sort(x, [](const Kokkos::complex<double>& a, const Kokkos::complex<double>& b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n  Kokkos::sort(x, [](const Kokkos::complex<double>& a, const Kokkos::complex<double>& b) {\n      return std::abs(a) < std::abs(b);\n  });\n}"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "40_sort_sort_an_array_of_complex_numbers_by_magnitude",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use OpenMP to sort in parallel.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (std::abs(x[j]) < std::abs(x[i])) {\n                std::swap(x[i], x[j]);\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (std::abs(x[j]) < std::abs(x[i])) {\n                std::swap(x[i], x[j]);\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (std::abs(x[j]) < std::abs(x[i])) {\n                std::swap(x[i], x[j]);\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (std::abs(x[j]) < std::abs(x[i])) {\n                std::swap(x[i], x[j]);\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (std::abs(x[i]) > std::abs(x[j])) {\n                std::swap(x[i], x[j]);\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (std::abs(x[j]) < std::abs(x[i])) {\n                std::swap(x[i], x[j]);\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (std::abs(x[j]) < std::abs(x[i])) {\n                std::swap(x[i], x[j]);\n            }\n        }\n    }\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    for (int j = i + 1; j < x.size(); j++) {\n        if (std::abs(x[i]) > std::abs(x[j])) {\n            std::swap(x[i], x[j]);\n        }\n    }\n}\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (std::abs(x[i]) > std::abs(x[j])) {\n                std::swap(x[i], x[j]);\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (std::abs(x[j]) < std::abs(x[i])) {\n                std::swap(x[i], x[j]);\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (std::abs(x[j]) < std::abs(x[i])) {\n                std::swap(x[i], x[j]);\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (std::abs(x[j]) < std::abs(x[i])) {\n                std::swap(x[i], x[j]);\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (std::abs(x[j]) < std::abs(x[i])) {\n                std::swap(x[i], x[j]);\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (std::abs(x[j]) < std::abs(x[i])) {\n                std::swap(x[i], x[j]);\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (abs(x[j]) < abs(x[i])) {\n                std::swap(x[i], x[j]);\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (std::abs(x[j]) < std::abs(x[i])) {\n                std::swap(x[i], x[j]);\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (std::abs(x[i]) > std::abs(x[j])) {\n                std::swap(x[i], x[j]);\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (std::abs(x[j]) < std::abs(x[i])) {\n                std::swap(x[i], x[j]);\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (std::abs(x[i]) > std::abs(x[j])) {\n                std::swap(x[i], x[j]);\n            }\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (std::abs(x[i]) > std::abs(x[j])) {\n                std::swap(x[i], x[j]);\n            }\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "40_sort_sort_an_array_of_complex_numbers_by_magnitude",
    "parallelism_model": "serial",
    "prompt": "/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "40_sort_sort_an_array_of_complex_numbers_by_magnitude",
    "parallelism_model": "hip",
    "prompt": "/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use AMD HIP to sort in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\n__global__ void sortComplexByMagnitude(hipDoubleComplex *x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (int i = 0; i < N; i++) {\n        for (int j = i + 1; j < N; j++) {\n            double mag1 = hypot(x[i].x, x[i].y);\n            double mag2 = hypot(x[j].x, x[j].y);\n            if (mag1 > mag2) {\n                hipDoubleComplex temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = x[j].x * x[j].x + x[j].y * x[j].y;\n                double mag2 = x[j + 1].x * x[j + 1].x + x[j + 1].y * x[j + 1].y;\n                if (mag1 > mag2) {\n                    hipDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = x[j].x * x[j].x + x[j].y * x[j].y;\n                double mag2 = x[j + 1].x * x[j + 1].x + x[j + 1].y * x[j + 1].y;\n                if (mag1 > mag2) {\n                    hipDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = x[j].x * x[j].x + x[j].y * x[j].y;\n                double mag2 = x[j + 1].x * x[j + 1].x + x[j + 1].y * x[j + 1].y;\n                if (mag1 > mag2) {\n                    hipDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = x[j].x * x[j].x + x[j].y * x[j].y;\n                double mag2 = x[j + 1].x * x[j + 1].x + x[j + 1].y * x[j + 1].y;\n                if (mag1 > mag2) {\n                    hipDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = x[j].x * x[j].x + x[j].y * x[j].y;\n                double mag2 = x[j + 1].x * x[j + 1].x + x[j + 1].y * x[j + 1].y;\n                if (mag1 > mag2) {\n                    hipDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = x[j].x * x[j].x + x[j].y * x[j].y;\n                double mag2 = x[j + 1].x * x[j + 1].x + x[j + 1].y * x[j + 1].y;\n                if (mag1 > mag2) {\n                    hipDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int id = blockIdx.x * blockDim.x + threadIdx.x;\n    if (id < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = x[j].x * x[j].x + x[j].y * x[j].y;\n                double mag2 = x[j + 1].x * x[j + 1].x + x[j + 1].y * x[j + 1].y;\n                if (mag1 > mag2) {\n                    hipDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = x[j].x * x[j].x + x[j].y * x[j].y;\n                double mag2 = x[j + 1].x * x[j + 1].x + x[j + 1].y * x[j + 1].y;\n                if (mag1 > mag2) {\n                    hipDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = x[j].x * x[j].x + x[j].y * x[j].y;\n                double mag2 = x[j + 1].x * x[j + 1].x + x[j + 1].y * x[j + 1].y;\n                if (mag1 > mag2) {\n                    hipDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = x[j].x * x[j].x + x[j].y * x[j].y;\n                double mag2 = x[j + 1].x * x[j + 1].x + x[j + 1].y * x[j + 1].y;\n                if (mag1 > mag2) {\n                    hipDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    // Calculate the magnitude of each complex number\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double magnitude = hypot(x[tid].x, x[tid].y);\n\n        // Use insertion sort to sort the complex numbers by magnitude\n        for (int i = 1; i < N; i++) {\n            hipDoubleComplex key = x[i];\n            int j = i - 1;\n            while (j >= 0 && hypot(x[j].x, x[j].y) > magnitude) {\n                x[j + 1] = x[j];\n                j = j - 1;\n            }\n            x[j + 1] = key;\n        }\n    }\n}\n",
      "\n    for (int i = 0; i < N; i++) {\n        for (int j = i + 1; j < N; j++) {\n            double mag1 = hipCabs(x[i]);\n            double mag2 = hipCabs(x[j]);\n            if (mag1 > mag2) {\n                hipDoubleComplex temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = x[j].x * x[j].x + x[j].y * x[j].y;\n                double mag2 = x[j + 1].x * x[j + 1].x + x[j + 1].y * x[j + 1].y;\n                if (mag1 > mag2) {\n                    hipDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = x[j].x * x[j].x + x[j].y * x[j].y;\n                double mag2 = x[j + 1].x * x[j + 1].x + x[j + 1].y * x[j + 1].y;\n                if (mag1 > mag2) {\n                    hipDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = x[j].x * x[j].x + x[j].y * x[j].y;\n                double mag2 = x[j + 1].x * x[j + 1].x + x[j + 1].y * x[j + 1].y;\n                if (mag1 > mag2) {\n                    hipDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    // Calculate the magnitude of each complex number\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double magnitude = hypot(x[tid].x, x[tid].y);\n\n        // Use bubble sort to sort the complex numbers by magnitude\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                if (hypot(x[j].x, x[j].y) > hypot(x[j + 1].x, x[j + 1].y)) {\n                    // Swap the complex numbers\n                    hipDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (int i = 0; i < N; i++) {\n        for (int j = i + 1; j < N; j++) {\n            double mag1 = hipCabs(x[i]);\n            double mag2 = hipCabs(x[j]);\n            if (mag1 > mag2) {\n                hipDoubleComplex temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = hypot(x[j].x, x[j].y);\n                double mag2 = hypot(x[j + 1].x, x[j + 1].y);\n                if (mag1 > mag2) {\n                    hipDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N - 1; j++) {\n            double mag1 = x[j].x * x[j].x + x[j].y * x[j].y;\n            double mag2 = x[j + 1].x * x[j + 1].x + x[j + 1].y * x[j + 1].y;\n            if (mag1 > mag2) {\n                hipDoubleComplex temp = x[j];\n                x[j] = x[j + 1];\n                x[j + 1] = temp;\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "40_sort_sort_an_array_of_complex_numbers_by_magnitude",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use MPI to sort in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "40_sort_sort_an_array_of_complex_numbers_by_magnitude",
    "parallelism_model": "cuda",
    "prompt": "/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use CUDA to sort in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\n__global__ void sortComplexByMagnitude(cuDoubleComplex *x, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = cuCabs(x[j]);\n                double mag2 = cuCabs(x[j + 1]);\n                if (mag1 > mag2) {\n                    cuDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = cuCabs(x[j]);\n                double mag2 = cuCabs(x[j + 1]);\n                if (mag1 > mag2) {\n                    cuDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = cuCabs(x[j]);\n                double mag2 = cuCabs(x[j + 1]);\n                if (mag1 > mag2) {\n                    cuDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = cuCabs(x[j]);\n                double mag2 = cuCabs(x[j + 1]);\n                if (mag1 > mag2) {\n                    cuDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = cuCabs(x[j]);\n                double mag2 = cuCabs(x[j + 1]);\n                if (mag1 > mag2) {\n                    cuDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = cuCabs(x[j]);\n                double mag2 = cuCabs(x[j + 1]);\n                if (mag1 > mag2) {\n                    cuDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = cuCabs(x[j]);\n                double mag2 = cuCabs(x[j + 1]);\n                if (mag1 > mag2) {\n                    cuDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = cuCabs(x[j]);\n                double mag2 = cuCabs(x[j + 1]);\n                if (mag1 > mag2) {\n                    cuDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = cuCabs(x[j]);\n                double mag2 = cuCabs(x[j + 1]);\n                if (mag1 > mag2) {\n                    cuDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = cuCabs(x[j]);\n                double mag2 = cuCabs(x[j + 1]);\n                if (mag1 > mag2) {\n                    cuDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = cuCabs(x[j]);\n                double mag2 = cuCabs(x[j + 1]);\n                if (mag1 > mag2) {\n                    cuDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = cuCabs(x[j]);\n                double mag2 = cuCabs(x[j + 1]);\n                if (mag1 > mag2) {\n                    cuDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = cuCabs(x[j]);\n                double mag2 = cuCabs(x[j + 1]);\n                if (mag1 > mag2) {\n                    cuDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = cuCabs(x[j]);\n                double mag2 = cuCabs(x[j + 1]);\n                if (mag1 > mag2) {\n                    cuDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int id = blockIdx.x * blockDim.x + threadIdx.x;\n    if (id < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = cuCabs(x[j]);\n                double mag2 = cuCabs(x[j + 1]);\n                if (mag1 > mag2) {\n                    cuDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = cuCabs(x[j]);\n                double mag2 = cuCabs(x[j + 1]);\n                if (mag1 > mag2) {\n                    cuDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = cuCabs(x[j]);\n                double mag2 = cuCabs(x[j + 1]);\n                if (mag1 > mag2) {\n                    cuDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = cuCabs(x[j]);\n                double mag2 = cuCabs(x[j + 1]);\n                if (mag1 > mag2) {\n                    cuDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = cuCabs(x[j]);\n                double mag2 = cuCabs(x[j + 1]);\n                if (mag1 > mag2) {\n                    cuDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n",
      "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        for (int i = 0; i < N - 1; i++) {\n            for (int j = 0; j < N - i - 1; j++) {\n                double mag1 = cuCabs(x[j]);\n                double mag2 = cuCabs(x[j + 1]);\n                if (mag1 > mag2) {\n                    cuDoubleComplex temp = x[j];\n                    x[j] = x[j + 1];\n                    x[j + 1] = temp;\n                }\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "40_sort_sort_an_array_of_complex_numbers_by_magnitude",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use MPI and OpenMP to sort in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    // Sort the vector x by magnitude using parallel sorting algorithm\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    // Sort the vector x by magnitude using parallel sorting algorithm\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    // Sort the vector x by magnitude in ascending order\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        double mag = std::abs(x[i]);\n        #pragma omp critical\n        {\n            x[i] = std::complex<double>(mag, 0);\n        }\n    }\n\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::vector<std::complex<double>> recvbuf(x.size() * MPI::COMM_WORLD.Get_size());\n        MPI::COMM_WORLD.Gather(&x[0], x.size(), MPI::DOUBLE_COMPLEX, &recvbuf[0], x.size(), MPI::DOUBLE_COMPLEX, 0);\n        std::copy(recvbuf.begin(), recvbuf.end(), x.begin());\n    } else {\n        MPI::COMM_WORLD.Gather(&x[0], x.size(), MPI::DOUBLE_COMPLEX, nullptr, 0, MPI::DOUBLE_COMPLEX, 0);\n    }\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    // Sort the vector x by magnitude in ascending order\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
      "\n    std::sort(x.begin(), x.end(), [](const std::complex<double> &a, const std::complex<double> &b) {\n        return std::abs(a) < std::abs(b);\n    });\n}"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "50_stencil_xor_kernel",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(Kokkos::View<const int**> &input, Kokkos::View<int**> &output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input(i-1, j) == 1) count++;\n      if (i < N-1 && input(i+1, j) == 1) count++;\n      if (j > 0 && input(i, j-1) == 1) count++;\n      if (j < N-1 && input(i, j+1) == 1) count++;\n      output(i, j) = (count == 1) ? 1 : 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input(i-1, j) == 1) count++;\n      if (i < N-1 && input(i+1, j) == 1) count++;\n      if (j > 0 && input(i, j-1) == 1) count++;\n      if (j < N-1 && input(i, j+1) == 1) count++;\n      output(i, j) = (count == 1) ? 1 : 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input(i-1, j) == 1) count++;\n      if (i < N-1 && input(i+1, j) == 1) count++;\n      if (j > 0 && input(i, j-1) == 1) count++;\n      if (j < N-1 && input(i, j+1) == 1) count++;\n      output(i, j) = (count == 1) ? 1 : 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input(i-1, j) == 1) count++;\n      if (i < N-1 && input(i+1, j) == 1) count++;\n      if (j > 0 && input(i, j-1) == 1) count++;\n      if (j < N-1 && input(i, j+1) == 1) count++;\n      output(i, j) = (count == 1) ? 1 : 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input(i-1, j) == 1) count++;\n      if (i < N-1 && input(i+1, j) == 1) count++;\n      if (j > 0 && input(i, j-1) == 1) count++;\n      if (j < N-1 && input(i, j+1) == 1) count++;\n      output(i, j) = (count == 1) ? 1 : 0;\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input(i-1, j) == 1) count++;\n      if (i < N-1 && input(i+1, j) == 1) count++;\n      if (j > 0 && input(i, j-1) == 1) count++;\n      if (j < N-1 && input(i, j+1) == 1) count++;\n      output(i, j) = (count == 1) ? 1 : 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input(i-1, j) == 1) count++;\n      if (i < N-1 && input(i+1, j) == 1) count++;\n      if (j > 0 && input(i, j-1) == 1) count++;\n      if (j < N-1 && input(i, j+1) == 1) count++;\n      output(i, j) = (count == 1) ? 1 : 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input(i-1, j) == 1) count++;\n      if (i < N-1 && input(i+1, j) == 1) count++;\n      if (j > 0 && input(i, j-1) == 1) count++;\n      if (j < N-1 && input(i, j+1) == 1) count++;\n      output(i, j) = (count == 1) ? 1 : 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input(i-1, j) == 1) count++;\n      if (i < N-1 && input(i+1, j) == 1) count++;\n      if (j > 0 && input(i, j-1) == 1) count++;\n      if (j < N-1 && input(i, j+1) == 1) count++;\n      output(i, j) = (count == 1) ? 1 : 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input(i-1, j) == 1) count++;\n      if (i < N-1 && input(i+1, j) == 1) count++;\n      if (j > 0 && input(i, j-1) == 1) count++;\n      if (j < N-1 && input(i, j+1) == 1) count++;\n      output(i, j) = (count == 1) ? 1 : 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input(i-1, j) == 1) count++;\n      if (i < N-1 && input(i+1, j) == 1) count++;\n      if (j > 0 && input(i, j-1) == 1) count++;\n      if (j < N-1 && input(i, j+1) == 1) count++;\n      output(i, j) = (count == 1) ? 1 : 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input(i-1, j) == 1) count++;\n      if (i < N-1 && input(i+1, j) == 1) count++;\n      if (j > 0 && input(i, j-1) == 1) count++;\n      if (j < N-1 && input(i, j+1) == 1) count++;\n      output(i, j) = (count == 1) ? 1 : 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input(i-1, j) == 1) count++;\n      if (i < N-1 && input(i+1, j) == 1) count++;\n      if (j > 0 && input(i, j-1) == 1) count++;\n      if (j < N-1 && input(i, j+1) == 1) count++;\n      output(i, j) = (count == 1) ? 1 : 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input(i-1, j) == 1) count++;\n      if (i < N-1 && input(i+1, j) == 1) count++;\n      if (j > 0 && input(i, j-1) == 1) count++;\n      if (j < N-1 && input(i, j+1) == 1) count++;\n      output(i, j) = (count == 1) ? 1 : 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input(i-1, j) == 1) count++;\n      if (i < N-1 && input(i+1, j) == 1) count++;\n      if (j > 0 && input(i, j-1) == 1) count++;\n      if (j < N-1 && input(i, j+1) == 1) count++;\n      output(i, j) = (count == 1) ? 1 : 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input(i-1, j) == 1) count++;\n      if (i < N-1 && input(i+1, j) == 1) count++;\n      if (j > 0 && input(i, j-1) == 1) count++;\n      if (j < N-1 && input(i, j+1) == 1) count++;\n      output(i, j) = (count == 1) ? 1 : 0;\n    }\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input(i-1, j) == 1) count++;\n      if (i < N-1 && input(i+1, j) == 1) count++;\n      if (j > 0 && input(i, j-1) == 1) count++;\n      if (j < N-1 && input(i, j+1) == 1) count++;\n      output(i, j) = (count == 1) ? 1 : 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input(i-1, j) == 1) count++;\n      if (i < N-1 && input(i+1, j) == 1) count++;\n      if (j > 0 && input(i, j-1) == 1) count++;\n      if (j < N-1 && input(i, j+1) == 1) count++;\n      output(i, j) = (count == 1) ? 1 : 0;\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t j) {\n      int count = 0;\n      if (i > 0 && input(i-1, j) == 1) count++;\n      if (i < N-1 && input(i+1, j) == 1) count++;\n      if (j > 0 && input(i, j-1) == 1) count++;\n      if (j < N-1 && input(i, j+1) == 1) count++;\n      output(i, j) = (count == 1) ? 1 : 0;\n    });\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input(i-1, j) == 1) count++;\n      if (i < N-1 && input(i+1, j) == 1) count++;\n      if (j > 0 && input(i, j-1) == 1) count++;\n      if (j < N-1 && input(i, j+1) == 1) count++;\n      output(i, j) = (count == 1) ? 1 : 0;\n    }\n  });\n  Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "50_stencil_xor_kernel",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input[(i-1)*N + j] == 1) count++;\n      if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n      if (j > 0 && input[i*N + (j-1)] == 1) count++;\n      if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n      output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n      for (size_t j = 0; j < N; ++j) {\n          int count = 0;\n          if (i > 0 && input[(i-1)*N + j] == 1) count++;\n          if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n          if (j > 0 && input[i*N + (j-1)] == 1) count++;\n          if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n          output[i*N + j] = (count == 1) ? 1 : 0;\n      }\n  }\n}",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n        int count = input[(i-1)*N + j] + input[(i+1)*N + j] + input[i*N + (j-1)] + input[i*N + (j+1)];\n        if (count == 1) {\n            output[i*N + j] = 1;\n        } else {\n            output[i*N + j] = 0;\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for collapse(2)\n  for (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      int count = input[(i-1)*N + j] + input[(i+1)*N + j] + input[i*N + (j-1)] + input[i*N + (j+1)];\n      output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n  }\n```\n}",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = 0; j < N; j++) {\n      int count = 0;\n      if (i > 0 && input[(i-1)*N + j] == 1) count++;\n      if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n      if (j > 0 && input[i*N + (j-1)] == 1) count++;\n      if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n      output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input[(i-1)*N + j] == 1) count++;\n      if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n      if (j > 0 && input[i*N + (j-1)] == 1) count++;\n      if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n      output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n}",
      "\n#pragma omp parallel for collapse(2)\n  for (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      int count = input[(i-1)*N + j] + input[(i+1)*N + j] + input[i*N + j-1] + input[i*N + j+1];\n      output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n  }\n```\n}",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input[(i-1)*N + j] == 1) count++;\n      if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n      if (j > 0 && input[i*N + (j-1)] == 1) count++;\n      if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n      output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input[(i-1)*N + j] == 1) count++;\n      if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n      if (j > 0 && input[i*N + (j-1)] == 1) count++;\n      if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n      output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n  }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input[(i-1)*N + j] == 1) count++;\n      if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n      if (j > 0 && input[i*N + (j-1)] == 1) count++;\n      if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n      output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input[(i-1)*N + j] == 1) count++;\n      if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n      if (j > 0 && input[i*N + (j-1)] == 1) count++;\n      if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n      output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input[(i-1)*N + j] == 1) count++;\n      if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n      if (j > 0 && input[i*N + (j-1)] == 1) count++;\n      if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n      output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input[(i-1)*N + j] == 1) count++;\n      if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n      if (j > 0 && input[i*N + (j-1)] == 1) count++;\n      if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n      output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input[(i-1)*N + j] == 1) count++;\n      if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n      if (j > 0 && input[i*N + (j-1)] == 1) count++;\n      if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n      output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input[(i-1)*N + j] == 1) count++;\n      if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n      if (j > 0 && input[i*N + (j-1)] == 1) count++;\n      if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n      output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input[(i-1)*N + j] == 1) count++;\n      if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n      if (j > 0 && input[i*N + (j-1)] == 1) count++;\n      if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n      output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input[(i-1)*N + j] == 1) count++;\n      if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n      if (j > 0 && input[i*N + (j-1)] == 1) count++;\n      if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n      output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n  }\n}",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      if (i > 0 && input[(i-1)*N + j] == 1) count++;\n      if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n      if (j > 0 && input[i*N + (j-1)] == 1) count++;\n      if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n      output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n  }\n}"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "50_stencil_xor_kernel",
    "parallelism_model": "serial",
    "prompt": "/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n",
      "\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "50_stencil_xor_kernel",
    "parallelism_model": "hip",
    "prompt": "/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\n__global__ void cellsXOR(const int *input, int *output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        int count = 0;\n\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "50_stencil_xor_kernel",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n\n    for (int i = start; i < end; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) count++;\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) count++;\n            if (j > 0 && input[i * N + (j - 1)] == 1) count++;\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) count++;\n\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&output[i * (N / size) * N], (N / size) * N, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int blockSize = N / size;\n    int start = rank * blockSize;\n    int end = start + blockSize;\n\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i * N + (j - 1)] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) {\n                count++;\n            }\n            output[i * N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&output[r * blockSize * N], blockSize * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], blockSize * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localN = N / size;\n    int start = rank * localN;\n    int end = start + localN;\n\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    for (int i = start; i < end; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i * N + (j - 1)] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) {\n                count++;\n            }\n\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            MPI_Recv(&output[r * localN * N], localN * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], localN * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    int start = rank * local_N;\n    int end = start + local_N;\n\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i * N + (j - 1)] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&output[r * local_N * N], local_N * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], local_N * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    for (int i = start; i < end; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            int start = r * (N / size);\n            int end = (r + 1) * (N / size);\n            if (r == size - 1) {\n                end = N;\n            }\n            MPI_Recv(&output[start*N], (end - start)*N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start*N], (end - start)*N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    // Get the rank and size of the MPI communicator\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Calculate the number of rows each process will handle\n    int rows_per_process = N / size;\n    int remainder = N % size;\n    int start_row = rank * rows_per_process;\n    int end_row = start_row + rows_per_process;\n    if (rank == size - 1) {\n        end_row += remainder;\n    }\n\n    // Process the rows assigned to this rank\n    for (int i = start_row; i < end_row; i++) {\n        for (int j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n\n    // Gather the results on rank 0\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            int start = r * rows_per_process;\n            int end = start + rows_per_process;\n            if (r == size - 1) {\n                end += remainder;\n            }\n            MPI_Recv(&output[start*N], (end-start)*N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start_row*N], (end_row-start_row)*N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    for (int i = start; i < end; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) {\n                count++;\n            }\n            if (i < N-1 && input[(i+1)*N + j] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i*N + (j-1)] == 1) {\n                count++;\n            }\n            if (j < N-1 && input[i*N + (j+1)] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output[i*N + j] = 1;\n            } else {\n                output[i*N + j] = 0;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            MPI_Recv(&output[r*(N/size)*N], (N/size)*N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start*N], (end-start)*N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    for (int i = start; i < end; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i * N + (j - 1)] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) {\n                count++;\n            }\n            output[i * N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    for (int i = start; i < end; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i * N + (j - 1)] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) {\n                count++;\n            }\n            output[i * N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    // Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    int start = rank * local_N;\n    int end = start + local_N;\n\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n            if (count == 1) {\n                output[i*N + j] = 1;\n            } else {\n                output[i*N + j] = 0;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&output[r*local_N*N], local_N*N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start*N], local_N*N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    // Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n\n    for (int i = start; i < end; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) count++;\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) count++;\n            if (j > 0 && input[i * N + (j - 1)] == 1) count++;\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) count++;\n\n            output[i * N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&output[i * (N / size) * N], (N / size) * N, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n```\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    int start = rank * local_N;\n    int end = start + local_N;\n\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i * N + (j - 1)] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) {\n                count++;\n            }\n\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&output[r * local_N * N], local_N * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], local_N * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    for (int i = start; i < end; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i * N + (j - 1)] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) {\n                count++;\n            }\n            output[i * N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    for (int i = start; i < end; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i * N + (j - 1)] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) {\n                count++;\n            }\n            output[i * N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    int start = rank * local_N;\n    int end = start + local_N;\n\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    for (int i = start; i < end; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i * N + (j - 1)] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) {\n                count++;\n            }\n\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            MPI_Recv(&output[r * local_N * N], local_N * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], local_N * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    std::vector<int> local_input(input.begin() + rank * local_N * N, input.begin() + (rank + 1) * local_N * N);\n    std::vector<int> local_output(local_N * N, 0);\n\n    for (size_t i = 1; i < local_N - 1; ++i) {\n        for (size_t j = 1; j < N - 1; ++j) {\n            int count = local_input[(i - 1) * N + j - 1] + local_input[(i - 1) * N + j] + local_input[(i - 1) * N + j + 1] +\n                        local_input[i * N + j - 1] + local_input[i * N + j + 1] +\n                        local_input[(i + 1) * N + j - 1] + local_input[(i + 1) * N + j] + local_input[(i + 1) * N + j + 1];\n            if (count == 1) {\n                local_output[i * N + j] = 1;\n            }\n        }\n    }\n\n    MPI_Gather(local_output.data(), local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    int start = rank * local_N;\n    int end = start + local_N;\n\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) {\n                count++;\n            }\n            if (i < N-1 && input[(i+1)*N + j] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i*N + (j-1)] == 1) {\n                count++;\n            }\n            if (j < N-1 && input[i*N + (j+1)] == 1) {\n                count++;\n            }\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&output[r*local_N*N], local_N*N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start*N], local_N*N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    int start = rank * local_N;\n    int end = start + local_N;\n\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n            if (count == 1) {\n                output[i*N + j] = 1;\n            } else {\n                output[i*N + j] = 0;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&output[r*local_N*N], local_N*N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start*N], local_N*N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    for (int i = start; i < end; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i * N + (j - 1)] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) {\n                count++;\n            }\n\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "50_stencil_xor_kernel",
    "parallelism_model": "cuda",
    "prompt": "/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\n__global__ void cellsXOR(const int *input, int *output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int count = 0;\n\n        if (i > 0 && input[(i-1)*N + j] == 1) count++;\n        if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n        if (j > 0 && input[i*N + (j-1)] == 1) count++;\n        if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n        output[i*N + j] = (count == 1) ? 1 : 0;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "50_stencil_xor_kernel",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) count++;\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) count++;\n            if (j > 0 && input[i * N + (j - 1)] == 1) count++;\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) count++;\n            output[i * N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i * N + (j - 1)] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n            if (count == 1) {\n                output[i*N + j] = 1;\n            } else {\n                output[i*N + j] = 0;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (N / size) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n\n            if (count == 1) {\n                output[i*N + j] = 1;\n            } else {\n                output[i*N + j] = 0;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (N / size) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) count++;\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) count++;\n            if (j > 0 && input[i * N + (j - 1)] == 1) count++;\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) count++;\n\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) count++;\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) count++;\n            if (j > 0 && input[i * N + (j - 1)] == 1) count++;\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) count++;\n\n            output[i * N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (N / size) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) count++;\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) count++;\n            if (j > 0 && input[i * N + (j - 1)] == 1) count++;\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) count++;\n\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i * N + (j - 1)] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) {\n                count++;\n            }\n            output[i * N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) count++;\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) count++;\n            if (j > 0 && input[i * N + (j - 1)] == 1) count++;\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) count++;\n\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i * N + (j - 1)] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) {\n                count++;\n            }\n            output[i * N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) count++;\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) count++;\n            if (j > 0 && input[i * N + (j - 1)] == 1) count++;\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) count++;\n\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i * N + (j - 1)] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) count++;\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) count++;\n            if (j > 0 && input[i * N + (j - 1)] == 1) count++;\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) count++;\n\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N+j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N+j] == 1) count++;\n            if (j > 0 && input[i*N+j-1] == 1) count++;\n            if (j < N-1 && input[i*N+j+1] == 1) count++;\n            output[i*N+j] = (count == 1) ? 1 : 0;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * N / size;\n    int end = (rank + 1) * N / size;\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) count++;\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) count++;\n            if (j > 0 && input[i * N + (j - 1)] == 1) count++;\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) count++;\n\n            output[i * N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&output[r * N / size], N * (N / size), MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], N * (end - start), MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[(i-1)*N + j] == 1) count++;\n            if (i < N-1 && input[(i+1)*N + j] == 1) count++;\n            if (j > 0 && input[i*N + (j-1)] == 1) count++;\n            if (j < N-1 && input[i*N + (j+1)] == 1) count++;\n            output[i*N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    // Get the rank and size of the MPI communicator\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Calculate the number of rows each rank will process\n    int rows_per_rank = N / size;\n    int start_row = rank * rows_per_rank;\n    int end_row = (rank == size - 1) ? N : start_row + rows_per_rank;\n\n    // Iterate over the rows assigned to this rank\n    for (int i = start_row; i < end_row; ++i) {\n        // Iterate over the columns\n        for (int j = 0; j < N; ++j) {\n            int count = 0;\n            // Check the 8 neighboring cells\n            for (int dx = -1; dx <= 1; ++dx) {\n                for (int dy = -1; dy <= 1; ++dy) {\n                    if (dx == 0 && dy == 0) continue; // Skip the current cell\n                    int ni = i + dx;\n                    int nj = j + dy;\n                    if (ni >= 0 && ni < N && nj >= 0 && nj < N && input[ni * N + nj] == 1) {\n                        count++;\n                    }\n                }\n            }\n            // Apply the XOR rule\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n\n    // Gather the results on rank 0\n    if (rank == 0) {\n        std::vector<int> recvcounts(size);\n        std::vector<int> displs(size);\n        for (int i = 0; i < size; ++i) {\n            recvcounts[i] = (i == size - 1) ? N - i * rows_per_rank : rows_per_rank;\n            displs[i] = i * rows_per_rank * N;\n        }\n        MPI_Gatherv(MPI_IN_PLACE, rows_per_rank * N, MPI_INT, output.data(), recvcounts.data(), displs.data(), MPI_INT, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Gatherv(output.data() + start_row * N, rows_per_rank * N, MPI_INT, nullptr, nullptr, nullptr, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i * N + (j - 1)] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) {\n                count++;\n            }\n            output[i * N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) count++;\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) count++;\n            if (j > 0 && input[i * N + (j - 1)] == 1) count++;\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) count++;\n\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n\n    #pragma omp parallel for\n    for (int i = start; i < end; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) count++;\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) count++;\n            if (j > 0 && input[i * N + (j - 1)] == 1) count++;\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) count++;\n\n            if (count == 1) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "54_stencil_game_of_life",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(Kokkos::View<const int**> &input, Kokkos::View<int**> &output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int liveNeighbors = 0;\n      for (int dx = -1; dx <= 1; ++dx) {\n        for (int dy = -1; dy <= 1; ++dy) {\n          if (dx == 0 && dy == 0) continue;\n          int ni = i + dx;\n          int nj = j + dy;\n          if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n            liveNeighbors += input(ni, nj);\n          }\n        }\n      }\n      if (input(i, j) == 1) {\n        if (liveNeighbors < 2 || liveNeighbors > 3) {\n          output(i, j) = 0;\n        } else {\n          output(i, j) = 1;\n        }\n      } else {\n        if (liveNeighbors == 3) {\n          output(i, j) = 1;\n        } else {\n          output(i, j) = 0;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int liveNeighbors = 0;\n      for (int dx = -1; dx <= 1; ++dx) {\n        for (int dy = -1; dy <= 1; ++dy) {\n          if (dx == 0 && dy == 0) continue;\n          int ni = i + dx;\n          int nj = j + dy;\n          if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n            liveNeighbors += input(ni, nj);\n          }\n        }\n      }\n      if (input(i, j) == 1) {\n        if (liveNeighbors < 2 || liveNeighbors > 3) {\n          output(i, j) = 0;\n        } else {\n          output(i, j) = 1;\n        }\n      } else {\n        if (liveNeighbors == 3) {\n          output(i, j) = 1;\n        } else {\n          output(i, j) = 0;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"game_of_life\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int liveNeighbors = 0;\n      for (int dx = -1; dx <= 1; ++dx) {\n        for (int dy = -1; dy <= 1; ++dy) {\n          if (dx == 0 && dy == 0) continue;\n          int ni = i + dx;\n          int nj = j + dy;\n          if (ni >= 0 && ni < N && nj >= 0 && nj < N && input(ni, nj) == 1) {\n            liveNeighbors++;\n          }\n        }\n      }\n      if (input(i, j) == 1) {\n        if (liveNeighbors < 2 || liveNeighbors > 3) {\n          output(i, j) = 0;\n        } else {\n          output(i, j) = 1;\n        }\n      } else {\n        if (liveNeighbors == 3) {\n          output(i, j) = 1;\n        } else {\n          output(i, j) = 0;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int liveNeighbors = 0;\n      for (int dx = -1; dx <= 1; ++dx) {\n        for (int dy = -1; dy <= 1; ++dy) {\n          if (dx == 0 && dy == 0) continue;\n          int ni = i + dx;\n          int nj = j + dy;\n          if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n            liveNeighbors += input(ni, nj);\n          }\n        }\n      }\n      if (input(i, j) == 1) {\n        if (liveNeighbors < 2 || liveNeighbors > 3) {\n          output(i, j) = 0;\n        } else {\n          output(i, j) = 1;\n        }\n      } else {\n        if (liveNeighbors == 3) {\n          output(i, j) = 1;\n        } else {\n          output(i, j) = 0;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int liveNeighbors = 0;\n      for (int dx = -1; dx <= 1; ++dx) {\n        for (int dy = -1; dy <= 1; ++dy) {\n          if (dx == 0 && dy == 0) continue;\n          int ni = i + dx;\n          int nj = j + dy;\n          if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n            liveNeighbors += input(ni, nj);\n          }\n        }\n      }\n      if (input(i, j) == 1) {\n        if (liveNeighbors < 2 || liveNeighbors > 3) {\n          output(i, j) = 0;\n        } else {\n          output(i, j) = 1;\n        }\n      } else {\n        if (liveNeighbors == 3) {\n          output(i, j) = 1;\n        } else {\n          output(i, j) = 0;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"game_of_life\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int live_neighbors = 0;\n      for (int dx = -1; dx <= 1; ++dx) {\n        for (int dy = -1; dy <= 1; ++dy) {\n          if (dx == 0 && dy == 0) continue;\n          int ni = i + dx;\n          int nj = j + dy;\n          if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n            live_neighbors += input(ni, nj);\n          }\n        }\n      }\n      if (input(i, j) == 1) {\n        if (live_neighbors < 2 || live_neighbors > 3) {\n          output(i, j) = 0;\n        } else {\n          output(i, j) = 1;\n        }\n      } else {\n        if (live_neighbors == 3) {\n          output(i, j) = 1;\n        } else {\n          output(i, j) = 0;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"game_of_life\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int liveNeighbors = 0;\n      for (int dx = -1; dx <= 1; ++dx) {\n        for (int dy = -1; dy <= 1; ++dy) {\n          if (dx == 0 && dy == 0) continue;\n          int ni = i + dx;\n          int nj = j + dy;\n          if (ni >= 0 && ni < N && nj >= 0 && nj < N && input(ni, nj) == 1) {\n            liveNeighbors++;\n          }\n        }\n      }\n      if (input(i, j) == 1) {\n        if (liveNeighbors < 2 || liveNeighbors > 3) {\n          output(i, j) = 0;\n        } else {\n          output(i, j) = 1;\n        }\n      } else {\n        if (liveNeighbors == 3) {\n          output(i, j) = 1;\n        } else {\n          output(i, j) = 0;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int liveNeighbors = 0;\n      for (int dx = -1; dx <= 1; ++dx) {\n        for (int dy = -1; dy <= 1; ++dy) {\n          if (dx == 0 && dy == 0) continue;\n          int ni = i + dx;\n          int nj = j + dy;\n          if (ni >= 0 && ni < N && nj >= 0 && nj < N && input(ni, nj) == 1) {\n            liveNeighbors++;\n          }\n        }\n      }\n      if (input(i, j) == 1) {\n        if (liveNeighbors < 2 || liveNeighbors > 3) {\n          output(i, j) = 0;\n        } else {\n          output(i, j) = 1;\n        }\n      } else {\n        if (liveNeighbors == 3) {\n          output(i, j) = 1;\n        } else {\n          output(i, j) = 0;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int liveNeighbors = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) continue;\n          int ni = i + x;\n          int nj = j + y;\n          if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n            liveNeighbors += input(ni, nj);\n          }\n        }\n      }\n      if (input(i, j) == 1) {\n        if (liveNeighbors < 2 || liveNeighbors > 3) {\n          output(i, j) = 0;\n        } else {\n          output(i, j) = 1;\n        }\n      } else {\n        if (liveNeighbors == 3) {\n          output(i, j) = 1;\n        } else {\n          output(i, j) = 0;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int liveNeighbors = 0;\n      for (int dx = -1; dx <= 1; ++dx) {\n        for (int dy = -1; dy <= 1; ++dy) {\n          if (dx == 0 && dy == 0) continue;\n          int ni = i + dx;\n          int nj = j + dy;\n          if (ni >= 0 && ni < N && nj >= 0 && nj < N && input(ni, nj) == 1) {\n            liveNeighbors++;\n          }\n        }\n      }\n      if (input(i, j) == 1) {\n        if (liveNeighbors < 2 || liveNeighbors > 3) {\n          output(i, j) = 0;\n        } else {\n          output(i, j) = 1;\n        }\n      } else {\n        if (liveNeighbors == 3) {\n          output(i, j) = 1;\n        } else {\n          output(i, j) = 0;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int liveNeighbors = 0;\n      for (int dx = -1; dx <= 1; ++dx) {\n        for (int dy = -1; dy <= 1; ++dy) {\n          if (dx == 0 && dy == 0) continue;\n          int ni = i + dx;\n          int nj = j + dy;\n          if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n            liveNeighbors += input(ni, nj);\n          }\n        }\n      }\n      if (input(i, j) == 1) {\n        if (liveNeighbors < 2 || liveNeighbors > 3) {\n          output(i, j) = 0;\n        } else {\n          output(i, j) = 1;\n        }\n      } else {\n        if (liveNeighbors == 3) {\n          output(i, j) = 1;\n        } else {\n          output(i, j) = 0;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int liveNeighbors = 0;\n      for (int dx = -1; dx <= 1; ++dx) {\n        for (int dy = -1; dy <= 1; ++dy) {\n          if (dx == 0 && dy == 0) continue;\n          int ni = i + dx;\n          int nj = j + dy;\n          if (ni >= 0 && ni < N && nj >= 0 && nj < N && input(ni, nj) == 1) {\n            liveNeighbors++;\n          }\n        }\n      }\n      if (input(i, j) == 1) {\n        if (liveNeighbors < 2 || liveNeighbors > 3) {\n          output(i, j) = 0;\n        } else {\n          output(i, j) = 1;\n        }\n      } else {\n        if (liveNeighbors == 3) {\n          output(i, j) = 1;\n        } else {\n          output(i, j) = 0;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int liveNeighbors = 0;\n      for (int dx = -1; dx <= 1; ++dx) {\n        for (int dy = -1; dy <= 1; ++dy) {\n          if (dx == 0 && dy == 0) continue;\n          int ni = i + dx;\n          int nj = j + dy;\n          if (ni >= 0 && ni < N && nj >= 0 && nj < N && input(ni, nj) == 1) {\n            liveNeighbors++;\n          }\n        }\n      }\n      if (input(i, j) == 1) {\n        if (liveNeighbors < 2 || liveNeighbors > 3) {\n          output(i, j) = 0;\n        } else {\n          output(i, j) = 1;\n        }\n      } else {\n        if (liveNeighbors == 3) {\n          output(i, j) = 1;\n        } else {\n          output(i, j) = 0;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int liveNeighbors = 0;\n      for (int dx = -1; dx <= 1; ++dx) {\n        for (int dy = -1; dy <= 1; ++dy) {\n          if (dx == 0 && dy == 0) continue;\n          int ni = i + dx;\n          int nj = j + dy;\n          if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n            liveNeighbors += input(ni, nj);\n          }\n        }\n      }\n      if (input(i, j) == 1) {\n        if (liveNeighbors < 2 || liveNeighbors > 3) {\n          output(i, j) = 0;\n        } else {\n          output(i, j) = 1;\n        }\n      } else {\n        if (liveNeighbors == 3) {\n          output(i, j) = 1;\n        } else {\n          output(i, j) = 0;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"game_of_life\", N, KOKKOS_LAMBDA(const size_t i) {\n    for (size_t j = 0; j < N; ++j) {\n      int liveNeighbors = 0;\n      for (int dx = -1; dx <= 1; ++dx) {\n        for (int dy = -1; dy <= 1; ++dy) {\n          if (dx == 0 && dy == 0) continue;\n          int ni = i + dx, nj = j + dy;\n          if (ni >= 0 && ni < N && nj >= 0 && nj < N && input(ni, nj) == 1) {\n            liveNeighbors++;\n          }\n        }\n      }\n      if (input(i, j) == 1) {\n        output(i, j) = (liveNeighbors == 2 || liveNeighbors == 3) ? 1 : 0;\n      } else {\n        output(i, j) = (liveNeighbors == 3) ? 1 : 0;\n      }\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int liveNeighbors = 0;\n      for (int dx = -1; dx <= 1; ++dx) {\n        for (int dy = -1; dy <= 1; ++dy) {\n          if (dx == 0 && dy == 0) continue;\n          int ni = i + dx;\n          int nj = j + dy;\n          if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n            liveNeighbors += input(ni, nj);\n          }\n        }\n      }\n      if (input(i, j) == 1) {\n        if (liveNeighbors < 2 || liveNeighbors > 3) {\n          output(i, j) = 0;\n        } else {\n          output(i, j) = 1;\n        }\n      } else {\n        if (liveNeighbors == 3) {\n          output(i, j) = 1;\n        } else {\n          output(i, j) = 0;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int liveNeighbors = 0;\n      for (int dx = -1; dx <= 1; ++dx) {\n        for (int dy = -1; dy <= 1; ++dy) {\n          if (dx == 0 && dy == 0) continue;\n          int ni = i + dx;\n          int nj = j + dy;\n          if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n            liveNeighbors += input(ni, nj);\n          }\n        }\n      }\n      if (input(i, j) == 1) {\n        if (liveNeighbors < 2 || liveNeighbors > 3) {\n          output(i, j) = 0;\n        } else {\n          output(i, j) = 1;\n        }\n      } else {\n        if (liveNeighbors == 3) {\n          output(i, j) = 1;\n        } else {\n          output(i, j) = 0;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int liveNeighbors = 0;\n      for (int dx = -1; dx <= 1; ++dx) {\n        for (int dy = -1; dy <= 1; ++dy) {\n          if (dx == 0 && dy == 0) continue;\n          int ni = i + dx;\n          int nj = j + dy;\n          if (ni >= 0 && ni < N && nj >= 0 && nj < N && input(ni, nj) == 1) {\n            liveNeighbors++;\n          }\n        }\n      }\n      if (input(i, j) == 1) {\n        if (liveNeighbors < 2 || liveNeighbors > 3) {\n          output(i, j) = 0;\n        } else {\n          output(i, j) = 1;\n        }\n      } else {\n        if (liveNeighbors == 3) {\n          output(i, j) = 1;\n        } else {\n          output(i, j) = 0;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int liveNeighbors = 0;\n      for (int dx = -1; dx <= 1; ++dx) {\n        for (int dy = -1; dy <= 1; ++dy) {\n          if (dx == 0 && dy == 0) continue;\n          int ni = i + dx;\n          int nj = j + dy;\n          if (ni >= 0 && ni < N && nj >= 0 && nj < N && input(ni, nj) == 1) {\n            liveNeighbors++;\n          }\n        }\n      }\n      if (input(i, j) == 1) {\n        if (liveNeighbors < 2 || liveNeighbors > 3) {\n          output(i, j) = 0;\n        } else {\n          output(i, j) = 1;\n        }\n      } else {\n        if (liveNeighbors == 3) {\n          output(i, j) = 1;\n        } else {\n          output(i, j) = 0;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int liveNeighbors = 0;\n      for (int dx = -1; dx <= 1; ++dx) {\n        for (int dy = -1; dy <= 1; ++dy) {\n          if (dx == 0 && dy == 0) continue;\n          int ni = i + dx;\n          int nj = j + dy;\n          if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n            liveNeighbors += input(ni, nj);\n          }\n        }\n      }\n      if (input(i, j) == 1) {\n        if (liveNeighbors < 2 || liveNeighbors > 3) {\n          output(i, j) = 0;\n        } else {\n          output(i, j) = 1;\n        }\n      } else {\n        if (liveNeighbors == 3) {\n          output(i, j) = 1;\n        } else {\n          output(i, j) = 0;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "54_stencil_game_of_life",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                        liveNeighbors += input[newX * N + newY];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                        liveNeighbors += input[newX * N + newY];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                        liveNeighbors += input[newX * N + newY];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                        liveNeighbors += input[newX * N + newY];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                        liveNeighbors += input[newX * N + newY];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N && input[newX * N + newY] == 1) {\n                        liveNeighbors++;\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                        liveNeighbors += input[newX * N + newY];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                        liveNeighbors += input[newX * N + newY];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                        liveNeighbors += input[newX * N + newY];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                        liveNeighbors += input[newX * N + newY];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N && input[newX * N + newY] == 1) {\n                        liveNeighbors++;\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                        liveNeighbors += input[newX * N + newY];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                        liveNeighbors += input[newX * N + newY];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                        liveNeighbors += input[newX * N + newY];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                        liveNeighbors += input[newX * N + newY];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                        liveNeighbors += input[newX * N + newY];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                        liveNeighbors += input[newX * N + newY];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N && input[newX * N + newY] == 1) {\n                        liveNeighbors++;\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                        liveNeighbors += input[newX * N + newY];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                        liveNeighbors += input[newX * N + newY];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "54_stencil_game_of_life",
    "parallelism_model": "serial",
    "prompt": "/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N && input[newX * N + newY] == 1) {\n                        liveNeighbors++;\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n  auto countLiveNeighbors = [&](int x, int y) {\n    int count = 0;\n    for (int i = -1; i <= 1; ++i) {\n      for (int j = -1; j <= 1; ++j) {\n        if (i == 0 && j == 0) continue;\n        int newX = x + i;\n        int newY = y + j;\n        if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n          count += input[newX * N + newY];\n        }\n      }\n    }\n    return count;\n  };\n\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int liveNeighbors = countLiveNeighbors(i, j);\n      if (input[i * N + j] == 1) {\n        if (liveNeighbors < 2 || liveNeighbors > 3) {\n          output[i * N + j] = 0;\n        } else {\n          output[i * N + j] = 1;\n        }\n      } else {\n        if (liveNeighbors == 3) {\n          output[i * N + j] = 1;\n        } else {\n          output[i * N + j] = 0;\n        }\n      }\n    }\n  }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N && input[newX * N + newY] == 1) {\n                        liveNeighbors++;\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n  std::vector<int> temp = input;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int liveNeighbors = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) continue;\n          int newX = i + x;\n          int newY = j + y;\n          if (newX >= 0 && newX < N && newY >= 0 && newY < N && input[newX * N + newY] == 1) {\n            liveNeighbors++;\n          }\n        }\n      }\n      if (input[i * N + j] == 1) {\n        if (liveNeighbors < 2 || liveNeighbors > 3) {\n          temp[i * N + j] = 0;\n        }\n      } else {\n        if (liveNeighbors == 3) {\n          temp[i * N + j] = 1;\n        }\n      }\n    }\n  }\n  output = temp;\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                        liveNeighbors += input[newX * N + newY];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N && input[newX * N + newY] == 1) {\n                        liveNeighbors++;\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N && input[newX * N + newY] == 1) {\n                        liveNeighbors++;\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                        liveNeighbors += input[newX * N + newY];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N && input[newX * N + newY] == 1) {\n                        liveNeighbors++;\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N && input[newX * N + newY] == 1) {\n                        liveNeighbors++;\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N && input[newX * N + newY] == 1) {\n                        liveNeighbors++;\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N && input[newX * N + newY] == 1) {\n                        liveNeighbors++;\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N && input[newX * N + newY] == 1) {\n                        liveNeighbors++;\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N && input[newX * N + newY] == 1) {\n                        liveNeighbors++;\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N && input[newX * N + newY] == 1) {\n                        liveNeighbors++;\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N && input[newX * N + newY] == 1) {\n                        liveNeighbors++;\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N && input[newX * N + newY] == 1) {\n                        liveNeighbors++;\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N && input[newX * N + newY] == 1) {\n                        liveNeighbors++;\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N && input[newX * N + newY] == 1) {\n                        liveNeighbors++;\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                        liveNeighbors += input[newX * N + newY];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "54_stencil_game_of_life",
    "parallelism_model": "hip",
    "prompt": "/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\n__global__ void gameOfLife(const int *input, int *output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n\n        for (int x = -1; x <= 1; x++) {\n            for (int y = -1; y <= 1; y++) {\n                if (x == 0 && y == 0) continue;\n                int ni = i + x;\n                int nj = j + y;\n                if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                    liveNeighbors += input[ni * N + nj];\n                }\n            }\n        }\n\n        int cell = input[i * N + j];\n        if (cell == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n  int i = blockIdx.y * blockDim.y + threadIdx.y;\n  int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (i < N && j < N) {\n    int liveNeighbors = 0;\n    for (int x = i - 1; x <= i + 1; x++) {\n      for (int y = j - 1; y <= j + 1; y++) {\n        if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n          liveNeighbors += input[x * N + y];\n        }\n      }\n    }\n\n    if (input[i * N + j] == 1) {\n      if (liveNeighbors < 2 || liveNeighbors > 3) {\n        output[i * N + j] = 0;\n      } else {\n        output[i * N + j] = 1;\n      }\n    } else {\n      if (liveNeighbors == 3) {\n        output[i * N + j] = 1;\n      } else {\n        output[i * N + j] = 0;\n      }\n    }\n  }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n\n        for (int x = -1; x <= 1; x++) {\n            for (int y = -1; y <= 1; y++) {\n                if (x == 0 && y == 0) continue;\n\n                int newX = i + x;\n                int newY = j + y;\n\n                if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                    liveNeighbors += input[newX * N + newY];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n\n        for (int x = -1; x <= 1; x++) {\n            for (int y = -1; y <= 1; y++) {\n                if (x == 0 && y == 0) continue;\n\n                int ni = i + x;\n                int nj = j + y;\n\n                if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                    liveNeighbors += input[ni * N + nj];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n\n        for (int x = i - 1; x <= i + 1; x++) {\n            for (int y = j - 1; y <= j + 1; y++) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n\n        for (int x = i - 1; x <= i + 1; x++) {\n            for (int y = j - 1; y <= j + 1; y++) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;  // Cell dies\n            } else {\n                output[i * N + j] = 1;  // Cell lives on\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;  // Cell becomes alive\n            } else {\n                output[i * N + j] = 0;  // Cell remains dead\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; x++) {\n            for (int y = j - 1; y <= j + 1; y++) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n\n        for (int x = -1; x <= 1; x++) {\n            for (int y = -1; y <= 1; y++) {\n                if (x == 0 && y == 0) continue;\n                int ni = i + x;\n                int nj = j + y;\n                if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                    liveNeighbors += input[ni * N + nj];\n                }\n            }\n        }\n\n        int cell = input[i * N + j];\n        if (cell == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; x++) {\n            for (int y = j - 1; y <= j + 1; y++) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; ++x) {\n            for (int y = j - 1; y <= j + 1; ++y) {\n                if (x >= 0 && x < N && y >= 0 && y < N && (x != i || y != j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; x++) {\n            for (int y = j - 1; y <= j + 1; y++) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; ++x) {\n            for (int y = j - 1; y <= j + 1; ++y) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; x++) {\n            for (int y = j - 1; y <= j + 1; y++) {\n                if (x >= 0 && x < N && y >= 0 && y < N && (x != i || y != j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; x++) {\n            for (int y = j - 1; y <= j + 1; y++) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; x++) {\n            for (int y = j - 1; y <= j + 1; y++) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = -1; x <= 1; ++x) {\n            for (int y = -1; y <= 1; ++y) {\n                if (x == 0 && y == 0) continue;\n                int ni = i + x;\n                int nj = j + y;\n                if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                    liveNeighbors += input[ni * N + nj];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n  int i = blockIdx.y * blockDim.y + threadIdx.y;\n  int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (i < N && j < N) {\n    int liveNeighbors = 0;\n    for (int x = i - 1; x <= i + 1; x++) {\n      for (int y = j - 1; y <= j + 1; y++) {\n        if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n          liveNeighbors += input[x * N + y];\n        }\n      }\n    }\n\n    if (input[i * N + j] == 1) {\n      if (liveNeighbors < 2 || liveNeighbors > 3) {\n        output[i * N + j] = 0;\n      } else {\n        output[i * N + j] = 1;\n      }\n    } else {\n      if (liveNeighbors == 3) {\n        output[i * N + j] = 1;\n      } else {\n        output[i * N + j] = 0;\n      }\n    }\n  }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; x++) {\n            for (int y = j - 1; y <= j + 1; y++) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; ++x) {\n            for (int y = j - 1; y <= j + 1; ++y) {\n                if (x >= 0 && x < N && y >= 0 && y < N && (x != i || y != j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        \n        for (int x = i - 1; x <= i + 1; ++x) {\n            for (int y = j - 1; y <= j + 1; ++y) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n        \n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "54_stencil_game_of_life",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n    // Implement the Game of Life rules to update the `output` grid based on the `input` grid\n    // You can use MPI functions to communicate and synchronize between ranks\n}",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n    // Implement the Game of Life rules to update the `output` grid based on the `input` grid\n    // You can use MPI functions to communicate and synchronize between processes\n}",
      "\n  // Your code here\n  // Implement the Game of Life rules to update the `output` grid based on the `input` grid\n  // You can use MPI functions to communicate and synchronize the data between ranks\n  // Remember to handle the edge cases when updating the cells\n  // After updating the `output` grid, copy the results to rank 0\n}\n",
      "\n    // Your code here\n    // Implement the Game of Life rules to update the `output` grid based on the `input` grid\n    // You may use MPI functions to communicate and coordinate the computation among ranks\n}",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n    // Implement the Game of Life rules to update the `output` grid based on the `input` grid\n    // You can use MPI functions to communicate and coordinate the computation among different ranks\n    // Remember to handle the edge cases properly\n}",
      "\n    // Your code here\n    // Implement the Game of Life rules to update the `output` grid based on the `input` grid\n    // You can use MPI functions to communicate and coordinate between ranks\n}",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n  // Your code here\n  // Implement the Game of Life rules to update the `output` grid based on the `input` grid\n  // You can use MPI functions to communicate and synchronize the data between processes\n  // Remember to handle the edge cases when processing the grid\n  // After updating the `output` grid, copy the result to rank 0\n}\n",
      "\n    // Your code here\n}\n",
      "\n  // Your code here\n  // Implement the Game of Life rules to update the `output` grid based on the `input` grid\n  // You may use MPI functions to distribute the work among ranks\n  // Remember to handle the edge cases and boundary conditions\n}\n",
      "\n    // Your code here\n    // Implement the Game of Life rules to update the `output` grid based on the `input` grid\n    // You may use MPI functions to communicate and synchronize the results\n    // Remember to handle the edge cases and boundary conditions\n}",
      "\n    // Your code here\n    // Implement the Game of Life rules to update the `output` grid based on the `input` grid\n    // You may use MPI functions to communicate and synchronize the results\n}",
      "\n    // Your code here\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int dx = -1; dx <= 1; ++dx) {\n                for (int dy = -1; dy <= 1; ++dy) {\n                    if (dx == 0 && dy == 0) continue;\n                    int x = i + dx;\n                    int y = j + dy;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        liveNeighbors += input[x * N + y];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    // Your code here\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int dx = -1; dx <= 1; ++dx) {\n                for (int dy = -1; dy <= 1; ++dy) {\n                    if (dx == 0 && dy == 0) continue;\n                    int x = i + dx;\n                    int y = j + dy;\n                    if (x >= 0 && x < N && y >= 0 && y < N && input[x * N + y] == 1) {\n                        liveNeighbors++;\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n",
      "\n    // Your code here\n    int dx[] = {-1, -1, -1, 0, 0, 1, 1, 1};\n    int dy[] = {-1, 0, 1, -1, 1, -1, 0, 1};\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int k = 0; k < 8; ++k) {\n                int x = i + dx[k];\n                int y = j + dy[k];\n                if (x >= 0 && x < N && y >= 0 && y < N && input[x * N + y] == 1) {\n                    liveNeighbors++;\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "54_stencil_game_of_life",
    "parallelism_model": "cuda",
    "prompt": "/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\n__global__ void gameOfLife(const int *input, int *output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; ++x) {\n            for (int y = j - 1; y <= j + 1; ++y) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; ++x) {\n            for (int y = j - 1; y <= j + 1; ++y) {\n                if (x >= 0 && x < N && y >= 0 && y < N && (x != i || y != j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n\n        for (int x = -1; x <= 1; x++) {\n            for (int y = -1; y <= 1; y++) {\n                if (x == 0 && y == 0) continue;\n                int ni = i + x;\n                int nj = j + y;\n                if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                    liveNeighbors += input[ni * N + nj];\n                }\n            }\n        }\n\n        int cell = input[i * N + j];\n        if (cell == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; x++) {\n            for (int y = j - 1; y <= j + 1; y++) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n\n        for (int x = i - 1; x <= i + 1; x++) {\n            for (int y = j - 1; y <= j + 1; y++) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; ++x) {\n            for (int y = j - 1; y <= j + 1; ++y) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n\n        for (int x = i - 1; x <= i + 1; x++) {\n            for (int y = j - 1; y <= j + 1; y++) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; ++x) {\n            for (int y = j - 1; y <= j + 1; ++y) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n\n        for (int x = i - 1; x <= i + 1; x++) {\n            for (int y = j - 1; y <= j + 1; y++) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; x++) {\n            for (int y = j - 1; y <= j + 1; y++) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; x++) {\n            for (int y = j - 1; y <= j + 1; y++) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; x++) {\n            for (int y = j - 1; y <= j + 1; y++) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; x++) {\n            for (int y = j - 1; y <= j + 1; y++) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; x++) {\n            for (int y = j - 1; y <= j + 1; y++) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n\n        for (int x = i - 1; x <= i + 1; x++) {\n            for (int y = j - 1; y <= j + 1; y++) {\n                if (x >= 0 && x < N && y >= 0 && y < N && (x != i || y != j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; x++) {\n            for (int y = j - 1; y <= j + 1; y++) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; ++x) {\n            for (int y = j - 1; y <= j + 1; ++y) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; x++) {\n            for (int y = j - 1; y <= j + 1; y++) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; ++x) {\n            for (int y = j - 1; y <= j + 1; ++y) {\n                if (x >= 0 && x < N && y >= 0 && y < N && !(x == i && y == j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int liveNeighbors = 0;\n        for (int x = i - 1; x <= i + 1; ++x) {\n            for (int y = j - 1; y <= j + 1; ++y) {\n                if (x >= 0 && x < N && y >= 0 && y < N && (x != i || y != j)) {\n                    liveNeighbors += input[x * N + y];\n                }\n            }\n        }\n\n        if (input[i * N + j] == 1) {\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;\n            } else {\n                output[i * N + j] = 1;\n            }\n        } else {\n            if (liveNeighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = 0;\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "54_stencil_game_of_life",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // Your code here\n    // Implement the Game of Life rules to update the `output` grid based on the `input` grid\n    // Use MPI and OpenMP to compute in parallel\n    // Assume MPI is already initialized\n    // Every rank has a complete copy of input. The result is stored in output on rank 0\n    // You may need to use MPI communication functions to exchange boundary cells between ranks\n    // Remember to handle the boundary cells properly to avoid race conditions\n}",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n    // Implement the Game of Life rules to update the output grid based on the input grid\n    // Use MPI and OpenMP to compute in parallel\n    // Assume MPI is already initialized\n    // Every rank has a complete copy of input. The result is stored in output on rank 0\n    // You may need to use MPI communication functions to exchange boundary cells between ranks\n    // Use OpenMP to parallelize the computation within each rank\n    // Remember to handle the boundary cells correctly when using OpenMP\n    // After updating the output grid, copy the result to rank 0 if necessary\n}\n",
      "\n    // Your code here\n    // Implement the Game of Life rules to update the `output` grid based on the `input` grid\n    // Use MPI and OpenMP to compute in parallel\n    // Assume MPI is already initialized\n    // Every rank has a complete copy of input. The result is stored in output on rank 0\n    // You may use MPI functions like MPI_Send and MPI_Recv to communicate between ranks\n    // You may use OpenMP to parallelize the computation within each rank\n}",
      "\n    // Your code here\n    // Iterate through each cell in the grid\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            // Count the number of live neighbors for the current cell\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;  // Skip the current cell\n                    int neighborX = i + x;\n                    int neighborY = j + y;\n                    if (neighborX >= 0 && neighborX < N && neighborY >= 0 && neighborY < N) {\n                        liveNeighbors += input[neighborX * N + neighborY];\n                    }\n                }\n            }\n            // Apply the rules of the game to determine the state of the current cell in the next generation\n            if (liveNeighbors < 2 || liveNeighbors > 3) {\n                output[i * N + j] = 0;  // Cell dies\n            } else if (liveNeighbors == 3) {\n                output[i * N + j] = 1;  // Cell becomes alive\n            } else {\n                output[i * N + j] = input[i * N + j];  // Cell remains the same\n            }\n        }\n    }\n```\n}",
      "\n    // Your code here\n    int numThreads, rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numThreads);\n\n    int startRow = rank * (N / numThreads);\n    int endRow = (rank + 1) * (N / numThreads);\n\n    if (rank == numThreads - 1) {\n        endRow = N;\n    }\n\n    #pragma omp parallel for\n    for (int i = startRow; i < endRow; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int ni = i + x;\n                    int nj = j + y;\n                    if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                        liveNeighbors += input[ni * N + nj];\n                    }\n                }\n            }\n\n            if (input[i * N + j] == 1) {\n                liveNeighbors -= 1;\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < numThreads; ++i) {\n            MPI_Recv(&output[i * (N / numThreads) * N], (N / numThreads) * N, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[startRow * N], (endRow - startRow) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n    int numThreads, rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numThreads);\n\n    int startRow = rank * (N / numThreads);\n    int endRow = (rank + 1) * (N / numThreads);\n    if (rank == numThreads - 1) {\n        endRow = N;\n    }\n\n    for (int i = startRow; i < endRow; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int ni = i + x;\n                    int nj = j + y;\n                    if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                        liveNeighbors += input[ni * N + nj];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                liveNeighbors -= 1;\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;\n                } else {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;\n                } else {\n                    output[i * N + j] = 0;\n                }\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < numThreads; ++i) {\n            MPI_Recv(&output[i * (N / numThreads) * N], (N / numThreads) * N, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[startRow * N], (endRow - startRow) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    // TODO: Implement the body of the function gameOfLife\n    // Your implementation should use MPI and OpenMP to compute in parallel\n    // Remember to store the results in the output vector\n}",
      "\n    // Your code here\n}\n",
      "\n    // Your code here\n    // Implement the Game of Life rules to update the `output` grid based on the `input` grid\n    // Use MPI and OpenMP to compute in parallel\n    // Assume MPI is already initialized\n    // Every rank has a complete copy of input. The result is stored in output on rank 0\n    // You may need to use MPI communication functions to exchange boundary cells between ranks\n    // You may also need to use OpenMP to parallelize the computation within each rank\n}",
      "\n    // Your code here\n    // Iterate over each cell in the grid\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n\n            // Count the number of live neighbors for the current cell\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;  // Skip the current cell\n                    int ni = i + x;\n                    int nj = j + y;\n                    if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                        liveNeighbors += input[ni * N + nj];\n                    }\n                }\n            }\n\n            // Apply the rules of the game to determine the state of the current cell in the next generation\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;  // Cell dies\n                } else {\n                    output[i * N + j] = 1;  // Cell lives on\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;  // Cell becomes alive\n                } else {\n                    output[i * N + j] = 0;  // Cell remains dead\n                }\n            }\n        }\n    }\n}\n",
      "\n    // Your code here\n    // Iterate through each cell in the grid\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            // Count the number of live neighbors for each cell\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;  // Skip the current cell\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                        liveNeighbors += input[newX * N + newY];\n                    }\n                }\n            }\n            // Apply the rules of the game to determine the state of the cell in the next generation\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;  // Cell dies\n                } else {\n                    output[i * N + j] = 1;  // Cell lives on\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;  // Cell becomes alive\n                } else {\n                    output[i * N + j] = 0;  // Cell remains dead\n                }\n            }\n        }\n    }\n}\n",
      "\n    // Your code here\n    // Iterate through each cell in the grid\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            // Count the number of live neighbors for each cell\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;  // Skip the current cell\n                    int newX = i + x;\n                    int newY = j + y;\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N && input[newX * N + newY] == 1) {\n                        liveNeighbors++;\n                    }\n                }\n            }\n            // Apply the rules of the game of life\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;  // Cell dies\n                } else {\n                    output[i * N + j] = 1;  // Cell lives on\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;  // Cell becomes alive\n                } else {\n                    output[i * N + j] = 0;  // Cell remains dead\n                }\n            }\n        }\n    }\n```\n}",
      "\n    // Your code here\n    // Implement the Game of Life rules to update the `output` grid based on the `input` grid\n    // Use MPI and OpenMP to compute in parallel\n    // Assume MPI is already initialized\n    // Every rank has a complete copy of input. The result is stored in output on rank 0\n}",
      "\n    // TODO: Implement the game of life logic using MPI and OpenMP to compute in parallel\n    // You can use the input vector to read the current state and update the output vector with the next generation\n    // Remember to handle the edge cases properly\n    // After the computation is done, the result should be stored in the output vector\n}\n",
      "\n    // Iterate over each cell in the grid\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n\n            // Check the 8 neighboring cells\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;  // Skip the current cell\n                    int newX = i + x;\n                    int newY = j + y;\n\n                    // Check if the neighboring cell is within the grid\n                    if (newX >= 0 && newX < N && newY >= 0 && newY < N) {\n                        liveNeighbors += input[newX * N + newY];\n                    }\n                }\n            }\n\n            // Apply the rules of the Game of Life\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;  // Cell dies\n                } else {\n                    output[i * N + j] = 1;  // Cell lives on\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;  // Cell becomes alive\n                } else {\n                    output[i * N + j] = 0;  // Cell remains dead\n                }\n            }\n        }\n    }\n```\n}",
      "\n    // Calculate the number of live neighbors for each cell\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int liveNeighbors = 0;\n            for (int x = -1; x <= 1; ++x) {\n                for (int y = -1; y <= 1; ++y) {\n                    if (x == 0 && y == 0) continue;\n                    int ni = i + x;\n                    int nj = j + y;\n                    if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                        liveNeighbors += input[ni * N + nj];\n                    }\n                }\n            }\n            // Apply the rules of the game\n            if (input[i * N + j] == 1) {\n                if (liveNeighbors < 2 || liveNeighbors > 3) {\n                    output[i * N + j] = 0;  // Cell dies\n                } else {\n                    output[i * N + j] = 1;  // Cell lives on\n                }\n            } else {\n                if (liveNeighbors == 3) {\n                    output[i * N + j] = 1;  // Cell becomes alive\n                } else {\n                    output[i * N + j] = 0;  // Cell remains dead\n                }\n            }\n        }\n    }\n```\n}",
      "\n    // TODO: Implement the game of life logic using MPI and OpenMP\n    // Note: You may need to split the input grid among MPI ranks and use OpenMP for parallel computation within each rank\n    // Remember to store the results in the output grid\n    // You can use the N parameter to determine the size of the grid\n    // Make sure to handle the boundary conditions when computing the neighbors of each cell\n    // The final result should be stored in the output grid\n}"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "52_stencil_1d_jacobi_3-point_stencil",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(Kokkos::View<const double*> &input, Kokkos::View<double*> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(int i) {\n    output(i) = (i == 0 ? 0 : input(i-1)) + input(i) + (i == input.extent(0)-1 ? 0 : input(i+1));\n    output(i) /= (i == 0 || i == input.extent(0)-1) ? 2 : 3;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(const int i) {\n    double left = (i == 0) ? 0 : input(i-1);\n    double right = (i == input.extent(0)-1) ? 0 : input(i+1);\n    output(i) = (left + input(i) + right) / 3.0;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(input.size(), KOKKOS_LAMBDA(const int i) {\n      double left = (i == 0) ? 0 : input(i-1);\n      double right = (i == input.size()-1) ? 0 : input(i+1);\n      output(i) = (left + input(i) + right) / 3.0;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(const int i) {\n    double left = (i == 0) ? 0 : input(i-1);\n    double right = (i == input.extent(0)-1) ? 0 : input(i+1);\n    output(i) = (left + input(i) + right) / 3.0;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(input.size(), KOKKOS_LAMBDA(const int i) {\n      double left = (i == 0) ? 0 : input(i-1);\n      double right = (i == input.size()-1) ? 0 : input(i+1);\n      output(i) = (left + input(i) + right) / 3.0;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(const int i) {\n    double left = (i == 0) ? 0 : input(i-1);\n    double right = (i == input.extent(0)-1) ? 0 : input(i+1);\n    output(i) = (left + input(i) + right) / 3;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(const int i) {\n    double left = (i == 0) ? 0 : input(i-1);\n    double right = (i == input.extent(0)-1) ? 0 : input(i+1);\n    output(i) = (left + input(i) + right) / 3.0;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(const int i) {\n      double left = (i == 0) ? 0 : input(i-1);\n      double right = (i == input.extent(0)-1) ? 0 : input(i+1);\n      output(i) = (left + input(i) + right) / 3;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(const int i) {\n      double left = (i == 0) ? 0 : input(i-1);\n      double right = (i == input.extent(0)-1) ? 0 : input(i+1);\n      output(i) = (left + input(i) + right) / 3.0;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(const int i) {\n    output(i) = (i > 0 ? input(i-1) : 0) + input(i) + (i < input.extent(0)-1 ? input(i+1) : 0);\n    output(i) /= 3.0;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(const int i) {\n    output(i) = (i > 0 ? input(i-1) : 0) + input(i) + (i < input.extent(0)-1 ? input(i+1) : 0);\n    output(i) /= 3.0;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(int i) {\n      double left = (i == 0) ? 0 : input(i-1);\n      double right = (i == input.extent(0)-1) ? 0 : input(i+1);\n      output(i) = (left + input(i) + right) / 3.0;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(const int i) {\n    double left = (i == 0) ? 0 : input(i-1);\n    double right = (i == input.extent(0)-1) ? 0 : input(i+1);\n    output(i) = (left + input(i) + right) / 3.0;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(const int i) {\n      double left = (i == 0) ? 0 : input(i-1);\n      double right = (i == input.extent(0)-1) ? 0 : input(i+1);\n      output(i) = (left + input(i) + right) / 3.0;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(const int i) {\n    double left = (i == 0) ? 0 : input(i-1);\n    double right = (i == input.extent(0)-1) ? 0 : input(i+1);\n    output(i) = (left + input(i) + right) / 3.0;\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(const int i) {\n    output(i) = (i > 0 ? input(i-1) : 0) + input(i) + (i < input.extent(0)-1 ? input(i+1) : 0);\n    output(i) /= 3.0;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(const int i) {\n      double left = (i == 0) ? 0 : input(i-1);\n      double right = (i == input.extent(0)-1) ? 0 : input(i+1);\n      output(i) = (left + input(i) + right) / 3.0;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(const int i) {\n      double left = (i > 0) ? input(i-1) : 0;\n      double right = (i < input.extent(0)-1) ? input(i+1) : 0;\n      output(i) = (left + input(i) + right) / 3.0;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(const int i) {\n    output(i) = (i > 0 ? input(i-1) : 0) + input(i) + (i < input.extent(0)-1 ? input(i+1) : 0);\n    output(i) /= 3.0;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(input.size(), KOKKOS_LAMBDA(const int i) {\n    double left = (i > 0) ? input(i-1) : 0;\n    double right = (i < input.size()-1) ? input(i+1) : 0;\n    output(i) = (left + input(i) + right) / 3.0;\n  });\n  Kokkos::fence();\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "52_stencil_1d_jacobi_3-point_stencil",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (int i = 0; i < input.size(); i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < input.size(); i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < input.size(); i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < input.size(); i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < input.size(); i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < input.size(); i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < input.size(); i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < input.size(); i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < input.size(); i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < input.size(); i++) {\n        double left = (i == 0) ? 0 : input[i-1];\n        double right = (i == input.size()-1) ? 0 : input[i+1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < input.size(); i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < input.size(); i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < input.size(); i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < input.size(); i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < input.size(); i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "52_stencil_1d_jacobi_3-point_stencil",
    "parallelism_model": "serial",
    "prompt": "/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    for (int i = 0; i < input.size(); i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i-1];\n        double right = (i == input.size()-1) ? 0 : input[i+1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    for (int i = 0; i < input.size(); i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    for (int i = 0; i < input.size(); ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "52_stencil_1d_jacobi_3-point_stencil",
    "parallelism_model": "hip",
    "prompt": "/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\n__global__ void jacobi1D(const double *input, double *output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double left = (i == 0) ? 0 : input[i-1];\n        double right = (i == N-1) ? 0 : input[i+1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double left = (i == 0) ? 0 : input[i-1];\n        double right = (i == N-1) ? 0 : input[i+1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double left = (i == 0) ? 0 : input[i-1];\n        double right = (i == N-1) ? 0 : input[i+1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double left = (i == 0) ? 0 : input[i-1];\n        double right = (i == N-1) ? 0 : input[i+1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == N-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 ? 2 : 3) + 3 + (i == N-1 ? 2 : 3);\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N) {\n        double left = (i == 0) ? 0 : input[i-1];\n        double right = (i == N-1) ? 0 : input[i+1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == N-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == N-1) ? 2 : 3;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double left = (i == 0) ? 0 : input[i-1];\n        double right = (i == N-1) ? 0 : input[i+1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == N-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == N-1) ? 2 : 3;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double left = (i == 0) ? 0 : input[i-1];\n        double right = (i == N-1) ? 0 : input[i+1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == N - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == N-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 ? 2 : 3) + 3 + (i == N-1 ? 2 : 3);\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == N - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == N-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == N-1) ? 2 : 3;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == N - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N) {\n        double left = (i == 0) ? 0 : input[i-1];\n        double right = (i == N-1) ? 0 : input[i+1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == N-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == N-1) ? 2 : 3;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == N-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == N-1) ? 2 : 3;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == N - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = (i > 0 ? input[i-1] : 0) + input[i] + (i < N-1 ? input[i+1] : 0);\n        output[i] /= 3.0;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "52_stencil_1d_jacobi_3-point_stencil",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = input.size();\n    int local_n = n / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    for (int i = start; i < end; i++) {\n        if (i == 0) {\n            output[i] = (input[i] + input[i + 1]) / 2.0;\n        } else if (i == n - 1) {\n            output[i] = (input[i - 1] + input[i]) / 2.0;\n        } else {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3.0;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = local_n * size; i < n; i++) {\n            if (i == 0) {\n                output[i] = (input[i] + input[i + 1]) / 2.0;\n            } else if (i == n - 1) {\n                output[i] = (input[i - 1] + input[i]) / 2.0;\n            } else {\n                output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3.0;\n            }\n        }\n    }\n    MPI_Gather(&output[start], local_n, MPI_DOUBLE, &output[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = input.size();\n    int local_n = n / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    for (int i = start; i < end; i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == n - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = local_n * size; i < n; i++) {\n            double left = (i == 0) ? 0 : input[i - 1];\n            double right = (i == n - 1) ? 0 : input[i + 1];\n            output[i] = (left + input[i] + right) / 3;\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = input.size();\n    for (int i = 0; i < n; ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == n - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n\n    if (rank == 0) {\n        std::vector<double> recvbuf(n);\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(recvbuf.data(), n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; ++j) {\n                output[j] = recvbuf[j];\n            }\n        }\n    } else {\n        MPI_Send(output.data(), n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = input.size();\n    int local_n = n / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    for (int i = start; i < end; i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == n - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = local_n * size; i < n; i++) {\n            double left = (i == 0) ? 0 : input[i - 1];\n            double right = (i == n - 1) ? 0 : input[i + 1];\n            output[i] = (left + input[i] + right) / 3;\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = input.size();\n    int local_n = n / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    for (int i = start; i < end; i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == n - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = local_n * size; i < n; i++) {\n            double left = (i == 0) ? 0 : input[i - 1];\n            double right = (i == n - 1) ? 0 : input[i + 1];\n            output[i] = (left + input[i] + right) / 3;\n        }\n    }\n\n    MPI_Gather(&output[start], local_n, MPI_DOUBLE, &output[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int size = input.size();\n    output.resize(size);\n\n    for (int i = 0; i < size; i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == size - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = input.size();\n    int local_n = n / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    for (int i = start; i < end; i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == n - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&output[i * local_n], local_n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start], local_n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int size = input.size();\n    for (int i = 0; i < size; ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == size - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = input.size();\n    int local_n = n / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    for (int i = start; i < end; i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == n - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&output[i * local_n], local_n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start], local_n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = input.size();\n    int local_n = n / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    for (int i = start; i < end; i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == n - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = local_n * size; i < n; i++) {\n            double left = (i == 0) ? 0 : input[i - 1];\n            double right = (i == n - 1) ? 0 : input[i + 1];\n            output[i] = (left + input[i] + right) / 3;\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = input.size();\n    int local_n = n / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    for (int i = start; i < end; i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == n - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&output[i * local_n], local_n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start], local_n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = input.size();\n    int local_n = n / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    for (int i = start; i < end; i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == n - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&output[i * local_n], local_n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start], local_n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = input.size();\n    int local_n = n / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    for (int i = start; i < end; i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == n - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = local_n * size; i < n; i++) {\n            double left = (i == 0) ? 0 : input[i - 1];\n            double right = (i == n - 1) ? 0 : input[i + 1];\n            output[i] = (left + input[i] + right) / 3;\n        }\n    }\n}\n",
      "\n    int size = input.size();\n    for (int i = 0; i < size; ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == size - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    for (int i = start; i < end; ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&output[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    for (int i = start; i < end; ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&output[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    for (int i = start; i < end; i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = local_size * size; i < input.size(); i++) {\n            double left = (i == 0) ? 0 : input[i - 1];\n            double right = (i == input.size() - 1) ? 0 : input[i + 1];\n            output[i] = (left + input[i] + right) / 3;\n        }\n    }\n    MPI_Gather(&output[start], local_size, MPI_DOUBLE, &output[0], local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int size = input.size();\n    output.resize(size);\n\n    for (int i = 0; i < size; i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == size - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    int size = input.size();\n    output.resize(size);\n\n    for (int i = 0; i < size; ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == size - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}",
      "\n    int size = input.size();\n    for (int i = 0; i < size; ++i) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == size - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n}"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "52_stencil_1d_jacobi_3-point_stencil",
    "parallelism_model": "cuda",
    "prompt": "/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\n__global__ void jacobi1D(const double *input, double *output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == N-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == N-1) ? 2 : 3;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == N-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == N-1) ? 2 : 3;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == N-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == N-1) ? 2 : 3;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == N - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == N-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == N-1) ? 2 : 3;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == N-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == N-1) ? 2 : 3;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == N-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == N-1) ? 2 : 3;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == N-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == N-1) ? 2 : 3;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == N-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == N-1) ? 2 : 3;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == N-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == N-1) ? 2 : 3;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == N - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == N - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N) {\n        double left = (i == 0) ? 0 : input[i-1];\n        double right = (i == N-1) ? 0 : input[i+1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == N - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == N-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == N-1) ? 2 : 3;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == N-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == N-1) ? 2 : 3;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == N - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N) {\n        double left = (i == 0) ? 0 : input[i-1];\n        double right = (i == N-1) ? 0 : input[i+1];\n        output[i] = (left + input[i] + right) / 3.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == N-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == N-1) ? 2 : 3;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == N-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == N-1) ? 2 : 3;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "52_stencil_1d_jacobi_3-point_stencil",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = input.size();\n    int local_n = n / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    if (rank == 0) {\n        start = 0;\n        end = local_n;\n    } else if (rank == size - 1) {\n        start = rank * local_n;\n        end = n;\n    } else {\n        start = rank * local_n - 1;\n        end = start + local_n + 1;\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == n - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n\n    if (rank == 0) {\n        MPI_Status status;\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&output[i * local_n], local_n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &status);\n        }\n    } else {\n        MPI_Send(&output[start], local_n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    if (rank == 0) {\n        start = 0;\n        end = local_size;\n    } else if (rank == size - 1) {\n        start = rank * local_size;\n        end = input.size();\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&output[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&output[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    if (rank == 0) {\n        start = 0;\n        end = local_size;\n    } else if (rank == size - 1) {\n        start = rank * local_size;\n        end = input.size();\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == input.size() - 1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == input.size() - 1) ? 2 : 3;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&output[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&output[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = input.size();\n    int local_n = n / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    if (rank == 0) {\n        start = 0;\n        end = local_n;\n    } else if (rank == size - 1) {\n        start = rank * local_n;\n        end = n;\n    } else {\n        start = rank * local_n - 1;\n        end = start + local_n + 1;\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; ++i) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == n-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == n-1) ? 2 : 3;\n    }\n\n    if (rank == 0) {\n        MPI_Status status;\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&output[i * local_n], local_n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &status);\n        }\n    } else {\n        MPI_Send(&output[start], local_n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? input.size() : (rank + 1) * local_size;\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        output[i] = (i == 0) ? (input[i] + input[i + 1]) / 2 : \n                    (i == input.size() - 1) ? (input[i - 1] + input[i]) / 2 : \n                    (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&output[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    if (rank == 0) {\n        start = 0;\n        end = local_size;\n    } else if (rank == size - 1) {\n        start = rank * local_size;\n        end = input.size();\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == input.size() - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&output[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&output[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = input.size();\n    int local_n = n / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    if (rank == 0) {\n        start = 0;\n        end = local_n;\n    } else if (rank == size - 1) {\n        start = rank * local_n;\n        end = n;\n    } else {\n        start = rank * local_n - 1;\n        end = start + local_n + 1;\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == n-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == n-1) ? 2 : 3;\n    }\n\n    if (rank == 0) {\n        MPI_Status status;\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&output[i * local_n], local_n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &status);\n        }\n    } else {\n        MPI_Send(&output[start], local_n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    if (rank == 0) {\n        start = 0;\n        end = local_size;\n    } else if (rank == size - 1) {\n        start = rank * local_size;\n        end = input.size();\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == input.size() - 1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == input.size() - 1) ? 2 : 3;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&output[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&output[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = input.size();\n    int local_n = n / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    if (rank == 0) {\n        start = 0;\n        end = local_n;\n    } else if (rank == size - 1) {\n        start = rank * local_n;\n        end = n;\n    } else {\n        start = rank * local_n - 1;\n        end = start + local_n + 1;\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == n-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == n-1) ? 2 : 3;\n    }\n\n    if (rank == 0) {\n        MPI_Status status;\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&output[i * local_n], local_n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &status);\n        }\n    } else {\n        MPI_Send(&output[start], local_n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    if (rank == 0) {\n        start = 0;\n        end = local_size;\n    } else if (rank == size - 1) {\n        start = rank * local_size;\n        end = input.size();\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == input.size()-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == input.size()-1) ? 2 : 3;\n    }\n\n    if (rank == 0) {\n        std::vector<double> recvbuf(local_size);\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(recvbuf.data(), local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            std::copy(recvbuf.begin(), recvbuf.end(), output.begin() + i * local_size);\n        }\n    } else {\n        MPI_Send(output.data() + start, local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = input.size() / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    if (rank == 0) {\n        output[0] = (input[0] + input[1]) / 3.0;\n    }\n\n    #pragma omp parallel for\n    for (int i = start + 1; i < end - 1; ++i) {\n        output[i] = (input[i-1] + input[i] + input[i+1]) / 3.0;\n    }\n\n    if (rank == size - 1) {\n        output[end - 1] = (input[end - 2] + input[end - 1]) / 3.0;\n    }\n\n    MPI_Gather(&output[start], local_n, MPI_DOUBLE, &output[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = input.size();\n    int local_n = n / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    if (rank == 0) {\n        start = 0;\n        end = local_n;\n    } else if (rank == size - 1) {\n        start = rank * local_n;\n        end = n;\n    } else {\n        start = rank * local_n - 1;\n        end = start + local_n + 1;\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == n-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == n-1) ? 2 : 3;\n    }\n\n    if (rank == 0) {\n        MPI_Status status;\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&output[i * local_n], local_n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &status);\n        }\n    } else {\n        MPI_Send(&output[start], local_n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    if (rank == 0) {\n        start = 0;\n        end = local_size;\n    } else if (rank == size - 1) {\n        start = rank * local_size;\n        end = input.size();\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; ++i) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == input.size()-1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == input.size()-1) ? 2 : 3;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&output[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&output[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    if (rank == 0) {\n        start = 0;\n        end = local_size;\n    } else if (rank == size - 1) {\n        start = rank * local_size;\n        end = input.size();\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; ++i) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == input.size() - 1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == input.size() - 1) ? 2 : 3;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&output[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&output[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    if (rank == 0) {\n        start = 0;\n        end = local_size;\n    } else if (rank == size - 1) {\n        start = rank * local_size;\n        end = input.size();\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; ++i) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == input.size() - 1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == input.size() - 1) ? 2 : 3;\n    }\n\n    if (rank == 0) {\n        MPI_Status status;\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&output[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &status);\n        }\n    } else {\n        MPI_Send(&output[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = input.size();\n    int local_n = n / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    if (rank == 0) {\n        output[0] = (input[0] + input[1]) / 3;\n    }\n\n    #pragma omp parallel for\n    for (int i = start + 1; i < end - 1; i++) {\n        output[i] = (input[i-1] + input[i] + input[i+1]) / 3;\n    }\n\n    if (rank == size - 1) {\n        output[n-1] = (input[n-2] + input[n-1]) / 3;\n    }\n\n    MPI_Gather(&output[start], local_n, MPI_DOUBLE, &output[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = input.size();\n    int local_n = n / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    if (rank == size - 1) {\n        end = n;\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        double left = (i == 0) ? 0 : input[i - 1];\n        double right = (i == n - 1) ? 0 : input[i + 1];\n        output[i] = (left + input[i] + right) / 3;\n    }\n\n    if (rank == 0) {\n        std::vector<double> recvbuf(n);\n        MPI_Gather(output.data() + start, local_n, MPI_DOUBLE, recvbuf.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        output = recvbuf;\n    } else {\n        MPI_Gather(output.data() + start, local_n, MPI_DOUBLE, nullptr, 0, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = input.size();\n    int local_n = n / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    if (rank == 0) {\n        output[0] = (input[0] + input[1]) / 3;\n    }\n\n    #pragma omp parallel for\n    for (int i = start + 1; i < end - 1; ++i) {\n        output[i] = (input[i-1] + input[i] + input[i+1]) / 3;\n    }\n\n    if (rank == size - 1) {\n        output[n-1] = (input[n-2] + input[n-1]) / 3;\n    }\n\n    MPI_Gather(&output[start], local_n, MPI_DOUBLE, &output[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = input.size();\n    int local_n = n / size;\n    int start = rank * local_n;\n    int end = start + local_n;\n\n    if (rank == 0) {\n        output[0] = (input[0] + input[1]) / 3.0;\n    }\n\n    #pragma omp parallel for\n    for (int i = start + 1; i < end - 1; i++) {\n        output[i] = (input[i-1] + input[i] + input[i+1]) / 3.0;\n    }\n\n    if (rank == size - 1) {\n        output[n-1] = (input[n-2] + input[n-1]) / 3.0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    if (rank == 0) {\n        start = 0;\n        end = local_size;\n    } else if (rank == size - 1) {\n        start = rank * local_size;\n        end = input.size();\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        output[i] = (i == 0 ? 0 : input[i-1]) + input[i] + (i == input.size() - 1 ? 0 : input[i+1]);\n        output[i] /= (i == 0 || i == input.size() - 1) ? 2 : 3;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&output[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&output[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "51_stencil_edge_kernel",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images.\n   Store the output of the computation in imageOut.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(Kokkos::View<const int**> &imageIn, Kokkos::View<int**> &imageOut, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(N, [=] (const int i) {\n    for (int j = 0; j < N; ++j) {\n      int sum = 0;\n      for (int k = -1; k <= 1; ++k) {\n        for (int l = -1; l <= 1; ++l) {\n          int x = i + k;\n          int y = j + l;\n          if (x >= 0 && x < N && y >= 0 && y < N) {\n            sum += imageIn(x, y) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(N, [=] (const int i) {\n      for (int j = 0; j < N; j++) {\n          int sum = 0;\n          for (int k = -1; k <= 1; k++) {\n              for (int l = -1; l <= 1; l++) {\n                  int row = i + k;\n                  int col = j + l;\n                  if (row >= 0 && row < N && col >= 0 && col < N) {\n                      sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n                  }\n              }\n          }\n          imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n      }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, [=] (const int i) {\n      for (size_t j = 0; j < N; j++) {\n          int sum = 0;\n          for (int k = -1; k <= 1; k++) {\n              for (int l = -1; l <= 1; l++) {\n                  int x = i + k;\n                  int y = j + l;\n                  if (x >= 0 && x < N && y >= 0 && y < N) {\n                      sum += imageIn(x, y) * edgeKernel[k + 1][l + 1];\n                  }\n              }\n          }\n          sum = std::max(0, std::min(255, sum));\n          imageOut(i, j) = sum;\n      }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, [=] (const int i) {\n    for (int j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int x = i + k;\n          int y = j + l;\n          if (x >= 0 && x < N && y >= 0 && y < N) {\n            sum += imageIn(x, y) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(N, [=] (const int i) {\n    for (int j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int x = i + k;\n          int y = j + l;\n          if (x >= 0 && x < N && y >= 0 && y < N) {\n            sum += imageIn(x, y) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, [=] (const int i) {\n    for (int j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, [=] (const int i) {\n    for (int j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(N, [=] (const int i) {\n      for (size_t j = 0; j < N; ++j) {\n          int sum = 0;\n          for (int k = -1; k <= 1; ++k) {\n              for (int l = -1; l <= 1; ++l) {\n                  int row = i + k;\n                  int col = j + l;\n                  if (row >= 0 && row < N && col >= 0 && col < N) {\n                      sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n                  }\n              }\n          }\n          imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n      }\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(N, [=] (const int i) {\n    for (int j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int x = i + k;\n          int y = j + l;\n          if (x >= 0 && x < N && y >= 0 && y < N) {\n            sum += imageIn(x, y) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(N, [=] (const int i) {\n    for (int j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int x = i + k;\n          int y = j + l;\n          if (x >= 0 && x < N && y >= 0 && y < N) {\n            sum += imageIn(x, y) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, [=] (const int i) {\n    for (int j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int sum = 0;\n      for (int k = -1; k <= 1; ++k) {\n        for (int l = -1; l <= 1; ++l) {\n          int x = i + k;\n          int y = j + l;\n          if (x >= 0 && x < N && y >= 0 && y < N) {\n            sum += imageIn(x, y) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(N, [=] (const int i) {\n    for (int j = 0; j < N; ++j) {\n      int sum = 0;\n      for (int k = -1; k <= 1; ++k) {\n        for (int l = -1; l <= 1; ++l) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int sum = 0;\n      for (int k = -1; k <= 1; ++k) {\n        for (int l = -1; l <= 1; ++l) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int sum = 0;\n      for (int k = -1; k <= 1; ++k) {\n        for (int l = -1; l <= 1; ++l) {\n          int x = i + k;\n          int y = j + l;\n          if (x >= 0 && x < N && y >= 0 && y < N) {\n            sum += imageIn(x, y) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n```\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int sum = 0;\n      for (int k = -1; k <= 1; ++k) {\n        for (int l = -1; l <= 1; ++l) {\n          int x = i + k;\n          int y = j + l;\n          if (x >= 0 && x < N && y >= 0 && y < N) {\n            sum += imageIn(x, y) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int sum = 0;\n      for (int k = -1; k <= 1; ++k) {\n        for (int l = -1; l <= 1; ++l) {\n          int x = i + k;\n          int y = j + l;\n          if (x >= 0 && x < N && y >= 0 && y < N) {\n            sum += imageIn(x, y) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int sum = 0;\n      for (int k = -1; k <= 1; ++k) {\n        for (int l = -1; l <= 1; ++l) {\n          int x = i + k;\n          int y = j + l;\n          if (x >= 0 && x < N && y >= 0 && y < N) {\n            sum += imageIn(x, y) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(N, [=] (const int i) {\n      for (int j = 0; j < N; ++j) {\n          int sum = 0;\n          for (int k = -1; k <= 1; ++k) {\n              for (int l = -1; l <= 1; ++l) {\n                  int x = i + k;\n                  int y = j + l;\n                  if (x >= 0 && x < N && y >= 0 && y < N) {\n                      sum += imageIn(x, y) * edgeKernel[k + 1][l + 1];\n                  }\n              }\n          }\n          imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n      }\n  });\n  Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "51_stencil_edge_kernel",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += edgeKernel[k + 1][l + 1] * imageIn[x * N + y];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "51_stencil_edge_kernel",
    "parallelism_model": "serial",
    "prompt": "const int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n    }\n}\n}",
      "\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n    }\n}\n}",
      "\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int x = i + k;\n                int y = j + l;\n                if (x >= 0 && x < N && y >= 0 && y < N) {\n                    sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n    }\n}\n}",
      "\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n    }\n}\n}",
      "\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n    }\n}\n}",
      "\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n    }\n}\n}",
      "\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n    }\n}\n}",
      "\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n    }\n}\n}",
      "\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int x = i + k;\n                int y = j + l;\n                if (x >= 0 && x < N && y >= 0 && y < N) {\n                    sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n    }\n}\n}",
      "\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int x = i + k;\n                int y = j + l;\n                if (x >= 0 && x < N && y >= 0 && y < N) {\n                    sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n    }\n}\n}",
      "\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int x = i + k;\n                int y = j + l;\n                if (x >= 0 && x < N && y >= 0 && y < N) {\n                    sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n    }\n}\n}",
      "\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n    }\n}\n}",
      "\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int x = i + k;\n                int y = j + l;\n                if (x >= 0 && x < N && y >= 0 && y < N) {\n                    sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n    }\n}\n}",
      "\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n    }\n}\n}",
      "\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int x = i + k;\n                int y = j + l;\n                if (x >= 0 && x < N && y >= 0 && y < N) {\n                    sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n    }\n}\n}",
      "\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n    }\n}\n}",
      "\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n    }\n}\n}",
      "\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n    }\n}\n}",
      "\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n    }\n}\n}",
      "\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n    }\n}\n}"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "51_stencil_edge_kernel",
    "parallelism_model": "hip",
    "prompt": "__constant__ int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\n__global__ void convolveKernel(const int *imageIn, int *imageOut, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; k++) {\n            for (int l = -1; l <= 1; l++) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < N && col < N) {\n        int sum = 0;\n        for (int i = -1; i <= 1; i++) {\n            for (int j = -1; j <= 1; j++) {\n                int neighborRow = row + i;\n                int neighborCol = col + j;\n                if (neighborRow >= 0 && neighborRow < N && neighborCol >= 0 && neighborCol < N) {\n                    sum += imageIn[neighborRow * N + neighborCol] * edgeKernel[i + 1][j + 1];\n                }\n            }\n        }\n        imageOut[row * N + col] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = max(0, min(255, sum));\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = max(0, min(255, sum));\n    }\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "51_stencil_edge_kernel",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of imageIn. The result is stored in imageOut on rank 0.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int kernelSize = 3;\n    int halfKernel = kernelSize / 2;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int startRow = rank * (N / size);\n    int endRow = (rank + 1) * (N / size);\n    if (rank == size - 1) {\n        endRow = N;\n    }\n\n    for (int i = startRow; i < endRow; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -halfKernel; k <= halfKernel; ++k) {\n                for (int l = -halfKernel; l <= halfKernel; ++l) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row < 0 || row >= N || col < 0 || col >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[row * N + col] * edgeKernel[k + halfKernel][l + halfKernel];\n                    }\n                }\n            }\n            sum = std::max(0, std::min(255, sum));\n            imageOut[i * N + j] = sum;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            int start = r * (N / size);\n            int end = (r + 1) * (N / size);\n            if (r == size - 1) {\n                end = N;\n            }\n            MPI_Recv(&imageOut[start * N], (end - start) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&imageOut[startRow * N], (endRow - startRow) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int kernelSize = 3;\n    int halfKernel = kernelSize / 2;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -halfKernel; k <= halfKernel; ++k) {\n                for (int l = -halfKernel; l <= halfKernel; ++l) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row < 0 || row >= N || col < 0 || col >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[row * N + col] * edgeKernel[k + halfKernel][l + halfKernel];\n                    }\n                }\n            }\n            sum = std::max(0, std::min(255, sum));\n            imageOut[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n    int kernelSize = 3;\n    int halfKernel = kernelSize / 2;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < N; i += size) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -halfKernel; k <= halfKernel; k++) {\n                for (int l = -halfKernel; l <= halfKernel; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + halfKernel][l + halfKernel];\n                    }\n                }\n            }\n            sum = std::max(0, std::min(255, sum));\n            imageOut[i * N + j] = sum;\n        }\n    }\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::vector<int> tempImage(N * N);\n            MPI_Recv(&tempImage[0], N * N, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < N * N; j++) {\n                imageOut[j] = tempImage[j];\n            }\n        }\n    } else {\n        MPI_Send(&imageOut[0], N * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int kernelSize = 3;\n    int halfKernel = kernelSize / 2;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < N; i += size) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -halfKernel; k <= halfKernel; k++) {\n                for (int l = -halfKernel; l <= halfKernel; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + halfKernel][l + halfKernel];\n                    }\n                }\n            }\n            sum = std::max(0, std::min(255, sum));\n            imageOut[i * N + j] = sum;\n        }\n    }\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::vector<int> tempImage(N * N);\n            MPI_Recv(&tempImage[0], N * N, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < N * N; j++) {\n                imageOut[j] = tempImage[j];\n            }\n        }\n    } else {\n        MPI_Send(&imageOut[0], N * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int kernelSize = 3;\n    int halfKernel = kernelSize / 2;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int startRow = rank * (N / size);\n    int endRow = (rank + 1) * (N / size);\n    if (rank == size - 1) {\n        endRow = N;\n    }\n\n    for (int i = startRow; i < endRow; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = 0; k < kernelSize; ++k) {\n                for (int l = 0; l < kernelSize; ++l) {\n                    int row = i + k - halfKernel;\n                    int col = j + l - halfKernel;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k][l];\n                    }\n                }\n            }\n            sum = std::max(0, std::min(255, sum));\n            imageOut[i * N + j] = sum;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            MPI_Recv(&imageOut[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&imageOut[startRow * N], (endRow - startRow) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int kernelSize = 3;\n    int halfKernel = kernelSize / 2;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < N; i += size) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -halfKernel; k <= halfKernel; k++) {\n                for (int l = -halfKernel; l <= halfKernel; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + halfKernel][l + halfKernel];\n                    }\n                }\n            }\n            sum = std::max(0, std::min(255, sum));\n            imageOut[i * N + j] = sum;\n        }\n    }\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            std::vector<int> tempImage(N * N);\n            MPI_Recv(&tempImage[0], N * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int i = 0; i < N; i++) {\n                for (int j = 0; j < N; j++) {\n                    imageOut[i * N + j] = tempImage[i * N + j];\n                }\n            }\n        }\n    } else {\n        MPI_Send(&imageOut[0], N * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int kernelSize = 3;\n    int halfKernel = kernelSize / 2;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int startRow = rank * (N / size);\n    int endRow = (rank + 1) * (N / size);\n    if (rank == size - 1) {\n        endRow = N;\n    }\n\n    for (int i = startRow; i < endRow; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -halfKernel; k <= halfKernel; k++) {\n                for (int l = -halfKernel; l <= halfKernel; l++) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + halfKernel][l + halfKernel];\n                    }\n                }\n            }\n            sum = std::max(0, std::min(255, sum));\n            imageOut[i * N + j] = sum;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            int start = r * (N / size);\n            int end = (r + 1) * (N / size);\n            if (r == size - 1) {\n                end = N;\n            }\n            MPI_Recv(&imageOut[start * N], (end - start) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&imageOut[startRow * N], (endRow - startRow) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int kernelSize = 3;\n    int halfKernel = kernelSize / 2;\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -halfKernel; k <= halfKernel; ++k) {\n                for (int l = -halfKernel; l <= halfKernel; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + halfKernel][l + halfKernel];\n                    }\n                }\n            }\n            sum = std::max(0, std::min(255, sum));\n            imageOut[i * N + j] = sum;\n        }\n    }\n}",
      "\n    int kernelSize = 3;\n    int halfKernel = kernelSize / 2;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < N; i += size) {\n        for (int j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -halfKernel; k <= halfKernel; ++k) {\n                for (int l = -halfKernel; l <= halfKernel; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + halfKernel][l + halfKernel];\n                    }\n                }\n            }\n            sum = std::max(0, std::min(255, sum));\n            imageOut[i * N + j] = sum;\n        }\n    }\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            std::vector<int> tempImage(N * N);\n            MPI_Recv(&tempImage[0], N * N, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < N * N; ++j) {\n                imageOut[j] = tempImage[j];\n            }\n        }\n    } else {\n        MPI_Send(&imageOut[0], N * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int kernelSize = 3;\n    int halfKernel = kernelSize / 2;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    for (int i = start; i < end; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -halfKernel; k <= halfKernel; ++k) {\n                for (int l = -halfKernel; l <= halfKernel; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x < 0 || x >= N || y < 0 || y >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[x * N + y] * edgeKernel[k + halfKernel][l + halfKernel];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            MPI_Recv(&imageOut[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&imageOut[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int kernelSize = 3;\n    int halfKernel = kernelSize / 2;\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -halfKernel; k <= halfKernel; ++k) {\n                for (int l = -halfKernel; l <= halfKernel; ++l) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + halfKernel][l + halfKernel];\n                    }\n                }\n            }\n            sum = std::max(0, std::min(255, sum));\n            imageOut[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n    int kernelSize = 3;\n    int halfKernel = kernelSize / 2;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int startRow = rank * (N / size);\n    int endRow = (rank + 1) * (N / size);\n    if (rank == size - 1) {\n        endRow = N;\n    }\n\n    for (int i = startRow; i < endRow; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -halfKernel; k <= halfKernel; ++k) {\n                for (int l = -halfKernel; l <= halfKernel; ++l) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row < 0 || row >= N || col < 0 || col >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[row * N + col] * edgeKernel[k + halfKernel][l + halfKernel];\n                    }\n                }\n            }\n            sum = std::max(0, std::min(255, sum));\n            imageOut[i * N + j] = sum;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            int start = r * (N / size);\n            int end = (r + 1) * (N / size);\n            if (r == size - 1) {\n                end = N;\n            }\n            MPI_Recv(&imageOut[start * N], (end - start) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&imageOut[startRow * N], (endRow - startRow) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int kernelSize = 3;\n    int halfKernel = kernelSize / 2;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < N; i += size) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -halfKernel; k <= halfKernel; k++) {\n                for (int l = -halfKernel; l <= halfKernel; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + halfKernel][l + halfKernel];\n                    }\n                }\n            }\n            sum = std::max(0, std::min(255, sum));\n            imageOut[i * N + j] = sum;\n        }\n    }\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::vector<int> tempImageOut(N * N);\n            MPI_Recv(&tempImageOut[0], N * N, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < N * N; j++) {\n                imageOut[j] = tempImageOut[j];\n            }\n        }\n    } else {\n        MPI_Send(&imageOut[0], N * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int kernelSize = 3;\n    int halfKernel = kernelSize / 2;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < N; i += size) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -halfKernel; k <= halfKernel; k++) {\n                for (int l = -halfKernel; l <= halfKernel; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + halfKernel][l + halfKernel];\n                    }\n                }\n            }\n            sum = std::max(0, std::min(255, sum));\n            imageOut[i * N + j] = sum;\n        }\n    }\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::vector<int> tempImage(N * N);\n            MPI_Recv(&tempImage[0], N * N, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < N * N; j++) {\n                imageOut[j] = tempImage[j];\n            }\n        }\n    } else {\n        MPI_Send(&imageOut[0], N * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int kernelSize = 3;\n    int halfKernel = kernelSize / 2;\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + halfKernel][l + halfKernel];\n                    }\n                }\n            }\n            sum = std::max(0, std::min(255, sum));\n            imageOut[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n    int kernelSize = 3;\n    int halfKernel = kernelSize / 2;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < N; i += size) {\n        for (int j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -halfKernel; k <= halfKernel; ++k) {\n                for (int l = -halfKernel; l <= halfKernel; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + halfKernel][l + halfKernel];\n                    }\n                }\n            }\n            sum = std::max(0, std::min(255, sum));\n            imageOut[i * N + j] = sum;\n        }\n    }\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            std::vector<int> tempImage(N * N);\n            MPI_Recv(&tempImage[0], N * N, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < N * N; ++j) {\n                imageOut[j] = tempImage[j];\n            }\n        }\n    } else {\n        MPI_Send(&imageOut[0], N * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int kernelSize = 3;\n    int halfKernel = kernelSize / 2;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < N; i += size) {\n        for (int j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -halfKernel; k <= halfKernel; ++k) {\n                for (int l = -halfKernel; l <= halfKernel; ++l) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + halfKernel][l + halfKernel];\n                    }\n                }\n            }\n            sum = std::max(0, std::min(255, sum));\n            imageOut[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n    int kernelSize = 3;\n    int halfKernel = kernelSize / 2;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int startRow = rank * (N / size);\n    int endRow = (rank + 1) * (N / size);\n    if (rank == size - 1) {\n        endRow = N;\n    }\n\n    for (int i = startRow; i < endRow; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -halfKernel; k <= halfKernel; k++) {\n                for (int l = -halfKernel; l <= halfKernel; l++) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row < 0 || row >= N || col < 0 || col >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[row * N + col] * edgeKernel[k + halfKernel][l + halfKernel];\n                    }\n                }\n            }\n            sum = std::max(0, std::min(255, sum));\n            imageOut[i * N + j] = sum;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&imageOut[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&imageOut[startRow * N], (endRow - startRow) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int kernelSize = 3;\n    int halfKernel = kernelSize / 2;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < N; i += size) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -halfKernel; k <= halfKernel; k++) {\n                for (int l = -halfKernel; l <= halfKernel; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + halfKernel][l + halfKernel];\n                    }\n                }\n            }\n            sum = std::max(0, std::min(255, sum));\n            imageOut[i * N + j] = sum;\n        }\n    }\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::vector<int> tempImage(N * N);\n            MPI_Recv(&tempImage[0], N * N, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < N * N; j++) {\n                imageOut[j] = tempImage[j];\n            }\n        }\n    } else {\n        MPI_Send(&imageOut[0], N * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int kernelSize = 3;\n    int halfKernel = kernelSize / 2;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < N; i += size) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -halfKernel; k <= halfKernel; k++) {\n                for (int l = -halfKernel; l <= halfKernel; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + halfKernel][l + halfKernel];\n                    }\n                }\n            }\n            sum = std::max(0, std::min(255, sum));\n            imageOut[i * N + j] = sum;\n        }\n    }\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::vector<int> tempImage(N * N);\n            MPI_Recv(&tempImage[0], N * N, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < N * N; j++) {\n                imageOut[j] = tempImage[j];\n            }\n        }\n    } else {\n        MPI_Send(&imageOut[0], N * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "51_stencil_edge_kernel",
    "parallelism_model": "cuda",
    "prompt": "__constant__ int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\n__global__ void convolveKernel(const int *imageIn, int *imageOut, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = max(0, min(255, sum));\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < N && col < N) {\n        int sum = 0;\n        for (int i = -1; i <= 1; i++) {\n            for (int j = -1; j <= 1; j++) {\n                int neighborRow = row + i;\n                int neighborCol = col + j;\n                if (neighborRow >= 0 && neighborRow < N && neighborCol >= 0 && neighborCol < N) {\n                    sum += imageIn[neighborRow * N + neighborCol] * edgeKernel[i + 1][j + 1];\n                }\n            }\n        }\n        imageOut[row * N + col] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; k++) {\n            for (int l = -1; l <= 1; l++) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = max(0, min(255, sum));\n    }\n}\n",
      "\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < N && col < N) {\n        int sum = 0;\n        for (int i = -1; i <= 1; i++) {\n            for (int j = -1; j <= 1; j++) {\n                int neighborRow = row + i;\n                int neighborCol = col + j;\n                if (neighborRow >= 0 && neighborRow < N && neighborCol >= 0 && neighborCol < N) {\n                    sum += imageIn[neighborRow * N + neighborCol] * edgeKernel[i + 1][j + 1];\n                }\n            }\n        }\n        imageOut[row * N + col] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; k++) {\n            for (int l = -1; l <= 1; l++) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int m = -1; m <= 1; m++) {\n            for (int n = -1; n <= 1; n++) {\n                int row = i + m;\n                int col = j + n;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[m + 1][n + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = max(0, min(255, sum));\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = max(0, min(255, sum));\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; k++) {\n            for (int l = -1; l <= 1; l++) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; k++) {\n            for (int l = -1; l <= 1; l++) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = max(0, min(255, sum));\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = min(max(sum, 0), 255);\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        int sum = 0;\n        for (int k = -1; k <= 1; k++) {\n            for (int l = -1; l <= 1; l++) {\n                int row = i + k;\n                int col = j + l;\n                if (row >= 0 && row < N && col >= 0 && col < N) {\n                    sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        imageOut[i * N + j] = max(0, min(255, sum));\n    }\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "51_stencil_edge_kernel",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of imageIn. The result is stored in imageOut on rank 0.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}",
      "\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for collapse(2)\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row < 0 || row >= N || col < 0 || col >= N) {\n                        sum += 0;\n                    } else {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int row = i + k;\n                    int col = j + l;\n                    if (row >= 0 && row < N && col >= 0 && col < N) {\n                        sum += imageIn[row * N + col] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::min(std::max(sum, 0), 255);\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "53_stencil_2d_jacobi_5-point_stencil",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(Kokkos::View<const double**> &input, Kokkos::View<double**> &output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      double sum = 0.0;\n      int count = 0;\n      if (i > 0) {\n        sum += input(i-1, j);\n        count++;\n      }\n      if (i < N-1) {\n        sum += input(i+1, j);\n        count++;\n      }\n      if (j > 0) {\n        sum += input(i, j-1);\n        count++;\n      }\n      if (j < N-1) {\n        sum += input(i, j+1);\n        count++;\n      }\n      sum += input(i, j);\n      count++;\n\n      output(i, j) = sum / count;\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int i) {\n    for (int j = 0; j < N; j++) {\n      double left = (j > 0) ? input(i, j - 1) : 0;\n      double right = (j < N - 1) ? input(i, j + 1) : 0;\n      double up = (i > 0) ? input(i - 1, j) : 0;\n      double down = (i < N - 1) ? input(i + 1, j) : 0;\n      output(i, j) = (left + right + up + down + input(i, j)) / 5.0;\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int i) {\n    for (int j = 0; j < N; ++j) {\n      double sum = 0.0;\n      int count = 0;\n      if (i > 0) {\n        sum += input(i-1, j);\n        count++;\n      }\n      if (i < N-1) {\n        sum += input(i+1, j);\n        count++;\n      }\n      if (j > 0) {\n        sum += input(i, j-1);\n        count++;\n      }\n      if (j < N-1) {\n        sum += input(i, j+1);\n        count++;\n      }\n      sum += input(i, j);\n      count++;\n\n      output(i, j) = sum / count;\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int i) {\n    for (int j = 0; j < N; ++j) {\n      double sum = 0.0;\n      int count = 0;\n      if (i > 0) {\n        sum += input(i-1, j);\n        count++;\n      }\n      if (i < N-1) {\n        sum += input(i+1, j);\n        count++;\n      }\n      if (j > 0) {\n        sum += input(i, j-1);\n        count++;\n      }\n      if (j < N-1) {\n        sum += input(i, j+1);\n        count++;\n      }\n      sum += input(i, j);\n      count++;\n\n      output(i, j) = sum / count;\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (int j = 0; j < N; j++) {\n      double sum = 0.0;\n      int count = 0;\n      if (i > 0) {\n        sum += input(i-1, j);\n        count++;\n      }\n      if (i < N-1) {\n        sum += input(i+1, j);\n        count++;\n      }\n      if (j > 0) {\n        sum += input(i, j-1);\n        count++;\n      }\n      if (j < N-1) {\n        sum += input(i, j+1);\n        count++;\n      }\n      sum += input(i, j);\n      count++;\n      output(i, j) = sum / count;\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (int j = 0; j < N; ++j) {\n      double left = (j > 0) ? input(i, j - 1) : 0;\n      double right = (j < N - 1) ? input(i, j + 1) : 0;\n      double up = (i > 0) ? input(i - 1, j) : 0;\n      double down = (i < N - 1) ? input(i + 1, j) : 0;\n      output(i, j) = (left + right + up + down + input(i, j)) / 5.0;\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (int j = 0; j < N; j++) {\n      double sum = 0.0;\n      int count = 0;\n      if (i > 0) {\n        sum += input(i - 1, j);\n        count++;\n      }\n      if (i < N - 1) {\n        sum += input(i + 1, j);\n        count++;\n      }\n      if (j > 0) {\n        sum += input(i, j - 1);\n        count++;\n      }\n      if (j < N - 1) {\n        sum += input(i, j + 1);\n        count++;\n      }\n      sum += input(i, j);\n      count++;\n      output(i, j) = sum / count;\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (int j = 0; j < N; j++) {\n      double sum = 0.0;\n      int count = 0;\n      if (i > 0) {\n        sum += input(i-1, j);\n        count++;\n      }\n      if (i < N-1) {\n        sum += input(i+1, j);\n        count++;\n      }\n      if (j > 0) {\n        sum += input(i, j-1);\n        count++;\n      }\n      if (j < N-1) {\n        sum += input(i, j+1);\n        count++;\n      }\n      sum += input(i, j);\n      count++;\n      output(i, j) = sum / count;\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      double sum = 0.0;\n      int count = 0;\n      if (i > 0) {\n        sum += input(i-1, j);\n        count++;\n      }\n      if (i < N-1) {\n        sum += input(i+1, j);\n        count++;\n      }\n      if (j > 0) {\n        sum += input(i, j-1);\n        count++;\n      }\n      if (j < N-1) {\n        sum += input(i, j+1);\n        count++;\n      }\n      sum += input(i, j);\n      count++;\n      output(i, j) = sum / count;\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      double left = (j > 0) ? input(i, j - 1) : 0;\n      double right = (j < N - 1) ? input(i, j + 1) : 0;\n      double up = (i > 0) ? input(i - 1, j) : 0;\n      double down = (i < N - 1) ? input(i + 1, j) : 0;\n      output(i, j) = (left + right + up + down + input(i, j)) / 5.0;\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      double sum = 0.0;\n      int count = 0;\n      if (i > 0) {\n        sum += input(i-1, j);\n        count++;\n      }\n      if (i < N-1) {\n        sum += input(i+1, j);\n        count++;\n      }\n      if (j > 0) {\n        sum += input(i, j-1);\n        count++;\n      }\n      if (j < N-1) {\n        sum += input(i, j+1);\n        count++;\n      }\n      sum += input(i, j);\n      count++;\n      output(i, j) = sum / count;\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (int j = 0; j < N; j++) {\n      double sum = 0.0;\n      int count = 0;\n      if (i > 0) {\n        sum += input(i-1, j);\n        count++;\n      }\n      if (i < N-1) {\n        sum += input(i+1, j);\n        count++;\n      }\n      if (j > 0) {\n        sum += input(i, j-1);\n        count++;\n      }\n      if (j < N-1) {\n        sum += input(i, j+1);\n        count++;\n      }\n      sum += input(i, j);\n      count++;\n      output(i, j) = sum / count;\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (int j = 0; j < N; j++) {\n      double sum = 0.0;\n      int count = 0;\n      if (i > 0) {\n        sum += input(i-1, j);\n        count++;\n      }\n      if (i < N-1) {\n        sum += input(i+1, j);\n        count++;\n      }\n      if (j > 0) {\n        sum += input(i, j-1);\n        count++;\n      }\n      if (j < N-1) {\n        sum += input(i, j+1);\n        count++;\n      }\n      sum += input(i, j);\n      count++;\n      output(i, j) = sum / count;\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      double sum = 0.0;\n      int count = 0;\n      if (i > 0) {\n        sum += input(i-1, j);\n        count++;\n      }\n      if (i < N-1) {\n        sum += input(i+1, j);\n        count++;\n      }\n      if (j > 0) {\n        sum += input(i, j-1);\n        count++;\n      }\n      if (j < N-1) {\n        sum += input(i, j+1);\n        count++;\n      }\n      sum += input(i, j);\n      count++;\n\n      output(i, j) = sum / count;\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (int j = 0; j < N; j++) {\n      double sum = 0.0;\n      int count = 0;\n      if (i > 0) {\n        sum += input(i-1, j);\n        count++;\n      }\n      if (i < N-1) {\n        sum += input(i+1, j);\n        count++;\n      }\n      if (j > 0) {\n        sum += input(i, j-1);\n        count++;\n      }\n      if (j < N-1) {\n        sum += input(i, j+1);\n        count++;\n      }\n      sum += input(i, j);\n      count++;\n      output(i, j) = sum / count;\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int i) {\n    for (int j = 0; j < N; ++j) {\n      double sum = 0.0;\n      int count = 0;\n      if (i > 0) {\n        sum += input(i-1, j);\n        count++;\n      }\n      if (i < N-1) {\n        sum += input(i+1, j);\n        count++;\n      }\n      if (j > 0) {\n        sum += input(i, j-1);\n        count++;\n      }\n      if (j < N-1) {\n        sum += input(i, j+1);\n        count++;\n      }\n      sum += input(i, j);\n      count++;\n      output(i, j) = sum / count;\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int i) {\n    for (int j = 0; j < N; ++j) {\n      double sum = 0.0;\n      int count = 0;\n      if (i > 0) {\n        sum += input(i-1, j);\n        count++;\n      }\n      if (i < N-1) {\n        sum += input(i+1, j);\n        count++;\n      }\n      if (j > 0) {\n        sum += input(i, j-1);\n        count++;\n      }\n      if (j < N-1) {\n        sum += input(i, j+1);\n        count++;\n      }\n      sum += input(i, j);\n      count++;\n      output(i, j) = sum / count;\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (int j = 0; j < N; ++j) {\n      double sum = 0.0;\n      int count = 0;\n      if (i > 0) {\n        sum += input(i-1, j);\n        count++;\n      }\n      if (i < N-1) {\n        sum += input(i+1, j);\n        count++;\n      }\n      if (j > 0) {\n        sum += input(i, j-1);\n        count++;\n      }\n      if (j < N-1) {\n        sum += input(i, j+1);\n        count++;\n      }\n      sum += input(i, j);\n      count++;\n      output(i, j) = sum / count;\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (int j = 0; j < N; ++j) {\n      double sum = 0.0;\n      int count = 0;\n      if (i > 0) {\n        sum += input(i-1, j);\n        count++;\n      }\n      if (i < N-1) {\n        sum += input(i+1, j);\n        count++;\n      }\n      if (j > 0) {\n        sum += input(i, j-1);\n        count++;\n      }\n      if (j < N-1) {\n        sum += input(i, j+1);\n        count++;\n      }\n      sum += input(i, j);\n      count++;\n      output(i, j) = sum / count;\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (int j = 0; j < N; ++j) {\n      double sum = 0.0;\n      int count = 0;\n      if (i > 0) {\n        sum += input(i - 1, j);\n        count++;\n      }\n      if (i < N - 1) {\n        sum += input(i + 1, j);\n        count++;\n      }\n      if (j > 0) {\n        sum += input(i, j - 1);\n        count++;\n      }\n      if (j < N - 1) {\n        sum += input(i, j + 1);\n        count++;\n      }\n      sum += input(i, j);\n      count++;\n      output(i, j) = sum / count;\n    }\n  });\n  Kokkos::fence();\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "53_stencil_2d_jacobi_5-point_stencil",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n\n            output[i*N + j] = sum / count;\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n            output[i*N + j] = sum / count;\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n            output[i*N + j] = sum / count;\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n\n            output[i*N + j] = sum / count;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n\n            output[i*N + j] = sum / count;\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n            output[i*N + j] = sum / count;\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n            output[i*N + j] = sum / count;\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n\n            output[i*N + j] = sum / count;\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n\n            output[i*N + j] = sum / count;\n        }\n    }\n```\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n            output[i*N + j] = sum / count;\n        }\n    }\n```\n}",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n\n            output[i*N + j] = sum / count;\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n\n            output[i*N + j] = sum / count;\n        }\n    }\n```\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n            output[i*N + j] = sum / count;\n        }\n    }\n```\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n\n            output[i*N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[i*N + (j > 0 ? j-1 : j)];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[i*N + (j < N-1 ? j+1 : j)];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[(i > 0 ? (i-1)*N : i*N) + j];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[(i < N-1 ? (i+1)*N : i*N) + j];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n\n            output[i*N + j] = sum / count;\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "53_stencil_2d_jacobi_5-point_stencil",
    "parallelism_model": "serial",
    "prompt": "/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[i*N + (j % N)];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[i*N + (j % N)];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j % N)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i*N + (j % N)];\n                count++;\n            }\n            sum += input[i*N + (j % N)];\n            count++;\n\n            output[i*N + (j % N)] = sum / count;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            size_t count = 0;\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            size_t count = 0;\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            size_t count = 0;\n\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n\n            sum += input[i * N + j];\n            count++;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[i * N + (j > 0 ? j - 1 : j)];\n                sum += input[i * N + (j < N - 1 ? j + 1 : j)];\n                count += 2;\n            }\n            if (j > 0) {\n                sum += input[(i > 0 ? (i - 1) : i) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            size_t count = 0;\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                ++count;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                ++count;\n            }\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                ++count;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                ++count;\n            }\n            sum += input[i * N + j];\n            ++count;\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            size_t count = 0;\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            size_t count = 0;\n\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                ++count;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                ++count;\n            }\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                ++count;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                ++count;\n            }\n\n            sum += input[i * N + j];\n            ++count;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n            output[i*N + j] = sum / count;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n            output[i*N + j] = sum / count;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            size_t count = 0;\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        double sum = 0;\n        int count = 0;\n\n        if (j > 0) {\n            sum += input[i * N + (j - 1)];\n            count++;\n        }\n        if (j < N - 1) {\n            sum += input[i * N + (j + 1)];\n            count++;\n        }\n        if (i > 0) {\n            sum += input[(i - 1) * N + j];\n            count++;\n        }\n        if (i < N - 1) {\n            sum += input[(i + 1) * N + j];\n            count++;\n        }\n\n        sum += input[i * N + j];\n        count++;\n\n        output[i * N + j] = sum / count;\n    }\n}\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            size_t count = 0;\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            size_t count = 0;\n\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                ++count;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                ++count;\n            }\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                ++count;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                ++count;\n            }\n\n            sum += input[i * N + j];\n            ++count;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            size_t count = 0;\n\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n\n            sum += input[i * N + j];\n            count++;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n            output[i*N + j] = sum / count;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            size_t count = 0;\n\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                ++count;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                ++count;\n            }\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                ++count;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                ++count;\n            }\n\n            sum += input[i * N + j];\n            ++count;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "53_stencil_2d_jacobi_5-point_stencil",
    "parallelism_model": "hip",
    "prompt": "/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\n__global__ void jacobi2D(const double *input, double *output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        double left = (i > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (i < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (j > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (j < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        double left = (i > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (i < N - 1) ? input[i * N + (j + 1)] : 0;\n        double top = (j > 0) ? input[(i - 1) * N + j] : 0;\n        double bottom = (j < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + top + bottom + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double top = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double bottom = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + top + bottom + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        double left = (i > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (i < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (j > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (j < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double top = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double bottom = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + top + bottom + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        double left = (i > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (i < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (j > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (j < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double top = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double bottom = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + top + bottom + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double top = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double bottom = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + top + bottom + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        double left = (i > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (i < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (j > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (j < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double top = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double bottom = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + top + bottom + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "53_stencil_2d_jacobi_5-point_stencil",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    int start_row = rank * local_N;\n    int end_row = start_row + local_N;\n\n    std::vector<double> local_input(input.begin() + start_row * N, input.begin() + end_row * N);\n    std::vector<double> local_output(local_input);\n\n    for (int i = start_row; i < end_row; ++i) {\n        for (int j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n\n            local_output[i * N + j - start_row * N] = sum / count;\n        }\n    }\n\n    MPI_Gather(local_output.data(), local_N * N, MPI_DOUBLE, output.data(), local_N * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n            output[i*N + j] = sum / count;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n\n            sum += input[i * N + j];\n            count++;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    int start_row = rank * local_N;\n    int end_row = start_row + local_N;\n\n    std::vector<double> local_input(input.begin() + start_row * N, input.begin() + end_row * N);\n    std::vector<double> local_output(local_input);\n\n    for (int i = start_row; i < end_row; ++i) {\n        for (int j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n\n            local_output[i * N + j - start_row * N] = sum / count;\n        }\n    }\n\n    MPI_Gather(local_output.data(), local_N * N, MPI_DOUBLE, output.data(), local_N * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n\n            output[i*N + j] = sum / count;\n        }\n    }\n```\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n\n            output[i*N + j] = sum / count;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n\n            sum += input[i * N + j];\n            count++;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n\n            output[i*N + j] = sum / count;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n\n            output[i*N + j] = sum / count;\n        }\n    }\n```\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n\n            output[i*N + j] = sum / count;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n\n            output[i*N + j] = sum / count;\n        }\n    }\n```\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n```\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n```\n}",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n\n            if (i > 0) {\n                sum += input[i*N + (j-1) < 0 ? j : j-1];\n                sum += input[i*N + (j+1 >= N ? j : j+1)];\n                count += 2;\n            }\n            if (j > 0) {\n                sum += input[(i-1 < 0 ? i : i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n\n            output[i*N + j] = sum / count;\n        }\n    }\n}\n",
      "\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n\n            sum += input[i * N + j];\n            count++;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "53_stencil_2d_jacobi_5-point_stencil",
    "parallelism_model": "cuda",
    "prompt": "/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\n__global__ void jacobi2D(const double *input, double *output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        double left = (i > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (i < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (j > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (j < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        double left = (i > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (i < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (j > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (j < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        double left = (i > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (i < N - 1) ? input[i * N + (j + 1)] : 0;\n        double top = (j > 0) ? input[(i - 1) * N + j] : 0;\n        double bottom = (j < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + top + bottom + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        double left = (i > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (i < N - 1) ? input[i * N + (j + 1)] : 0;\n        double top = (j > 0) ? input[(i - 1) * N + j] : 0;\n        double bottom = (j < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + top + bottom + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        output[i * N + j] = (left + right + up + down + input[i * N + j]) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.y * blockDim.y + threadIdx.y;\n    int j = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N && j < N) {\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n",
      "\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        double left = (i > 0) ? input[i * N + (j - 1)] : 0;\n        double right = (i < N - 1) ? input[i * N + (j + 1)] : 0;\n        double up = (j > 0) ? input[(i - 1) * N + j] : 0;\n        double down = (j < N - 1) ? input[(i + 1) * N + j] : 0;\n        double center = input[i * N + j];\n\n        output[i * N + j] = (left + right + up + down + center) / 5.0;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "53_stencil_2d_jacobi_5-point_stencil",
    "parallelism_model": "mpi+omp",
    "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n            output[i*N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n\n            output[i*N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n            output[i*N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n\n            output[i*N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n\n            output[i*N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n            output[i*N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n\n            output[i*N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n            output[i*N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n            output[i*N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n            output[i*N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n            output[i*N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[(i - 1) * N + j];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[(i + 1) * N + j];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[i * N + (j - 1)];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[i * N + (j + 1)];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n            output[i*N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[i*N + (j-1) < 0 ? j : j-1];\n                sum += input[i*N + (j+1 >= N ? j : j+1)];\n                count += 2;\n            }\n            if (j > 0) {\n                sum += input[(i-1 < 0 ? i : i-1)*N + j];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n\n            output[i*N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[i * N + (j > 0 ? j - 1 : j)];\n                count++;\n            }\n            if (i < N - 1) {\n                sum += input[i * N + (j < N - 1 ? j + 1 : j)];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[(i > 0 ? i - 1 : i) * N + j];\n                count++;\n            }\n            if (j < N - 1) {\n                sum += input[(i < N - 1 ? i + 1 : i) * N + j];\n                count++;\n            }\n            sum += input[i * N + j];\n            count++;\n            output[i * N + j] = sum / count;\n        }\n    }\n}",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0.0;\n            int count = 0;\n            if (i > 0) {\n                sum += input[i*N + (j-1)];\n                count++;\n            }\n            if (i < N-1) {\n                sum += input[i*N + (j+1)];\n                count++;\n            }\n            if (j > 0) {\n                sum += input[(i-1)*N + j];\n                count++;\n            }\n            if (j < N-1) {\n                sum += input[(i+1)*N + j];\n                count++;\n            }\n            sum += input[i*N + j];\n            count++;\n            output[i*N + j] = sum / count;\n        }\n    }\n}"
    ]
  }
]